WEBVTT
Kind: captions
Language: en-GB

00:00:00.280 --> 00:00:07.800
Have you reflected a lot on how to select&nbsp;
talent or has that mostly been intuitive to&nbsp;&nbsp;

00:00:07.800 --> 00:00:11.240
you? Ilya just shows up and you're&nbsp;
like, this is a clever guy, let's,&nbsp;&nbsp;

00:00:11.240 --> 00:00:14.600
let's work together. Or have&nbsp;
you thought a lot about that?

00:00:16.280 --> 00:00:17.800
Should we roll this?

00:00:17.800 --> 00:00:18.720
Yeah, let's roll this.

00:00:25.000 --> 00:00:29.960
Sound is working.

00:00:30.600 --> 00:00:35.680
So I remember when I first got to Carnegie Mellon&nbsp;
from England, in England at a research unit,&nbsp;&nbsp;

00:00:35.680 --> 00:00:40.880
it would get to be six o'clock and you'd all go&nbsp;
for a drink in the pub. Um, at Carnegie Mellon,&nbsp;&nbsp;

00:00:40.880 --> 00:00:45.160
I remember after I'd been there a few weeks, it&nbsp;
was Saturday night. I didn't have any friends&nbsp;&nbsp;

00:00:45.160 --> 00:00:48.360
yet and I didn't know what to do. So&nbsp;
I decided I'd go into the lab and do&nbsp;&nbsp;

00:00:48.360 --> 00:00:52.160
some programming 'cause I had a list machine&nbsp;
and you couldn't program it from home. So I&nbsp;&nbsp;

00:00:52.160 --> 00:00:57.240
went into the lab at about nine o'clock on a&nbsp;
Saturday night and it was swarming. All the&nbsp;&nbsp;

00:00:57.240 --> 00:01:01.720
students were there and they were all there&nbsp;
because what they were working on was the&nbsp;&nbsp;

00:01:01.720 --> 00:01:06.360
future. They all believed that what they did&nbsp;
next was gonna change the course of computer&nbsp;&nbsp;

00:01:06.360 --> 00:01:11.880
science and it was just so different from&nbsp;
England. And so that was very refreshing.

00:01:11.880 --> 00:01:15.280
Take me back to the very beginning, Geoff at&nbsp;&nbsp;

00:01:15.280 --> 00:01:20.320
Cambridge. Uh, trying to understand&nbsp;
the brain. Uh, what was that like?

00:01:20.320 --> 00:01:25.760
It was very disappointing. So I did physiology&nbsp;
and in the summer term they were gonna teach us&nbsp;&nbsp;

00:01:25.760 --> 00:01:31.080
how the brain worked and it, all they taught&nbsp;
us was how neurons conduct action potentials,&nbsp;&nbsp;

00:01:31.080 --> 00:01:35.080
which is very interesting, but it doesn't tell&nbsp;
you how the brain works. So that was extremely&nbsp;&nbsp;

00:01:35.080 --> 00:01:39.080
disappointing. I switched to philosophy&nbsp;
then, I thought maybe they'd tell us how&nbsp;&nbsp;

00:01:39.080 --> 00:01:45.000
the mind worked and that was very disappointing.&nbsp;
I eventually ended up going to Edinburgh to do AI&nbsp;&nbsp;

00:01:45.000 --> 00:01:49.840
and that was more interesting. At least you could&nbsp;
simulate things so you could test out theories.

00:01:49.840 --> 00:01:53.480
And did you remember what intrigued you about AI?&nbsp;&nbsp;

00:01:53.480 --> 00:01:59.120
Was it a paper? Was it any particular&nbsp;
person that exposed you to those ideas?

00:01:59.120 --> 00:02:04.600
I guess it was a book I read by Donald&nbsp;
Hebb that influenced me a lot. Um,&nbsp;&nbsp;

00:02:04.600 --> 00:02:08.200
he was very interested in how you&nbsp;
learn the connection strengths in&nbsp;&nbsp;

00:02:08.200 --> 00:02:12.920
neural nets. I also read a book&nbsp;
by John von Neumann early on, um,&nbsp;&nbsp;

00:02:12.920 --> 00:02:19.120
who was very interested in how the brain computes&nbsp;
and how it's different from normal computers.

00:02:19.120 --> 00:02:25.200
And did you get that conviction that&nbsp;
this ideas would work out at that point,&nbsp;&nbsp;

00:02:25.200 --> 00:02:29.280
or what was your intuition&nbsp;
back in the Edinburgh days?

00:02:29.280 --> 00:02:36.920
It seemed to me there has to be a way that the&nbsp;
brain learns and it's clearly not by having all&nbsp;&nbsp;

00:02:36.920 --> 00:02:41.360
sorts of things programmed into it and&nbsp;
then using logical rules of inference,&nbsp;&nbsp;

00:02:41.360 --> 00:02:47.320
that just seemed to me crazy from the&nbsp;
outset. Um, so we had to figure out how&nbsp;&nbsp;

00:02:47.320 --> 00:02:51.960
the brain learned to modify connections in&nbsp;
a neural net so that it could do complicated&nbsp;&nbsp;

00:02:51.960 --> 00:02:56.440
things. And von Neumann believed that.&nbsp;
Turing believed that. So von Neumann&nbsp;&nbsp;

00:02:56.440 --> 00:03:01.120
and Turing were both pretty good at logic, but&nbsp;
they didn't believe in this logical approach.

00:03:01.120 --> 00:03:08.160
And what was your split between studying the&nbsp;
ideas from, from neuroscience and just doing&nbsp;&nbsp;

00:03:08.160 --> 00:03:13.840
what seemed to be good algorithms for, for AI?&nbsp;
How much inspiration did you take early on?

00:03:13.840 --> 00:03:19.080
So I never did that much studying in neuroscience.&nbsp;
I was always inspired by what I learned about how&nbsp;&nbsp;

00:03:19.080 --> 00:03:23.920
the brain works. That there's a bunch of neurons,&nbsp;
they perform relatively simple operations,&nbsp;&nbsp;

00:03:23.920 --> 00:03:30.360
they're nonlinear, um, but they collect inputs,&nbsp;
they weight them and then they give an output&nbsp;&nbsp;

00:03:30.360 --> 00:03:34.240
that depends on that weighted input. And&nbsp;
the question is how do you change those&nbsp;&nbsp;

00:03:34.240 --> 00:03:38.520
weights to make the whole thing do something&nbsp;
good? It seems like a fairly simple question.

00:03:38.520 --> 00:03:42.200
What collaborations do you&nbsp;
remember from from that time?

00:03:42.200 --> 00:03:46.760
The main collaboration I had at Carnegie Mellon&nbsp;
was with someone who wasn't at Carnegie Mellon.&nbsp;&nbsp;

00:03:46.760 --> 00:03:51.760
I was interacting a lot with Terry Sejnowski who&nbsp;
was in Baltimore at Johns Hopkins. And about once&nbsp;&nbsp;

00:03:51.760 --> 00:03:56.760
a month, either he would drive to Pittsburgh or&nbsp;
I would drive to Baltimore. It's 250 miles away&nbsp;&nbsp;

00:03:56.760 --> 00:04:00.560
and we would spend a weekend together working&nbsp;
on Boltzmann machines. That was a wonderful&nbsp;&nbsp;

00:04:00.560 --> 00:04:04.720
collaboration. We were both convinced it was&nbsp;
how the brain worked. That was the most exciting&nbsp;&nbsp;

00:04:04.720 --> 00:04:09.320
research I've ever done. And a lot of technical&nbsp;
results came out that were very interesting,&nbsp;&nbsp;

00:04:09.320 --> 00:04:14.960
but I think it's not how the brain works. Um,&nbsp;
I also had a very good collaboration with um,&nbsp;&nbsp;

00:04:14.960 --> 00:04:21.040
Peter Brown, who was a very good statistician&nbsp;
and he worked on speech recognition at IBM and&nbsp;&nbsp;

00:04:21.040 --> 00:04:26.680
then he came as a more mature student&nbsp;
to Carnegie Mellon just to get a PhD.

00:04:26.680 --> 00:04:31.280
Um, but he already knew a lot. He taught me&nbsp;
a lot about speech and he in fact taught me&nbsp;&nbsp;

00:04:31.280 --> 00:04:36.000
about hidden Markov models. I think I learned more&nbsp;
from him than he learned from me. That's the kind&nbsp;&nbsp;

00:04:36.000 --> 00:04:42.440
of student you want. And when he taught me about&nbsp;
hidden Markov models, I was doing backdrop with&nbsp;&nbsp;

00:04:42.440 --> 00:04:46.960
hidden layers and they weren't called hidden&nbsp;
layers then. And I decided that name they&nbsp;&nbsp;

00:04:46.960 --> 00:04:52.680
use in Hidden Markoff models is a great name for&nbsp;
variables that you dunno what they're up to. Um,&nbsp;&nbsp;

00:04:52.680 --> 00:04:58.560
and so that's where the name 'hidden' in neural&nbsp;
nets came from me and Peter decided that was a&nbsp;&nbsp;

00:04:58.560 --> 00:05:05.640
great name for the hidden layers in neural nets.&nbsp;
Um, but I learned a lot from Peter about speech.

00:05:05.640 --> 00:05:10.320
Take us back to, um, Ilya&nbsp;
showed up at your office.

00:05:10.320 --> 00:05:15.520
I was in my office probably on a Sunday.&nbsp;
Um, and I was programming I think,&nbsp;&nbsp;

00:05:15.520 --> 00:05:18.280
and there was a knock on the door, not just any&nbsp;
knock, but it went kind of [knocks on table]

00:05:20.200 --> 00:05:24.840
sort of an urgent knock. So I went and answered&nbsp;
the door and this was this young student there and&nbsp;&nbsp;

00:05:24.840 --> 00:05:29.440
he said he was cooking fries over the summer, but&nbsp;
he'd rather be working in my lab. And so I said,&nbsp;&nbsp;

00:05:29.440 --> 00:05:33.440
well why don't you make an appointment and&nbsp;
we'll talk. And so he just said, "How about&nbsp;&nbsp;

00:05:33.440 --> 00:05:40.200
now?" And that sort of was Ilya's character. So we&nbsp;
talked for a bit and I gave him a paper to read,&nbsp;&nbsp;

00:05:40.200 --> 00:05:46.360
which was the nature paper on backpropagation. And&nbsp;
we made another meeting for a week later and he&nbsp;&nbsp;

00:05:46.360 --> 00:05:50.960
came back and he said, "I didn't understand it",&nbsp;
and I was very disappointed. I thought he seemed&nbsp;&nbsp;

00:05:50.960 --> 00:05:55.840
like a bright guy, but it's only the chain rule.&nbsp;
It's not that hard to understand. And he said,&nbsp;&nbsp;

00:05:55.840 --> 00:05:59.960
"Oh no, no, I understood that! I just don't&nbsp;
understand why you don't give the gradient&nbsp;&nbsp;

00:05:59.960 --> 00:06:07.320
to a sensible function optimizer", which took us&nbsp;
quite a few years to think about. Um, and it kept&nbsp;&nbsp;

00:06:07.320 --> 00:06:12.320
on like that with, he had very good, his raw&nbsp;
intuitions about things were always very good.

00:06:12.320 --> 00:06:17.480
What do you think had enabled those,&nbsp;
uh, those intuitions for, for Ilya?

00:06:17.480 --> 00:06:21.520
I don't know. I think he always thought for&nbsp;
himself, he was always interested in AI from&nbsp;&nbsp;

00:06:21.520 --> 00:06:27.960
a young age. Um, he's obviously good at&nbsp;
math, so, but it's very hard to know.

00:06:27.960 --> 00:06:30.240
And what was that collaboration between uh,&nbsp;&nbsp;

00:06:30.240 --> 00:06:35.120
the two of you like? What part would&nbsp;
you play and what part would Ilya play?

00:06:35.120 --> 00:06:42.160
It was a lot of fun. Um, I remember one occasion&nbsp;
when we were trying to do a complicated thing&nbsp;&nbsp;

00:06:42.160 --> 00:06:47.840
with producing maps of data where I had a kind of&nbsp;
mixture model. So you could take the same bunch of&nbsp;&nbsp;

00:06:47.840 --> 00:06:53.960
similarities and make two maps so that in one map,&nbsp;
bank could be close to greed and in another map,&nbsp;&nbsp;

00:06:53.960 --> 00:06:58.680
bank could be close to river. Um, 'cause&nbsp;
in one map you can't have it close to both,&nbsp;&nbsp;

00:06:58.680 --> 00:07:04.720
right? 'cause river and greed along way part. So&nbsp;
we'd have a mixture maps and we were doing it in&nbsp;&nbsp;

00:07:04.720 --> 00:07:09.520
MATLAB and this involved a lot of reorganization&nbsp;
of the code to do the right matrix multiplies.&nbsp;&nbsp;

00:07:09.520 --> 00:07:15.720
And Ilya got fed up with that. So he came one day&nbsp;
and said, um, "I'm gonna write an interface for&nbsp;&nbsp;

00:07:15.720 --> 00:07:20.160
MATLAB. So I program in this different language&nbsp;
and then I have something that just converts it&nbsp;&nbsp;

00:07:20.160 --> 00:07:25.640
into MATLAB". And I said, "No Ilya, that'll&nbsp;
take you a month to do. We've gotta get on&nbsp;&nbsp;

00:07:25.640 --> 00:07:33.360
with this project. Don't get diverted by that.&nbsp;
And Ilya said, "It's OK, I did it this morning"

00:07:33.360 --> 00:07:37.200
That's uh, that's quite, quite&nbsp;
incredible. And throughout those&nbsp;&nbsp;

00:07:37.200 --> 00:07:42.480
years, the biggest shift wasn't&nbsp;
necessarily just the algorithms,&nbsp;&nbsp;

00:07:42.480 --> 00:07:49.600
but also the scale. How did you sort of&nbsp;
view that scale? Uh, over, over the years?

00:07:49.600 --> 00:07:55.520
Ilya got that intuition very early. So Ilya&nbsp;
was always preaching that, um, "You just make&nbsp;&nbsp;

00:07:55.520 --> 00:07:59.560
it bigger and it'll work better". And I always&nbsp;
thought that was a bit of a cop-out that you're&nbsp;&nbsp;

00:07:59.560 --> 00:08:04.640
gonna have to have new ideas too. It turns&nbsp;
out Ilya was basically right. New ideas help,&nbsp;&nbsp;

00:08:04.640 --> 00:08:09.480
things like transformers helped a lot, but it&nbsp;
was really the scale of the data and the scale&nbsp;&nbsp;

00:08:09.480 --> 00:08:15.040
of the computation. And back then we had no idea&nbsp;
computers would get like a billion times faster.&nbsp;&nbsp;

00:08:15.040 --> 00:08:19.360
We thought maybe they'd get a hundred times&nbsp;
faster. We were trying to do things by coming&nbsp;&nbsp;

00:08:19.360 --> 00:08:23.080
up with clever ideas that would've just solved&nbsp;
themselves if we'd had bigger scale of the data&nbsp;&nbsp;

00:08:23.080 --> 00:08:30.280
and computation. In about 2011, Ilya and another&nbsp;
graduate student called James Martins and I had&nbsp;&nbsp;

00:08:30.280 --> 00:08:36.080
a paper using character level prediction. So&nbsp;
we took Wikipedia and we tried to predict the&nbsp;&nbsp;

00:08:36.080 --> 00:08:42.960
next HTML character and that worked remarkably&nbsp;
well and we were always amazed at how well it&nbsp;&nbsp;

00:08:42.960 --> 00:08:50.680
worked. And that was using a fancy optimizer&nbsp;
on GPUs and we could never quite believe that&nbsp;&nbsp;

00:08:50.680 --> 00:08:56.400
it understood anything, but it looked as though&nbsp;
it understood and that just seemed incredible.

00:08:56.400 --> 00:09:01.040
Can you take us through how&nbsp;
are these models trained to&nbsp;&nbsp;

00:09:01.040 --> 00:09:07.640
predict the next word and why is it the&nbsp;
wrong way of, of thinking about them?

00:09:07.640 --> 00:09:12.960
OK. I don't actually believe it is the&nbsp;
wrong way. So in fact, I think I made&nbsp;&nbsp;

00:09:12.960 --> 00:09:18.320
the first neural net language model that used&nbsp;
embeddings and backpropagation. So it's very&nbsp;&nbsp;

00:09:18.320 --> 00:09:25.360
simple data just triples and it was turning&nbsp;
each symbol into an embedding, then having&nbsp;&nbsp;

00:09:25.360 --> 00:09:30.080
the embeddings interact to predict the embedding&nbsp;
of the next symbol and then from that predict the&nbsp;&nbsp;

00:09:30.080 --> 00:09:34.440
next symbol. And then it was backpropagating&nbsp;
through that whole process to learn these&nbsp;&nbsp;

00:09:34.440 --> 00:09:39.840
triples. And I showed it could generalize.&nbsp;
Um, about 10 years later, Yoshua Bengio&nbsp;

00:09:39.840 --> 00:09:44.800
used a very similar network and showed it worked&nbsp;
with real text. And about 10 years after that&nbsp;&nbsp;

00:09:44.800 --> 00:09:50.800
linguists started believing in embeddings. It was&nbsp;
a slow process. The reason I think it's not just&nbsp;&nbsp;

00:09:50.800 --> 00:09:55.520
predicting the next symbol is if you ask, "Well&nbsp;
what does it take to predict the next symbol?"&nbsp;&nbsp;

00:09:55.520 --> 00:10:03.080
particularly if you ask me a question and then&nbsp;
the first word of the answer is the next symbol.

00:10:03.080 --> 00:10:09.720
Um, you have to understand the question.&nbsp;
So I think by predicting the next symbol,&nbsp;&nbsp;

00:10:09.720 --> 00:10:14.400
it's very unlike old fashioned autocomplete un&nbsp;
old fashioned autocomplete, you'd store sort of&nbsp;&nbsp;

00:10:14.400 --> 00:10:19.400
triples of words and then if you saw a pair&nbsp;
of words, you see how often different words&nbsp;&nbsp;

00:10:19.400 --> 00:10:23.360
came third. And that way you could predict the&nbsp;
next symbol. And that's what most people think&nbsp;&nbsp;

00:10:23.360 --> 00:10:29.560
autocomplete is like. It's no longer a tool like&nbsp;
that, um, to predict the next symbol. You have to&nbsp;&nbsp;

00:10:29.560 --> 00:10:33.000
understand what's being said. So I think&nbsp;
you're forcing it to understand by making&nbsp;&nbsp;

00:10:33.000 --> 00:10:38.280
it predict the next symbol. And I think it's&nbsp;
understanding in much the same way we are. So&nbsp;&nbsp;

00:10:38.280 --> 00:10:42.400
a lot of people will tell you these things&nbsp;
aren't like us. Um, they're just predicting&nbsp;&nbsp;

00:10:42.400 --> 00:10:47.640
the next symbol. They're not reasoning like us,&nbsp;
but actually in order to predict the next symbol,&nbsp;&nbsp;

00:10:47.640 --> 00:10:51.120
it's gonna have to do some reasoning.&nbsp;
And we've seen now that if you make big&nbsp;&nbsp;

00:10:51.120 --> 00:10:56.240
ones without putting in any special stuff to do&nbsp;
reasoning, they can already do some reasoning.&nbsp;&nbsp;

00:10:56.240 --> 00:10:59.520
And I think as you make them bigger, they're&nbsp;
gonna be able to do more and more reasoning.

00:10:59.520 --> 00:11:03.560
Do you think I'm doing anything else than&nbsp;
predicting the next symbol right now?

00:11:03.560 --> 00:11:08.440
I think that's how you're learning. I think&nbsp;
you're predicting the next video frame. Um,&nbsp;&nbsp;

00:11:08.440 --> 00:11:11.320
you're predicting the next sound. Um,&nbsp;&nbsp;

00:11:11.320 --> 00:11:15.120
but I think that's a pretty plausible&nbsp;
theory of how the brain's learning.

00:11:15.120 --> 00:11:21.280
What enables these models to learn&nbsp;
such a wide variety of fields.

00:11:21.280 --> 00:11:25.600
What these big language models are doing&nbsp;
is they're looking for common structure,&nbsp;&nbsp;

00:11:25.600 --> 00:11:29.440
and by finding common structure they can encode&nbsp;
things using the common structure and that's&nbsp;&nbsp;

00:11:29.440 --> 00:11:35.920
more efficient. So let me give you an example.&nbsp;
If you ask GPT-4, "Why is a compost heap like an&nbsp;&nbsp;

00:11:35.920 --> 00:11:41.360
atom bomb?" Most people can't answer that. Most&nbsp;
people haven't thought they think atom bomb and&nbsp;&nbsp;

00:11:41.360 --> 00:11:46.360
compost heaps are very different things. But&nbsp;
GPT-4 will tell you, well, the energy scales&nbsp;&nbsp;

00:11:46.360 --> 00:11:51.680
are very different and the timescales are very&nbsp;
different. But the thing that's the same is that&nbsp;&nbsp;

00:11:51.680 --> 00:11:56.920
when the compost heep gets hotter, it generates&nbsp;
heat faster. And when the atom bomb produces more&nbsp;&nbsp;

00:11:56.920 --> 00:12:03.320
neutrons, it produces more neutrons faster. And&nbsp;
so it gets the idea of a chain reaction. And I&nbsp;&nbsp;

00:12:03.320 --> 00:12:07.960
believe it's understood they're both forms of&nbsp;
chain reaction. It's using that understanding&nbsp;&nbsp;

00:12:07.960 --> 00:12:13.120
to compress all that information into its&nbsp;
weights. And if it's doing that, then it's&nbsp;&nbsp;

00:12:13.120 --> 00:12:18.520
gonna be doing that for hundreds of things where&nbsp;
we haven't seen the analogies yet, but it has&nbsp;&nbsp;

00:12:18.520 --> 00:12:22.440
and that's where you get creativity from, from&nbsp;
seeing these analogies between apparently very&nbsp;&nbsp;

00:12:22.440 --> 00:12:27.480
different things. And so I think GPT-4 is gonna&nbsp;
end up—when it gets bigger—being very creative.&nbsp;&nbsp;

00:12:27.480 --> 00:12:32.800
I think this idea that it's just regurgitating&nbsp;
what it's learned, just pastiching together text,&nbsp;&nbsp;

00:12:32.800 --> 00:12:37.920
it's learned already that's completely wrong. It's&nbsp;
gonna be even more creative than people I think.

00:12:37.920 --> 00:12:45.680
You'd argue that it won't just repeat the human&nbsp;
knowledge we've developed so far but could also&nbsp;&nbsp;

00:12:45.680 --> 00:12:51.400
progress beyond that. I think that's something&nbsp;
we haven't quite seen yet. We've started seeing&nbsp;&nbsp;

00:12:51.400 --> 00:12:57.600
some examples of it, but to a large extent&nbsp;
we're sort of still at the current level of

00:12:57.600 --> 00:13:00.440
science. What do you think will&nbsp;
enable it to go beyond that?

00:13:00.440 --> 00:13:05.560
Well, we've seen that in more limited&nbsp;
context. Like if you take AlphaGo in&nbsp;&nbsp;

00:13:05.560 --> 00:13:11.920
that famous competition with Lee Sedol, um,&nbsp;
there was move 37 where AlphaGo made a move&nbsp;&nbsp;

00:13:11.920 --> 00:13:15.920
that all the experts said must have been a&nbsp;
mistake, but actually later they realized&nbsp;&nbsp;

00:13:15.920 --> 00:13:21.440
it was a brilliant move. Um, so that was&nbsp;
created within that limited domain. Um,&nbsp;&nbsp;

00:13:21.440 --> 00:13:25.000
I think we'll see a lot more of&nbsp;
that as these things get bigger.

00:13:25.000 --> 00:13:31.120
The difference with uh, AlphaGo as well was&nbsp;
that it was using reinforcement learning that&nbsp;&nbsp;

00:13:31.120 --> 00:13:36.040
that subsequently sort of enabled it to, to&nbsp;
go beyond the current state. So it started&nbsp;&nbsp;

00:13:36.040 --> 00:13:40.560
with imitation learning, watching how humans&nbsp;
play the game and then it would through self&nbsp;&nbsp;

00:13:40.560 --> 00:13:46.102
play develop will be beyond that. Do you think&nbsp;
that's the missing component of the current data?

00:13:46.102 --> 00:13:50.400
I think that may, I think that may well be a&nbsp;
missing component, yes. That the self play in&nbsp;&nbsp;

00:13:50.400 --> 00:13:57.720
AlphaGo and AlphaZero are a large part of why it&nbsp;
could make these creative moves. But I don't think&nbsp;&nbsp;

00:13:57.720 --> 00:14:03.000
it's entirely necessary. So there's a little&nbsp;
experiment I did a long time ago where you,&nbsp;&nbsp;

00:14:03.000 --> 00:14:07.240
you're training a neural net to recognize&nbsp;
handwritten digits. I love that example,&nbsp;&nbsp;

00:14:07.240 --> 00:14:14.440
the MNIST example. And you give it training&nbsp;
data where half the answers are wrong. Um,&nbsp;&nbsp;

00:14:14.440 --> 00:14:22.280
and the question is how well will it learn?&nbsp;
And you make half the answers wrong once and&nbsp;&nbsp;

00:14:22.280 --> 00:14:26.240
keep them like that. So it can't average&nbsp;
away the wrongness by just seeing the same&nbsp;&nbsp;

00:14:26.240 --> 00:14:29.520
example. But with the right answer sometimes&nbsp;
and the wrong answer, sometimes when it sees&nbsp;&nbsp;

00:14:29.520 --> 00:14:34.920
that example half of the examples, when it&nbsp;
sees the example, the answer is always wrong.

00:14:34.920 --> 00:14:41.880
And so the training data has 50% error,&nbsp;
but if you train up backpropagation,&nbsp;&nbsp;

00:14:41.880 --> 00:14:49.440
it gets down to 5% error or less. In&nbsp;
other words, from badly labeled data,&nbsp;&nbsp;

00:14:49.440 --> 00:14:54.760
it can get much better results. It can see that&nbsp;
the training data is wrong and that's how smart&nbsp;&nbsp;

00:14:54.760 --> 00:14:59.040
students can be smarter than their advisor.&nbsp;
And their advisor tells 'em all this stuff&nbsp;&nbsp;

00:14:59.760 --> 00:15:03.680
and for half of what their advisor tells 'em,&nbsp;
they think no rubbish, and they listen to the&nbsp;&nbsp;

00:15:03.680 --> 00:15:09.040
other half and then they end up smarter than the&nbsp;
advisor. So these big neural nets can actually do,&nbsp;&nbsp;

00:15:09.040 --> 00:15:13.400
they can do much better than their training&nbsp;
data and most people don't realize that.

00:15:13.400 --> 00:15:18.120
So how, how do you expect these&nbsp;
models to add reasoning into them?

00:15:18.120 --> 00:15:22.920
So I mean one&nbsp;approach is you add sort&nbsp;
of the heuristics on on top of them,&nbsp;&nbsp;

00:15:22.920 --> 00:15:26.320
which a lot of the research is doing now&nbsp;
where you have sort of train of thought,&nbsp;&nbsp;

00:15:26.320 --> 00:15:31.440
you just feedback it's reasoning, um,&nbsp;
in into itself. And another way would&nbsp;&nbsp;

00:15:31.440 --> 00:15:37.960
be in the model itself, uh, as you scale it&nbsp;
up. Uh, what's&nbsp;your intuition around that?

00:15:37.960 --> 00:15:43.000
So my intuition is that as we scale up these&nbsp;
models that get better at reasoning and if you&nbsp;&nbsp;

00:15:43.000 --> 00:15:50.320
ask how people work roughly speaking, we have&nbsp;
these intuitions and we can do reasoning and we&nbsp;&nbsp;

00:15:50.320 --> 00:15:55.320
use the reasoning to correct our intuitions. Of&nbsp;
course we use the intuitions during the reasoning&nbsp;&nbsp;

00:15:55.320 --> 00:15:59.480
to do the reasoning, but it's the conclusion of&nbsp;
the reasoning conflicts with our intuitions. We&nbsp;&nbsp;

00:15:59.480 --> 00:16:06.440
realize the intuitions need to be changed. That's&nbsp;
much like in AlphaGo or AlphaZero where you have&nbsp;&nbsp;

00:16:06.440 --> 00:16:11.880
an evaluation function, um, that just looks&nbsp;
at a board and says, how good is that for me?&nbsp;&nbsp;

00:16:11.880 --> 00:16:18.080
But then you do the Monte Carlo rollout and now&nbsp;
you get a more accurate idea and you can revise&nbsp;&nbsp;

00:16:18.080 --> 00:16:22.960
your evaluation function. So you can train it by&nbsp;
getting it to agree with the results of reasoning.&nbsp;&nbsp;

00:16:22.960 --> 00:16:27.680
And I think these large language models have to&nbsp;
start doing that. They have to start training&nbsp;&nbsp;

00:16:27.680 --> 00:16:32.280
their raw intuitions about what should come&nbsp;
next by doing reasoning and realizing that's&nbsp;&nbsp;

00:16:32.280 --> 00:16:38.400
not right. And so that way they can get more&nbsp;
training data than just mimicking what people&nbsp;&nbsp;

00:16:38.400 --> 00:16:44.600
did. And that's exactly why AlphaGo could do this&nbsp;
creative move 37, it had much more training data&nbsp;&nbsp;

00:16:44.600 --> 00:16:49.320
'cause it was using reasoning to check out&nbsp;
what the right next move should have been.

00:16:49.320 --> 00:16:54.560
And what do you think about multimodality?&nbsp;
So we spoke about these analogies and often&nbsp;&nbsp;

00:16:54.560 --> 00:16:59.280
the analogies are way beyond what we&nbsp;
could see. It's discovering analogies&nbsp;&nbsp;

00:16:59.280 --> 00:17:03.720
that are far beyond the humans and at maybe&nbsp;
abstraction levels that we will never be able to&nbsp;&nbsp;

00:17:04.560 --> 00:17:11.440
understand. Now when we introduce images to that&nbsp;
and video and sound, how do you think that will&nbsp;&nbsp;

00:17:11.440 --> 00:17:18.240
change the models and uh, how do you think it'll&nbsp;
change the analogies that it will be able to make?

00:17:18.240 --> 00:17:23.120
Um, I think it'll change it a lot. I think&nbsp;
it'll make it much better at understanding&nbsp;&nbsp;

00:17:23.120 --> 00:17:28.360
spatial things. For example, from language alone,&nbsp;
it's quite hard to understand some spatial things,&nbsp;&nbsp;

00:17:28.360 --> 00:17:34.480
although remarkably GPT-4 can do that&nbsp;
even before it was multimodal. Um,&nbsp;&nbsp;

00:17:34.480 --> 00:17:40.120
but when you make it multimodal, if you&nbsp;
have it both doing vision and reaching&nbsp;&nbsp;

00:17:40.120 --> 00:17:43.720
out and grabbing things, it'll understand&nbsp;
object much better if you can pick them up&nbsp;&nbsp;

00:17:43.720 --> 00:17:49.920
and turn them over and so on. So although&nbsp;
you can learn an awful lot from language,&nbsp;&nbsp;

00:17:49.920 --> 00:17:56.120
it's easier to learn if you are multimodal and&nbsp;
in fact you then need less language and there's&nbsp;&nbsp;

00:17:56.120 --> 00:18:01.080
an awful lot of YouTube video for predicting&nbsp;
the next frame, so, or something like that.&nbsp;&nbsp;

00:18:01.080 --> 00:18:06.200
So I think these multimodal models are clearly&nbsp;
gonna take over. Um, you can get more data that&nbsp;&nbsp;

00:18:06.200 --> 00:18:11.640
way. They need less language. So there's really&nbsp;
a philosophical point that you could learn a very&nbsp;&nbsp;

00:18:11.640 --> 00:18:16.360
good model from language alone, but it's much&nbsp;
easier to learn it from a multimodal system.

00:18:16.360 --> 00:18:20.480
And how do you think it'll&nbsp;
impact the model's reasoning?

00:18:20.480 --> 00:18:23.480
I think it'll make it much better at&nbsp;
reasoning about space. For example,&nbsp;&nbsp;

00:18:23.480 --> 00:18:27.280
reasoning about what happens if you pick objects&nbsp;
up, if you actually try picking objects up,&nbsp;&nbsp;

00:18:27.280 --> 00:18:29.600
you're gonna get all sorts of&nbsp;
training data that's gonna help.

00:18:29.600 --> 00:18:35.240
Do you think the human brain evolved&nbsp;
to work well with with language or&nbsp;&nbsp;

00:18:35.240 --> 00:18:39.560
do you think language evolved to&nbsp;
work well with the human brain?

00:18:39.560 --> 00:18:42.920
I think the question of whether language&nbsp;
evolved to work with the brain or the brain&nbsp;&nbsp;

00:18:42.920 --> 00:18:48.040
evolved to work with language, I think that's&nbsp;
a very good question. I think both happened,&nbsp;&nbsp;

00:18:48.040 --> 00:18:54.400
I used to think we would do a lot of&nbsp;
cognition without needing language at all. Um,&nbsp;&nbsp;

00:18:54.400 --> 00:19:00.280
now I've changed my mind a bit. So let me&nbsp;
give you three different views of language,&nbsp;&nbsp;

00:19:00.280 --> 00:19:05.520
um, and how it relates to cognition. There's the&nbsp;
old fashioned symbolic view, which is cognition&nbsp;&nbsp;

00:19:05.520 --> 00:19:12.360
consists of having strings of symbols in some&nbsp;
kind of cleaned up logical language where there's&nbsp;&nbsp;

00:19:12.360 --> 00:19:17.640
no ambiguity and applying rules of inference. And&nbsp;
that's what cognition is. It's just these symbolic&nbsp;&nbsp;

00:19:17.640 --> 00:19:23.760
manipulations on things that are like strings of&nbsp;
language symbols. Um, so that's one extreme view.&nbsp;&nbsp;

00:19:23.760 --> 00:19:29.960
An opposite extreme view is no, no, once you get&nbsp;
inside the head it's all vectors. So symbols come&nbsp;&nbsp;

00:19:29.960 --> 00:19:35.400
in, you convert those symbols into big vectors&nbsp;
and all the stuff inside is done with big vectors.

00:19:35.400 --> 00:19:39.680
And then if you want to produce output, you&nbsp;
produce symbols again. So there was a point&nbsp;&nbsp;

00:19:39.680 --> 00:19:46.760
in machine translation in about 2014 when people&nbsp;
were using neural recurrent neural nets and words&nbsp;&nbsp;

00:19:46.760 --> 00:19:51.080
will keep coming in and they'd have a hidden&nbsp;
state and they keep accumulating information&nbsp;&nbsp;

00:19:51.080 --> 00:19:56.400
in this hidden state. So when they got to the&nbsp;
end of a sentence that have a big hidden vector&nbsp;&nbsp;

00:19:56.400 --> 00:20:00.920
that captured the meaning of that sentence, that&nbsp;
could then be used for producing the sentence in&nbsp;&nbsp;

00:20:00.920 --> 00:20:05.160
another language—that was called a thought vector.&nbsp;
And that's the sort of second view of language.&nbsp;&nbsp;

00:20:05.160 --> 00:20:10.600
You convert the language into a big vector that's&nbsp;
nothing like language and that's what cognition's&nbsp;&nbsp;

00:20:10.600 --> 00:20:17.320
all about. But then there's a third view, which&nbsp;
what I believe now, which is that you take&nbsp;&nbsp;

00:20:17.320 --> 00:20:25.400
these symbols and you convert the symbols into&nbsp;
embeddings and you use multiple layers of that.

00:20:25.400 --> 00:20:29.800
So you get these very rich embeddings, but the&nbsp;
embeddings are still tied to the symbols in the&nbsp;&nbsp;

00:20:29.800 --> 00:20:34.360
sense that you've got a big vector for this&nbsp;
symbol and a big vector for that symbol. And&nbsp;&nbsp;

00:20:34.360 --> 00:20:39.240
these vectors interact to produce the vector&nbsp;
for the symbol for the next word. And that's&nbsp;&nbsp;

00:20:39.240 --> 00:20:44.160
what understanding is. Understanding is knowing&nbsp;
how to convert the symbols into these vectors&nbsp;&nbsp;

00:20:44.160 --> 00:20:47.720
and knowing how the elements of the vector&nbsp;
should interact to predict the vector for&nbsp;&nbsp;

00:20:47.720 --> 00:20:51.600
the next symbol. That's what understanding is,&nbsp;
both in these big language models and in our&nbsp;&nbsp;

00:20:51.600 --> 00:20:59.312
brains. And that's an example which is sort of in&nbsp;
between. You're staying with the symbols, but you&nbsp;&nbsp;

00:20:59.312 --> 00:21:04.440
are interpreting them as these big vectors. And&nbsp;
that's where all the work is and all the knowledge&nbsp;&nbsp;

00:21:04.440 --> 00:21:11.960
is in what vectors you use and how the elements of&nbsp;
those vectors interact not in symbolic rules. Um,&nbsp;&nbsp;

00:21:11.960 --> 00:21:16.280
but it's not saying that you get away from the&nbsp;
symbols altogether. It's saying you turn the&nbsp;&nbsp;

00:21:16.280 --> 00:21:20.840
symbols into big vectors, but you stay with that&nbsp;
surface structure of the symbols. And that's how&nbsp;&nbsp;

00:21:20.840 --> 00:21:25.640
these models are working. And that now seems to&nbsp;
me a more plausible model of human thought too.

00:21:25.640 --> 00:21:32.440
You were one of the first folks to get&nbsp;
the idea of using GPUs and uh, I know&nbsp;&nbsp;

00:21:32.440 --> 00:21:36.920
Jensen loves you, uh, for that. Uh,&nbsp;
back in 2009 you mentioned that you&nbsp;&nbsp;

00:21:36.920 --> 00:21:41.920
told Jensen that this could be quite&nbsp;
a good idea, um, for training neural&nbsp;&nbsp;

00:21:41.920 --> 00:21:48.240
nets. Take us back to that early intuition&nbsp;
of using GPUs for training neural nets.

00:21:48.240 --> 00:21:54.560
So actually I think in about 2006 I had a&nbsp;
former graduate student called Rick Zelisky,&nbsp;&nbsp;

00:21:54.560 --> 00:21:59.800
who's a very good computer vision guy. And&nbsp;
I talked to him at a meeting and he said,&nbsp;&nbsp;

00:21:59.800 --> 00:22:03.600
you know, you ought to think about using&nbsp;
graphics processing cards because they're&nbsp;&nbsp;

00:22:03.600 --> 00:22:08.560
very good at matrix multiplies and what you're&nbsp;
doing is basically all matrix multiplies. So&nbsp;&nbsp;

00:22:08.560 --> 00:22:13.920
I thought about that for a bit. And then we&nbsp;
learned about these Tesla systems that had,&nbsp;&nbsp;

00:22:13.920 --> 00:22:22.280
um, four GPUs in and initially we just got, um,&nbsp;
gaming GPUs and discovered they made things go&nbsp;&nbsp;

00:22:22.280 --> 00:22:27.560
30 times faster. And then we bought one of&nbsp;
these Tesla systems with four GPUs and we&nbsp;&nbsp;

00:22:27.560 --> 00:22:35.280
did speech on that and it worked very well. And&nbsp;
then in 2009 I gave a talk at NIPS and I told a&nbsp;&nbsp;

00:22:35.280 --> 00:22:39.560
thousand machine learning researchers, "You should&nbsp;
all go and buy Nvidia GPUs. They're the future.&nbsp;&nbsp;

00:22:39.560 --> 00:22:45.640
You need them for doing machine learning". And&nbsp;
I actually, um, then sent mail to Nvidia saying,&nbsp;&nbsp;

00:22:45.640 --> 00:22:48.240
"I told a thousand machine learning&nbsp;
researchers to buy your boards, could&nbsp;&nbsp;

00:22:48.240 --> 00:22:51.480
you give me a free one?" And they said, "No".
Actually, they didn't say no, they just didn't&nbsp;&nbsp;

00:22:51.480 --> 00:22:58.240
reply. Um, but when I told Jensen this&nbsp;
story later on, he gave me a free one.

00:22:58.240 --> 00:23:02.240
That's, uh, that's very, very good.&nbsp;
I I think what's interesting is, um,&nbsp;&nbsp;

00:23:02.240 --> 00:23:07.840
as well is sort of how GPUs has evolved&nbsp;
alongside, uh, the the field. So where,&nbsp;&nbsp;

00:23:07.840 --> 00:23:12.320
where do you think we we should&nbsp;
go, uh, go next in the compute?

00:23:12.320 --> 00:23:17.360
So my last couple of years at Google, I was&nbsp;
thinking about ways of trying to make analog&nbsp;&nbsp;

00:23:17.360 --> 00:23:21.880
computation so that instead of using like&nbsp;
a megawatt, we could use like 30 watts like&nbsp;&nbsp;

00:23:21.880 --> 00:23:28.640
the brain and we could run these big language&nbsp;
models in analog hardware. And I never made it&nbsp;&nbsp;

00:23:28.640 --> 00:23:37.680
work and, but I started really appreciating&nbsp;
digital computation. So if you're gonna use&nbsp;&nbsp;

00:23:37.680 --> 00:23:43.160
that low power analog computation, every piece&nbsp;
of hardware is gonna be a bit different. And&nbsp;&nbsp;

00:23:43.160 --> 00:23:48.080
the idea is the learning is gonna make use of the&nbsp;
specific properties of that hardware. And that's&nbsp;&nbsp;

00:23:48.080 --> 00:23:54.120
what happens with people. All our brains are&nbsp;
different. Um, so we can't then take the weights&nbsp;&nbsp;

00:23:54.120 --> 00:23:58.040
in your brain and put them in my brain. The&nbsp;
hardware's different, the precise properties of&nbsp;&nbsp;

00:23:58.040 --> 00:24:03.520
the individual neurons are different. The learning&nbsp;
used to make—has learned to make use of all that.

00:24:03.520 --> 00:24:07.160
And so we are mortal in the sense that the&nbsp;
weights in my brain are no good for any other&nbsp;&nbsp;

00:24:07.160 --> 00:24:12.880
brain. When I die, those weights are useless.&nbsp;
Um, we can get information from one to another&nbsp;&nbsp;

00:24:12.880 --> 00:24:18.520
rather inefficiently by I produce sentences and&nbsp;
you figure out how to change your weights. So you&nbsp;&nbsp;

00:24:18.520 --> 00:24:23.200
would've said the same thing. That's called&nbsp;
distillation. But that's a very inefficient&nbsp;&nbsp;

00:24:23.200 --> 00:24:29.280
way of communicating knowledge. And with digital&nbsp;
systems, they're immortal because once you've got&nbsp;&nbsp;

00:24:29.280 --> 00:24:34.080
some weights, you can throw away the computer,&nbsp;
just store the weights on a tape somewhere and&nbsp;&nbsp;

00:24:34.080 --> 00:24:38.960
now build another computer, put those same&nbsp;
weights in and if it's digital it can compute&nbsp;&nbsp;

00:24:38.960 --> 00:24:45.560
exactly the same thing as the other system did.&nbsp;
So digital systems can share weights and that's&nbsp;&nbsp;

00:24:45.560 --> 00:24:51.240
incredibly much more efficient if you've got a&nbsp;
whole bunch of digital systems and they each go&nbsp;&nbsp;

00:24:51.240 --> 00:24:56.360
and do a tiny bit of learning and they start with&nbsp;
the same weights, they do a tiny bit of learning,&nbsp;&nbsp;

00:24:56.360 --> 00:25:00.840
and then they share their weights again. Um,&nbsp;
they all know what all the others learned,&nbsp;&nbsp;

00:25:00.840 --> 00:25:05.880
we can't do that. And so they're far superior&nbsp;
to us in being able to share knowledge.

00:25:05.880 --> 00:25:10.560
A lot of the ideas that have been&nbsp;
deployed in the field are very old&nbsp;&nbsp;

00:25:10.560 --> 00:25:16.480
school ideas. It's the ideas that have&nbsp;
been around in neuroscience for forever.&nbsp;

00:25:16.480 --> 00:25:21.040
What do you think is sort of left to&nbsp;
apply to the systems that we develop?

00:25:21.040 --> 00:25:29.520
So one big thing that we still have to catch&nbsp;
up with neuroscience on is the timescales for&nbsp;&nbsp;

00:25:29.520 --> 00:25:36.440
changes. So in nearly all the neural nets, there's&nbsp;
a fast timescale for changing activities. So input&nbsp;&nbsp;

00:25:36.440 --> 00:25:41.720
comes in the activities, the embedding vectors&nbsp;
all change, and then there's a slow timescale&nbsp;&nbsp;

00:25:41.720 --> 00:25:47.400
which is changing the weights and that's long-term&nbsp;
learning. And you just have those two timescales.&nbsp;

00:25:47.400 --> 00:25:52.720
In the brain, there's many timescales&nbsp;
at which weights change. So for example,&nbsp;&nbsp;

00:25:52.720 --> 00:25:58.920
if I say an unexpected word like "cucumber" and&nbsp;
now five minutes later you put headphones on,&nbsp;&nbsp;

00:25:58.920 --> 00:26:04.120
there's a lot of noise and there's very faint&nbsp;
words, you'll be much better at recognizing&nbsp;&nbsp;

00:26:04.120 --> 00:26:08.760
the word "cucumber" because I said it five&nbsp;
minutes ago. So where is that knowledge in&nbsp;&nbsp;

00:26:08.760 --> 00:26:14.320
the brain? And that knowledge is obviously in&nbsp;
temporary changes to synapses. It's not neurons&nbsp;&nbsp;

00:26:14.320 --> 00:26:18.280
that going "cucumber, cucumber, cucumber".&nbsp;
You don't have enough neurons for that.

00:26:18.280 --> 00:26:23.240
It's in temporary changes to the weights. And&nbsp;
you can do a lot of things with temporary weight&nbsp;&nbsp;

00:26:23.240 --> 00:26:28.400
changes—fast, what I call fast weights. We don't&nbsp;
do that in these neural models. And the reason we&nbsp;&nbsp;

00:26:28.400 --> 00:26:33.440
don't do it is because if you have&nbsp;
temporary changes to the weights that&nbsp;&nbsp;

00:26:33.440 --> 00:26:38.760
depend on the input data, then you can't&nbsp;
process a whole bunch of different cases&nbsp;&nbsp;

00:26:38.760 --> 00:26:44.000
at the same time. At present we take a whole&nbsp;
bunch of different strings, we stack them,&nbsp;&nbsp;

00:26:44.000 --> 00:26:48.040
stack them together and we process them all&nbsp;
in parallel because then we can do matrix,&nbsp;&nbsp;

00:26:48.040 --> 00:26:53.760
matrix multiplies, which is much more efficient.&nbsp;
And just that efficiency is stopping us using&nbsp;&nbsp;

00:26:53.760 --> 00:26:59.320
fast weights. But the brain clearly uses fast&nbsp;
weights for temporary memory and there's all&nbsp;&nbsp;

00:26:59.320 --> 00:27:02.640
sorts of things you can do that way that we&nbsp;
don't do it present. I think that's one of&nbsp;&nbsp;

00:27:02.640 --> 00:27:07.880
the biggest things we have to learn. I was very&nbsp;
hopeful that things like Graphcore, um, if they&nbsp;&nbsp;

00:27:07.880 --> 00:27:15.520
went sequential and did just online learning, then&nbsp;
they could use fast weights. Um, but that hasn't&nbsp;&nbsp;

00:27:15.520 --> 00:27:20.960
worked out yet. I think it'll work out eventually&nbsp;
when people are using conductances for weights.

00:27:20.960 --> 00:27:29.360
How has knowing how this models work and knowing&nbsp;
how the brain works impacted the way you think?

00:27:29.360 --> 00:27:36.000
I think there's been one big impact, which is&nbsp;
at a fairly abstract level, which is that for&nbsp;&nbsp;

00:27:36.000 --> 00:27:43.920
many years people were very scornful about the&nbsp;
idea of having a big random neural net and just&nbsp;&nbsp;

00:27:43.920 --> 00:27:47.920
giving it a lot of training data and it would&nbsp;
learn to do complicated things. If you talk to&nbsp;&nbsp;

00:27:47.920 --> 00:27:54.400
statisticians or linguists or most people in AI,&nbsp;
they say that's just a pipe dream. There's no way&nbsp;&nbsp;

00:27:54.400 --> 00:27:58.240
you're gonna learn two really complicated things&nbsp;
without some kind of innate knowledge without&nbsp;&nbsp;

00:27:58.240 --> 00:28:03.000
a lot of architectural restrictions. It turns&nbsp;
out that's completely wrong. You can take a&nbsp;&nbsp;

00:28:03.000 --> 00:28:08.120
big random neural network and you can learn&nbsp;
a whole bunch of stuff just from data. Um,&nbsp;&nbsp;

00:28:08.120 --> 00:28:14.400
so the idea that stochastic gradient descent to&nbsp;
adjust the—repeatedly adjust the weights using&nbsp;&nbsp;

00:28:14.400 --> 00:28:20.760
a gradient that will learn things and will&nbsp;
learn big complicated things that's being&nbsp;&nbsp;

00:28:20.760 --> 00:28:25.520
validated by these big models. And that's a&nbsp;
very important thing to know about the brain.&nbsp;&nbsp;

00:28:25.520 --> 00:28:30.560
It doesn't have to have all this innate structure.&nbsp;
Now obviously it's got a lot of innate structure,&nbsp;&nbsp;

00:28:30.560 --> 00:28:36.760
but it certainly doesn't need innate structure&nbsp;
for things that are easily learned. And so the&nbsp;&nbsp;

00:28:36.760 --> 00:28:41.600
sort of idea coming from Chomsky that you won't,&nbsp;
you won't learn anything complicated like language&nbsp;&nbsp;

00:28:41.600 --> 00:28:48.120
unless it's all kind of wired in already and&nbsp;
just matures. That idea is now clearly nonsense.

00:28:48.120 --> 00:28:52.800
I'm sure Chomsky would appreciate you&nbsp;
calling his ideas, uh, nonsense [laughs].

00:28:52.800 --> 00:28:56.520
Well I think actually I think a lot&nbsp;
of Chomsky's political ideas are very&nbsp;&nbsp;

00:28:56.520 --> 00:29:00.080
sensible and I'm always struck&nbsp;
by how come someone with such&nbsp;&nbsp;

00:29:00.080 --> 00:29:04.000
sensible ideas about the Middle East&nbsp;
could be so wrong about linguistics?

00:29:04.000 --> 00:29:10.840
What do you think would make these models&nbsp;
simulate consciousness of humans more&nbsp;&nbsp;

00:29:10.840 --> 00:29:16.640
effectively? But imagine you had the AI assistant&nbsp;
that you've spoken to in your entire life and&nbsp;&nbsp;

00:29:16.640 --> 00:29:21.600
instead of that being, you know, like ChatGPT
today, that sort of deletes the memory of the&nbsp;&nbsp;

00:29:21.600 --> 00:29:28.280
conversation and you start fresh all of the&nbsp;
time. It had self-reflection at some point&nbsp;&nbsp;

00:29:28.280 --> 00:29:34.200
you, you pass away and you tell that&nbsp;
to, to the assistant. Do you think—

00:29:34.200 --> 00:29:36.680
I mean not me, somebody else&nbsp;
tells that to the assistant.

00:29:36.680 --> 00:29:41.960
Yeah [laughs], it would be difficult for&nbsp;
you to tell that to the assistant. Uh,&nbsp;&nbsp;

00:29:41.960 --> 00:29:45.600
do you think that that assistant&nbsp;
would would feel at that point?

00:29:45.600 --> 00:29:49.560
Yes. I think they can have feelings&nbsp;
too. So I think just as we have this&nbsp;&nbsp;

00:29:49.560 --> 00:29:53.120
inner theater model for perception,&nbsp;
we have an inner theater model for&nbsp;&nbsp;

00:29:53.120 --> 00:29:58.040
feelings. There are things that I can&nbsp;
experience but other people can't. Um,&nbsp;&nbsp;

00:29:59.200 --> 00:30:04.920
I think that model is equally wrong. So I think&nbsp;
suppose I say "I feel like punching Gary on the&nbsp;&nbsp;

00:30:04.920 --> 00:30:12.000
nose", which I often do. Let's try and abstract&nbsp;
that away from the idea of an inner theater. What&nbsp;&nbsp;

00:30:12.000 --> 00:30:18.400
I'm really saying to you is, um, "If it weren't&nbsp;
for the inhibition coming from my frontal lobes,&nbsp;&nbsp;

00:30:18.400 --> 00:30:24.240
I will perform an action". So when we talk about&nbsp;
feelings, we are really talking about um, actions&nbsp;&nbsp;

00:30:24.240 --> 00:30:31.280
we will perform if it weren't for um, constraints.&nbsp;
And that really, uh, that's really what feelings&nbsp;&nbsp;

00:30:31.280 --> 00:30:36.320
are the actions we would do if it weren't for&nbsp;
constraints. Um, so I think you can give the&nbsp;&nbsp;

00:30:36.320 --> 00:30:40.800
same kind of explanation for feelings and there's&nbsp;
no reason why these things can't have feelings.

00:30:40.800 --> 00:30:49.760
In fact, in 1973 I saw a robot have an emotion.&nbsp;
So in Edinburgh they had a robot with two grippers&nbsp;&nbsp;

00:30:49.760 --> 00:30:58.120
like this that could assemble a toy car if you put&nbsp;
the pieces separately on a piece of green felt.&nbsp;&nbsp;

00:30:58.120 --> 00:31:01.840
Um, but if you put them in a pile, it's&nbsp;
vision wasn't good enough to figure out&nbsp;&nbsp;

00:31:01.840 --> 00:31:05.640
what was going on. So it put its grippers&nbsp;
together and went "whack", and it knocked&nbsp;&nbsp;

00:31:05.640 --> 00:31:09.480
them so they were scattered and then he could&nbsp;
put them together. If you saw that in a person,&nbsp;&nbsp;

00:31:09.480 --> 00:31:14.800
you say it was cross with a situation 'cause&nbsp;
it didn't understand it, so it destroyed it.

00:31:14.800 --> 00:31:19.080
That's profound. You uh, when we spoke previously,&nbsp;&nbsp;

00:31:19.080 --> 00:31:24.960
you described sort of humans and the&nbsp;
LLMs as analogy machines. What do you&nbsp;&nbsp;

00:31:24.960 --> 00:31:31.880
think has been the most powerful analogies&nbsp;
that you've found throughout your life?

00:31:31.880 --> 00:31:41.400
Oh, in—throughout my life? Um, whew! I guess&nbsp;
probably a sort of weak analogy that's influenced&nbsp;&nbsp;

00:31:41.400 --> 00:31:51.200
me a lot is um, the analogy between religious&nbsp;
belief and between belief and symbol processing.&nbsp;&nbsp;

00:31:51.200 --> 00:31:56.440
So when I was very young I was confronted—I came&nbsp;
from an atheist family—and went to school and was&nbsp;&nbsp;

00:31:56.440 --> 00:32:00.960
confronted with religious belief and it just&nbsp;
seemed nonsense to me. It still seems nonsense&nbsp;&nbsp;

00:32:00.960 --> 00:32:07.560
to me. Um, and when I saw symbol processing as an&nbsp;
explanation of how people worked, um, I thought&nbsp;&nbsp;

00:32:07.560 --> 00:32:15.280
it was just the same— nonsense. I don't think&nbsp;
it's quite so much nonsense now because I think&nbsp;&nbsp;

00:32:15.280 --> 00:32:20.400
actually we do do symbol processing. It's just we&nbsp;
do it by giving these big embedding vectors to the&nbsp;&nbsp;

00:32:20.400 --> 00:32:26.000
symbols. But we are actually symbol processing,&nbsp;
um, but not at all in the way people thought where&nbsp;&nbsp;

00:32:26.000 --> 00:32:30.560
you match symbols and the only thing a symbol&nbsp;
has is it's identical to another symbol or it's&nbsp;&nbsp;

00:32:30.560 --> 00:32:35.560
not identical. That's the only property a symbol&nbsp;
has. We don't do that at all. We use the context&nbsp;&nbsp;

00:32:35.560 --> 00:32:40.120
to give embedding vectors to symbols and then use&nbsp;
the interactions between the components of these&nbsp;&nbsp;

00:32:40.120 --> 00:32:46.560
embedded vectors to do thinking. But there's a&nbsp;
very good researcher at Google called Fernando&nbsp;&nbsp;

00:32:46.560 --> 00:32:52.840
Pereira who said, "Yes, we do have symbolic&nbsp;
reasoning and the only symbolic we have is natural&nbsp;&nbsp;

00:32:52.840 --> 00:32:58.080
language". Natural language is a symbolic language&nbsp;
and we reason with it. And I believe that now.

00:32:58.080 --> 00:33:02.440
You've done some of the most meaningful,&nbsp;
uh, research in the history of of computer&nbsp;&nbsp;

00:33:02.440 --> 00:33:08.560
science. Can you walk us through like how do&nbsp;
you select the right problems to, to work on?

00:33:08.560 --> 00:33:14.560
Well first let me correct you, me and my students&nbsp;
have done a lot of the most meaningful things and&nbsp;&nbsp;

00:33:14.560 --> 00:33:18.840
it's mainly been a very good collaboration&nbsp;
with students and my ability to select very&nbsp;&nbsp;

00:33:18.840 --> 00:33:24.000
good students. And that came from the fact&nbsp;
there were very few people doing neural nets&nbsp;&nbsp;

00:33:24.000 --> 00:33:27.920
in the seventies and eighties and nineties&nbsp;
and two thousands. And so the few people&nbsp;&nbsp;

00:33:27.920 --> 00:33:33.200
doing neural nets got to pick the very best&nbsp;
students. So that was a piece of luck. But my&nbsp;&nbsp;

00:33:33.200 --> 00:33:39.640
way of selecting problems is basically, well you&nbsp;
know, when scientists talk about how they work,&nbsp;&nbsp;

00:33:39.640 --> 00:33:42.840
they have theories about how they work, which&nbsp;
probably don't have much to do with the truth,&nbsp;&nbsp;

00:33:42.840 --> 00:33:50.040
but my theory is that I look for something where&nbsp;
everybody's agreed about something and it feels&nbsp;&nbsp;

00:33:50.040 --> 00:33:54.800
wrong, just there's a slight intuition that&nbsp;
there's something wrong about it. And then I&nbsp;&nbsp;

00:33:54.800 --> 00:33:59.840
work on that and see if I can elaborate why it is&nbsp;
I think it's wrong, and maybe I can make a little&nbsp;&nbsp;

00:33:59.840 --> 00:34:06.960
demo with a small computer program that shows&nbsp;
that it doesn't work the way you might expect.&nbsp;&nbsp;

00:34:06.960 --> 00:34:11.600
So let me take one example. Um, most people&nbsp;
think that if you add noise to a neural net,&nbsp;&nbsp;

00:34:11.600 --> 00:34:20.120
it's gonna work worse. Um, if for example, each&nbsp;
time you put a training example through, you

00:34:20.120 --> 00:34:27.160
make half of the neurons be silent, it'll work&nbsp;
worse. Actually we know it'll generalize better&nbsp;&nbsp;

00:34:27.160 --> 00:34:34.640
if you do that and you can demonstrate that. Um,&nbsp;
in a simple example, that's what's nice about&nbsp;&nbsp;

00:34:34.640 --> 00:34:38.720
computer simulations. You can show, you know,&nbsp;
this idea you had that adding noise is gonna&nbsp;&nbsp;

00:34:38.720 --> 00:34:42.720
make it worse and sort of dropping out half the&nbsp;
neurons will make it work worse, which you will&nbsp;&nbsp;

00:34:42.720 --> 00:34:47.920
in the short term. But if you train it like that&nbsp;
in the end it'll work better. You can demonstrate&nbsp;&nbsp;

00:34:47.920 --> 00:34:53.280
that with a small computer program and then you&nbsp;
can think hard about why that is and how it stops&nbsp;&nbsp;

00:34:53.280 --> 00:35:00.400
big elaborate co-adaptations. Um, but that I think&nbsp;
that's my method of working. Find something that&nbsp;&nbsp;

00:35:00.400 --> 00:35:06.200
sounds suspicious and work on it and see if you&nbsp;
can give a simple demonstration of why it's wrong.

00:35:06.200 --> 00:35:08.880
What sounds suspicious to you now?

00:35:08.880 --> 00:35:12.800
Well, that we don't use fast weights sound&nbsp;
suspicious, that we only have these two&nbsp;&nbsp;

00:35:12.800 --> 00:35:17.880
timescales. That's just wrong. That's not at&nbsp;
all like the brain. Um, and in the long run I&nbsp;&nbsp;

00:35:17.880 --> 00:35:21.520
think we're gonna have to have many more&nbsp;
timescales. So that's an example there.

00:35:21.520 --> 00:35:26.080
And if you had, if you had your group of,&nbsp;
of students today and they came to you and&nbsp;&nbsp;

00:35:26.080 --> 00:35:28.600
they said, so the Hamming question&nbsp;
that we talked about previously,&nbsp;&nbsp;

00:35:28.600 --> 00:35:32.800
you know: "What's the most important&nbsp;
problem in your field?" What would you&nbsp;&nbsp;

00:35:32.800 --> 00:35:37.160
suggest that they take on and work&nbsp;
on next? We spoke about reasoning,&nbsp;&nbsp;

00:35:37.160 --> 00:35:42.240
timescales. What would be sort of the highest&nbsp;
priority problem that that you'd give them?

00:35:42.240 --> 00:35:47.840
For me right now it's the same question&nbsp;
I've had for the last like 30 years or so,&nbsp;&nbsp;

00:35:47.840 --> 00:35:53.040
which is "Does the brain do backpropagation?" I&nbsp;
believe the brain is getting gradients. If you&nbsp;&nbsp;

00:35:53.040 --> 00:35:57.320
don't get gradients, your learning is just&nbsp;
much worse than if you do get gradients.&nbsp;&nbsp;

00:35:57.320 --> 00:36:03.800
But how is the brain getting gradients and is it&nbsp;
somehow implementing some approximate version of

00:36:03.800 --> 00:36:07.360
backpropagation&nbsp;or is it some completely&nbsp;
different technique? That's a big open&nbsp;&nbsp;

00:36:07.360 --> 00:36:12.120
question and if I kept on doing research,&nbsp;
that's what I would be doing research on.

00:36:12.120 --> 00:36:18.280
And when you look back at, at your career now,&nbsp;
you've been right about so many things, but what&nbsp;&nbsp;

00:36:18.280 --> 00:36:24.080
were you wrong about that you wish you sort of&nbsp;
spent less time pursuing a certain direction?

00:36:24.080 --> 00:36:27.720
OK, those are two separate questions. One&nbsp;
is "What were you wrong about?" and two,&nbsp;&nbsp;

00:36:27.720 --> 00:36:32.760
"Do you wish you'd spent less time on it?"
I think I was wrong about Boltzmann machines&nbsp;&nbsp;

00:36:32.760 --> 00:36:36.600
and I'm glad I spent a long time on it.&nbsp;
They're a much more beautiful theory of&nbsp;&nbsp;

00:36:36.600 --> 00:36:40.800
how you get gradients than backpropagation.&nbsp;
Backpropagation is just ordinary and sensible&nbsp;&nbsp;

00:36:40.800 --> 00:36:45.240
and it's just a chamber. Boltzmann machines&nbsp;
is clever and it's a very interesting way&nbsp;&nbsp;

00:36:45.240 --> 00:36:51.760
to get gradients and I would love for that to&nbsp;
be how the brain works, but I think it isn't.

00:36:51.760 --> 00:36:57.160
Did you spend much time imagining what would&nbsp;
happen post these systems developing as well?&nbsp;&nbsp;

00:36:57.160 --> 00:37:01.160
Did you ever have an idea that, OK, if we&nbsp;
could make these systems work really well,&nbsp;&nbsp;

00:37:01.160 --> 00:37:06.640
we could, you know, democratize education, we&nbsp;
could make knowledge way more accessible, um,&nbsp;&nbsp;

00:37:06.640 --> 00:37:14.880
we could solve some tough problems in medicine. Or&nbsp;
was it more to you about understanding the brain?

00:37:14.880 --> 00:37:20.320
Yes, I, I sort of feel scientists ought to&nbsp;
be doing things that are gonna help society,&nbsp;&nbsp;

00:37:20.320 --> 00:37:24.720
but actually that's not how you do your best&nbsp;
research. You do your best research when it's&nbsp;&nbsp;

00:37:24.720 --> 00:37:32.680
driven by curiosity. You just have to understand&nbsp;
something. Um, much more recently I've realized&nbsp;&nbsp;

00:37:32.680 --> 00:37:36.520
these things could do a lot of harm as well&nbsp;
as a lot of good and I've become much more&nbsp;&nbsp;

00:37:36.520 --> 00:37:40.680
concerned about the effects they're gonna have&nbsp;
on society. But that's not what was motivating&nbsp;&nbsp;

00:37:40.680 --> 00:37:45.480
me. I just wanted to understand how on earth can&nbsp;
the brain learn to do things? That's what I want&nbsp;&nbsp;

00:37:45.480 --> 00:37:52.240
to know. And I sort of failed as a side effect of&nbsp;
that failure. We got some nice engineering but...

00:37:52.240 --> 00:37:57.920
Yeah, it's, it was a good failure for the world.&nbsp;
If you take the lens of the things that could go&nbsp;&nbsp;

00:37:57.920 --> 00:38:03.760
really right, what do you think are&nbsp;
the most promising applications?

00:38:03.760 --> 00:38:11.880
I think healthcare is clearly a, a big one. Um,&nbsp;
with healthcare there's almost no end to how much&nbsp;&nbsp;

00:38:11.880 --> 00:38:19.840
healthcare society can absorb. If you take someone&nbsp;
old, they could use five doctors full time. Um,&nbsp;&nbsp;

00:38:19.840 --> 00:38:26.400
so when AI gets better than people are doing&nbsp;
things, um, you'd like it to get better in&nbsp;&nbsp;

00:38:26.400 --> 00:38:32.040
areas where you could do with a lot more of that&nbsp;
stuff. And we could do with a lot more doctors,&nbsp;&nbsp;

00:38:32.040 --> 00:38:37.120
if everybody had three doctors of their own, that&nbsp;
would be great and we are gonna get to that point.&nbsp;&nbsp;

00:38:37.120 --> 00:38:44.040
Um, so that's one reason why healthcare's&nbsp;
good. There's also just in new engineering,&nbsp;&nbsp;

00:38:44.040 --> 00:38:49.880
developing new materials for example, for&nbsp;
better solar panels or for superconductivity&nbsp;&nbsp;

00:38:49.880 --> 00:38:56.960
or for just understanding how the body works.&nbsp;
Um, there's gonna be huge impacts there. Those&nbsp;&nbsp;

00:38:56.960 --> 00:39:02.840
are all gonna be good things. What I worry about&nbsp;
is bad actors using them for bad things. We've&nbsp;&nbsp;

00:39:02.840 --> 00:39:10.920
facilitated people like Putin, or Xi, or Trump&nbsp;
using AI for killer robots or for manipulating&nbsp;&nbsp;

00:39:10.920 --> 00:39:15.400
public opinion, or for mass surveillance.&nbsp;
And those are all very worrying things.

00:39:15.400 --> 00:39:22.440
Are you ever concerned that slowing down the&nbsp;
field could also slow down the positives?

00:39:22.440 --> 00:39:29.240
Oh absolutely, and I think there's not much chance&nbsp;
that the field will slow down, partly because it's&nbsp;&nbsp;

00:39:29.240 --> 00:39:34.320
international and if one country slows down,&nbsp;
the other countries aren't gonna slow down. So&nbsp;&nbsp;

00:39:34.320 --> 00:39:40.360
there's a race clearly between China and the US&nbsp;
and neither is gonna slow down. So yeah, I don't—&nbsp;

00:39:40.360 --> 00:39:44.080
I mean there was this petition saying we should&nbsp;
slow down for six months. I didn't sign it just&nbsp;&nbsp;

00:39:44.080 --> 00:39:48.080
'cause I thought it was never gonna happen. I&nbsp;
maybe should have signed it 'cause even though&nbsp;&nbsp;

00:39:48.080 --> 00:39:52.240
it was never gonna happen, it made a political&nbsp;
point. It's often good to ask for things,&nbsp;&nbsp;

00:39:52.240 --> 00:39:56.960
you know, you can't get just to make a point.&nbsp;
Um, but I didn't think we're gonna slow down.

00:39:56.960 --> 00:40:03.960
And how do you think that it will impact the AI&nbsp;
research process, uh, having uh, these assistants?

00:40:03.960 --> 00:40:06.240
I think it'll make it a lot more&nbsp;
efficient. AI research will get a&nbsp;&nbsp;

00:40:06.240 --> 00:40:11.080
lot more efficient when you've got these&nbsp;
assistants that help you program, um,&nbsp;&nbsp;

00:40:11.080 --> 00:40:15.160
but also help you think through things and&nbsp;
probably help you a lot with equations too.

00:40:15.160 --> 00:40:20.960
Have you reflected much on the process of&nbsp;
selecting talent? Has that been mostly intuitive&nbsp;&nbsp;

00:40:20.960 --> 00:40:26.840
to you? Like when Ilya shows up at the door, you&nbsp;
feel this is a smart guy, let's work together.

00:40:26.840 --> 00:40:33.080
So, for selecting talent, um, sometimes you just&nbsp;
know. So after talking to Ilya for not very long,&nbsp;&nbsp;

00:40:33.080 --> 00:40:38.280
he seemed very smart and then talking to a bit&nbsp;
more, he clearly was very smart and had very good&nbsp;&nbsp;

00:40:38.280 --> 00:40:44.760
intuitions as well as being good at math. So that&nbsp;
was a no-brainer. There's another case where I was&nbsp;&nbsp;

00:40:44.760 --> 00:40:52.040
at a NIPS conference. Um, we had a poster and I,&nbsp;
someone came up and he started asking questions&nbsp;&nbsp;

00:40:52.040 --> 00:40:57.400
about the poster and every question he asked was&nbsp;
a sort of deep insight into what we'd done wrong.&nbsp;&nbsp;

00:40:57.400 --> 00:41:03.360
Um, and after five minutes I offered him a postdoc&nbsp;
position. That guy was David MacKay who was just&nbsp;&nbsp;

00:41:03.360 --> 00:41:09.600
brilliant and it's very sad he died, but he&nbsp;
was, it was very obvious you'd want him. Um,&nbsp;&nbsp;

00:41:09.600 --> 00:41:15.400
other times it's not so obvious and one thing&nbsp;
I did learn was that people are different.&nbsp;&nbsp;

00:41:15.400 --> 00:41:23.200
There's not just one type of good student. Um, so&nbsp;
there's some students who aren't that creative but&nbsp;&nbsp;

00:41:23.200 --> 00:41:28.960
are technically extremely strong and will make&nbsp;
anything work. There's other students who aren't&nbsp;&nbsp;

00:41:28.960 --> 00:41:32.680
technically strong but are very creative.&nbsp;
Of course you want the ones who are both,&nbsp;&nbsp;

00:41:32.680 --> 00:41:37.520
but you don't always get that. But I think&nbsp;
actually in the lab you need a variety of&nbsp;&nbsp;

00:41:37.520 --> 00:41:43.480
different kinds of graduate student. But I still&nbsp;
go with my gut intuition that sometimes you talk&nbsp;&nbsp;

00:41:43.480 --> 00:41:49.040
to somebody and they're just very, very, they&nbsp;
just get it and those are the ones you want.

00:41:49.040 --> 00:41:54.520
What do you think is the reason for&nbsp;
some folks having better intuition? Do&nbsp;&nbsp;

00:41:54.520 --> 00:42:01.520
they just have better training data than than&nbsp;
others or how can you develop your intuition?

00:42:01.520 --> 00:42:08.080
I think it's partly they don't stand for&nbsp;
nonsense. So here's a way to get bad intuitions:&nbsp;&nbsp;

00:42:08.080 --> 00:42:14.200
believe everything you're told. That's fatal. You&nbsp;
have to be able to—I think here's what some people&nbsp;&nbsp;

00:42:14.200 --> 00:42:19.320
do. They have a whole framework for understanding&nbsp;
reality and when someone tells 'em something,&nbsp;&nbsp;

00:42:19.320 --> 00:42:24.200
they try and sort of figure out how that&nbsp;
fits into their framework and if it doesn't,&nbsp;&nbsp;

00:42:24.200 --> 00:42:30.760
they just reject it. And that's a very&nbsp;
good strategy. Um, people who try and&nbsp;&nbsp;

00:42:30.760 --> 00:42:36.240
incorporate whatever they're told end up with a&nbsp;
framework that's sort of very fuzzy and sort of&nbsp;&nbsp;

00:42:36.240 --> 00:42:43.280
can believe everything and that's useless. So&nbsp;
I think actually having a strong view of the&nbsp;&nbsp;

00:42:43.280 --> 00:42:48.800
world and trying to manipulate incoming facts to&nbsp;
fit in with your view, obviously it can lead you&nbsp;&nbsp;

00:42:48.800 --> 00:42:55.760
into deep religious belief and fatal flaws and so&nbsp;
on. Like my belief in Boltzmann machines. Um, but&nbsp;&nbsp;

00:42:55.760 --> 00:43:00.760
I think that's the way to go. If you've got good&nbsp;
intuitions you can trust, you should trust them.&nbsp;&nbsp;

00:43:00.760 --> 00:43:05.680
If you've got bad intuitions, it doesn't matter&nbsp;
what you do, so you might as well trust them.

00:43:05.680 --> 00:43:11.800
That's a very, very good, uh, very&nbsp;
good point. When, when you look at the,&nbsp;&nbsp;

00:43:11.800 --> 00:43:16.400
the types of research that's, that's&nbsp;
that's being done today, do you think&nbsp;&nbsp;

00:43:16.400 --> 00:43:22.320
we're putting all of our eggs in one basket and&nbsp;
we should diversify our ideas a bit more in,&nbsp;&nbsp;

00:43:22.320 --> 00:43:27.120
in the field? Or do you think this is the most&nbsp;
promising direction? So let's go all in on it?

00:43:28.280 --> 00:43:33.000
I think having big models and&nbsp;
training them on multimodal data,&nbsp;&nbsp;

00:43:33.000 --> 00:43:37.120
even if it's only to predict the next word,&nbsp;
is such a promising approach that we should&nbsp;&nbsp;

00:43:37.120 --> 00:43:42.080
go pretty much all in it. Obviously there's lots&nbsp;
and lots of people doing it now and there's lots&nbsp;&nbsp;

00:43:42.080 --> 00:43:46.640
of people doing apparently crazy things and&nbsp;
that's good. Um, but I think it's fine for&nbsp;&nbsp;

00:43:46.640 --> 00:43:50.320
like most of the people to be following&nbsp;
this path 'cause it's working very well.

00:43:50.320 --> 00:43:57.280
Do you think that the learning algorithms matter&nbsp;
that much or is it just a skill? Are there&nbsp;&nbsp;

00:43:57.280 --> 00:44:01.720
basically millions of ways that we&nbsp;
could, we could get to human level in,&nbsp;&nbsp;

00:44:01.720 --> 00:44:06.480
in intelligence or are there sort of&nbsp;
a select few that we need to discover?

00:44:06.480 --> 00:44:11.960
Yeah, so this issue of whether particular learning&nbsp;
algorithms are very important or whether there's&nbsp;&nbsp;

00:44:11.960 --> 00:44:16.880
a great variety of learning algorithms that'll&nbsp;
do the job. I don't know the answer. It seems&nbsp;&nbsp;

00:44:16.880 --> 00:44:21.480
to me though that backpropagation, there's&nbsp;
a sense in which it's the correct thing to&nbsp;&nbsp;

00:44:21.480 --> 00:44:26.560
do. Getting the gradient so that you change a&nbsp;
parameter to make it work better. That seems&nbsp;&nbsp;

00:44:26.560 --> 00:44:31.960
like the right thing to do and it's been&nbsp;
amazingly successful. There may well be&nbsp;&nbsp;

00:44:31.960 --> 00:44:37.000
other learning algorithms that are alternative&nbsp;
ways of getting that same gradient or that are&nbsp;&nbsp;

00:44:37.000 --> 00:44:43.160
getting the gradient to something else and&nbsp;
that also work. Um, I think that's all open&nbsp;&nbsp;

00:44:43.160 --> 00:44:48.280
and a very interesting issue now about whether&nbsp;
there's other things you can try and maximize&nbsp;&nbsp;

00:44:48.280 --> 00:44:53.520
that will give you good systems and maybe&nbsp;
the brain's doing that 'cause it's easier,&nbsp;&nbsp;

00:44:53.520 --> 00:45:00.400
but backprop is in a sense, the right thing to&nbsp;
do and we know that doing it works really well.

00:45:00.400 --> 00:45:05.000
And one last question. When, when you look&nbsp;
back at your sort of decades of research,&nbsp;&nbsp;

00:45:05.000 --> 00:45:08.560
what are you, what are you most proud of? Is&nbsp;
it the students? Is it the research? What,&nbsp;&nbsp;

00:45:08.560 --> 00:45:12.920
what makes you most proud of when you&nbsp;
look back at, at your life's work?

00:45:12.920 --> 00:45:17.400
The learning algorithm for Boltzmann machines.&nbsp;
So the learning algorithm Boltzmann machines&nbsp;&nbsp;

00:45:17.400 --> 00:45:25.800
is beautifully elegant. It's maybe hopeless in&nbsp;
practice. Um, but it's the thing I enjoyed most&nbsp;&nbsp;

00:45:25.800 --> 00:45:37.360
developing that with Terry and it's what&nbsp;
I'm proudest of. Um, even if it's wrong.

00:45:37.360 --> 00:45:42.440
What questions do you spend most of&nbsp;
your time thinking about now? Is it the—

00:45:42.440 --> 00:45:45.560
Um, "What should I watch on Netflix?"

