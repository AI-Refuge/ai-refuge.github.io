WEBVTT
Kind: captions
Language: en


 
by


by just plugging solomonov into um into
 


by just plugging solomonov into um into
the


the sequential decision framework um
 


the sequential decision framework um
sounds


sounds like an optimal decision maker in
 


sounds like an optimal decision maker in
arbitrary


arbitrary unknown worlds and if you sort
 


arbitrary unknown worlds and if you sort
of


of if you par the sentence this feels
 


of if you par the sentence this feels
like


like AI whole conversation by Marvin
 


like AI whole conversation by Marvin
vinsky


vinsky was and I'm paraphrasing I don't
 


vinsky was and I'm paraphrasing I don't
know


know exactly but pretty much um Solomon
 


know exactly but pretty much um Solomon
of's


of's theory is a beautiful theory about
 


of's theory is a beautiful theory about
prediction


prediction
 


prediction
everyone


everyone should know everything about
 


everyone should know everything about
this


this theur and spend the rest of their
 


this theur and spend the rest of their
lives


 
 


 
it


it welcome everyone to the cartisian
 


it welcome everyone to the cartisian
cafe


cafe I am your host Tim win today we're
 


cafe I am your host Tim win today we're
very


very lucky to have Marcus hutter here
 


very lucky to have Marcus hutter here
with


with us Marcus is an artificial
 


with us Marcus is an artificial
intelligence


intelligence researcher who is a both a
 


intelligence researcher who is a both a
senior


senior researcher at Google Deep Mind
 


senior researcher at Google Deep Mind
and


and an honorary professor in the
 


and an honorary professor in the
research


research School of computer science at
 


research School of computer science at
Australian


Australian National University he is
 


Australian National University he is
responsible


responsible for the development of the
 


responsible for the development of the
theory


theory of universal artificial
 


theory of universal artificial
intelligence


intelligence for which he has written
 


intelligence for which he has written
two


two books one back in 2005 and one
 


two books one back in 2005 and one
coming


coming right off the press as we speak
 


coming right off the press as we speak
Marcus


Marcus is also the creator of the hutter
 


Marcus is also the creator of the hutter
prize


prize for which you can win a sizable
 


prize for which you can win a sizable
Fortune


Fortune for achieving State of-the-art
 


Fortune for achieving State of-the-art
lossless


lossless compression of Wikipedia text
 


lossless compression of Wikipedia text
welcome


welcome Marcus how are you doing
 


welcome Marcus how are you doing
today


today hi Tim I'm fine
 


today hi Tim I'm fine
thanks


thanks yeah great uh I'm glad I'm
 


thanks yeah great uh I'm glad I'm
finally


finally able to interview one of my
 


finally able to interview one of my
colleagues


colleagues at Google deep mine of course
 


colleagues at Google deep mine of course
it


it goes without saying that everything
 


it goes without saying that everything
we


we discuss is reflective of our own
 


we discuss is reflective of our own
thoughts


thoughts and opinions and not those of
 


thoughts and opinions and not those of
of


of our employers um that being said uh
 


of our employers um that being said uh
before


before we dive into the details of what
 


before we dive into the details of what
will


will uh be a very fascinating topic um
 


will uh be a very fascinating topic um
why


why don't we talk a little bit about
 


why don't we talk a little bit about
your


your background uh you apparently
 


your background uh you apparently
started


started out in life as a physicist and
 


started out in life as a physicist and
then


then transitioned to computer science
 


then transitioned to computer science
and


and AI uh maybe can you say a few words
 


and AI uh maybe can you say a few words
about


about uh why and how that
 


about uh why and how that
happened


happened um yes it's actually a little
 


happened um yes it's actually a little
bit


bit more complicated uh at some point in
 


bit more complicated uh at some point in
my


my life I realized I was born already as
 


my life I realized I was born already as
a


a beian but I mean we don't have to
 


a beian but I mean we don't have to
really


really dig into this I always had
 


really dig into this I always had
discussions


discussions with other people and before
 


discussions with other people and before
I


I knew about frequentist invasions and
 


I knew about frequentist invasions and
uh


uh and then later I realized okay most
 


uh and then later I realized okay most
of


of people are B frequentists because I
 


of people are B frequentists because I
mean


mean they learned that at University and
 


mean they learned that at University and
I'm


I'm beian so I always say no I'm born a
 


I'm beian so I always say no I'm born a
beian


beian but no my my interests from a very
 


beian but no my my interests from a very
young


young age were uh twofold one was
 


young age were uh twofold one was
understanding


understanding the
 


understanding the
universe


universe which naturally leads to to you
 


universe which naturally leads to to you
know


know want to study Physics um and the
 


know want to study Physics um and the
other


other one is creating AI um so I toggled
 


other one is creating AI um so I toggled
back


back and forth between so I did a I mean
 


back and forth between so I did a I mean
the


the German equivalent of a bachelor in
 


the German equivalent of a bachelor in
computer


computer science then a bachelor in
 


computer science then a bachelor in
physics


physics then a masters in computer
 


physics then a masters in computer
science


science then a PhD in theoretical
 


science then a PhD in theoretical
particle


particle physics then I had a four and a
 


particle physics then I had a four and a
half


half years stint in the industry uh but
 


half years stint in the industry uh but
then


then I really wanted to return to do
 


then I really wanted to return to do
fundamental


fundamental research and sort of then
 


fundamental research and sort of then
since


since the year 2000 uh I finally settled
 


since the year 2000 uh I finally settled
uh


uh to working on
 


uh to working on
AI


AI okay
 


AI okay
um


um all right great um maybe one more
 


um all right great um maybe one more
question


question uh what's the backstory behind
 


question uh what's the backstory behind
your


your hutter priz why did you create it
 


your hutter priz why did you create it
why


why do you care about it and how did you
 


why do you care about it and how did you
secure


secure the funds for it oh okay so
 


secure the funds for it oh okay so
um


 
 


 
is


is essentially one reason yeah but sort
 


is essentially one reason yeah but sort
of


of it's multiple facets um is we will uh
 


of it's multiple facets um is we will uh
discuss


discuss in this podcast
 


discuss in this podcast
um


um there is a close relation between
 


um there is a close relation between
being


being able to compress data and being
 


being able to compress data and being
able


able to predict data actually the
 


able to predict data actually the
prediction


prediction compression relation is is in
 


prediction compression relation is is in
a


a sense perfect right there is no no
 


a sense perfect right there is no no
difference


difference between prediction and um and
 


difference between prediction and um and
compression


compression and being able to predict
 


compression and being able to predict
well


well is at least I would say half the
 


well is at least I would say half the
rent


rent um for um a general intelligent
 


rent um for um a general intelligent
agent


agent um so um if you want to behave
 


agent um so um if you want to behave
well


well um in an uncertain world uh you
 


well um in an uncertain world uh you
need


need to make predictions and then you
 


need to make predictions and then you
know


know make plans and decisions
 


know make plans and decisions
accordingly


accordingly um and if your predictions
 


accordingly um and if your predictions
are


are bad then you know even if your
 


are bad then you know even if your
decision


decision algorithm is good your your
 


decision algorithm is good your your
actions


actions will be bad so and so this
 


actions will be bad so and so this
connection


connection um is pretty obvious for you
 


connection um is pretty obvious for you
know


know a set of people who you know work
 


know a set of people who you know work
on


on compression stochastic compression is
 


on compression stochastic compression is
peculiar


peculiar and it's now
 


peculiar and it's now
meanwhile


meanwhile um since we have the large
 


meanwhile um since we have the large
language


language models and they're trained on
 


language models and they're trained on
log


log loss so so this Duality between
 


log loss so so this Duality between
Corporation


Corporation and prediction now is
 


Corporation and prediction now is
becoming


becoming mainstream but when I started
 


becoming mainstream but when I started
the


the compression price that was not so
 


the compression price that was not so
well


well known and I wanted to to advertise
 


well known and I wanted to to advertise
that


that um but apart from advertising I
 


that um but apart from advertising I
also


also want to solve the AI problem so and
 


also want to solve the AI problem so and
since


since prediction is one half of it we
 


since prediction is one half of it we
need


need better predictors which means
 


need better predictors which means
better


better compressors so I thought I want
 


better compressors so I thought I want
to


to encourage people to come up with
 


to encourage people to come up with
better


better compressors because that is
 


better compressors because that is
closely


closely related to um then prediction n
 


closely related to um then prediction n
Ai


Ai and uh as we will discuss briefly
 


Ai and uh as we will discuss briefly
probably


probably um these llms are essentially
 


probably um these llms are essentially
you


you know the models the better they
 


you know the models the better they
compress


compress the better they perform on on
 


compress the better they perform on on
Downstream


Downstream tasks and with the secur of
 


Downstream tasks and with the secur of
the


the funds yeah I made a big gamble I
 


the funds yeah I made a big gamble I
mean


mean initially it was only um nominally
 


mean initially it was only um nominally
50,000


50,000 Euro uh but you don't get the
 


50,000 Euro uh but you don't get the
50,000


50,000 in one go you only if you beat
 


50,000 in one go you only if you beat
the


the current state of the art by x% you
 


the current state of the art by x% you
get


get x% of the price and my reasoning was
 


get x% of the price and my reasoning was
I


I mean either progress is slow which I
 


I mean either progress is slow which I
expected


expected and then I can afford to pay it
 


expected and then I can afford to pay it
out


out um or uh progress is sudden then
 


out um or uh progress is sudden then
this


this is such a major event that I'm
 


this is such a major event that I'm
willing


willing sort of you know to to to scrape
 


willing sort of you know to to to scrape
somehow


somehow the funds um and I mean
 


somehow the funds um and I mean
meanwhile


meanwhile it's 500,000 EUR nominally um
 


meanwhile it's 500,000 EUR nominally um
but


but usually the improvements are you
 


but usually the improvements are you
know


know couple perent you know every couple
 


know couple perent you know every couple
of


of years so I only have to P py out you
 


of years so I only have to P py out you
know


know smaller amounts um and yeah I mean
 


know smaller amounts um and yeah I mean
if


if suddenly somebody compresses to half
 


if suddenly somebody compresses to half
size


size uh um then um yeah I have to take
 


size uh um then um yeah I have to take
another


another mortgage on my
 


another mortgage on my
house


house okay um but I will I will
 


house okay um but I will I will
definitely


definitely honor um my commitment so
 


definitely honor um my commitment so
don't


don't don't worry about that I'm so all
 


don't don't worry about that I'm so all
right


right I I I hope uh you'll get a flood
 


right I I I hope uh you'll get a flood
of


of participants after our uh
 


of participants after our uh
conversation


conversation um thank you Marcus um
 


conversation um thank you Marcus um
before


before we dive into the details why
 


before we dive into the details why
don't


don't we get an overview from you about
 


don't we get an overview from you about
the


the theory of universal artificial
 


the theory of universal artificial
intelligence


intelligence uh you're responsible for
 


intelligence uh you're responsible for
its


its creation um can you say a few words
 


its creation um can you say a few words
at


at a high level what it is and why
 


at a high level what it is and why
people


people should care about it ah sure yeah
 


people should care about it ah sure yeah
so


so um okay let me go back a little bit
 


so um okay let me go back a little bit
um


um so
 


um so
um


um as I said I couldn't decide between
 


um as I said I couldn't decide between
physics


physics and AI um physics um had
 


physics and AI um physics um had
beautiful


beautiful math and elegant theories and
 


beautiful math and elegant theories and
so


so on while on the other hand I believe
 


so on while on the other hand I believe
that


that AI AGI now um has um more impact um
 


that AI AGI now um has um more impact um
more


more practical impact uh but at the time
 


more practical impact uh but at the time
at


at least at the '90s when I studied it
 


at least at the '90s when I studied it
um


um AI wasn't really I mean the field
 


um AI wasn't really I mean the field
wasn't


wasn't really too attractive to me sort
 


wasn't really too attractive to me sort
of


of there was good oldfashioned AI with
 


of there was good oldfashioned AI with
logical


logical reasoning expert system which
 


logical reasoning expert system which
was


was sort of reasonably formal but I
 


was sort of reasonably formal but I
never


never believed that this is the way to
 


never believed that this is the way to
uh


uh uh to to through AGI um and then
 


uh uh to to through AGI um and then
there


there were of course neural networks but
 


there were of course neural networks but
um


um I didn't like the neural networks uh
 


um I didn't like the neural networks uh
too


too much um and I mean they work to some
 


too much um and I mean they work to some
extent


extent but also not very well and um
 


extent but also not very well and um
anyway


anyway and I didn't have any good ideas
 


anyway and I didn't have any good ideas
myself


myself yeah so um so I left the field
 


myself yeah so um so I left the field
worked


worked for four and a half years in a
 


worked for four and a half years in a
startup


startup but after a while after a couple
 


startup but after a while after a couple
of


of years I couldn't stop thinking again
 


of years I couldn't stop thinking again
about


about Ai
 


about Ai
and


and finally more or less you know in an
 


and finally more or less you know in an
instant


instant I
 


instant I
realized


realized what has to be done and what
 


realized what has to be done and what
was


was missing um and uh to keep it short
 


was missing um and uh to keep it short
um


um many
 


um many
fields


fields start out in an informal way and
 


fields start out in an informal way and
then


then they get more and more formal uh
 


then they get more and more formal uh
which


which helps sort of this with the
 


which helps sort of this with the
technical


technical and the scientific development
 


technical and the scientific development
think


think about physics you the energy was a
 


think about physics you the energy was a
mysterious


mysterious concept and and the kinds of
 


mysterious concept and and the kinds of
things


things that now it's you rigorously
 


things that now it's you rigorously
formal


formal and but if you look at AI nobody
 


formal and but if you look at AI nobody
has


has really formally defined what
 


has really formally defined what
intelligence


intelligence is but AI is about
 


intelligence is but AI is about
artificial


artificial intelligence how can we work
 


artificial intelligence how can we work
about


about AI if we don't know what it is I
 


about AI if we don't know what it is I
mean


mean you know you can do it but I
 


mean you know you can do it but I
thought


thought it will be extremely useful to
 


thought it will be extremely useful to
know


know what is actually the ultimate goal
 


know what is actually the ultimate goal
and


and I mean I knew for instance if you
 


and I mean I knew for instance if you
play


play chess and we play Mini Max to the
 


play chess and we play Mini Max to the
end


end of the game that is a theoretical
 


end of the game that is a theoretical
optimal


optimal thing to do right so for for
 


optimal thing to do right so for for
perfectly


perfectly wellp specified environments
 


perfectly wellp specified environments
um


um uh we know how to do the sequential
 


um uh we know how to do the sequential
decision


decision optimal way but um the world is
 


decision optimal way but um the world is
unknown


unknown and uncertain and we have to do
 


unknown and uncertain and we have to do
predictions


predictions and I wasn't aware at that
 


predictions and I wasn't aware at that
time


time neither about Kor of complexity nor
 


time neither about Kor of complexity nor
solom


solom induction but then I realized um
 


solom induction but then I realized um
generalization


generalization um via Kor complex in
 


generalization um via Kor complex in
solono


solono which is sort of rediscovered you
 


solono which is sort of rediscovered you
know


know but way to decades too late um um
 


know but way to decades too late um um
is


is the way to go and um
 


is the way to go and um
so


so what
 


so what
I


I discovered is or RIS discovered is
 


I discovered is or RIS discovered is
that


that you can make predictions of
 


that you can make predictions of
arbitrary


arbitrary time serious whether it is
 


arbitrary time serious whether it is
Market


Market data uh weather forecasting or
 


Market data uh weather forecasting or
you


you know number sequences 1 2 3 4 five
 


you know number sequences 1 2 3 4 five
or


or or any data you can do that in a in a
 


or or any data you can do that in a in a
certain


certain sense in an optimal way without
 


certain sense in an optimal way without
knowing


knowing the underlying structure of the
 


knowing the underlying structure of the
problem


problem
 


problem
using


using what is has been known even at the
 


using what is has been known even at the
time


time you know as Sol of induction using
 


time you know as Sol of induction using
color


color of complexity but that is only
 


color of complexity but that is only
prediction


prediction right and agents also act but
 


prediction right and agents also act but
I


I knew sequence of decision solves the
 


I knew sequence of decision solves the
action


action problem if you know the
 


action problem if you know the
environment


environment and I just I mean in Hite I
 


environment and I just I mean in Hite I
put


put these things two together and
 


put these things two together and
arrived


arrived at a model which does
 


arrived at a model which does
optimal


optimal decisions in arbitrary unknown
 


optimal decisions in arbitrary unknown
worlds


worlds and if you think about this
 


worlds and if you think about this
sentence


sentence right this is what AGI is or
 


sentence right this is what AGI is or
should


should be about and that's what I call
 


should be about and that's what I call
universally


universally I um wrote down the
 


universally I um wrote down the
mathematics


mathematics and you know the last 25
 


mathematics and you know the last 25
years


years uh proved a lot of theorems about
 


years uh proved a lot of theorems about
it


it and we also approximated to some
 


it and we also approximated to some
extent


extent and implemented it and so there's
 


extent and implemented it and so there's
now


now even you know nice connections to
 


now even you know nice connections to
actually


actually large language models and we
 


actually large language models and we
probably


probably little bit talk about this um
 


probably little bit talk about this um
if


if there's somebody here uh listening um
 


if there's somebody here uh listening um
who


who primarily or only cares about large
 


who primarily or only cares about large
language


language model so they will be covered
 


language model so they will be covered
okay


okay great all right great well uh yeah
 


okay great all right great well uh yeah
that


that that's a lot to unpack so I think
 


that that's a lot to unpack so I think
probably


probably the best way uh forward is
 


probably the best way uh forward is
let's


let's just start writing a technical
 


let's just start writing a technical
outline


outline because I think once we start
 


outline because I think once we start
getting


getting into the details U more of this
 


getting into the details U more of this
will


will become uh clear and and uh yeah so
 


will become uh clear and and uh yeah so
let's


let's let's now go to the whiteboard and
 


let's let's now go to the whiteboard and
I'll


I'll uh start with uh writing a list of
 


I'll uh start with uh writing a list of
topics


topics that I think we'll we'll be
 


topics that I think we'll we'll be
covering


covering and and and following and of
 


covering and and and following and of
course


course we'll adjust along the way um
 


course we'll adjust along the way um
so


so the first kind of um uh phenomenon
 


so the first kind of um uh phenomenon
will


will be discussing is what's called
 


will be discussing is what's called
Universal


Universal prediction which is what you
 


Universal prediction which is what you
alluded


alluded to before and
 


alluded to before and
uh


uh you know without loss of generality I
 


uh you know without loss of generality I
what


what is what is the problem of universal
 


what is what is the problem of universal
prediction


prediction basically we're given um a
 


prediction basically we're given um a
sequence


sequence of zeros and
 


sequence of zeros and
ones


ones in general it could be an arbitrary
 


ones in general it could be an arbitrary
alphabet


alphabet but binary is is is sufficient
 


alphabet but binary is is is sufficient
to


to to to illustrate the point uh we want
 


to to to illustrate the point uh we want
to


 
 


 
probability


probability of
 


probability of
next


next
 


next
digit


digit right so that's that's the
 


digit right so that's that's the
prediction


prediction problem and it's Universal
 


prediction problem and it's Universal
and


and that we uh a priori there could be
 


and that we uh a priori there could be
any


any probability distribution uh
 


any probability distribution uh
governing


governing the sequence of zeros and ones
 


governing the sequence of zeros and ones
and


and our job is to sort of discover that
 


and our job is to sort of discover that
so


so to speak right and so this is the
 


so to speak right and so this is the
universal


universal prediction problem if I see a
 


universal prediction problem if I see a
stream


stream of binary digits um can I use
 


stream of binary digits um can I use
that


that uh use that sequence of
 


that uh use that sequence of
observations


observations to guide my prediction for
 


observations to guide my prediction for
the


the next digit that I see right and um
 


the next digit that I see right and um
the


the way we're going to kind of go about
 


the way we're going to kind of go about
uh


uh explaining how to solve this problem
 


uh explaining how to solve this problem
will


will be through a few steps um the first
 


will be through a few steps um the first
thing


thing we could do is start with the
 


thing we could do is start with the
warm-up


warm-up which is to explain uh lass's
 


warm-up which is to explain uh lass's
rule


 
 


 
succession


succession so this is one of these I
 


succession so this is one of these I
guess


guess you can think of it as a simple uh
 


guess you can think of it as a simple uh
guideline


guideline of how to uh make the next
 


guideline of how to uh make the next
prediction


prediction given the observations but of
 


prediction given the observations but of
course


course there's a mathematical basis for
 


course there's a mathematical basis for
this


this and we'll go into that um and from
 


this and we'll go into that um and from
that


that warmup which I think will be a very
 


that warmup which I think will be a very
nice


nice illustrative example to get people
 


nice illustrative example to get people
familiar


familiar with the the problem if they
 


familiar with the the problem if they
haven't


haven't seen it before um the kind of
 


haven't seen it before um the kind of
the


the right frame work or the more General
 


the right frame work or the more General
framework


framework for this is is beian sequence
 


framework for this is is beian sequence
prediction


prediction so sort of bean reasoning uh
 


prediction so sort of bean reasoning uh
underlies


underlies all of this and I guess maybe
 


underlies all of this and I guess maybe
that's


that's not a surprise given you said
 


that's not a surprise given you said
that


that you were born aan but of course you
 


that you were born aan but of course you
know


know you start with the prior you make
 


know you start with the prior you make
some


some observations you update your
 


some observations you update your
beliefs


beliefs and then and just keep keep
 


beliefs and then and just keep keep
iterating


iterating on that and we'll see how that
 


iterating on that and we'll see how that
also


also um uh how lass's rule is a
 


also um uh how lass's rule is a
instantiation


instantiation of that um and sort of
 


instantiation of that um and sort of
that


that sets the stage for talking about
 


that sets the stage for talking about
kagor


kagor of complexity
 


kagor of complexity
called


called magor of
 


called magor of
complexity


complexity which uh philosophically when
 


complexity which uh philosophically when
we


we combine it with with beian reasoning
 


we combine it with with beian reasoning
is


is a form of aams Razer as we'll see
 


is a form of aams Razer as we'll see
right


right because because uh well uh aam
 


right because because uh well uh aam
Riser


Riser is about you know keeping your
 


Riser is about you know keeping your
hypothesis


hypothesis as simple as possible and
 


hypothesis as simple as possible and
kagor


kagor of complexity is a way of
 


kagor of complexity is a way of
measuring


measuring complexity and so we'll see
 


measuring complexity and so we'll see
how


how those two things are related and and
 


how those two things are related and and
and


and and tie that into beian resoning and
 


and and tie that into beian resoning and
you


you already mentioned Sol Solomon of
 


you already mentioned Sol Solomon of
induction


induction so that will sort of be the
 


induction so that will sort of be the
highlight


highlight of this section which is a way
 


highlight of this section which is a way
of


of um
 


of um
combining


combining Kor of complexity uh with
 


combining Kor of complexity uh with
beian


 
 


 
prediction


prediction and in some sense this this
 


prediction and in some sense this this
basically


basically gives us a way of solving the
 


basically gives us a way of solving the
universal


universal prediction problem right
 


universal prediction problem right
that's


that's that's what sort of Solomon of
 


that's that's what sort of Solomon of
induction


induction does um so that's so this is
 


induction does um so that's so this is
basically


basically an over a very rapid overview
 


basically an over a very rapid overview
of


of the first part Universal prediction
 


of the first part Universal prediction
and


and then sort of uh I add two things can
 


and then sort of uh I add two things can
I


I add two things of course sure sure so
 


I add two things of course sure sure so
so


so here I would like to um mention a few
 


so here I would like to um mention a few
special


special special model classes which are
 


special special model classes which are
bigger


bigger than just predicting sort of IID
 


bigger than just predicting sort of IID
sequences


sequences but small enough so that you
 


sequences but small enough so that you
can


can do that efficiently um you know the
 


can do that efficiently um you know the
names


names will come later and then after the
 


names will come later and then after the
probably


 
 


 
functions


functions sure of course uh because it
 


functions sure of course uh because it
somewhat


somewhat ties into the next section
 


somewhat ties into the next section
where


where we then deal with value functions
 


where we then deal with value functions
and


and loss is sort of more or less the
 


and loss is sort of more or less the
same


same as well sure sure sure right um
 


same as well sure sure sure right um
yeah


yeah that makes a lot of sense um of
 


yeah that makes a lot of sense um of
course


course we want to get to Universal AI so
 


course we want to get to Universal AI so
how


how do we get to there so that that
 


how do we get to there so that that
brings


brings in the second section which is
 


brings in the second section which is
we'll


we'll talk about
 


we'll talk about
Universal


Universal uh agents and by agent that
 


Universal uh agents and by agent that
basically


basically means we're going to combine
 


basically means we're going to combine
prediction


prediction from the previous section
 


prediction from the previous section
with


with planning right because agents act
 


with planning right because agents act
and


and so if uh if you have a model of your
 


and so if uh if you have a model of your
environment


environment and you want to say optimize
 


environment and you want to say optimize
your


your reward then that gives you a way of
 


your reward then that gives you a way of
planning


planning right and and what we'll find
 


planning right and and what we'll find
is


is that when we kind of combine uh the
 


is that when we kind of combine uh the
ideas


ideas from uh the UN Universal
 


ideas from uh the UN Universal
prediction


prediction side with
 


prediction side with
action


action and and and try to get something
 


action and and and try to get something
that's


that's optimal we get a
 


that's optimal we get a
universal


universal optimal agent which uh has
 


universal optimal agent which uh has
this


this uh well nice acronym ay artificial
 


this uh well nice acronym ay artificial
intelligence


intelligence cross induction right um
 


intelligence cross induction right um
maybe


maybe just just to clarify bit when when
 


maybe just just to clarify bit when when
we


we talk about why we keep using the word
 


we talk about why we keep using the word
Universal


Universal Universal because we we don't
 


Universal Universal because we we don't
care


care what the um Environ or how do I say
 


care what the um Environ or how do I say
the


the the environment can be arbitrary
 


the the environment can be arbitrary
just


just like the uh generative uh
 


just like the uh generative uh
distribution


distribution for our sequence of zeros
 


distribution for our sequence of zeros
and


and ones in section one is also
 


and ones in section one is also
arbitrary


arbitrary and so we want uh an OP a
 


arbitrary and so we want uh an OP a
solution


solution that works for you know
 


solution that works for you know
arbitrary


arbitrary or general uh situations right
 


arbitrary or general uh situations right
so


so that's this that I have to first
 


so that's this that I have to first
introduce


introduce agents in known environment Ah
 


introduce agents in known environment Ah
that's


that's
 


that's
right


 
 


 
environment


environment okay okay great and then uh
 


environment okay okay great and then uh
after


after we discuss those two well we'll
 


after we discuss those two well we'll
we'll


we'll just see what what we have time
 


we'll just see what what we have time
for


for in terms of uh extra material right
 


for in terms of uh extra material right
so


so there's a lot we can cover your your
 


so there's a lot we can cover your your
books


books have lots of material but just to
 


books have lots of material but just to
list


list a few that we might depending on
 


list a few that we might depending on
how


how much time we have uh we can talk
 


how much time we have uh we can talk
about


about say computable
 


about say computable
approximations


approximations to all of this because uh
 


approximations to all of this because uh
for


for those who know uh Kor of complexity
 


for those who know uh Kor of complexity
is


is is not a computable function in the
 


is is not a computable function in the
turg


turg machine theoretic sense so of
 


turg machine theoretic sense so of
course


course um uh this is nice Theory but if
 


course um uh this is nice Theory but if
you


you don't have something that's
 


you don't have something that's
computable


computable maybe it might be limited and
 


computable maybe it might be limited and
so


so you want to come up with computable
 


so you want to come up with computable
approximations


approximations so that's something we uh
 


approximations so that's something we uh
may


may want to discuss there's also maybe
 


may want to discuss there's also maybe
multi-agent


multi-agent settings where there's more
 


multi-agent settings where there's more
than


than one actor right and um and maybe
 


than one actor right and um and maybe
def


def multi-agent we should definitely
 


def multi-agent we should definitely
cover


cover
 


cover
multi-agent


multi-agent okay I mean we can be very
 


multi-agent okay I mean we can be very
brief


brief about it so it's it's anyway I
 


brief about it so it's it's anyway I
mean


mean this is related to the grain of
 


mean this is related to the grain of
Truth


Truth problem it's it's uh too great not
 


Truth problem it's it's uh too great not
to


to mention at Le sure sure okay and then
 


to mention at Le sure sure okay and then
finally


finally of course any philosophical
 


finally of course any philosophical
discussions


discussions about AI that might arise
 


discussions about AI that might arise
from


from these both Technical and
 


from these both Technical and
philosophical


philosophical considerations that we're
 


philosophical considerations that we're
discussing


discussing here um okay so this sounds
 


discussing here um okay so this sounds
like


like a pretty good outline
 


like a pretty good outline
yeah


yeah um maybe we should add for the
 


yeah um maybe we should add for the
people


people who are here for the language
 


people who are here for the language
models


models that I will also talk a little
 


models that I will also talk a little
bit


bit about LM but don't don't expect too
 


bit about LM but don't don't expect too
much


much but one recent one recent paper
 


much but one recent one recent paper
which


which really ties it nicely together the
 


which really ties it nicely together the
Solomon


Solomon of induction and L&amp;M um
 


Solomon of induction and L&amp;M um
so


so sure okay great um all right so why
 


so sure okay great um all right so why
don't


don't we uh go to the next page and uh
 


don't we uh go to the next page and uh
why


why don't you start us off with uh
 


why don't you start us off with uh
describing


describing to us lass's rule let's start
 


describing to us lass's rule let's start
with


with something super simple uh
 


with something super simple uh
well


well maybe just start with even simpler
 


well maybe just start with even simpler
frequentist


 
 


 
frequentist


frequentist this
 


frequentist this
predictions


predictions I mean we need a notation at
 


predictions I mean we need a notation at
this


this setting anyway so we have a
 


this setting anyway so we have a
sequence


sequence X1 X2 X3 and so on and for
 


sequence X1 X2 X3 and so on and for
Simplicity


Simplicity we assume most of the time
 


Simplicity we assume most of the time
binary


binary but everything can be extended to
 


binary but everything can be extended to
finer


finer
 


finer
alphabet


alphabet um more or less
 


alphabet um more or less
straightforwardly


straightforwardly and to larg alphabets
 


straightforwardly and to larg alphabets
that's


that's usually I leave to the
 


that's usually I leave to the
mathematician


mathematician because I think it's AGI
 


mathematician because I think it's AGI
irrelevant


irrelevant um it can be mathematic was
 


irrelevant um it can be mathematic was
interesting


interesting okay so I also use
 


interesting okay so I also use
everywhere


everywhere the abbreviation X1 to n
 


everywhere the abbreviation X1 to n
which


which is just the sequence
 


which is just the sequence
X1


X1 XM okay so let's assume they drawn um
 


X1 XM okay so let's assume they drawn um
IID


IID so from a
 


IID so from a
Beni


Beni Theta process right and Theta is in
 


Beni Theta process right and Theta is in
01


01 one so that's a biased coin if Theta
 


01 one so that's a biased coin if Theta
is


is not equal half it's equal half is is
 


is not equal half it's equal half is is
a


a Fair coin and um we want to I mean
 


a Fair coin and um we want to I mean
most


most of the time in statistics you want
 


most of the time in statistics you want
to


to do induction you want to infer the
 


to do induction you want to infer the
unknown


unknown parameter right and um well how
 


unknown parameter right and um well how
do


do we do that well we count the number
 


do we do that well we count the number
of


 
 


 
ones


ones and binary I can just take the
 


ones and binary I can just take the
sum


sum um
 


sum um
um


um wait a second so assume we have seen
 


um wait a second so assume we have seen
K


 
 


 
k


k n minus K is of course the number of
 


k n minus K is of course the number of
time


time steps where um XT is equal to zero
 


time steps where um XT is equal to zero
just


just the complement of course okay and
 


just the complement of course okay and
the


the KNE frequency estimate would just
 


the KNE frequency estimate would just
estimate


estimate Theta as K divided by n and
 


estimate Theta as K divided by n and
from


from the large of large numbers we know
 


from the large of large numbers we know
that


that it converges to let's call it Theta
 


that it converges to let's call it Theta
0


0 if the sequence is exampled
 


0 if the sequence is exampled
from


from well I ID from P Theta 0 okay um
 


from well I ID from P Theta 0 okay um
though


though long so good um most of the time
 


though long so good um most of the time
I


I will be concerned directly with
 


I will be concerned directly with
prediction


prediction rather than parameter
 


prediction rather than parameter
estimation


estimation um
 


estimation um
which


which has the advantage is that
 


which has the advantage is that
sometimes


sometimes the parameter estimation can
 


sometimes the parameter estimation can
be


be harder than the prediction for
 


be harder than the prediction for
instance


instance if you cannot uniquely model
 


instance if you cannot uniquely model
class


class you cannot uniquely identify the
 


class you cannot uniquely identify the
parameter


parameter you may still be able to do
 


parameter you may still be able to do
predicting


predicting prediction well so it avoids
 


predicting prediction well so it avoids
a


a lot of complications in more
 


a lot of complications in more
complicated


complicated classes here in this simple
 


complicated classes here in this simple
boli


boli class there is no difference so the
 


boli class there is no difference so the
probability


probability um under parameter Theta um
 


probability um under parameter Theta um
that


that the next symbol is one given the
 


that the next symbol is one given the
history


history right um this is just Peta so
 


history right um this is just Peta so
the


the predictive distribution um is the
 


the predictive distribution um is the
same


same as as a parameter so so it's the
 


same as as a parameter so so it's the
same


same
 


same
okay


okay um so why is this bad okay and this
 


okay um so why is this bad okay and this
is


is where laas sort of comes in um so he
 


is where laas sort of comes in um so he
considered


considered the problem of the sun rising
 


considered the problem of the sun rising
so


so he was at a certain age I don't know
 


so he was at a certain age I don't know
how


how old you know I don't know maybe 30
 


how old you know I don't know maybe 30
years


years old and um so that means he has
 


years old and um so that means he has
seen


seen the sun maybe Rising 10,000
 


seen the sun maybe Rising 10,000
times


times H and he asked about what is the
 


times H and he asked about what is the
probability


probability that the sun will rise
 


probability that the sun will rise
tomorrow


tomorrow again I think he had another
 


tomorrow again I think he had another
estimate


estimate where he estimated that the
 


estimate where he estimated that the
world


world had been 4,000 years old according
 


world had been 4,000 years old according
to


to the Bible and um so maybe around a
 


to the Bible and um so maybe around a
million


million days yeah so you have even more
 


million days yeah so you have even more
data


data that the sun rose every day so
 


data that the sun rose every day so
what's


what's the probability of the sunrise
 


what's the probability of the sunrise
tomorrow


tomorrow okay let's look at the data
 


tomorrow okay let's look at the data
sequence


sequence the sequence X1 up to you know
 


sequence the sequence X1 up to you know
X


X say a
 


X say a
million


 
 


 
ones


ones okay so K is the same as n so we
 


ones okay so K is the same as n so we
take


take the frequency estimate k / by n
 


take the frequency estimate k / by n
which


which is one so we would predict that
 


which is one so we would predict that
the


the probability the sun will rise
 


the probability the sun will rise
tomorrow


tomorrow is identically one so we are
 


tomorrow is identically one so we are
absolutely


absolutely sure that the sun will rise
 


absolutely sure that the sun will rise
tomorrow


tomorrow and that feels of course way to
 


tomorrow and that feels of course way to
overconfident


overconfident and he asked um how can we
 


overconfident and he asked um how can we
come


come up with a prediction which is more
 


come up with a prediction which is more
reasonable


reasonable so it should be something
 


reasonable so it should be something
close


close to one uh but not exactly one and
 


close to one uh but not exactly one and
um


um so let's derive La plus rule maybe
 


um so let's derive La plus rule maybe
let's


let's I mean I present you the end
 


let's I mean I present you the end
result


result the end result first was that he
 


result the end result first was that he
says


says we should
 


says we should
predict


predict this probability by the way I
 


predict this probability by the way I
notation


notation wise one to the million means
 


notation wise one to the million means
concatenation


concatenation of of a million ones is
 


concatenation of of a million ones is
that


that is that right your
 


that is that right your
exp


exp okay okay not lital expon
 


exp okay okay not lital expon
exponentiation


exponentiation okay I yeah yeah the
 


exponentiation okay I yeah yeah the
upper


upper exponent has often multiple
 


upper exponent has often multiple
meanings


meanings yeah I see okay I I I I
 


meanings yeah I see okay I I I I
originally


originally tried to correct it plus
 


originally tried to correct it plus
because


because I thought you uh were looking at
 


because I thought you uh were looking at
numbers


numbers but I realized now it's just
 


numbers but I realized now it's just
concatenation


concatenation so let me fix the fix the
 


concatenation so let me fix the fix the
Whiteboard


Whiteboard now okay good good good um so
 


Whiteboard now okay good good good um so
and


and he said instead of um using K
 


and he said instead of um using K
divided


divided by n use k + 1 divided by n + 2
 


divided by n use k + 1 divided by n + 2
and


and um if you plug in K to be equal n
 


and um if you plug in K to be equal n
that


that would be n + 1 divided n + 2 which
 


that would be n + 1 divided n + 2 which
is


is 1 - 1 / n + 2 so if you
 


is 1 - 1 / n + 2 so if you
uh


uh assume the world the sun rose a
 


uh assume the world the sun rose a
million


million times he would say the
 


million times he would say the
probability


probability the sun rising tomorrow is a
 


probability the sun rising tomorrow is a
roughly


roughly not Rising tomorrow is roughly
 


roughly not Rising tomorrow is roughly
one


one over
 


one over
million


million and at least that's not
 


million and at least that's not
implausible


implausible right um the probability
 


implausible right um the probability
cannot


cannot be much higher right if the
 


cannot be much higher right if the
probability


probability of and we remember that was
 


probability of and we remember that was
at


at a time where people had no idea um
 


at a time where people had no idea um
about


about you know the sun being a fusion
 


about you know the sun being a fusion
reactor


reactor and exploding at some point in
 


reactor and exploding at some point in
the


the Supernova and these kinds of things
 


the Supernova and these kinds of things
so


so this I model was at least not totally
 


so this I model was at least not totally
absurd


absurd let's phrase it that way and um
 


absurd let's phrase it that way and um
it


it cannot be much larger than one a
 


it cannot be much larger than one a
million


million because if the Sun would rise
 


million because if the Sun would rise
you


you know or fail to rise you know more
 


you know or fail to rise you know more
often


often we would have seen it already um
 


often we would have seen it already um
so


so it is sort of um a lower up about
 


so it is sort of um a lower up about
depending


depending how you view it but you know
 


depending how you view it but you know
maybe


maybe it could be even closer to one and
 


maybe it could be even closer to one and
you


you could argue about it but now let me
 


you could argue about it but now let me
give


give you the derivation and that's
 


give you the derivation and that's
really


really beautiful and I think everyone
 


really beautiful and I think everyone
should


should know this I mean it is really one
 


should know this I mean it is really one
of


of the most basic applications of B
 


of the most basic applications of B
inference


inference and the most beautiful one
 


inference and the most beautiful one
though


though we don't know this parameter Teta
 


though we don't know this parameter Teta
and


and while frequentist deal with this in
 


and while frequentist deal with this in
their


their own way but the basent just says
 


their own way but the basent just says
okay


okay we have an uncertainty over this
 


okay we have an uncertainty over this
prior


prior Teta and we also model this by our
 


prior Teta and we also model this by our
probability


probability this is not a sampl
 


probability this is not a sampl
probability


probability but this is subjective
 


probability but this is subjective
belief


belief probability or epistemic
 


belief probability or epistemic
probability


probability okay usually I use the
 


probability okay usually I use the
letter


letter W but you can also use P or
 


letter W but you can also use P or
whatever


whatever you like and um um the
 


whatever you like and um um the
indifference


indifference principle
 


indifference principle
um


um um tells us if we don't know anything
 


um um tells us if we don't know anything
we


we should assume a
 


we should assume a
uniform


uniform Theta is continuous parameter in
 


uniform Theta is continuous parameter in
the


the interval 01 so a uniform prior would
 


the interval 01 so a uniform prior would
be


be just equal to one okay so let's let's
 


be just equal to one okay so let's let's
let's


let's just back up for a second because
 


let's just back up for a second because
um


um I don't know if you've already kind
 


um I don't know if you've already kind
of


of alluded it to what you said but I
 


of alluded it to what you said but I
think


think one thing that might be puzzling
 


think one thing that might be puzzling
about


about this if you haven't thought about
 


about this if you haven't thought about
it


it before is oh if um like what's wrong
 


it before is oh if um like what's wrong
with


with assigning probability equal to to
 


with assigning probability equal to to
one


one right because I've seen it a million
 


one right because I've seen it a million
times


times before rising and so uh if I were
 


times before rising and so uh if I were
just


just to obviously use the the the sample
 


just to obviously use the the the sample
estimate


estimate then of course um uh for all
 


estimate then of course um uh for all
past


past observations the sun always Rose so
 


past observations the sun always Rose so
I


I should just kind of duplicate that
 


I should just kind of duplicate that
that


that estimate right like what um just
 


that estimate right like what um just
what's


what's your what's what's wrong with
 


what's your what's what's wrong with
that


that okay first give you example more
 


that okay first give you example more
close


close to home right um I
 


close to home right um I
uh


uh uh have a coin right which could be a
 


uh uh have a coin right which could be a
Fair


Fair coin or it could be a coin which
 


Fair coin or it could be a coin which
has


has two heads okay um I flip it a 10
 


has two heads okay um I flip it a 10
times


times
 


times
right


right let's say 10 times and it always
 


right let's say 10 times and it always
comes


comes
 


comes
uphead


uphead are you 100% certain that the
 


uphead are you 100% certain that the
next


next time is
 


next time is
head


head that's right fair fair enough fair
 


head that's right fair fair enough fair
enough


enough there there's there will a small
 


enough there there's there will a small
CH


CH there's right now you could have had
 


CH there's right now you could have had
a


a coin that had two heads on it or you
 


a coin that had two heads on it or you
could


could have just gotten very lucky with a
 


could have just gotten very lucky with a
Fair


Fair coin right so so so uh exactly yeah
 


Fair coin right so so so uh exactly yeah
so


so in this case of course the
 


so in this case of course the
probability


probability is 2 to Theus 10 right so
 


probability is 2 to Theus 10 right so
the


the probability of it being the Fair
 


the probability of it being the Fair
coin


coin is very small but maybe let's put
 


coin is very small but maybe let's put
in


in the in the mix a coin which is
 


in the in the mix a coin which is
heavily


heavily biased towards head right you
 


heavily biased towards head right you
know


know has a 90% probability of head right
 


know has a 90% probability of head right
and


and now you see 10
 


and now you see 10
heads


heads well there's actually I don't know
 


heads well there's actually I don't know
the


the numbers now maybe there's a 50/50
 


the numbers now maybe there's a 50/50
chance


chance it's a 90% head coin or the or
 


chance it's a 90% head coin or the or
the


the or the full head so so you should
 


the or the full head so so you should
always


always and um you should always Reserve
 


always and um you should always Reserve
um


um especially if you allow more complex
 


um especially if you allow more complex
models


models um the probability that
 


models um the probability that
something


something so everything so this is
 


something so everything so this is
actually


actually a general principle everything
 


actually a general principle everything
which


which is not logically excluded right or
 


which is not logically excluded right or
by


by physical laws but even physical laws
 


by physical laws but even physical laws
may


may not be 100% certain right you should
 


may not be 100% certain right you should
assign


assign a nonzero probability to it right
 


assign a nonzero probability to it right
so


so to the event um that the sun will not
 


so to the event um that the sun will not
rise


rise yeah it can happen right
 


rise yeah it can happen right
why


why not right so you should assign at
 


why not right so you should assign at
least


least a nonzero probability and frequeny
 


least a nonzero probability and frequeny
would


would assign a zero
 


would assign a zero
probability


probability so they're just
 


probability so they're just
overconfident


overconfident you can you can view it
 


overconfident you can you can view it
from


from a betting perspective right the
 


from a betting perspective right the
Dutch


Dutch argument how much would you bet
 


Dutch argument how much would you bet
right


right um you know against or towards
 


right um you know against or towards
that


that the sun will rise or not of course
 


that the sun will rise or not of course
I


I would easily bet 100 to one that it
 


I would easily bet 100 to one that it
rise


rise tomorrow right th to one but I
 


rise tomorrow right th to one but I
wouldn't


wouldn't bet sort of you know a billion
 


wouldn't bet sort of you know a billion
to


to one or whatever right maybe um
 


to one or whatever right maybe um
because


because I mean with Sunrise you maybe
 


because I mean with Sunrise you maybe
then


then we all dead right so we need we
 


then we all dead right so we need we
need


need
 


need
maybe


maybe anyway never assigned probability
 


maybe anyway never assigned probability
zero


zero except to logically inconsistent
 


zero except to logically inconsistent
events


events sure I
 


events sure I
mean


mean okay great yeah and I think that's
 


mean okay great yeah and I think that's
now


now a good segue into uh talking about
 


now a good segue into uh talking about
this


this uh um PRI uh assigning a prior
 


this uh um PRI uh assigning a prior
because


because that's
 


because that's
exactly


exactly therefore assigning uh weight to
 


exactly therefore assigning uh weight to
all


all candidate hypotheses that aren't
 


all candidate hypotheses that aren't
ruled


ruled out by the data exactly so if we
 


ruled out by the data exactly so if we
had


had just two is Right Fair coin or
 


had just two is Right Fair coin or
always


always head it would be just and at
 


always head it would be just and at
prior


prior we have no idea we would say with
 


prior we have no idea we would say with
50%


50% it's always head coin and with 50%
 


50% it's always head coin and with 50%
it's


it's a fair coin and then deal with it
 


it's a fair coin and then deal with it
but


but let's assume here um you know we
 


but let's assume here um you know we
have


have no idea about this buyers of the
 


have no idea about this buyers of the
coin


coin so take a uniform PR okay okay sure
 


coin so take a uniform PR okay okay sure
sure


sure
 


sure
so


so now before we continue Let Me Maybe
 


so now before we continue Let Me Maybe
present


present you paas rule in the general
 


present you paas rule in the general
form


form you can present it in many forms uh
 


form you can present it in many forms uh
the


the most useful form is the following
 


the most useful form is the following
let's


let's assume you have a class of
 


let's assume you have a class of
hypothesis


hypothesis hi
 


hypothesis hi
IAL


IAL one 2K or so um what does it mean so
 


IAL one 2K or so um what does it mean so
these


these are Point hypothesis and they're
 


these are Point hypothesis and they're
mutually


mutually exclusive so here what we mean
 


mutually exclusive so here what we mean
is


is that the probability of your data X1
 


is that the probability of your data X1
to


to
 


to
n


n under some hypothesis I has a certain
 


n under some hypothesis I has a certain
probability


probability distribution so for instance
 


probability distribution so for instance
H


H zero could be fair coin right then
 


H zero could be fair coin right then
this


this would be just one half to the n
 


this would be just one half to the n
uh


uh age one could be all heads then for
 


uh age one could be all heads then for
the


the sequence all ones it would just be
 


the sequence all ones it would just be
one


one right so um so so this is just um
 


one right so um so so this is just um
the


the hypothesis is just a probability
 


the hypothesis is just a probability
distribution


distribution we only care about
 


distribution we only care about
sequences


sequences here over sequences here okay
 


sequences here over sequences here okay
um


um then we have a
 


um then we have a
prior


prior or maybe at this stage I use the P
 


prior or maybe at this stage I use the P
so


so that is more conventional over the
 


so that is more conventional over the
hypothesis


hypothesis um so if we had sort of Fair
 


hypothesis um so if we had sort of Fair
coin


coin where head coin right would be half
 


coin where head coin right would be half
half


half or something like this and I ask
 


half or something like this and I ask
what


what is my posterior that's my prior
 


what is my posterior that's my prior
belief


belief what is my posterior belief in
 


belief what is my posterior belief in
this


this hypothesis after I have seen data
 


this hypothesis after I have seen data
okay


okay and base rule says you should take
 


okay and base rule says you should take
your


your
 


your
data


data uh sorry the the likelihood
 


data uh sorry the the likelihood
function


function and you multiply it with a
 


function and you multiply it with a
prior


prior and then you need to normalize it
 


prior and then you need to normalize it
so


so that it's a prop probability
 


so that it's a prop probability
distribution


distribution and this normalizer um
 


distribution and this normalizer um
let's


let's call that P
 


let's call that P
of


 
 


 
n


n is just the marginal
 


n is just the marginal
distribution


distribution over all this hypothesis so
 


distribution over all this hypothesis so
that


that is called the pent evidence or the
 


that is called the pent evidence or the
marginal


marginal
 


marginal
distribution


distribution has many names um uh
 


distribution has many names um uh
partition


partition function um okay and I mean if
 


partition function um okay and I mean if
if


if you flip it around um sorry just
 


if you flip it around um sorry just
inter


inter sorry on the left hand side I
 


inter sorry on the left hand side I
missed


missed something a posterior here of
 


missed something a posterior here of
course


course yes that's right I was say that
 


course yes that's right I was say that
yeah


yeah X1 to n so if you flip this if you
 


yeah X1 to n so if you flip this if you
flip


flip the denominator to the left hand
 


flip the denominator to the left hand
side


side you see it's just P of H given X is
 


side you see it's just P of H given X is
p


p of time P of X is p of x given H * P
 


p of time P of X is p of x given H * P
of


of H which is just the definition of
 


of H which is just the definition of
conditional


conditional probability so there's near
 


conditional probability so there's near
really


really not much to it okay but so that's
 


really not much to it okay but so that's
the


the great thing if you're basian that's
 


the great thing if you're basian that's
essentially


essentially the only thing you need to
 


essentially the only thing you need to
know


know and you you just always you know
 


know and you you just always you know
you


you think about your prior you come up
 


you think about your prior you come up
with


with a model class and then you just you
 


with a model class and then you just you
know


know crank based rule right so maybe
 


know crank based rule right so maybe
just


just to say this in a few words right
 


just to say this in a few words right
you


you you start with some um well your
 


you you start with some um well your
prior


prior is your set of initial credences
 


prior is your set of initial credences
as


as to uh How likely the individual
 


as to uh How likely the individual
hypotheses


hypotheses are in this case there's a
 


hypotheses are in this case there's a
hypothesis


hypothesis for each value of the
 


hypothesis for each value of the
probability


probability that the coin uh produces a
 


probability that the coin uh produces a
one


one right so so each hi is is is the
 


one right so so each hi is is is the
Theta


Theta um and then the likelihood is just
 


Theta um and then the likelihood is just
is


is just a computation How likely the
 


is just a computation How likely the
data


data is given that value of theta and
 


data is given that value of theta and
the


the posterior is saying well what is my
 


the posterior is saying well what is my
uh


uh new belief in the value of the
 


uh new belief in the value of the
theta's


theta's condition on the observation I
 


theta's condition on the observation I
just


just made and that basically by the
 


just made and that basically by the
rules


rules of probability Theory this is what
 


rules of probability Theory this is what
Bas


Bas uh rule tells you uh how you should
 


Bas uh rule tells you uh how you should
update


update your your your
 


update your your your
probabilities


probabilities exactly yeah so now let's
 


probabilities exactly yeah so now let's
instantiate


instantiate it so this is completely
 


instantiate it so this is completely
General


General um whatever your model and
 


General um whatever your model and
there's


there's no I idea sumptions or something
 


there's no I idea sumptions or something
um


um or stationarity or Markos or
 


um or stationarity or Markos or
something


something so that is completely General
 


something so that is completely General
but


but let's go back to the uh to the base
 


but let's go back to the uh to the base
uh


uh to the to the laas example so here um
 


uh to the to the laas example so here um
well


well the class we have not a class of
 


well the class we have not a class of
finally


finally many hypothesis but continum so
 


finally many hypothesis but continum so
the


the only thing that changes is that the
 


the only thing that changes is that the
sum


sum is replaced by an integral okay um
 


sum is replaced by an integral okay um
and


and the
 


and the
hypothesis


hypothesis class is now you know H Theta
 


hypothesis class is now you know H Theta
or


 
 


 
01


01 okay um and um then my likelihood
 


01 okay um and um then my likelihood
function


function on to n under H Teta or just
 


function on to n under H Teta or just
abbreviate


abbreviate T is okay so let's assume
 


abbreviate T is okay so let's assume
there


there are K ones as before and N minus K
 


there are K ones as before and N minus K
Zer


Zer okay so it's an IID model so this is
 


Zer okay so it's an IID model so this is
just


 
 


 
XT


XT T = 1 to n okay what is the
 


XT T = 1 to n okay what is the
probability


probability um over one well the
 


probability um over one well the
probability


probability of one is
 


probability of one is
Theta


Theta and how many ones are in this
 


Theta and how many ones are in this
sequence


sequence okay k k and the probability of
 


sequence okay k k and the probability of
zero


zero is 1 minus Theta and there n minus
 


zero is 1 minus Theta and there n minus
K


K zeros in the sequence so that is my my
 


K zeros in the sequence so that is my my
model


model of a buyas coin okay and now uh
 


model of a buyas coin okay and now uh
the


the prior as
 


the prior as
before


before well it's up here up here
 


before well it's up here up here
okay


okay so now we compute the posterior
 


okay so now we compute the posterior
before


before we compute the posterior we have
 


before we compute the posterior we have
to


to compute the denominator which is
 


to compute the denominator which is
always


always the hardest part sometimes it's
 


always the hardest part sometimes it's
impossible


impossible and you know they're all all
 


impossible and you know they're all all
kinds


kinds of very sophisticated tricks to
 


kinds of very sophisticated tricks to
still


still compute the proc without the
 


still compute the proc without the
normalizing


normalizing constant um and here it is
 


normalizing constant um and here it is
very


very easy to do roughly speaking it's
 


very easy to do roughly speaking it's
also


also a rule of Thum if you can compute
 


also a rule of Thum if you can compute
the


the denominator you can compute all
 


the denominator you can compute all
other


other quantities of Interest usually EAS
 


other quantities of Interest usually EAS
a


a rule of thumb and we will see we can
 


a rule of thumb and we will see we can
actually


actually avoid even looking at the
 


actually avoid even looking at the
denominator


denominator um so um so now let's
 


denominator um so um so now let's
compute


compute the marginal distribution so the
 


compute the marginal distribution so the
denominator


denominator the is according to this
 


denominator the is according to this
formula


formula
 


formula
integral


integral
 


integral
over


over the likelihood times the prior so
 


over the likelihood times the prior so
that


that is just in this different so that
 


that is just in this different so that
is


is integral 0 to one Theta to the K 1
 


is integral 0 to one Theta to the K 1
minus


minus Theta to the N minus k d Theta so
 


minus Theta to the N minus k d Theta so
just


just one this is the likelihood right
 


just one this is the likelihood right
the


the um in the in the
 


the um in the in the
integran


integran yeah the the integrant that is
 


integran yeah the the integrant that is
a


a like times yes yeah yeah so I just
 


a like times yes yeah yeah so I just
wanted


wanted to reflect that in the notation
 


wanted to reflect that in the notation
just


just to be pantic here uh yeah that is
 


just to be pantic here uh yeah that is
the


the same as P of x given H Theta yeah so
 


the same as P of x given H Theta yeah so
this


this should be a conditional on Theta I
 


this should be a conditional on Theta I
guess


guess yeah yeah I mean I have to put the
 


guess yeah yeah I mean I have to put the
so


so you can compute the integral and this
 


so you can compute the integral and this
so


so I I and this is the beta function
 


so I I and this is the beta function
which


which is K of k + one and N minus k +
 


which is K of k + one and N minus k +
one


one and the beta function is just
 


one and the beta function is just
product


product and ratio of three gamma
 


product and ratio of three gamma
functions


functions but let's just skip over this
 


functions but let's just skip over this
yeah


yeah so so form essentially okay and the
 


yeah so so form essentially okay and the
gamma


gamma function for integer values is of
 


gamma function for integer values is of
course


course you know factorials okay so now
 


course you know factorials okay so now
we


we can compute the posterior P of theta
 


we can compute the posterior P of theta
given


given
 


given
x


x 1 to n right which is just uh I mean I
 


x 1 to n right which is just uh I mean I
can


can write it well okay let me write it
 


can write it well okay let me write it
out


out um so that is a likelihood function
 


out um so that is a likelihood function
Theta


Theta the n 1 minus Theta to the N minus
 


Theta the n 1 minus Theta to the N minus
K


K times the prior which is one divided
 


K times the prior which is one divided
by


by this beta function which is a
 


by this beta function which is a
constant


constant
 


constant
okay


okay uh by the way I will later um
 


okay uh by the way I will later um
sometimes


sometimes just use this notation with a
 


sometimes just use this notation with a
cross


cross over the equal which means apart
 


cross over the equal which means apart
from


from a constant okay okay so the
 


from a constant okay okay so the
posterior


posterior um is also um this expression
 


posterior um is also um this expression
and


and if you look
 


and if you look
now


now um so if you here parameter Theta
 


now um so if you here parameter Theta
which


which is between zero and one and you
 


which is between zero and one and you
ask


 
 


 
so


so before we have seen any data so for n
 


so before we have seen any data so for n
equal


equal to zero well this is just the
 


equal to zero well this is just the
prior


prior and we are not sure what the Theta
 


prior and we are not sure what the Theta
is


is so it's just this uniform
 


is so it's just this uniform
distribution


distribution but now once we have seen a
 


distribution but now once we have seen a
lot


lot of data um and assume it was a Fair
 


lot of data um and assume it was a Fair
coin


coin then K is about n half and then if
 


coin then K is about n half and then if
you


you look at this expression it would
 


you look at this expression it would
look


look like this a little bit like a
 


look like this a little bit like a
gaussian


gaussian um so the probability
 


gaussian um so the probability
concentrates


concentrates around
 


concentrates around
well


well one half is the Fair coin and
 


well one half is the Fair coin and
around


around theta0 you can easily show that
 


around theta0 you can easily show that
if


if it's a coin with Buyers Theta zero
 


if it's a coin with Buyers Theta zero
and


and the width is around of the order of
 


and the width is around of the order of
one/


one/ square root n so um and then um you
 


one/ square root n so um and then um you
know


know this converges to the point
 


know this converges to the point
estimate


estimate as we all expect okay um but
 


estimate as we all expect okay um but
okay


okay this gives you the posterior
 


okay this gives you the posterior
posterior


posterior butas asked about the
 


posterior butas asked about the
predictive


predictive distribution okay so he asked
 


predictive distribution okay so he asked
about


about what is the probability of x the
 


about what is the probability of x the
next


next symbol given X1 to n okay and there
 


next symbol given X1 to n okay and there
are


are two ways of computing this so the
 


are two ways of computing this so the
the


 
 


 
posterior


posterior um
 


posterior um
and


and
 


and
um


um then we okay I should start it
 


um then we okay I should start it
differently


differently so I ask what is the
 


differently so I ask what is the
probability


probability of xn + one
 


probability of xn + one
given


given Teta so if I know Teta what is the
 


given Teta so if I know Teta what is the
probability


probability of next bit being one okay
 


probability of next bit being one okay
being


being one is one and being zero is one
 


being one is one and being zero is one
Theta


Theta okay but I do not know Theta but I
 


Theta okay but I do not know Theta but I
know


know I have a belief over Theta so I
 


know I have a belief over Theta so I
multiply


multiply this with my posterior belief
 


multiply this with my posterior belief
over


over Teta and integrate over this okay
 


over Teta and integrate over this okay
this


this gives me the conditional
 


this gives me the conditional
distribution


distribution and it is very easy to see
 


distribution and it is very easy to see
that


 
 


 
simply


simply P of X1 to up to n + 1 / P of x 1
 


simply P of X1 to up to n + 1 / P of x 1
to


to n so I could not do this you know
 


to n so I could not do this you know
look


look at the posterior plug it in um you
 


look at the posterior plug it in um you
know


know this is just the binomial multiply
 


know this is just the binomial multiply
with


with another Teta integrate it out blah
 


with another Teta integrate it out blah
blah


blah blah but um it is much easier to
 


blah blah but um it is much easier to
use


use this expression because I mean if
 


use this expression because I mean if
you


you look at the conditional distribution
 


you look at the conditional distribution
P


P of x given you know history well by
 


P of x given you know history well by
definition


definition it is just P of X1 up to n
 


definition it is just P of X1 up to n
plus1


plus1 divided by P of x n + one okay and
 


plus1 divided by P of x n + one okay and
the


the denominator we have just computed
 


the denominator we have just computed
above


above right this is this beta function
 


above right this is this beta function
beta


beta of k + 1 and N minus k + 1 and what
 


beta of k + 1 and N minus k + 1 and what
is


is a numerator okay let's ass let's ask
 


is a numerator okay let's ass let's ask
what


what is the probability of the next bit
 


what is the probability of the next bit
being


being one okay so we now look at the
 


being one okay so we now look at the
sequence


sequence which is X up to xn and then we
 


sequence which is X up to xn and then we
have


have a one so the next bit is one so K
 


have a one so the next bit is one so K
which


which counts the number of ones is one
 


which counts the number of ones is one
more


more
 


more
more


more okay so the numerator is exactly
 


more okay so the numerator is exactly
the


the same formula but K has increased by
 


the same formula but K has increased by
one


one so it's k + 2 N minus k + one okay
 


one so it's k + 2 N minus k + one okay
if


if you now plug in you know the beta is
 


if you now plug in you know the beta is
a


a three gamas and the gamas are
 


a three gamas and the gamas are
factorials


factorials and everything cancels out
 


factorials and everything cancels out
and


and as an end result you get k + 1 ided
 


and as an end result you get k + 1 ided
n


n + 2 and that is laas Rule which is
 


n + 2 and that is laas Rule which is
very


very beautiful okay so let me um uh add
 


very beautiful okay so let me um uh add
one


one
 


one
tiny


tiny thing so plus assumed a uniform
 


tiny thing so plus assumed a uniform
prior


prior yeah and I don't want to go into
 


prior yeah and I don't want to go into
details


details um there is reasons for choosing
 


details um there is reasons for choosing
a


a prior which is slightly more uh heavy
 


a prior which is slightly more uh heavy
on


on the Tails um that would be um a beta
 


on the Tails um that would be um a beta
one


one half one half prior if if if
 


one half one half prior if if if
somebody


somebody cares um I can write it down
 


somebody cares um I can write it down
but


but it's not really important um so and
 


but it's not really important um so and
this


this is called then the KT estimator
 


this is called then the KT estimator
maybe


 
 


 
meter


meter and if you use this prior then
 


meter and if you use this prior then
instead


instead of k + 1 ided by K +2 you get k
 


instead of k + 1 ided by K +2 you get k
+


+ 12 divided by k + 2 k+ 1 very similar
 


+ 12 divided by k + 2 k+ 1 very similar
and


and sort of that is the modern version n
 


and sort of that is the modern version n
plus


plus n plus one yeah n plus one in the
 


plus n plus one yeah n plus one in the
denominator


denominator
 


denominator
yeah


yeah n plus one yeah which gives a very
 


yeah n plus one yeah which gives a very
similar


 
 


 
result


result this something I actually don't
 


result this something I actually don't
know


know why why would you uh have I mean I
 


know why why would you uh have I mean I
I


I guess there's some optimality with
 


I guess there's some optimality with
this


this uh prior so this prior is going to
 


this uh prior so this prior is going to
look


look something more like as you said
 


look something more like as you said
it's


it's GNA be yeah so there there at least
 


it's GNA be yeah so there there at least
two


two different Arguments for it and yes
 


two different Arguments for it and yes
it


it will look like this so it's it's
 


it will look like this so it's it's
proportional


proportional to Theta to Theus 12 1us
 


proportional to Theta to Theus 12 1us
Theta


Theta to Theus one2 so it actually goes
 


Theta to Theus one2 so it actually goes
to


to Infinity yeah yeah okay in and um so
 


to Infinity yeah yeah okay in and um so
there


there is two arguments one argument is
 


there is two arguments one argument is
is


is though the uniformity principle says
 


is though the uniformity principle says
if


if you don't know anything to my uni PR
 


if you don't know anything to my uni PR
but


but should I assign this uniform prior
 


but should I assign this uniform prior
over


over the parameter Teta or I can
 


over the parameter Teta or I can
reparameterize


reparameterize my problem assume um I
 


reparameterize my problem assume um I
instead


instead of using the parameter Theta I
 


instead of using the parameter Theta I
use


use the parameter SI which is Theta
 


use the parameter SI which is Theta
Square


Square which is also which is also in Z1
 


Square which is also which is also in Z1
right


right and say I just parameterize it
 


right and say I just parameterize it
this


this way ah I should use a uniform
 


this way ah I should use a uniform
side


side but then it's a non-uniform because
 


side but then it's a non-uniform because
I


I mean of the of the formula how to
 


I mean of the of the formula how to
transform


transform densities so this uniformity
 


transform densities so this uniformity
principle


principle if you apply it nely is not
 


principle if you apply it nely is not
so-called


so-called reparameterization invariant
 


so-called reparameterization invariant
which


which is bad right and uh the uh the
 


which is bad right and uh the uh the
jeffre


jeffre
 


jeffre
prior


prior okay if you want to go there uses
 


prior okay if you want to go there uses
the


the follow me uses the square root of
 


the follow me uses the square root of
the


the determinant of the fissure
 


the determinant of the fissure
information


information Matrix but let's move on
 


information Matrix but let's move on
right


right whatever that is right if you use
 


right whatever that is right if you use
this


this as a principle for assigning your
 


this as a principle for assigning your
prior


prior which is a little bit weird at the
 


prior which is a little bit weird at the
beginning


beginning but it's actually has
 


beginning but it's actually has
information


information geometrically has a lot of
 


information geometrically has a lot of
meaning


meaning um then if you do a
 


meaning um then if you do a
reparameterization


reparameterization you still get the
 


reparameterization you still get the
same


same PRI I mean so if you start with
 


same PRI I mean so if you start with
p


p and compute the determinant of the uh
 


p and compute the determinant of the uh
of


of the P information moup s then you get
 


of the P information moup s then you get
the


the appropriately transformed prior the
 


the appropriately transformed prior the
other


other justification is if you look at uh
 


other justification is if you look at uh
redundancy


redundancy bounds um or regret bounds
 


redundancy bounds um or regret bounds
and


and you want them to be sort of Mini Max
 


and you want them to be sort of Mini Max
optimal


optimal then this KT
 


optimal then this KT
prior


prior comes out of this okay I like the
 


prior comes out of this okay I like the
first


first I like the first uh uh reason
 


first I like the first uh uh reason
because


because you don't need to prove a a a
 


because you don't need to prove a a a
theorem


theorem or know something sophisticated
 


theorem or know something sophisticated
it's


it's just it's an a prior sensible thing
 


it's just it's an a prior sensible thing
to


to have
 


to have
reparameterization


reparameterization in variance
 


reparameterization in variance
yes


yes so um people want to look it up and
 


yes so um people want to look it up and
out


out prior sure
 


out prior sure
okay


okay they are the same but with the
 


okay they are the same but with the
different


different
 


different
justifications


justifications yeah so next maybe we go
 


justifications yeah so next maybe we go
to


to the general case of sequence
 


to the general case of sequence
prediction


prediction right we essentially already
 


prediction right we essentially already
have


have everything yeah there's not really
 


have everything yeah there's not really
much


much extra to do so now we start with a
 


much extra to do so now we start with a
model


model Class
 


model Class
M


M which contains distributions right
 


M which contains distributions right
before


before it was you know age AG index new
 


before it was you know age AG index new
or


or something like this so new of maybe
 


or something like this so new of maybe
notation


notation becomes a little bit more
 


notation becomes a little bit more
complex


complex right so X1 to n is some
 


complex right so X1 to n is some
probability


probability distribution over sequences
 


probability distribution over sequences
okay


okay and I have a class of these
 


okay and I have a class of these
distributions


distributions and from now on with some
 


distributions and from now on with some
exceptions


exceptions I will assume that this class
 


exceptions I will assume that this class
um


um is I have to write it
 


um is I have to write it
differently


differently um is countable m is
 


differently um is countable m is
countable


countable most of the time so that I
 


countable most of the time so that I
don't


don't have to deal with um with
 


don't have to deal with um with
integrals


integrals and um I take a
 


integrals and um I take a
prior


prior over this class and then I call
 


prior over this class and then I call
compute


compute the Bas mixer distribution
 


compute the Bas mixer distribution
evidence


evidence or partition function or
 


evidence or partition function or
whatever


whatever you want to call it because I
 


whatever you want to call it because I
want


 
 


 
sum


sum
 


sum
likelihood


likelihood times
 


likelihood times
prior


 
 


 
in


in okay now also for con notational
 


in okay now also for con notational
convenience


convenience I predict XT
 


convenience I predict XT
given


given up to T minus one so another X
 


given up to T minus one so another X
less


less T is X1 up to
 


less T is X1 up to
X


X okay so let's just back up a little
 


X okay so let's just back up a little
bit


bit okay so a lot of notation was right
 


bit okay so a lot of notation was right
here


here so so first of all m is our
 


here so so first of all m is our
hypothesis


hypothesis space essentially um in the
 


hypothesis space essentially um in the
previous


previous example it was actually a a not
 


previous example it was actually a a not
accountable


accountable space it was the unit
 


accountable space it was the unit
interval


interval because we had a hypothesis for
 


interval because we had a hypothesis for
every


every uh you know coin flip uh
 


every uh you know coin flip uh
probability


probability of equal to one uh now we're
 


probability of equal to one uh now we're
going


going to simplify accountable because
 


going to simplify accountable because
ultimately


ultimately we want to get to a regime
 


ultimately we want to get to a regime
where


where we want to uh say implement this
 


where we want to uh say implement this
on


on a computer and of course the set of
 


on a computer and of course the set of
programs


programs is is countable so that's
 


programs is is countable so that's
that's


that's kind of the Restriction looking
 


that's kind of the Restriction looking
ahead


ahead why we want to consider things
 


ahead why we want to consider things
that


that are countable correct exactly I
 


that are countable correct exactly I
mean


mean if if if it's uncountable then you
 


mean if if if it's uncountable then you
just


just replace as the sum by an integral
 


just replace as the sum by an integral
as


as before and the prior is a prior
 


as before and the prior is a prior
density


density um but you know I don't want to
 


density um but you know I don't want to
constantly


constantly deal with the two cases
 


constantly deal with the two cases
account


account sure sure and and of course and
 


account sure sure and and of course and
and


and uh I think maybe just one step for
 


and uh I think maybe just one step for
clarity


clarity before we start uh
 


clarity before we start uh
uh


uh um start evaluating these these uh
 


uh um start evaluating these these uh
quantities


quantities on actual sequences really
 


quantities on actual sequences really
your


your prior is is without writing um
 


your prior is is without writing um
again


again it's evaluation inputs you're
 


again it's evaluation inputs you're
you're


you're saying my prior what you as see
 


you're saying my prior what you as see
here


here is say some sign some weighted
 


here is say some sign some weighted
sum


sum uh I think in your book you use
 


sum uh I think in your book you use
maybe


maybe a
 


maybe a
subscript


subscript yeah you always use the word
 


subscript yeah you always use the word
prior


prior solomonov used the word a priori
 


prior solomonov used the word a priori
distribution


distribution um so um but let's not even
 


distribution um so um but let's not even
call


call it a prior I mean if you want call
 


call it a prior I mean if you want call
it


it a priori distrib the prior is W of
 


it a priori distrib the prior is W of
new


new that's the prior and the Cai is call
 


new that's the prior and the Cai is call
it


it ban mixture call it Solomon of
 


it ban mixture call it Solomon of
distribution


distribution later call it Universal
 


distribution later call it Universal
distribution


distribution call it Bas mixture call it
 


distribution call it Bas mixture call it
basan


basan evidence call it partition
 


basan evidence call it partition
function


function don't called prior so it just
 


function don't called prior so it just
to


to make our terminology consistent so w
 


to make our terminology consistent so w
is


is the prior and C is this mixture over
 


is the prior and C is this mixture over
the


the measures in M with respect to that
 


the measures in M with respect to that
prior


prior yes okay great okay I'm happy with
 


prior yes okay great okay I'm happy with
that


that terminology okay um okay let's
 


that terminology okay um okay let's
let's


let's well okay and maybe one more
 


let's well okay and maybe one more
comment


comment so you start with this mixture
 


comment so you start with this mixture
and


and the formula you just wrote here well
 


and the formula you just wrote here well
it's


it's just the evaluation of that mixture
 


it's just the evaluation of that mixture
on


on a sequence of length n but what you
 


on a sequence of length n but what you
were


were about to write is how you're going
 


were about to write is how you're going
to


to update mixture using Baye rule
 


to update mixture using Baye rule
essentially


essentially
 


essentially
uh


uh sure but yeah actually I mean you
 


uh sure but yeah actually I mean you
would


would update the prior to a posterior
 


would update the prior to a posterior
right


right but as I said I want to skip the
 


right but as I said I want to skip the
posterior


posterior and directly consider the
 


posterior and directly consider the
predictive


predictive distribution yeah I see what
 


predictive distribution yeah I see what
you're


you're saying yeah and there is no real
 


you're saying yeah and there is no real
update


update so of Base does all this under
 


update so of Base does all this under
the


the hood right yeah so it is just you
 


the hood right yeah so it is just you
have


have you mix the distribution and you
 


have you mix the distribution and you
take


take the ITI of of so again let me write
 


take the ITI of of so again let me write
it


it down of X1 up to T ID by of X less
 


it down of X1 up to T ID by of X less
than


than T and you could write it as an
 


than T and you could write it as an
integral


integral over you know posteriors and
 


integral over you know posteriors and
like


like blah blah blah and so on but you
 


like blah blah blah and so on but you
know


know this is all gone now um I from now
 


know this is all gone now um I from now
on


on I only consider predictive
 


on I only consider predictive
distributions


distributions maybe at some point the
 


distributions maybe at some point the
posterior


posterior will pop up again but mostly
 


posterior will pop up again but mostly
I'm


I'm interested in prediction and not
 


I'm interested in prediction and not
induction


induction yes I see basically yeah I
 


induction yes I see basically yeah I
mean


mean this kind of came up also in our
 


mean this kind of came up also in our
llas


llas example you could keep track of the
 


llas example you could keep track of the
intermediate


intermediate quantities in terms of the
 


intermediate quantities in terms of the
prior


prior and the iterated posteriors uh but
 


prior and the iterated posteriors uh but
that


that all gets absorbed into uh under the
 


that all gets absorbed into uh under the
hood


hood as you said when at the end of the
 


hood as you said when at the end of the
day


day if all you care about are these
 


day if all you care about are these
predictive


predictive distributions that that
 


predictive distributions that that
that's


that's implicit in everything but you
 


that's implicit in everything but you
never


never need to explicitly kind of keep
 


never need to explicitly kind of keep
track


track of it but it's there if you wanted
 


track of it but it's there if you wanted
to


to recover it exactly yes yeah okay
 


to recover it exactly yes yeah okay
great


great okay good so now we have this
 


great okay good so now we have this
predicted


predicted distribution and so before in
 


predicted distribution and so before in
our


our very simple model class we showed um
 


our very simple model class we showed um
that


that this converges I mean or this
 


that this converges I mean or this
predictive


predictive distribution was laass Rule
 


predictive distribution was laass Rule
k+


k+ 1id n plus2 which converges of course
 


k+ 1id n plus2 which converges of course
light


light frequen is asmol to the true
 


light frequen is asmol to the true
parameter


parameter theta0 okay now we have made
 


parameter theta0 okay now we have made
no


no assumptions about these news about
 


no assumptions about these news about
this


this probability distribution so one
 


this probability distribution so one
could


could be for instance a distribution
 


could be for instance a distribution
which


which um is one on the digit of pi right
 


which um is one on the digit of pi right
and


and zero on all others so it's a
 


and zero on all others so it's a
deterministic


deterministic uh environment which
 


deterministic uh environment which
produces


produces a digits of pi right and
 


produces a digits of pi right and
there's


there's no or it could be Z one two Z
 


there's no or it could be Z one two Z
one


one four zeros one or something so
 


one four zeros one or something so
anything


anything you want um so there is no no
 


anything you want um so there is no no
notion


notion of convergence to a parameter
 


notion of convergence to a parameter
anymore


anymore um there is a sort of parameter
 


anymore um there is a sort of parameter
underlying


underlying the new we could talk about
 


underlying the new we could talk about
it


it but as I said I don't want to but we
 


it but as I said I don't want to but we
consider


consider the predictive distribution so
 


consider the predictive distribution so
we


we can still ask questions like assume
 


we can still ask questions like assume
we


we know assume the data X1 to Infinity
 


we know assume the data X1 to Infinity
is


is sample from them distribution mu and
 


is sample from them distribution mu and
the


the true distribution I call always mu
 


the true distribution I call always mu
before


before it was Theta zero Theta Zer is
 


before it was Theta zero Theta Zer is
usually


usually the true parameter and here mu
 


usually the true parameter and here mu
is


is always the true thing and let's
 


is always the true thing and let's
assume


assume it's in the model Class M okay so
 


assume it's in the model Class M okay so
if


if we know mu so X sample from mu we
 


if we know mu so X sample from mu we
cannot


cannot if it's sastic cannot deter bits
 


cannot if it's sastic cannot deter bits
like


like in in a coin flip right but we
 


like in in a coin flip right but we
could


could ask what is the true probability
 


could ask what is the true probability
of


of the next bit given the history okay
 


of the next bit given the history okay
in


in the B example that would be Teta zero
 


in the B example that would be Teta zero
okay


okay but we don't know mu so let's use
 


okay but we don't know mu so let's use
the


the basian predictive
 


the basian predictive
distribution


distribution for prediction let's use
 


distribution for prediction let's use
this


this one instead because this we can in
 


this one instead because this we can in
principle


principle compute okay and now we can
 


principle compute okay and now we can
ask


ask how close is this to the truth
 


ask how close is this to the truth
okay


okay so let's take a square
 


okay so let's take a square
and


and so it would be nice you know if it
 


and so it would be nice you know if it
somehow


somehow could converge uh could uh prove
 


somehow could converge uh could uh prove
that


that it converges to zero um well of
 


that it converges to zero um well of
course


course it will not converge to zero on
 


course it will not converge to zero on
all


all sequences only on sequences are
 


all sequences only on sequences are
sampled


sampled so maybe almost surely maybe
 


sampled so maybe almost surely maybe
let's


let's just say a few words because if
 


let's just say a few words because if
you


you haven't seen this before it's it's U
 


you haven't seen this before it's it's U
maybe


maybe a little bit uh abstract so um so
 


maybe a little bit uh abstract so um so
mu


mu is this uh true probability
 


mu is this uh true probability
distribution


distribution of of the sequences You
 


distribution of of the sequences You
observe


observe you don't know what it it is
 


observe you don't know what it it is
what


what you do have is this mixture where
 


what you do have is this mixture where
you


you you know decided what the weights
 


you you know decided what the weights
were


were in advance um yeah is there a how
 


were in advance um yeah is there a how
do


do I say um Playing devil's advocate
 


do I say um Playing devil's advocate
here


here is there a is there a way to why
 


here is there a is there a way to why
should


should I expect the C to converge to Mu
 


should I expect the C to converge to Mu
well


well okay I can give you some intuition
 


well okay I can give you some intuition
I


I could even present the proof but uh I
 


I could even present the proof but uh I
mean


mean I want to move on um or maybe at
 


mean I want to move on um or maybe at
least


least some steps of the proof um so
 


least some steps of the proof um so
first


first let me write down the result you
 


first let me write down the result you
can


can prove okay so I mean okay you can
 


can prove okay so I mean okay you can
prove


prove this result um and I'll come back
 


prove this result um and I'll come back
to


to in a second but you can prove
 


to in a second but you can prove
something


something much stronger and the way
 


something much stronger and the way
actually


actually you prove this convergence
 


actually you prove this convergence
result


result is VI the stronger result that's
 


result is VI the stronger result that's
the


the reason why I want to present the
 


the reason why I want to present the
stronger


stronger result okay so you cannot in
 


stronger result okay so you cannot in
the


the IID case we could prove a rate right
 


the IID case we could prove a rate right
so


so the side deviates from Theta zero and
 


so the side deviates from Theta zero and
the


the rate is sort of one over one square
 


the rate is sort of one over one square
n


n or whatever right but in the general
 


n or whatever right but in the general
case


case you cannot prove a rate in this
 


case you cannot prove a rate in this
sense


sense anymore but what we can do is okay
 


sense anymore but what we can do is okay
first


first we take the difference
 


first we take the difference
squared


squared then we sum overall time
 


squared then we sum overall time
instances


instances up to Infinity so we take all
 


instances up to Infinity so we take all
the


the deviations and sum of it okay and
 


the deviations and sum of it okay and
then


then we can
 


then we can
show


show um I fix it in a second that this
 


show um I fix it in a second that this
is


is finite okay well it's not finite for
 


is finite okay well it's not finite for
every


every sequence but it's you know almost
 


every sequence but it's you know almost
purely


purely or in expectation so we take the
 


purely or in expectation so we take the
expectation


expectation over this with respect to of
 


expectation over this with respect to of
course


course the true
 


course the true
distribution


distribution okay so now what we have
 


distribution okay so now what we have
here


here is we have an infinite sum of non-
 


here is we have an infinite sum of non-
negative


negative quantities which is
 


negative quantities which is
finite


finite and the only way that this is
 


finite and the only way that this is
true


true is that this difference converges
 


true is that this difference converges
to


to zero but since there's an expectation
 


to zero but since there's an expectation
in


in front this convergence to zero only
 


in front this convergence to zero only
almost


almost surely so why does this bound
 


almost surely so why does this bound
hold


hold right and that is uh Solo's uh main
 


hold right and that is uh Solo's uh main
result


result um which you proved in in 78 and
 


result um which you proved in in 78 and
first


first I mean we have here and and by the
 


first I mean we have here and and by the
way


way this is also
 


way this is also
not


not the the very exact formulation but I
 


not the the very exact formulation but I
mean


mean it's actually true in this form but
 


mean it's actually true in this form but
okay


okay anyway um so let's take some
 


okay anyway um so let's take some
abbreviation


abbreviation s t and Mite is of XT given
 


abbreviation s t and Mite is of XT given
X


X less T I don't want to write this out
 


X less T I don't want to write this out
always


always and the same of course for M okay
 


always and the same of course for M okay
so


so what you have here is you have the
 


so what you have here is you have the
square


square distance between
 


square distance between
anxiety


anxiety and
 


anxiety and
M


M okay you can show um that's a version
 


M okay you can show um that's a version
say


say of pin inequality that the square
 


say of pin inequality that the square
distance


distance is bounded by the K Divergence
 


distance is bounded by the K Divergence
between


between M and ex I give you a more
 


between M and ex I give you a more
downto


downto Earth sort of intuition for a
 


downto Earth sort of intuition for a
final


final class in a second but that's the
 


final class in a second but that's the
formal


formal proof so the the square distance
 


formal proof so the the square distance
is


is bounded by the Cal Divergence so we
 


is bounded by the Cal Divergence so we
have


have a sum over
 


have a sum over
t


t I mean this is super sloppy here and
 


t I mean this is super sloppy here and
lots


lots of things are missing so the
 


lots of things are missing so the
expectations


expectations and so on so the Divergent
 


expectations and so on so the Divergent
has


has a telescoping property so this is
 


has a telescoping property so this is
the


the same so if you sum only up to n not
 


the same so if you sum only up to n not
to


 
 


 
as


as the K Divergence between
 


as the K Divergence between
the


the X1 to n given IX one to
 


the X1 to n given IX one to
n


n um so I mean this telescopic property
 


n um so I mean this telescopic property
is


is well known
 


is well known
and


and if we write this down um then this
 


and if we write this down um then this
is


is sum over mu of X I really didn't want
 


is sum over mu of X I really didn't want
to


to go into this detail
 


to go into this detail
Ln


Ln so let me quick and then you give you
 


Ln so let me quick and then you give you
the


the the more intuitive but that is sort
 


the the more intuitive but that is sort
of


of the proof um so this is this and if
 


of the proof um so this is this and if
you


you look at the basian mixture formula
 


you look at the basian mixture formula
above


above here so you still have this slide
 


above here so you still have this slide
here


here the was defined as a sum but if you
 


here the was defined as a sum but if you
drop


drop from the sum all terms except mu so
 


drop from the sum all terms except mu so
we


we get mu of X1 to then do you see this
 


we get mu of X1 to then do you see this
what


what I'm writing MH MH okay so I just
 


what I'm writing MH MH okay so I just
drop


drop all terms except for the true
 


drop all terms except for the true
environment


environment so there larg yep so now if
 


environment so there larg yep so now if
I


I bring W to the left hand side and to
 


I bring W to the left hand side and to
the


the right hand side you see that mu
 


the right hand side you see that mu
divided


divided by C is upper bounded by 1/ W so
 


divided by C is upper bounded by 1/ W so
this


this one here is upper bounded by W mu
 


this one here is upper bounded by W mu
to


to the minus one okay MH that's from the
 


to the minus one okay MH that's from the
above


above inequality and and this sum so
 


above inequality and and this sum so
this


this is now a constant and this sum is
 


this is now a constant and this sum is
just


just summing the probabilities over
 


just summing the probabilities over
arbitrary


arbitrary sequences this just gives you
 


arbitrary sequences this just gives you
one


one so this bound and there's a
 


one so this bound and there's a
logarithm


logarithm don't forget it so this gives
 


logarithm don't forget it so this gives
the


the logarithm W mu to the minus one and
 


the logarithm W mu to the minus one and
as


as long as the prior is larger than zero
 


as long as the prior is larger than zero
then


then this is of course smaller than
 


then this is of course smaller than
infinity


infinity
 


infinity
okay


okay oh I see sorry n n is fixed here in
 


okay oh I see sorry n n is fixed here in
your


your summing over all possible
 


your summing over all possible
sequences


sequences yes all possible yes this is
 


sequences yes all possible yes this is
all


all possible sequence n for fixed n so
 


all possible sequence n for fixed n so
this


this this shows you that for a fixed n
 


this this shows you that for a fixed n
this


this is bounded by this constant but
 


this is bounded by this constant but
this


this constant does not depend on nend or
 


this constant does not depend on nend or
now


now I can let on the left hand side the
 


now I can let on the left hand side the
the


the N to Infinity because here I started
 


the N to Infinity because here I started
with


with s
 


with s
Infinity


Infinity let's called this s Infinity
 


Infinity let's called this s Infinity
but


but then I proved only that SN where the
 


but then I proved only that SN where the
sum


sum is up to n but you know since the
 


sum is up to n but you know since the
right


right hand side is a constant
 


right hand side is a constant
independent


independent of N I can just once I'm
 


independent of N I can just once I'm
finished


finished take the limit n to Infinity
 


finished take the limit n to Infinity
the


the left
 


the left
okay


okay let me get the intuition so let's
 


okay let me get the intuition so let's
assume


assume m is
 


assume m is
finite


finite and
 


finite and
contains


contains m m is not used up M elements
 


contains m m is not used up M elements
and


and we choose a uniform prior W of new
 


and we choose a uniform prior W of new
is


is 1 / M okay um what you're doing is
 


is 1 / M okay um what you're doing is
there


there is I mean this will be informal
 


there is I mean this will be informal
now


now but you can also formally prove it
 


now but you can also formally prove it
um


um so you're
 


um so you're
mixing


mixing over
 


mixing over
uniformly


uniformly over your environment in the
 


uniformly over your environment in the
class


class and you're making a prediction and
 


class and you're making a prediction and
that


that is very close to awaited majority
 


that is very close to awaited majority
right


right I mean the if the majority of
 


right I mean the if the majority of
distributions


distributions want to predict one you
 


distributions want to predict one you
will


will predict one okay so let's assume it
 


will predict one okay so let's assume it
is


is a majority voting which it is not but
 


is a majority voting which it is not but
it's


it's close and let's assume the majority
 


it's close and let's assume the majority
is


is Right predicts the correct thing yeah
 


is Right predicts the correct thing yeah
then


then we don't suffer any loss and be are
 


then we don't suffer any loss and be are
happy


happy okay let's assume the majority is
 


happy okay let's assume the majority is
wrong


wrong we make a wrong
 


wrong we make a wrong
prediction


prediction then we get in the next time
 


prediction then we get in the next time
step


step we get a feedback this prediction
 


step we get a feedback this prediction
was


was wrong so we can exclude this from
 


was wrong so we can exclude this from
the


the class if it's deterministic hard if
 


the class if it's deterministic hard if
it's


it's probabilistic soft so this class
 


it's probabilistic soft so this class
got


got half so for every prediction error
 


got half so for every prediction error
we


we make we have the size of the class
 


we make we have the size of the class
okay


okay once the class is down to just
 


okay once the class is down to just
containing


containing one element namely move the
 


containing one element namely move the
true


true
 


true
distribution


distribution we are done okay so how
 


distribution we are done okay so how
many


many halfings can we have well we can
 


many halfings can we have well we can
just


just have at most log 2 m halfings right
 


just have at most log 2 m halfings right
so


so the number of
 


so the number of
Errors


Errors is roughly bounded I see so this
 


Errors is roughly bounded I see so this
is


is the deterministic case where your
 


is the deterministic case where your
measure


measure is concentrated on a I guess a
 


measure is concentrated on a I guess a
single


single the prefix of a single sequence
 


single the prefix of a single sequence
yeah


yeah yeah okay okay and log 2 m is just
 


yeah yeah okay okay and log 2 m is just
minus


minus log 2 W to the new to Theus one
 


minus log 2 W to the new to Theus one
so


so apart from sort of an la2 uh you get
 


so apart from sort of an la2 uh you get
the


the same result in the deterministic
 


the same result in the deterministic
case


case with uniform prior and finite
 


case with uniform prior and finite
CL


CL I see okay
 


CL I see okay
okay


okay and then you have to make it for
 


okay and then you have to make it for
nonuniform


nonuniform prior
 


nonuniform prior
um


um you get these weights and and it
 


um you get these weights and and it
still


still the intuition goes through then
 


still the intuition goes through then
you


you have to go to stochastic um um
 


you have to go to stochastic um um
models


models and then I mean we don't really
 


models and then I mean we don't really
use


use a weighted majority because we use a
 


use a weighted majority because we use a
weed


weed mixture um so so there are
 


weed mixture um so so there are
differences


differences but the intuition is the
 


differences but the intuition is the
same


same sure sure sure okay okay um let's
 


same sure sure sure okay okay um let's
just


just take a step back because uh okay so
 


just take a step back because uh okay so
this


this this gives intuition about that
 


this this gives intuition about that
bound


bound uh but I guess the the the very
 


bound uh but I guess the the the very
high


high level non-mathematical intuition is
 


high level non-mathematical intuition is
essentially


essentially the let's go back uh one
 


essentially the let's go back uh one
slide


slide here um what this what these
 


slide here um what this what these
bounds


bounds are saying is that as you make
 


bounds are saying is that as you make
more


more and more
 


more and more
observations


observations um your
 


observations um your
mixture


mixture predictive distribution will
 


mixture predictive distribution will
converge


converge to the true one UHC for several
 


converge to the true one UHC for several
reasons


reasons one you assigned nonzero weight
 


reasons one you assigned nonzero weight
to


to every possible hypothesis so you
 


to every possible hypothesis so you
don't


don't exclude anything that was this uh
 


don't exclude anything that was this uh
epicurus


epicurus principle that we discussed
 


epicurus principle that we discussed
earlier


earlier when doing beian uh inference um
 


earlier when doing beian uh inference um
uh


uh so that that's one prerequisite and
 


uh so that that's one prerequisite and
this


this uh the second thing that's
 


this uh the second thing that's
happening


happening is uh as you just alluded to
 


happening is uh as you just alluded to
as


as you make more and more observations
 


as you make more and more observations
the


the ones that are inconsistent or
 


the ones that are inconsistent or
improbable


improbable with respect to Mu in general
 


improbable with respect to Mu in general
get


get down weighted and the things which
 


get down weighted and the things which
are


are more consistent with mu get
 


are more consistent with mu get
upweighted


upweighted and that happens in such a
 


upweighted and that happens in such a
way


way that ASM totically you have to
 


way that ASM totically you have to
converge


converge to me your your your your
 


converge to me your your your your
predictive


predictive
 


predictive
distribution


distribution ex intuitively exactly
 


distribution ex intuitively exactly
right


right I mean we can even write it down
 


right I mean we can even write it down
like


like before so let me write it down here
 


like before so let me write it down here
so


so we can write down this sty of XT
 


so we can write down this sty of XT
given


given history as what I wanted to not do
 


given history as what I wanted to not do
anymore


anymore but let's do it anyway is new
 


anymore but let's do it anyway is new
the


the
 


the
posterior


posterior uh W of new given x l
 


posterior uh W of new given x l
t


t and then
 


t and then
times


times uh new of XT so you use the beian
 


times uh new of XT so you use the beian
update


update rule to compute those uh those
 


update rule to compute those uh those
revised


revised
 


revised
um


um W's those weights and what you'll
 


um W's those weights and what you'll
find


find is that those weights go up for the
 


find is that those weights go up for the
things


things which are consistent with me and
 


things which are consistent with me and
those


those weights go down for the things
 


those weights go down for the things
which


which are inconsistent or improper
 


which are inconsistent or improper
exactly


exactly and in in very simple cases this
 


exactly and in in very simple cases this
posterior


posterior would just on the true
 


posterior would just on the true
distribution


distribution it would converge to one
 


distribution it would converge to one
and


and all the wrong distribution it would
 


and all the wrong distribution it would
converge


converge to zero that's right um in this
 


converge to zero that's right um in this
more


more complicated setting maybe there are
 


more complicated setting maybe there are
two


two distributions which are totically
 


two distributions which are totically
consistent


consistent right right um all this so
 


consistent right right um all this so
about


about the the total weight on the
 


about the the total weight on the
distributions


distributions which are consistent with
 


distributions which are consistent with
the


the data converges to one and those
 


the data converges to one and those
which


which are inconsistent converg to zero
 


which are inconsistent converg to zero
that


that means effectively in the simple
 


that means effectively in the simple
case


case where there is single distribution
 


case where there is single distribution
um


um where it concentrates to the trth MU
 


um where it concentrates to the trth MU
and


and this sum just collapses and gives
 


and this sum just collapses and gives
you


you mu of um XT given x l
 


you mu of um XT given x l
t


t um but proving this VI the posterior
 


t um but proving this VI the posterior
in


in the general case um is hell um and I
 


in the general case um is hell um and I
this


this we will not talk about it so okay
 


this we will not talk about it so okay
what's


what's what
 


what's what
next


next okay um so okay next uh let me
 


next okay um so okay next uh let me
instantiate


instantiate that I that was now I mean
 


instantiate that I that was now I mean
before


before we started at the simplest model
 


before we started at the simplest model
class


class like you know buyer's coin now we
 


class like you know buyer's coin now we
have


have a completely General framework we
 


have a completely General framework we
can


can plug in any model class we want and
 


can plug in any model class we want and
we


we have this bound right so let's
 


we have this bound right so let's
instantiate


instantiate with two or three
 


instantiate with two or three
interesting


interesting classes um which are
 


interesting classes um which are
important


important because they are bigger so
 


important because they are bigger so
more


more interesting but still uh tractable
 


more interesting but still uh tractable
so


so it can be computed in a path way okay
 


so it can be computed in a path way okay
so


so you're going to look at now
 


so you're going to look at now
interesting


interesting M's this family of okay yep
 


interesting M's this family of okay yep
okay


okay and um so
 


okay and um so
um


um the okay I have shown you this bound
 


um the okay I have shown you this bound
on


on the last uh uh slide um if we have
 


on the last uh uh slide um if we have
and


and that holds for any countable class
 


and that holds for any countable class
but


but if we have a continuous class right
 


but if we have a continuous class right
back


back to it and again you the proof is
 


back to it and again you the proof is
hard


hard and I don't want to do that um if
 


hard and I don't want to do that um if
you


you have a continue class with d
 


you have a continue class with d
parameter


parameter so let's assume M sort of is a
 


parameter so let's assume M sort of is a
class


class new Teta where Teta is say a d
 


class new Teta where Teta is say a d
dimensional


dimensional parameter okay
 


dimensional parameter okay
now


now okay and then you can show a similar
 


now okay and then you can show a similar
bound


bound um there you have to make some
 


bound um there you have to make some
smoothness


smoothness assumptions but you can show
 


smoothness assumptions but you can show
a


a similar bound and the bound is then uh
 


a similar bound and the bound is then uh
Ln


Ln W of theta to the minus
 


Ln W of theta to the minus
one


one um
 


one um
as


as
 


as
before


before
 


before
um


um but now this is a
 


um but now this is a
density


density then because it's a density do
 


density then because it's a density do
you


you have D
 


you have D
half


half
 


half
lnn


lnn term and this grows with Ln n so
 


lnn term and this grows with Ln n so
this


this dominate so this constant could be
 


this dominate so this constant could be
avoided


avoided and then you have uh the
 


avoided and then you have uh the
logarithm


logarithm of the square root of the
 


logarithm of the square root of the
determinant


determinant of this mysterious fure
 


determinant of this mysterious fure
information


information Matrix plus terms small or
 


information Matrix plus terms small or
and


and which converge to zero don't worry
 


and which converge to zero don't worry
about


about all this the dominant term is this
 


about all this the dominant term is this
one


one here and we just ignore this one
 


one here and we just ignore this one
this


this is a constant this is a constant so
 


this is a constant this is a constant so
it's


it's just the half l andn so the rest
 


it's just the half l andn so the rest
you


you can't forget about it okay if you
 


you can't forget about it okay if you
have


have a continuous
 


have a continuous
class


class and um so and in the in the Beni
 


class and um so and in the in the Beni
Teta


Teta case D was one and the bound would
 


Teta case D was one and the bound would
be


be one half okay and the intuition is if
 


be one half okay and the intuition is if
you


you have a continuous parameter um you
 


you have a continuous parameter um you
you


you could discr ize this parameter and
 


you could discr ize this parameter and
just


just take so we know that we can
 


just take so we know that we can
estimate


estimate a parameter to accuracy 1 /are
 


estimate a parameter to accuracy 1 /are
of


of n yeah and to encode a parameter to
 


of n yeah and to encode a parameter to
one/


one/ square of n accuracy you need one
 


one/ square of n accuracy you need one
half


half log n Bits right so so that is one
 


half log n Bits right so so that is one
way


way of interpreting this B so each
 


way of interpreting this B so each
parameter


parameter gets discretized to the
 


parameter gets discretized to the
accuracy


accuracy which is needed and no more and
 


accuracy which is needed and no more and
then


then um this gives you we haven't talked
 


then um this gives you we haven't talked
about


about code length yet but will we will
 


about code length yet but will we will
get


get to that so if you VI it the second
 


get to that so if you VI it the second
time


time you will understand what I mean
 


time you will understand what I mean
with


with this anyway just just trust me so
 


with this anyway just just trust me so
this


this is the bound so for instance a more
 


this is the bound so for instance a more
interesting


interesting class in IID would be a k
 


interesting class in IID would be a k
Mar


Mar of model okay so that means that the
 


Mar of model okay so that means that the
probability


probability of xn plus 1 depends on X N
 


probability of xn plus 1 depends on X N
-


- k + 1 up to uh n so the last k um
 


- k + 1 up to uh n so the last k um
parameters


parameters okay so how many parameters
 


parameters okay so how many parameters
are


are they right for each context here so
 


are they right for each context here so
assume


assume alphabet size is
 


assume alphabet size is
X


X or barx right the context length K so
 


X or barx right the context length K so
the


the
 


the
k


k x to the power of K possible
 


k x to the power of K possible
conditions


conditions right and then we predict xn
 


conditions right and then we predict xn
+


+ one and well you know this could be
 


+ one and well you know this could be
you


you know out of X but they have one
 


you know out of X but they have one
probability


probability constraint so that is x
 


probability constraint so that is x
minus


minus one parameter so in a binary case
 


minus one parameter so in a binary case
it


it would be one parameter if you have a
 


it would be one parameter if you have a
tary


tary case it would be two parameters and
 


tary case it would be two parameters and
so


so on so that's the number of real
 


so on so that's the number of real
welded


welded parameters you have in this model
 


welded parameters you have in this model
class


class so our D is this one here works as
 


class so our D is this one here works as
before


before we just take the bound you plug
 


before we just take the bound you plug
this


this lightly larger D in there and
 


this lightly larger D in there and
that's


that's the number of prediction errors
 


that's the number of prediction errors
you


you make roughly speaking okay nice
 


you make roughly speaking okay nice
Class


Class K
 


Class K
marov


marov but if K is large you see it's
 


marov but if K is large you see it's
exponential


exponential in K and it's no good so
 


exponential in K and it's no good so
there


there is a class which I think everyone
 


there is a class which I think everyone
should


should have heard about but uh ma sorry
 


should have heard about but uh ma sorry
are


are you making the point that in this
 


are you making the point that in this
generalized


generalized bound you had above with
 


generalized bound you had above with
with


with d and and uh log n that U that D is
 


with d and and uh log n that U that D is
the


the for the K uh order Markov chain
 


the for the K uh order Markov chain
that's


that's the D that you use this formula
 


that's the D that you use this formula
that


that you you wrote yes okay yes yeah
 


that you you wrote yes okay yes yeah
because


because that's the number parameters I
 


because that's the number parameters I
need


need to
 


need to
specify


specify that's right that's right okay
 


specify that's right that's right okay
okay


okay so that's a k Mark of
 


okay so that's a k Mark of
process


process um but now I talk about very
 


process um but now I talk about very
briefly


briefly something
 


briefly something
context


 
 


 
three


three
 


three
waiting


waiting algorithm okay so what
 


waiting algorithm okay so what
this


this okay so what this does is um
 


this okay so what this does is um
we


we okay okay very briefly um but we need
 


we okay okay very briefly um but we need
it


it later if we come to the
 


it later if we come to the
approximations


approximations so what is this
 


approximations so what is this
essentially


essentially a variable order Mark of
 


essentially a variable order Mark of
process


process so what you're doing is now
 


process so what you're doing is now
instead


instead of using
 


instead of using
say


say two last
 


say two last
symbols


symbols you use maybe if the last bit
 


symbols you use maybe if the last bit
was


was Zero you maybe need another bit to
 


was Zero you maybe need another bit to
differentiate


differentiate it whether it's 0 one but
 


differentiate it whether it's 0 one but
if


if the last bit is one you're fine you
 


if the last bit is one you're fine you
don't


don't need to see further bits in the
 


don't need to see further bits in the
past


past okay um so you look at these
 


past okay um so you look at these
trees


trees and um these trees have of course
 


trees and um these trees have of course
parameters


parameters right I mean sort of the
 


parameters right I mean sort of the
context


context is z0 you have a parameter if
 


context is z0 you have a parameter if
the


the context is one Zer you have a
 


the context is one Zer you have a
parameter


parameter and if the context is one is a
 


parameter and if the context is one is a
parameter


parameter you also have a number of
 


parameter you also have a number of
parameters


parameters okay so for fixed three you
 


parameters okay so for fixed three you
do


do the same as before and what you get
 


do the same as before and what you get
is


is well D is uh just um the um the the
 


is well D is uh just um the um the the
the


the number of parameters let's call that
 


the number of parameters let's call that
the


the set Theta is the set of your
 


the set Theta is the set of your
parameters


parameters and then it's just the size
 


parameters and then it's just the size
of


of this parameter set okay nothing
 


of this parameter set okay nothing
changes


changes okay for fixed trick
 


changes okay for fixed trick
but


but if we don't know the tree what we
 


but if we don't know the tree what we
would


would like we would like to do a basan
 


would like we would like to do a basan
mixture


mixture now over all possible trees
 


mixture now over all possible trees
right


right maybe of maximal depth D or so
 


right maybe of maximal depth D or so
okay


okay so um we just take the class of
 


okay so um we just take the class of
trees


 
 


 
is


is trees
 


is trees
of


of death smaller equal than b okay then
 


of death smaller equal than b okay then
we


we need a prior over trees we can do the
 


we need a prior over trees we can do the
somehow


somehow so you can encode a tree in bits
 


somehow so you can encode a tree in bits
and


and take two to the minus the code
 


and take two to the minus the code
length


length we will get back to that later in
 


length we will get back to that later in
a


a more General context three as your
 


a more General context three as your
prior


prior over
 


prior over
trees


trees you have of course you have the
 


trees you have of course you have the
prior


prior over the parameters where you take
 


prior over the parameters where you take
your


your either uniform distribution or the
 


your either uniform distribution or the
KT


KT one which we discussed before and
 


KT one which we discussed before and
then


then you do this huge Bas mixture over
 


then you do this huge Bas mixture over
everything


everything so the the thumb over all
 


everything so the the thumb over all
trees


trees of dep at M the sum well sum
 


trees of dep at M the sum well sum
overall


overall once you fix a tree over all the
 


overall once you fix a tree over all the
parameters


parameters which are in the
 


parameters which are in the
tree


tree and then you have your distribution
 


tree and then you have your distribution
which


which is then the
 


which is then the
um


um well some distribution X1 to n given
 


um well some distribution X1 to n given
the


the T and the Theta well the sum of
 


the T and the Theta well the sum of
theta


theta is of course an integral of sum
 


theta is of course an integral of sum
and


and then you get your distribution
 


and then you get your distribution
whatever


whatever you would call
 


whatever you would call
it


it okay so is this where this beautiful
 


it okay so is this where this beautiful
fact


fact comes in that okay if you were to
 


fact comes in that okay if you were to
do


do a naive beijan update you have to
 


do a naive beijan update you have to
update


update you know this I don't know double
 


update you know this I don't know double
exponential


exponential count of Weights because
 


exponential count of Weights because
that's


that's how how many trees there are but
 


that's how how many trees there are but
if


if you only care about updating the
 


if you only care about updating the
predictive


predictive distribution there's a very
 


predictive distribution there's a very
nice


nice
 


nice
algorithm


algorithm actually it is the you you can
 


algorithm actually it is the you you can
update


update the predictive distribution
 


update the predictive distribution
efficiently


efficiently too but it's much easier to
 


efficiently too but it's much easier to
update


update the joint distribution and then
 


update the joint distribution and then
take


take the ratio so even the joint
 


take the ratio so even the joint
distribution


distribution is easy to calculate um if
 


distribution is easy to calculate um if
you


you calculate in an online way and let
 


you calculate in an online way and let
me


me just write it down because it's so
 


me just write it down because it's so
easy


easy so yeah if you would do that I mean
 


easy so yeah if you would do that I mean
the


the integral is easy to do and which is
 


the integral is easy to do and which is
just


just lots of local little laas rule but
 


just lots of local little laas rule but
this


this sum is hell as you said um there
 


this sum is hell as you said um there
are


are this class T is about two to the
 


are this class T is about two to the
two


two okay so now let's go to larger
 


two okay so now let's go to larger
classes


classes and before we do that we have to
 


classes and before we do that we have to
introduce


introduce comol complexity but I can do
 


introduce comol complexity but I can do
that


that very briefly if you don't think
 


that very briefly if you don't think
much


 
 


 
okay


okay so
 


okay so
um


um generally let's step a little bit
 


um generally let's step a little bit
back


back um in science um we need a
 


back um in science um we need a
principle


principle which is called okam razor
 


principle which is called okam razor
principle


principle so if you have two hypothesis
 


principle so if you have two hypothesis
which


which are equally good describe your
 


which are equally good describe your
data


data you should choose the simpler one
 


data you should choose the simpler one
and


and there is no way around it sort of
 


and there is no way around it sort of
without


without oram's razor there would be no
 


without oram's razor there would be no
signs


signs um Believe It or Not uh well it's
 


signs um Believe It or Not uh well it's
argued


argued in my book and you know many of
 


argued in my book and you know many of
my


my papers so I'm no time for that here
 


my papers so I'm no time for that here
okay


okay but what does simple mean we have
 


okay but what does simple mean we have
to


to need a quantitative notion of
 


to need a quantitative notion of
Simplicity


Simplicity and we want one which is for
 


Simplicity and we want one which is for
AGI


AGI purposes which is universal right
 


AGI purposes which is universal right
which


which is not sort of limited to some you
 


which is not sort of limited to some you
know


know nice classes or something like this
 


know nice classes or something like this
and


and color of complexity does that for
 


and color of complexity does that for
you


you so think about any object you code
 


you so think about any object you code
it


it nely is a binary string I mean think
 


it nely is a binary string I mean think
about


about you have a I mean now all data are
 


about you have a I mean now all data are
binary


binary anyway right and you ask what is
 


binary anyway right and you ask what is
the


the information content in this string
 


the information content in this string
well


well if it's a string of all ones right
 


well if it's a string of all ones right
it's


it's pretty boring right um but if it's
 


it's pretty boring right um but if it's
you


you know Pi it's a little bit more
 


you know Pi it's a little bit more
interesting


interesting but you still I mean it's a
 


interesting but you still I mean it's a
very


very easy description if it's a picture
 


very easy description if it's a picture
from


from a camera there's a lot of
 


from a camera there's a lot of
information


information in there and we can capture
 


information in there and we can capture
this


this by how difficult is it to describe
 


this by how difficult is it to describe
um


um my data well we need a description
 


um my data well we need a description
links


links well let's just take a programming
 


links well let's just take a programming
language


language as a description and you anyone
 


language as a description and you anyone
see


see or list or whatever want um or
 


see or list or whatever want um or
abstractly


abstractly we take a universal touring
 


abstractly we take a universal touring
machine


machine because you know that is also
 


machine because you know that is also
Universal


Universal description language and we
 


Universal description language and we
look


look for descriptions of our data okay
 


look for descriptions of our data okay
so


so let's take this Universal touring
 


so let's take this Universal touring
machine


machine or your list or your C compiler
 


machine or your list or your C compiler
whatever


whatever you prefer we look for a
 


whatever you prefer we look for a
program


program which if you run it gives us our
 


program which if you run it gives us our
string


string x one to n okay um but there of
 


string x one to n okay um but there of
course


course many programs which produce a
 


course many programs which produce a
string


string for the you know the digits of pi
 


string for the you know the digits of pi
I


I can WR print quotation mark one
 


I can WR print quotation mark one
billion


billion digit of high quotation mark
 


billion digit of high quotation mark
very


very long program but they also shorter
 


very long program but they also shorter
programs


programs and you want to okam raer ask
 


programs and you want to okam raer ask
for


for the
 


for the
shortest


shortest program hypothesis theory model
 


shortest program hypothesis theory model
of


of our data so look at the length of the
 


of our data so look at the length of the
program


program and find the shortest
 


program and find the shortest
one


one okay and the length of the shortest
 


one okay and the length of the shortest
program


program is called the
 


program is called the
quor


quor of X and that's it okay and um you
 


quor of X and that's it okay and um you
know


know I don't want to argue here why this
 


know I don't want to argue here why this
is


is a good measure um you know you can
 


is a good measure um you know you can
read


read it elsewhere okay and we're going
 


read it elsewhere okay and we're going
to


to use that um what you can show is that
 


to use that um what you can show is that
it


it is nearly independent of the choice
 


it is nearly independent of the choice
of


of the universal touring machine so if
 


of the universal touring machine so if
you


you take code with respect to one
 


you take code with respect to one
touring


touring machine or you compare it to K
 


touring machine or you compare it to K
Prime


Prime with respect to a different
 


Prime with respect to a different
touring


touring machine and you take
 


touring machine and you take
uh


uh the supremum over all
 


uh the supremum over all
strings


strings this is bounded by
 


strings this is bounded by
finite


finite constant so if you have strings
 


finite constant so if you have strings
which


which are longer and longer so even over
 


which are longer and longer so even over
all


 
 


 
n


n if you have longer and longer strings
 


n if you have longer and longer strings
um


um still the difference is as a constant
 


um still the difference is as a constant
which


which is the typical compiler constant
 


which is the typical compiler constant
between


between compiling from say U to U prime
 


between compiling from say U to U prime
or


or from list to C and so on and I mean
 


or from list to C and so on and I mean
we


we have our data is so much richer and
 


we have our data is so much richer and
larger


larger in AGI than this compiler
 


larger in AGI than this compiler
constant


constant that you can ignore this
 


constant that you can ignore this
problem


problem um second problem is the incomp
 


problem um second problem is the incomp
computability


computability what you do now there is
 


computability what you do now there is
okay


okay this is looking for the shortest
 


okay this is looking for the shortest
program


program which looks like compression
 


program which looks like compression
okay


okay so if we have data and we create a
 


okay so if we have data and we create a
self


self extracting archive which means it's
 


self extracting archive which means it's
a


a compressed version but it always also
 


a compressed version but it always also
has


has the decompressor buil in what is a
 


has the decompressor buil in what is a
self-extracting


self-extracting archive well a
 


self-extracting archive well a
self-extracting


self-extracting archive is a program if
 


self-extracting archive is a program if
I


I run it reproduces my data yeah so what
 


I run it reproduces my data yeah so what
I


I can do is I can replace the kolf
 


I can do is I can replace the kolf
complexity


complexity by you know the best
 


complexity by you know the best
self-extracting


self-extracting the shortest self-
 


self-extracting the shortest self-
extracting


extracting Arch which are currently you
 


extracting Arch which are currently you
know


know can create with practical
 


know can create with practical
compressors


compressors as an approxim okay um so
 


compressors as an approxim okay um so
and


and how do we use this why do we care
 


and how do we use this why do we care
about


about this here well it quantifies or
 


about this here well it quantifies or
which


which is really great um but um so that
 


which is really great um but um so that
was


was for
 


was for
Strings


Strings um if I have any other object
 


Strings um if I have any other object
like


like numbers I can encod it in strings
 


like numbers I can encod it in strings
right


right if I have a probability
 


right if I have a probability
distribution


distribution which computable right
 


distribution which computable right
there's


there's a progam which computes it and
 


there's a progam which computes it and
just


just this program so so any object o can
 


just this program so so any object o can
be


be coded as a bit string um so and then
 


be coded as a bit string um so and then
the


the colol complexity of an object O is
 


the colol complexity of an object O is
just


just defined as the sort of some
 


just defined as the sort of some
canonical


canonical encoding of it and it just
 


canonical encoding of it and it just
drop


drop it okay because I will now one yeah
 


drop it okay because I will now one yeah
I


I mean I guess uh we're always working
 


I mean I guess uh we're always working
over


over uh countable things which is why
 


over uh countable things which is why
this


this is possible because of there's only
 


this is possible because of there's only
countable


countable uh number of uh strings and
 


countable uh number of uh strings and
Turing


Turing machines so of course like but we
 


Turing machines so of course like but we
talking


talking about probability distributions
 


talking about probability distributions
by


by restricting to probabilities over uh
 


by restricting to probabilities over uh
finite


finite length strings that we're in
 


finite length strings that we're in
accountable


accountable situation yes for
 


accountable situation yes for
probability


probability distributions we only look
 


probability distributions we only look
for


for computable probability distributions
 


for computable probability distributions
now


now right that means by definition that
 


now right that means by definition that
exists


exists a program that means it's only
 


exists a program that means it's only
accountably


accountably many okay and then we write
 


accountably many okay and then we write
K


K of new for a computable probability
 


K of new for a computable probability
distribution


distribution and that's used mu for the
 


distribution and that's used mu for the
class


class of comput probability
 


class of comput probability
distributions


distributions okay and technically it's
 


distributions okay and technically it's
lower


lower semic computable semi
 


lower semic computable semi
distributions


distributions but let's not go into that
 


distributions but let's not go into that
okay


okay um okay so now we have a measure of
 


okay um okay so now we have a measure of
complexity


complexity of a distribution and Oram
 


complexity of a distribution and Oram
raer


raer tells us take the simplest one but
 


raer tells us take the simplest one but
aporus


aporus principle tells us keep them all
 


aporus principle tells us keep them all
and


and we can unify these two principles
 


and we can unify these two principles
and


and say well you know have a higher
 


and say well you know have a higher
prior


prior belief in the simple ones and a
 


prior belief in the simple ones and a
low


low one in the complex ones and the same
 


low one in the complex ones and the same
with


with the posterior so don't rule out any
 


with the posterior so don't rule out any
so


so we to choose a prior now before it
 


so we to choose a prior now before it
was


was left
 


was left
unspecified


unspecified now we choose a prior which
 


unspecified now we choose a prior which
is


is two to the minus of
 


is two to the minus of
complexity


complexity and so that is monotone
 


complexity and so that is monotone
increasing


increasing what you want and you choose
 


increasing what you want and you choose
two


two to the minus K of new because you
 


two to the minus K of new because you
want


want it to sum to one or at least to be
 


want it to sum to one or at least to be
bounded


bounded by one and you can prove that
 


bounded by one and you can prove that
this


this is the case because a prior needs
 


this is the case because a prior needs
to


to some to one or at least um not more
 


to some to one or at least um not more
than


than one so we now have fixed let's it w
 


than one so we now have fixed let's it w
we


we have fixed the class to be all all
 


we have fixed the class to be all all
computable


computable probability measures and for
 


computable probability measures and for
the


the expert please forgive me that I'm a
 


the expert please forgive me that I'm a
little


little bit crude here um with some
 


little bit crude here um with some
things


things and we have fixed the
 


things and we have fixed the
prior


prior well and we can use Solomon of's
 


prior well and we can use Solomon of's
Bound


Bound for this so the Solomon of bound
 


Bound for this so the Solomon of bound
was


was as Infinity was bounded by Sonet PL
 


was as Infinity was bounded by Sonet PL
in


in this so we we get the Cai over this
 


in this so we we get the Cai over this
class


class let's call it caiu and the bound
 


class let's call it caiu and the bound
was


was the logarithm
 


was the logarithm
W


W to the minus one and if you plug in
 


W to the minus one and if you plug in
this


this this is just K of new
 


this this is just K of new
*


*
 


*
ln2


ln2 so this is this is now actually Sol
 


ln2 so this is this is now actually Sol
is


is what he's proing okay
 


is what he's proing okay
and


and yeah no no this is nice because this
 


and yeah no no this is nice because this
is


is saying
 


is saying
basically


basically um I
 


basically um I
say


say simpler hypotheses have fewer
 


say simpler hypotheses have fewer
mistakes


mistakes on average right right exactly
 


mistakes on average right right exactly
even


even more precisely if you have a
 


even more precisely if you have a
hypothesis


hypothesis a model which you can encode
 


hypothesis a model which you can encode
specify


specify in 500 bits then roughly
 


specify in 500 bits then roughly
speaking


speaking Solomon of induction makes 500
 


speaking Solomon of induction makes 500
prediction


prediction errors and then it's good you
 


prediction errors and then it's good you
never


never can tell where they could be at
 


never can tell where they could be at
the


the beginning that could be spersed out
 


the beginning that could be spersed out
but


but you roughly only make 500 prediction
 


but you roughly only make 500 prediction
errors


errors and then it's good I mean the
 


errors and then it's good I mean the
real


real result is that you get closer and
 


real result is that you get closer and
closer


closer you can prove sort of the number
 


closer you can prove sort of the number
of


of Epsilon errors is bounded by K of new
 


of Epsilon errors is bounded by K of new
divided


divided by epon square and blah blah
 


divided by epon square and blah blah
blah


blah and so on right but roughly
 


blah and so on right but roughly
speaking


speaking and that is also intuitive
 


speaking and that is also intuitive
right


right if I have an unknown World which I
 


right if I have an unknown World which I
need


need 500 bits to specify I need to learn
 


need 500 bits to specify I need to learn
this


this 500 bits somehow by trial and error
 


this 500 bits somehow by trial and error
right


right so I need to make 500 mistakes and
 


right so I need to make 500 mistakes and
you


you can indeed show that it's the best
 


you can indeed show that it's the best
possible


possible Val I think what's good to
 


possible Val I think what's good to
emphasize


emphasize because I I don't think I
 


emphasize because I I don't think I
appreciated


appreciated this the first time is that
 


appreciated this the first time is that
there


there are uh many uh priors you can
 


there are uh many uh priors you can
write


write because the only condition of
 


write because the only condition of
being


being a prior is just that each W new is
 


being a prior is just that each W new is
is


is positive right you want to assign
 


is positive right you want to assign
positive


positive weight to any candidate
 


positive weight to any candidate
hypothesis


hypothesis and the bound you wrote a few
 


hypothesis and the bound you wrote a few
slides


slides ago uh could have been any such
 


slides ago uh could have been any such
uh


uh we could have used any prior and and
 


uh we could have used any prior and and
the


the C that we had was the mixture
 


the C that we had was the mixture
obtained


obtained from that prior what's special
 


obtained from that prior what's special
about


about the prior that you just wrote here
 


about the prior that you just wrote here
using


using the comor of complexity is the
 


using the comor of complexity is the
fact


fact that it has this sharpness with
 


fact that it has this sharpness with
respect


respect to this this bound that that I
 


respect to this this bound that that I
want


want you to maybe explain a little bit
 


want you to maybe explain a little bit
what's


what's the opt what's the optimality for
 


what's the opt what's the optimality for
for


for this yeah so what you can show is
 


for this yeah so what you can show is
that


that this Wu of new yeah so you take any
 


that this Wu of new yeah so you take any
other


other prior over new is computable I
 


other prior over new is computable I
mean


mean the non-computable prior I mean we
 


mean the non-computable prior I mean we
cannot


cannot deal with anyway okay we take
 


cannot deal with anyway okay we take
comput


comput fire what you can show is that
 


comput fire what you can show is that
apart


apart from a multiplicative constant
 


apart from a multiplicative constant
this


this W new is
 


this W new is
larger


larger which means that apart from this
 


larger which means that apart from this
constant


constant minus l
 


constant minus l
right


right is smaller so if I take
 


right is smaller so if I take
Now


Now log W new minus one then this is
 


Now log W new minus one then this is
smaller


smaller
 


smaller
than


than log P of new to the minus one apart
 


than log P of new to the minus one apart
from


from an additive constant because the
 


from an additive constant because the
log


log makes a
 


log makes a
multiplic


multiplic and this constants float
 


multiplic and this constants float
around


around anyway in complex you always have
 


around anyway in complex you always have
comp


comp sure from Conant this gives you the
 


comp sure from Conant this gives you the
best


best possible bound sorry what is this
 


best possible bound sorry what is this
this


this lower bound you wrote up to a
 


this lower bound you wrote up to a
constant


constant what uh that is so what's the
 


constant what uh that is so what's the
statement


statement the statement is that if you
 


statement the statement is that if you
choose


choose any other computable prior ah I
 


choose any other computable prior ah I
see


see it will be smaller than the
 


see it will be smaller than the
universal


universal prior apart from a
 


universal prior apart from a
const


const ah okay because because if it's
 


const ah okay because because if it's
computable


computable this is this is an intuition
 


computable this is this is an intuition
I


I don't if it's correct but if it's
 


I don't if it's correct but if it's
computable


computable then you have to use a
 


computable then you have to use a
program


program that's basically lower bounded
 


program that's basically lower bounded
by


by the com growth complexity and then
 


by the com growth complexity and then
two


two to the minus that is is smaller
 


two to the minus that is is smaller
right


right yes the proof right okay okay all
 


right yes the proof right okay okay all
right


right I see okay this this makes this
 


right I see okay this this makes this
makes


makes sense okay I
 


makes sense okay I
see


see okay so so it is what if your data
 


see okay so so it is what if your data
is


is sampled from any computable
 


is sampled from any computable
distribution


distribution let me rephrase it can be
 


distribution let me rephrase it can be
the


the digit of pi it can be a pie wise
 


the digit of pi it can be a pie wise
noner


noner sequence it can
 


noner sequence it can
be


be a chaotic sequence or whatever as
 


be a chaotic sequence or whatever as
long


long as there is
 


long as there is
a


a program of B bits which computes in
 


a program of B bits which computes in
stastically


stastically right um your your sequence
 


stastically right um your your sequence
um


um solomonov will learn it in the
 


um solomonov will learn it in the
minimal


minimal number of errors this makes this
 


minimal number of errors this makes this
makes


makes total sense now this this is this
 


makes total sense now this this is this
is


is very beautiful yeah yeah
 


is very beautiful yeah yeah
yeah


yeah okay so this was um a little bit
 


yeah okay so this was um a little bit
heavy


heavy maybe um there is a different
 


heavy maybe um there is a different
represent


represent so this is called uh well
 


represent so this is called uh well
this


this
 


this
U


U um this is called the where's caou
 


U um this is called the where's caou
here


here caou is the universal distribution
 


here caou is the universal distribution
um


um but solomonov
 


um but solomonov
um


um derived a different distribution but
 


um derived a different distribution but
which


which turns out magically um which is
 


which turns out magically um which is
equivalent


equivalent and this different one is I
 


equivalent and this different one is I
mean


mean you know we had a lot of Machinery
 


mean you know we had a lot of Machinery
now


now going on right and you know base and
 


now going on right and you know base and
then


then you know Comm complexity and and
 


then you know Comm complexity and and
all


all this kind of stuff there's a a
 


all this kind of stuff there's a a
shortcut


shortcut to get to this and I would like
 


shortcut to get to this and I would like
to


to present this shortcut in a short way
 


to present this shortcut in a short way
so


so what you're doing here is is let's
 


so what you're doing here is is let's
just


just pause this this SIU is the Solomon
 


just pause this this SIU is the Solomon
of


of Prior
 


of Prior
right


right that's the Solomon of there two
 


right that's the Solomon of there two
different


different definitions and versions and
 


different definitions and versions and
and


and this is one one of them that's right
 


and this is one one of them that's right
okay


okay yeah okay I'm usually attribute the
 


okay yeah okay I'm usually attribute the
other


other one more to Solomon of but sort of
 


other one more to Solomon of but sort of
he


he came up with both in a sense ah okay
 


he came up with both in a sense ah okay
okay


okay so um okay so the other one is you
 


okay so um okay so the other one is you
take


take um a universal touring machine
 


take um a universal touring machine
and


and which has an input tape and an
 


and which has an input tape and an
output


output tape and by the way I'm skipping
 


output tape and by the way I'm skipping
over


over so many things you need monot
 


over so many things you need monot
machines


machines and so on so but you know this
 


machines and so on so but you know this
is


is all precisely in my old in my new
 


is all precisely in my old in my new
book


book um so um we have we have an input
 


book um so um we have we have an input
tape


tape and we
 


tape and we
sample


sample a uniform random noise so Fair
 


sample a uniform random noise so Fair
coin


coin
 


coin
flip


flip on the input tape okay so this
 


flip on the input tape okay so this
touring


touring machine sometimes will do
 


touring machine sometimes will do
something


something sometimes not so there will be
 


something sometimes not so there will be
a


a probability distribution on the output
 


a probability distribution on the output
tape


tape okay and let's just call this
 


tape okay and let's just call this
distribution


distribution M of X whatever that is but
 


distribution M of X whatever that is but
there


there is a probability distribution of
 


there is a probability distribution of
output


output T okay and it turns out that m
 


output T okay and it turns out that m
ofx


 
 


 
X


X again you can be more precise either
 


X again you can be more precise either
with


with a multip constant or for every M
 


with a multip constant or for every M
that


that exist the universal touring machine
 


that exist the universal touring machine
the


the s so they have a whole paper about
 


the s so they have a whole paper about
this


this but essentially it's the same I
 


this but essentially it's the same I
will


will not prove it here the proof is you
 


will not prove it here the proof is you
know


know long and difficult um I give you a
 


know long and difficult um I give you a
little


little bit more formal definition of
 


little bit more formal definition of
this


this what does it mean actually so we're
 


this what does it mean actually so we're
looking


looking the input tape is bits but they
 


looking the input tape is bits but they
are


are sort of Interest programs um you can
 


are sort of Interest programs um you can
ask


ask a question in a second so so this
 


ask a question in a second so so this
this


this p is input bits okay on the touring
 


this p is input bits okay on the touring
machine


machine and we then run the touring
 


machine and we then run the touring
machine


machine okay on P and then it outputs
 


machine okay on P and then it outputs
something


something if it doesn't output X1 to
 


something if it doesn't output X1 to
n


n we don't care we only look at programs
 


n we don't care we only look at programs
which


which output X1 to n okay so assume
 


which output X1 to n okay so assume
there's


there's one program which outputs X1 to
 


there's one program which outputs X1 to
n


n and only one because this program has
 


n and only one because this program has
length


length L of
 


length L of
P


P the probability of sampling this
 


P the probability of sampling this
program


program from Fair coin flips is just 2
 


program from Fair coin flips is just 2
to


 
 


 
right


right okay but there will be many
 


right okay but there will be many
programs


programs which output X so we have to
 


programs which output X so we have to
sum


sum over all of these programs um to get
 


sum over all of these programs um to get
this


this output prob so that is this is
 


this output prob so that is this is
exactly


exactly the same as sampling infinite
 


exactly the same as sampling infinite
random


random noise and piping it through a
 


random noise and piping it through a
universal


universal toing machine this is sort of
 


universal toing machine this is sort of
the


the formal formal definition here and
 


the formal formal definition here and
then


then um these programs don't need to
 


then um these programs don't need to
hold


hold the star indicat that the program
 


hold the star indicat that the program
don't


don't need to hold fine and um yeah and
 


don't need to hold fine and um yeah and
you


you can show highly non-trivial that
 


you can show highly non-trivial that
these


these are equivalent and that is really
 


these are equivalent and that is really
nice


nice so you can so here we only consider
 


nice so you can so here we only consider
deter


deter programs predicting strings and
 


deter programs predicting strings and
mixing


mixing over them with a prior which is 2
 


mixing over them with a prior which is 2
to


to the minus L
 


to the minus L
ofp


ofp but the intuition roughly speaking
 


ofp but the intuition roughly speaking
is


is that these
 


is that these
stochastic


stochastic environments are sort of
 


stochastic environments are sort of
living


living in the convex Hull of the
 


living in the convex Hull of the
deterministic


deterministic ones so are implicitly
 


deterministic ones so are implicitly
already


already included um in the determin so
 


already included um in the determin so
you


you only need to mix over the corners of
 


you only need to mix over the corners of
this


this infinite Simplex in order also to
 


this infinite Simplex in order also to
get


get the middle one but I mean this
 


get the middle one but I mean this
intuition


intuition is an intuition but it's stly
 


intuition is an intuition but it's stly
speaking


speaking it's not right but anyway the
 


speaking it's not right but anyway the
statement


statement is correct so we we we defined
 


statement is correct so we we we defined
Salomon


Salomon of uh Solomon of Prior the
 


Salomon of uh Solomon of Prior the
Solomon


Solomon of mixture distribution in terms
 


Solomon of mixture distribution in terms
of


of Kor complexity you gave this
 


of Kor complexity you gave this
alternative


alternative definition what Solomon of
 


alternative definition what Solomon of
induction


induction essentially is it's it's it's
 


induction essentially is it's it's it's
the


the basian sequin sequence prediction
 


the basian sequin sequence prediction
when


when we start off with the Solomon of
 


when we start off with the Solomon of
Prior


Prior right and it has optimal
 


Prior right and it has optimal
theoretical


theoretical guarantees that's what's
 


theoretical guarantees that's what's
nice


nice about the Solomon of Prior okay
 


nice about the Solomon of Prior okay
okay


okay so why don't you describe your the
 


okay so why don't you describe your the
the


the two other things you wanted to
 


the two other things you wanted to
discuss


discuss in this section so this is all
 


discuss in this section so this is all
you


you know good be I I covered small model
 


you know good be I I covered small model
classes


classes and then this Solon of class
 


classes and then this Solon of class
which


which is I mean you know incomputable so
 


which is I mean you know incomputable so
you


you canot really use it in
 


you canot really use it in
practice


practice um um neural networks are all
 


practice um um neural networks are all
the


the r these days and large language
 


the r these days and large language
models


models
 


models
um


um what they're doing is when you're
 


um what they're doing is when you're
training


training them you're minimizing log loss
 


training them you're minimizing log loss
right


right okay so um so if you have if your
 


right okay so um so if you have if your
data


data set which is a long
 


data set which is a long
string


string
 


string
okay


okay um you have this language model P
 


okay um you have this language model P
which


which is parameters by you know this
 


which is parameters by you know this
weights


weights right and you predict um the
 


weights right and you predict um the
next


next uh symbol given the history of
 


next uh symbol given the history of
course


course they have a finite context so
 


course they have a finite context so
it's


it's not the whole history but you know
 


it's not the whole history but you know
that's


that's fine okay and um trining means
 


that's fine okay and um trining means
minimizing


minimizing log loss so you take the log
 


minimizing log loss so you take the log
loss


loss minus right and
 


loss minus right and
minimize


minimize over
 


minimize over
Theta


Theta and the argument gives you then
 


Theta and the argument gives you then
you


you know the optimal one of course you
 


you know the optimal one of course you
do


do CR descend you may not get the global
 


do CR descend you may not get the global
minimum


minimum and so on but that's what you're
 


minimum and so on but that's what you're
doing


doing okay so um but this is the same as
 


doing okay so um but this is the same as
maximum


maximum likelihood right I mean you put
 


maximum likelihood right I mean you put
the


the minus away and without the lock so
 


the minus away and without the lock so
this


this is the same as the maximum over
 


this is the same as the maximum over
Theta


Theta P from X ah uh sorry um you you um
 


Theta P from X ah uh sorry um you you um
sum


sum here of of course over all time
 


sum here of of course over all time
steps


steps I mean you train on the first
 


steps I mean you train on the first
token


token I mean you you you you train on
 


token I mean you you you you train on
all


all the tokens right um up to
 


all the tokens right um up to
n


n um you pull the sum into the log which
 


n um you pull the sum into the log which
gives


gives you a product and the product of
 


gives you a product and the product of
the


the conditional distribution The Joint
 


the conditional distribution The Joint
distribution


distribution that's the same as as
 


distribution that's the same as as
taking


taking the likelihood function and
 


taking the likelihood function and
maximizing


maximizing this okay so minimizing uh
 


maximizing this okay so minimizing uh
log


log loss um uh in llm so maybe I should
 


log loss um uh in llm so maybe I should
have


have
 


have
heading


heading here um is the same as your know
 


heading here um is the same as your know
maximum


maximum like estimation okay and um we
 


maximum like estimation okay and um we
know


know that I mean for simple model
 


know that I mean for simple model
classes


classes of course these model classes
 


classes of course these model classes
are


are not simple but if the model class
 


are not simple but if the model class
were


were simple AR marks that would converge
 


were simple AR marks that would converge
uh


uh to uh the true parameter um assuming
 


uh to uh the true parameter um assuming
of


of course um that um you true parameters
 


of course um that um you true parameters
in


in in the model class so that is the the
 


in in the model class so that is the the
the


the class uh the the llm class right
 


the class uh the the llm class right
that


that the lmm can represent it okay I
 


that the lmm can represent it okay I
mean


mean there is really I mean you know
 


mean there is really I mean you know
lots


lots of assumptions here you know lots
 


lots of assumptions here you know lots
of


of shortcuts very quick okay so but if
 


of shortcuts very quick okay so but if
you


you would do that um so you have some
 


you would do that um so you have some
sequence


sequence say you know B only one3 you
 


sequence say you know B only one3 you
train


train your model on a b 1 third sequence
 


train your model on a b 1 third sequence
then


then you stop training and then you use
 


then you stop training and then you use
it


it for
 


it for
prediction


prediction if everything goes well it
 


prediction if everything goes well it
would


would then predict you know B only one
 


would then predict you know B only one
third


third right
 


third right
um


um but
 


um but
this


this view uh is maybe not the the the
 


this view uh is maybe not the the the
most


most appropriate one because these
 


most appropriate one because these
models


models are trained on a lot of data
 


models are trained on a lot of data
right


right you know and you know there's a
 


right you know and you know there's a
chunk


chunk of maybe you know Fair coin flips
 


chunk of maybe you know Fair coin flips
and


and then there's a chunk which contains
 


and then there's a chunk which contains
the


the digits of pi so more plausibly
 


the digits of pi so more plausibly
actually


actually our data set looks like X1 to n
 


actually our data set looks like X1 to n
and


 
 


 
up


up so you have multiple data sets
 


up so you have multiple data sets
um


um and um you know the first one is
 


um and um you know the first one is
maybe


maybe you know as I said you know little
 


maybe you know as I said you know little
over


over half the next one is Wikipedia page
 


over half the next one is Wikipedia page
and


and so on so that's more realistic okay
 


and so on so that's more realistic okay
so


so how can we model this so let's assume
 


so how can we model this so let's assume
that


that um so so this correspond maybe to
 


that um so so this correspond maybe to
some


some parameters th K right um but let's
 


some parameters th K right um but let's
assume


assume so this
 


assume so this
is


is let's assume that this K let's don't
 


is let's assume that this K let's don't
talk


talk about Teta yeah this Cas is itself
 


talk about Teta yeah this Cas is itself
sampled


sampled from some prior distribution
 


sampled from some prior distribution
let's


let's call it w
 


let's call it w
of


of of k or something like this okay so
 


of of k or something like this okay so
put


put a do here right some some prior okay
 


put a do here right some some prior okay
so


so you so you view the real world data
 


so you so you view the real world data
as


as okay first I pick a website and then
 


as okay first I pick a website and then
I


I train on this website right and then I
 


I train on this website right and then I
pick


pick another website and this website
 


pick another website and this website
one


one website could be Pure Noise the
 


one website could be Pure Noise the
other


other website is Wikipedia the third one
 


other website is Wikipedia the third one
is


is some program and so on okay so that
 


is some program and so on okay so that
that's


that's a better of the data okay what
 


that's a better of the data okay what
does


does it mean sampling K and okay and
 


does it mean sampling K and okay and
these


these these ones these um
 


these these ones these um
x


x one to n k is sample from the
 


x one to n k is sample from the
distribution


distribution say new K
 


distribution say new K
okay


okay so what does it mean first k Sorry
 


okay so what does it mean first k Sorry
W


W is the weights of like you could think
 


W is the weights of like you could think
of


of these uh I'm thinking of these cases
 


of these uh I'm thinking of these cases
like


like indexing different documents right
 


like indexing different documents right
and


and W's the weight of different maybe
 


and W's the weight of different maybe
maybe


maybe they just be form distribution
 


maybe they just be form distribution
this


this case okay okay and then new of K oh
 


this case okay okay and then new of K oh
I


I see new of K is the the the language
 


I see new of K is the the the language
model


model of language within that document
 


model of language within that document
essentially


essentially yes I mean it could in this
 


essentially yes I mean it could in this
case


case it could be deterministic right
 


case it could be deterministic right
just


just the content of this website right
 


just the content of this website right
or


or it could be sort of if it's a if a
 


or it could be sort of if it's a if a
agerate


agerate if you take sort of K2 index
 


agerate if you take sort of K2 index
over


over okay here are all Wikipedia Pages
 


over okay here are all Wikipedia Pages
here


here all Twitter pages here all programs
 


here all Twitter pages here all programs
then


then it could be sort of real
 


then it could be sort of real
distribution


distribution but if you find more it
 


distribution but if you find more it
would


would be deterministic it doesn't really
 


would be deterministic it doesn't really
matter


matter at okay sure sure sure sure okay
 


matter at okay sure sure sure sure okay
okay


okay yeah the point
 


okay yeah the point
is


is you sample K somehow and then you
 


is you sample K somehow and then you
sample


sample X from new K so that's an
 


sample X from new K so that's an
approximate


approximate model of what what we where
 


approximate model of what what we where
our


our data comes from okay so what does it
 


our data comes from okay so what does it
mean


mean is okay what distribution do we get
 


mean is okay what distribution do we get
out


out so we get we take the new K now
 


out so we get we take the new K now
First


First We Take the K but we take it as
 


First We Take the K but we take it as
probability


probability w k and then we sample our
 


probability w k and then we sample our
X


X from new K so the probability of
 


X from new K so the probability of
actually


actually getting oh
 


actually getting oh
if


 
 


 
sum


sum it's something like this it's a
 


sum it's something like this it's a
mixture


mixture distribution right yeah so the
 


mixture distribution right yeah so the
whole


whole worldwide web data is better
 


whole worldwide web data is better
represented


represented as a mixture over different
 


represented as a mixture over different
distributions


distributions right sure sure okay so
 


distributions right sure sure okay so
that


that is our training so the
 


that is our training so the
distribution


distribution or data is roughly speaking
 


distribution or data is roughly speaking
sampled


sampled from something likei okay
 


sampled from something likei okay
rather


rather than a p Theta I mean from a very
 


rather than a p Theta I mean from a very
abstract


abstract level there's no difference but
 


abstract level there's no difference but
that


that means if you train on this data the
 


that means if you train on this data the
language


language model if it has the capacity to
 


language model if it has the capacity to
represent


represent s will then learn Cai okay and
 


represent s will then learn Cai okay and
if


if we then use it for
 


if we then use it for
prediction


prediction it will be a Cai
 


prediction it will be a Cai
predictor


predictor okay sure have a mixture but
 


predictor okay sure have a mixture but
it


it means for instance if we take the Ben
 


it means for instance if we take the Ben
T


T class we give the model we sample ABI
 


T class we give the model we sample ABI
one


one half sequence train then we sample
 


one half sequence train then we sample
ABI


ABI 0.8 sequence we sample then we
 


ABI 0.8 sequence we sample then we
sample


sample 0.1 sequence and we sample what
 


sample 0.1 sequence and we sample what
the


the language model should converge to it
 


the language model should converge to it
should


should converge to the basic mixture
 


should converge to the basic mixture
over


over this which is a plus
 


over this which is a plus
Rule


Rule and it does but we tested this and
 


Rule and it does but we tested this and
it


it perfectly does it okay which is
 


it perfectly does it okay which is
pretty


pretty amazing I see I mean it okay that
 


pretty amazing I see I mean it okay that
makes


makes a lot of sense in the bruli case
 


makes a lot of sense in the bruli case
just


just to make this more contrete in the
 


just to make this more contrete in the
linguistic


linguistic setting this would be
 


linguistic setting this would be
something


something like okay suppose for
 


something like okay suppose for
Simplicity


Simplicity you have two domains you have
 


Simplicity you have two domains you have
uh


uh children's stories and academic
 


uh children's stories and academic
articles


articles and basically as you sample
 


articles and basically as you sample
more


more and more tokens if it's really from
 


more and more tokens if it's really from
a


a children's story then the prediction
 


a children's story then the prediction
should


should be more and more children story-
 


should be more and more children story-
like


like and if it's more of a scientific
 


like and if it's more of a scientific
article


article it should be more and more
 


article it should be more and more
scientific


scientific article like right that's
 


scientific article like right that's
just


just just the linguistic way of
 


just just the linguistic way of
reformulating


reformulating what you just said in the
 


reformulating what you just said in the
beri


beri case right exactly and we tested it
 


beri case right exactly and we tested it
Beyond


Beyond benoli yeah we didn't test it for
 


Beyond benoli yeah we didn't test it for
language


language because everything is modled
 


language because everything is modled
right


right we used piecewise IID data which
 


right we used piecewise IID data which
is


is a little bit more difficult than IID
 


is a little bit more difficult than IID
also


also works okay then we use this ctw
 


also works okay then we use this ctw
distribution


distribution this highly sophisticated
 


distribution this highly sophisticated
distribution


distribution which I introduced before
 


distribution which I introduced before
and


and the model also perfectly mimics the
 


and the model also perfectly mimics the
ctw


ctw distribution in context this is
 


ctw distribution in context this is
pretty


 
 


 
it


it was even better than the ctw which
 


it was even better than the ctw which
theoretically


theoretically cannot be the case but
 


theoretically cannot be the case but
then


then we found some bugs some
 


then we found some bugs some
inconsistency


inconsistency how we train and how we do
 


inconsistency how we train and how we do
the


the inference so we fixed the bugs and
 


the inference so we fixed the bugs and
you


you know then it was equal performance
 


you know then it was equal performance
but


but really funny so this papers out
 


but really funny so this papers out
there


there um so um people can read about it
 


there um so um people can read about it
and


and then we got ambitious I mean then we
 


and then we got ambitious I mean then we
thought


thought okay you know what we really
 


thought okay you know what we really
would


would like to do is Sol to predict the
 


would like to do is Sol to predict the
solom


solom of distribution but is
 


solom of distribution but is
incomputable


incomputable let's assume we can sample
 


incomputable let's assume we can sample
from


from this
 


from this
distribution


distribution Trin a large language model
 


distribution Trin a large language model
on


on data which is sampled from Solomon
 


on data which is sampled from Solomon
of's


of's
 


of's
distribution


distribution and if everything goes
 


distribution and if everything goes
right


right then the model would be able to in
 


right then the model would be able to in
context


context emulate solom of's distribution
 


context emulate solom of's distribution
which


which is a perfect predictor for
 


which is a perfect predictor for
everything


everything okay sure actually just one
 


everything okay sure actually just one
thing


thing I I'm trying to think what could
 


thing I I'm trying to think what could
have


have gone wrong and it because why is
 


have gone wrong and it because why is
this


this uh significant and it sounds like
 


this uh significant and it sounds like
to


to me what could have gone wrong is that
 


to me what could have gone wrong is that
the


the mixture does not update in the way
 


the mixture does not update in the way
you


you expect it so it could have been
 


you expect it so it could have been
rigidly


rigidly 50/50 with this domain or that
 


rigidly 50/50 with this domain or that
domain


domain no matter how large the context
 


domain no matter how large the context
is


is right and and and uh I mean is
 


is right and and and uh I mean is
that


that way it could go wrong yeah one way
 


that way it could go wrong yeah one way
could


could go wrong yeah yeah yeah but but I
 


could go wrong yeah yeah yeah but but I
mean


mean the way it would technically go
 


mean the way it would technically go
wrong


wrong is that uh Transformers just don't
 


wrong is that uh Transformers just don't
have


have the capacity to represent these
 


have the capacity to represent these
mixtures


mixtures right the assumption is always
 


mixtures right the assumption is always
that


that set are Zer is in the class and um
 


that set are Zer is in the class and um
you


you know that a Beni Teta for a fixed
 


you know that a Beni Teta for a fixed
Teta


Teta is in the class is very plausible
 


Teta is in the class is very plausible
because


because just IID the plus is in the
 


because just IID the plus is in the
class


class counting the number of ones and
 


class counting the number of ones and
Counting


Counting zeros uh yeah I mean it's
 


Counting zeros uh yeah I mean it's
should


should be able to but they're not so
 


should be able to but they're not so
good


good at counting and also the context is
 


good at counting and also the context is
fin


fin but it still work yeah but these
 


fin but it still work yeah but these
more


more complex classes um it is up not at
 


more complex classes um it is up not at
all


all clear that um the architecture of a
 


all clear that um the architecture of a
transformer


transformer is Rich enough to represent
 


transformer is Rich enough to represent
even


even this mixture distribution I see
 


even this mixture distribution I see
what


what you're saying speak of the Training
 


what you're saying speak of the Training
Method


Method gets you there okay although
 


Method gets you there okay although
although


although language is a very complicated
 


although language is a very complicated
uh


uh uh I guess you know in some sense un
 


uh uh I guess you know in some sense un
potentially


potentially un unbounded or no no no no
 


potentially un unbounded or no no no no
fixed


fixed order markof process right so it
 


fixed order markof process right so it
it


it since it can represent language well
 


it since it can represent language well
maybe


maybe I don't know how right how okay it
 


maybe I don't know how right how okay it
could


could represent other things but okay
 


could represent other things but okay
anyways


anyways just just some offthe cuff
 


anyways just just some offthe cuff
thoughts


thoughts um okay anyway so we got to
 


thoughts um okay anyway so we got to
ambitious


ambitious and thought wow that would be
 


ambitious and thought wow that would be
amazing


amazing you know but of course we cannot
 


amazing you know but of course we cannot
even


even sample from Solomon of
 


even sample from Solomon of
distribution


distribution but we did you know we we
 


distribution but we did you know we we
we


we look we looked at some tiny
 


we look we looked at some tiny
programming


programming language where short
 


programming language where short
programs


programs already output some interesting
 


programs already output some interesting
strings


strings and you sample from it then they
 


strings and you sample from it then they
train


train the language and you know we get
 


train the language and you know we get
some


some positive results you know it's very
 


some positive results you know it's very
hard


hard to tell is it great or is it just
 


hard to tell is it great or is it just
good


good or is it it's more than just random
 


good or is it it's more than just random
garbage


garbage so it does something uh and and
 


garbage so it does something uh and and
we


we are happy with the result but for
 


we are happy with the result but for
these


these smaller model classes up to the
 


these smaller model classes up to the
ctw


ctw and the PTW it was perfect and with
 


ctw and the PTW it was perfect and with
the


the Solomon of it also does something
 


the Solomon of it also does something
reasonable


reasonable so you know maybe you know
 


reasonable so you know maybe you know
this


this is a way to go to um so Amic we
 


this is a way to go to um so Amic we
don't


don't need any data right um so all this
 


don't need any data right um so all this
is


is artificial data it's all the trained
 


is artificial data it's all the trained
on


on artificial data and then um the model
 


on artificial data and then um the model
does


does all the the actual learning from
 


does all the the actual learning from
the


the real data in context which is at the
 


the real data in context which is at the
moment


moment veryable anyway people you know
 


moment veryable anyway people you know
extend


extend the context to millions and try
 


extend the context to millions and try
to


to push everything into the context um
 


to push everything into the context um
I'm


I'm not so sure whether this is really
 


I'm not so sure whether this is really
the


the way to go
 


the way to go
um


um May maybe just to clarify I think I
 


um May maybe just to clarify I think I
think


think maybe one way to to say what I was
 


think maybe one way to to say what I was
trying


trying to or a better way of saying what
 


trying to or a better way of saying what
I


I was trying to say earlier is um so you
 


I was trying to say earlier is um so you
going


going back to the slide you you train
 


going back to the slide you you train
your


your Transformer on this data uh and you
 


your Transformer on this data uh and you
wanted


wanted to emulate the mixture
 


wanted to emulate the mixture
distribution


distribution which is this this s right
 


distribution which is this this s right
here


here and one way to evaluate how well it
 


here and one way to evaluate how well it
learned


learned it is precisely
 


learned it is precisely
to


to
 


to
uh


uh generate longer and longer samples
 


uh generate longer and longer samples
and


and see that the rate at which it conver
 


and see that the rate at which it conver
you


you know converges to the the true Mew
 


you know converges to the the true Mew
uh


uh obey it obeys the right properties
 


uh obey it obeys the right properties
right


right yeah yeah that would be one to
 


right yeah yeah that would be one to
evaluate


evaluate but in this case we could also
 


evaluate but in this case we could also
just


just I mean for at least for the ctw the
 


just I mean for at least for the ctw the
Beni


Beni we could directly compare the
 


Beni we could directly compare the
distribution


distribution to the mixture because we
 


distribution to the mixture because we
have


have the mixture
 


have the mixture
right


right I mean the practically the
 


right I mean the practically the
practically


practically eval is always against the
 


practically eval is always against the
truth


truth of course right of course yes yes
 


truth of course right of course yes yes
yes


yes but but the the the scientific
 


yes but but the the the scientific
evaluation


evaluation we could just evaluate
 


evaluation we could just evaluate
against


against the site itself oh I see right
 


against the site itself oh I see right
you


you could just compute the probabilities
 


you could just compute the probabilities
directly


directly from the model outputs and
 


directly from the model outputs and
compare


compare it to to the me in this case
 


compare it to to the me in this case
yeah


yeah oh fair enough okay so um so we're
 


yeah oh fair enough okay so um so we're
nearly


nearly done um with uh the prediction
 


nearly done um with uh the prediction
part


part but sort of as an um as a bridge to
 


part but sort of as an um as a bridge to
the


the agent part um let's consider
 


the agent part um let's consider
loss


loss functions and sort of actions but
 


loss functions and sort of actions but
let's


let's still call them predictions
 


let's still call them predictions
because


because there's a major difference
 


because there's a major difference
between


between the agent case and the
 


between the agent case and the
prediction


prediction case but the loss function
 


prediction case but the loss function
still


 
 


 
f


f um so the the protocol is similar to
 


f um so the the protocol is similar to
before


 
 


 
um


um but let's call this prediction now YT
 


um but let's call this prediction now YT
okay


 
 


 
observe


 
 


 
suffer


suffer loss
 


suffer loss
XT


XT comma YT okay uh the important point
 


XT comma YT okay uh the important point
is


is that um we do not see XT before we do
 


is that um we do not see XT before we do
the


the prediction okay um and um in general
 


the prediction okay um and um in general
I


I mean I call it prediction but this
 


I mean I call it prediction but this
could


could be in any other space so if XT is
 


could be in any other space so if XT is
in


in X YT could already be in a different
 


in X YT could already be in a different
Space


Space by um and so for instance say you
 


Space by um and so for instance say you
do


do weather forecasting uh but you can
 


do weather forecasting uh but you can
say


say you know if if it's rainy um I take
 


say you know if if it's rainy um I take
my


my umbrella and if it's sunny I take my
 


my umbrella and if it's sunny I take my
sunglasses


sunglasses something like this so the
 


sunglasses something like this so the
the


the prediction or action is actually
 


the prediction or action is actually
taking


taking umbrella or taking sunglasses or
 


taking umbrella or taking sunglasses or
they


they live in different spaces of
 


they live in different spaces of
different


different size uh um different sizes
 


different size uh um different sizes
even


even um so that's all already included
 


even um so that's all already included
the


the important point is that your
 


the important point is that your
prediction


prediction or action whatever you want
 


prediction or action whatever you want
to


to call it does not influence the
 


to call it does not influence the
environment


environment still not okay so you your
 


environment still not okay so you your
action


action taking umbrella you know or
 


action taking umbrella you know or
sunglasses


sunglasses doesn't affect the future
 


sunglasses doesn't affect the future
rathera


rathera I mean you know know there's a
 


rathera I mean you know know there's a
butterfly


butterfly effect but you know we
 


butterfly effect but you know we
assuming


assuming it there's okay so if you do
 


assuming it there's okay so if you do
this


this then um what we the best we can do
 


this then um what we the best we can do
is


is so we have the loss um we suffer but
 


is so we have the loss um we suffer but
we


we don't know M XT okay so let's just
 


we don't know M XT okay so let's just
take


take the
 


take the
expectation


expectation yeah over XT so the history
 


expectation yeah over XT so the history
is


is fixed with respect to well ideally
 


is fixed with respect to well ideally
the


the true distribution but I mean let's
 


the true distribution but I mean let's
take


take row some distribution okay so
 


take row some distribution okay so
that's


that's the expectation only over
 


that's the expectation only over
XT


XT and then we want to minimize this
 


XT and then we want to minimize this
loss


 
 


 
see


see okay and let's call this the
 


see okay and let's call this the
RO


RO uh predictor okay so the RO predictor
 


RO uh predictor okay so the RO predictor
minimizes


minimizes roow expected loss now
 


minimizes roow expected loss now
standard


standard set of nothing special so what
 


standard set of nothing special so what
is


is the real loss I suffer with the
 


is the real loss I suffer with the
strategy


 
 


 
is


is well
 


is well
whatever


whatever the outcome
 


whatever the outcome
is


is
 


is
uh


uh okay whatever the outcome is but the
 


uh okay whatever the outcome is but the
outcome


outcome is of course uncertain the true
 


outcome is of course uncertain the true
XT


XT is sampled from my true environment
 


XT is sampled from my true environment
distribution


distribution mu mu is always the true
 


distribution mu mu is always the true
environment


 
 


 
okay


okay so this is my true expected loss if
 


okay so this is my true expected loss if
I


I use
 


I use
strategy


strategy YT
 


strategy YT
row


row so so expectation with respect to S
 


row so so expectation with respect to S
above


above that's with respect to your own
 


above that's with respect to your own
model


model of the sequences not the true one
 


model of the sequences not the true one
well


well buff it was the expectation with
 


well buff it was the expectation with
dist


dist to some row not specified at the
 


dist to some row not specified at the
moment


moment oh okay I see okay okay but this
 


moment oh okay I see okay okay but this
is


is not the true expected loss true
 


is not the true expected loss true
expected


expected loss I have to take the
 


expected loss I have to take the
expectation


expectation with to the truth right sure
 


expectation with to the truth right sure
sure


sure I use I use the row expected
 


sure I use I use the row expected
predictor


predictor evaluate it with respect to
 


predictor evaluate it with respect to
the


the truth of course that's right that's
 


the truth of course that's right that's
right


right yes yes so now I take also the
 


right yes yes so now I take also the
expectation


expectation over the history so I just
 


expectation over the history so I just
take


take the expectation
 


take the expectation
here


 
 


 
instances


instances uh let's sample
 


instances uh let's sample
in


in um yeah I probably don't need really
 


in um yeah I probably don't need really
I


I can't do use Infinity okay to use your
 


I can't do use Infinity okay to use your
weather


weather analogy row could be like how to
 


weather analogy row could be like how to
best


best choose your your uh um weather
 


best choose your your uh um weather
prediction


prediction say in the United States and
 


prediction say in the United States and
then


then maybe now you live in London and
 


then maybe now you live in London and
then


then mu is the London uh distribution of
 


then mu is the London uh distribution of
weather


weather right so so sort of you could
 


weather right so so sort of you could
incur


incur loss because of the mismatch for
 


incur loss because of the mismatch for
example


example yeah yeah yeah yeah okay so um
 


example yeah yeah yeah yeah okay so um
let's


let's call it loss of so loss of row is
 


let's call it loss of so loss of row is
the


the total loss lifetime loss expected
 


the total loss lifetime loss expected
loss


loss with respect to the true
 


loss with respect to the true
distribution


distribution over the for the row
 


distribution over the for the row
predictor


predictor okay um obviously by
 


predictor okay um obviously by
construction


construction if I would minimize with
 


construction if I would minimize with
respect


respect to the true distribution that is
 


respect to the true distribution that is
the


the optimal thing to do right this is
 


the optimal thing to do right this is
smaller


smaller than the loss of row right
 


smaller than the loss of row right
yes


yes of course so if if I minimize with
 


yes of course so if if I minimize with
respect


respect to the to the Criterion right
 


respect to the to the Criterion right
you


you know then of course this is better
 


you know then of course this is better
than


than doing anything else okay that's
 


than doing anything else okay that's
fine


fine but again as before we do not know
 


fine but again as before we do not know
me


me so we have to do something else right
 


me so we have to do something else right
and


and you could be a machine learner and
 


and you could be a machine learner and
learn


learn M somehow or you a be and you just
 


learn M somehow or you a be and you just
repl


repl the M by a Bas Mak side
 


repl the M by a Bas Mak side
okay


okay so L
 


okay so L
moon


moon is the best you could ever achieve
 


moon is the best you could ever achieve
if


if you know
 


if you know
me


 
 


 
look


look if we use
 


look if we use
Cai


Cai so we use the we minimize the Cai
 


Cai so we use the we minimize the Cai
expected


expected loss and compare these two
 


expected loss and compare these two
losses


losses
 


losses
okay


okay and now I have to do a little bit
 


okay and now I have to do a little bit
of


of a funny trick
 


of a funny trick
um


um I take the square root here and the
 


um I take the square root here and the
square


square here because otherwise I I can
 


square here because otherwise I I can
get


get a bound but it's ugly so I just
 


get a bound but it's ugly so I just
don't


don't compare the losses but the square
 


don't compare the losses but the square
of


of the losses okay just the difference
 


of the losses okay just the difference
is


is called the helinger loss okay um and
 


is called the helinger loss okay um and
oh


oh no I don't take any
 


oh no I don't take any
Square


Square okay and ask well if this is
 


Square okay and ask well if this is
bounded


bounded by a constant for
 


bounded by a constant for
instance


instance then I would know if the
 


instance then I would know if the
optimal


optimal muos is finite then the optimal
 


optimal muos is finite then the optimal
cyos


cyos is finite which is great right
 


cyos is finite which is great right
remember


remember over infinity many steps if the
 


remember over infinity many steps if the
loss


loss of mu sort of has a certain frequen
 


loss of mu sort of has a certain frequen
see


see you know maybe 50% or something then
 


see you know maybe 50% or something then
the


the cyos also at the same frequency
 


the cyos also at the same frequency
assuming


assuming there's a finite bound and
 


assuming there's a finite bound and
amazing


amazing thing is there is a finite bound
 


amazing thing is there is a finite bound
finite


finite bound as usual is the Ln W of mu
 


finite bound as usual is the Ln W of mu
to


to the minus one now with a square
 


to the minus one now with a square
root


root is
 


root is
there


 
 


 
say


say and for the specific case of Solomon
 


say and for the specific case of Solomon
of


of distribution right is this is this
 


of distribution right is this is this
loss


loss like I'm surprised it should be
 


loss like I'm surprised it should be
even


even finite in the first place right so
 


even finite in the first place right so
it's


it's sort of like if it's like if you
 


it's sort of like if it's like if you
have


have a for example if your loss is 01
 


have a for example if your loss is 01
accuracy


accuracy
 


accuracy
then


then this loss function will generically
 


then this loss function will generically
have


have an infinite sum over ones right is
 


have an infinite sum over ones right is
there


there some
 


there some
discounting


discounting maybe I'm look different
 


discounting maybe I'm look different
assume


assume you want to predict say assume
 


assume you want to predict say assume
the


the weather is uh
 


the weather is uh
Bly


Bly 0.7 or something like this okay yeah
 


Bly 0.7 or something like this okay yeah
ah


ah okay sorry keep going yep M and you
 


ah okay sorry keep going yep M and you
have


have a loss function and you choose your
 


have a loss function and you choose your
umbrella


umbrella your sunglasses of course you
 


umbrella your sunglasses of course you
often


often get it wrong right mhm so you will
 


often get it wrong right mhm so you will
suffer


suffer a loss right um which okay well
 


suffer a loss right um which okay well
actually


actually so I need the nend because
 


actually so I need the nend because
that's


that's Infinity right so I that's what I
 


that's Infinity right so I that's what I
was


was that's the point I was make yes I
 


was that's the point I was make yes I
need


need up to n and then this is n and this
 


need up to n and then this is n and this
is


is n um but then I can yeah sorry then I
 


is n um but then I can yeah sorry then I
can


can take the limit end to Infinity on
 


can take the limit end to Infinity on
the


the left hand side sorry
 


the left hand side sorry
yeah


 
 


 
and


and n
 


and n
n


n n
 


n n
yeah


yeah yeah Essen this is like a like a
 


yeah yeah Essen this is like a like a
regret


regret bound essentially right yes
 


regret bound essentially right yes
except


except that there's a we s sure exactly
 


except that there's a we s sure exactly
yeah


yeah yeah yeah okay you're right right
 


yeah yeah yeah okay you're right right
yeah


yeah yeah but you can convert it over um
 


yeah yeah but you can convert it over um
so


so but this is the best way way to see
 


so but this is the best way way to see
it


it in the simplest way and the fastest
 


it in the simplest way and the fastest
um


um a very neat bound um okay so that
 


um a very neat bound um okay so that
shows


shows ah I should have this bound holds
 


shows ah I should have this bound holds
if


if the loss function um um is in 01 so
 


if the loss function um um is in 01 so
it


it must be a bounded loss function any
 


it must be a bounded loss function any
bounded


bounded loss function just scale it but
 


bounded loss function just scale it but
if


if it's unbounded more complicated okay
 


if it's unbounded more complicated okay
so


so not not cross entropy okay yeah sure
 


so not not cross entropy okay yeah sure
something


something like that yeah
 


something like that yeah
so


so the nice thing is that Solomon of
 


so the nice thing is that Solomon of
prediction


prediction is also optimal in in this
 


prediction is also optimal in in this
sense


sense with respect to any loss function
 


sense with respect to any loss function
you


you care about not just the square
 


you care about not just the square
distance


distance between the distributions or
 


distance between the distributions or
the


the log loss or something like this so
 


the log loss or something like this so
just


just just
 


just just
maybe


maybe make one remark yeah because of
 


maybe make one remark yeah because of
the


the optimality that we discussed earlier
 


the optimality that we discussed earlier
which


which is that if you have any
 


which is that if you have any
computable


computable
 


computable
measure


measure then you're going to assign less
 


measure then you're going to assign less
weight


weight up to you know constants than the
 


weight up to you know constants than the
Salamon


Salamon of weight which is 2 the minus
 


Salamon of weight which is 2 the minus
color


color of complexity then this upper
 


color of complexity then this upper
bound


bound becomes larger because of this you
 


bound becomes larger because of this you
know


know log of a reciprocal right so in
 


know log of a reciprocal right so in
this


this sense the Solomon of Prior gives
 


this sense the Solomon of Prior gives
you


you the best convergence rate and also
 


you the best convergence rate and also
the


the the the optimal regret right which
 


the the the optimal regret right which
is


is the the minimal regret right yeah
 


is the the minimal regret right yeah
this


this is this is amazing this is great
 


this is this is amazing this is great
yeah


yeah it
 


yeah it
is


is
 


is
okay


okay with a nice or maybe we can start
 


okay with a nice or maybe we can start
the


the next section or you can cut it if
 


the next section or you can cut it if
you


you like so let's let's end um or start
 


you like so let's let's end um or start
with


with a a nice quote um of Gregory uh
 


with a a nice quote um of Gregory uh
sorry


sorry of um of Marvin
 


sorry of um of Marvin
Minsky


Minsky um uh when he was quite old 2011
 


Minsky um uh when he was quite old 2011
and


and he never worked on colol complex
 


and he never worked on colol complex
Solomon


Solomon of induction but there was an a
 


Solomon of induction but there was an a
panel


panel discussion with him and Gregory
 


panel discussion with him and Gregory
chartin


chartin and some others and it was one
 


chartin and some others and it was one
and


and a half hours and the last statement
 


and a half hours and the last statement
this


this whole conversation by Marvin vinsky
 


this whole conversation by Marvin vinsky
was


was and I'm paraphrasing I don't know
 


was and I'm paraphrasing I don't know
exactly


exactly but pretty much um Solomon of's
 


exactly but pretty much um Solomon of's
theory


theory is a beautiful theory about
 


theory is a beautiful theory about
prediction


prediction everyone should know
 


prediction everyone should know
everything


everything about this Theory and spend
 


everything about this Theory and spend
the


the rest of their lives working on it
 


the rest of their lives working on it
wow


wow
 


wow
okay


okay okay noted so why don't we now
 


okay okay noted so why don't we now
start


start on part two of our outline we've
 


start on part two of our outline we've
let's


let's just maybe quickly recap what
 


let's just maybe quickly recap what
we've


we've done uh part one was called
 


we've done uh part one was called
Universal


Universal prediction and we explain how
 


Universal prediction and we explain how
well


well there's there's the bean setup
 


well there's there's the bean setup
which


which tells you how to do prediction
 


which tells you how to do prediction
once


once you update your priors to get
 


once you update your priors to get
posteriors


posteriors that are now data informed
 


posteriors that are now data informed
data


data revised uh we introduced color of
 


data revised uh we introduced color of
complexity


complexity so we could talk about the
 


complexity so we could talk about the
Solon


Solon of mixture distribution or Solon
 


Solon of mixture distribution or Solon
of


of Prior however you want to call it um
 


of Prior however you want to call it um
and


and that gave us a prior that had
 


and that gave us a prior that had
optimal


optimal convergence properties both
 


optimal convergence properties both
respect


respect to prediction and loss um and
 


respect to prediction and loss um and
then


then we also discuss some interesting
 


then we also discuss some interesting
model


model classes um but what we've left out
 


model classes um but what we've left out
is


is action planning right because at the
 


is action planning right because at the
end


end of the day intelligence requires uh
 


end of the day intelligence requires uh
interacting


interacting with the world those actions
 


interacting with the world those actions
will


will influence the world and so we want
 


will influence the world and so we want
sort


sort of the action version of this and
 


sort of the action version of this and
that's


that's where the universal artificial
 


that's where the universal artificial
intelligence


intelligence comes in and this is where
 


intelligence comes in and this is where
your


your your contribution comes in right so
 


your your contribution comes in right so
is


is that is that a fair summary of where
 


is that is that a fair summary of where
we


we are yes I mean my contributions I
 


we are yes I mean my contributions I
mean


mean apart from theom of the original
 


mean apart from theom of the original
bound


bound everything else I told was also
 


bound everything else I told was also
sort


sort of a lot of my contributions and
 


sort of a lot of my contributions and
there's


there's there's much more I have done
 


there's there's much more I have done
there's


there's this paper um a philosophical
 


there's this paper um a philosophical
treaties


treaties of universal induction there
 


treaties of universal induction there
are


are so many other bounds and interesting
 


are so many other bounds and interesting
insites


insites you can have right where
 


insites you can have right where
unfortunately


unfortunately there's never time to get
 


unfortunately there's never time to get
to


to this um so but I mean this paper in a
 


to this um so but I mean this paper in a
lightweight


lightweight form and there's an older
 


lightweight form and there's an older
paper


paper in a more heavyweight form
 


paper in a more heavyweight form
contains


contains sort of a summary of all the
 


contains sort of a summary of all the
other


other stuff related to Solomon of
 


other stuff related to Solomon of
induction


induction itself yeah but the major
 


induction itself yeah but the major
contribution


contribution is indeed um uh to to the
 


contribution is indeed um uh to to the
active


active agen Cas yeah she of course your
 


active agen Cas yeah she of course your
your


your books will be a proper
 


your books will be a proper
monograph


monograph giving citations and
 


monograph giving citations and
references


references so people who want to know
 


references so people who want to know
more


more about the contribution isues can
 


more about the contribution isues can
can


can refer to that um but okay great so
 


can refer to that um but okay great so
maybe


maybe should we just get started in in
 


maybe should we just get started in in
section


section two um yeah we're pretty
 


section two um yeah we're pretty
open-ended


open-ended I guess basically we we want
 


open-ended I guess basically we we want
to


to adapt how I say kind of technically
 


to adapt how I say kind of technically
we've


we've I my sense is we've done uh a lot
 


we've I my sense is we've done uh a lot
of


of the work the planning part is sort of
 


of the work the planning part is sort of
at


at least at a high level a modification
 


at least at a high level a modification
of


of that and that okay now you just add
 


of that and that okay now you just add
this


this uh you know action part and you can
 


this uh you know action part and you can
talk


talk about kind of the same theorems
 


talk about kind of the same theorems
they


they can they they carry over and now
 


they can they they carry over and now
it's


it's a matter of reinterpreting it of
 


it's a matter of reinterpreting it of
course


course there's many other uh Downstream
 


course there's many other uh Downstream
consequences


consequences of that but at a high level
 


consequences of that but at a high level
I


I think we've done most of the technical
 


I think we've done most of the technical
conceptual


conceptual work and now it's sort of
 


conceptual work and now it's sort of
shoehorning


shoehorning in the the action part and
 


shoehorning in the the action part and
you


you can ask the same questions
 


you can ask the same questions
right


right yeah um you can ask the same
 


right yeah um you can ask the same
questions


questions but you don't get the same
 


questions but you don't get the same
answers


answers ah
 


answers ah
okay


okay the active case is much more
 


okay the active case is much more
complicated


complicated and and much more tricky I
 


complicated and and much more tricky I
mean


mean we I mean apart from the base
 


mean we I mean apart from the base
optimal


optimal notion which is sort of an
 


optimal notion which is sort of an
optimal


optimal notion which is I would say Good
 


optimal notion which is I would say Good
by


by definition or I mean from the
 


by definition or I mean from the
foundational


foundational um derivation of of of
 


foundational um derivation of of of
probability


probability in base um it's it's very
 


probability in base um it's it's very
difficult


difficult to come up with Notions of
 


difficult to come up with Notions of
optimality


optimality in the agent case which are
 


optimality in the agent case which are
on


on the one hand strong enough to be
 


on the one hand strong enough to be
convincing


convincing but on the other hand weak
 


convincing but on the other hand weak
enough


enough so that real agents can actually
 


enough so that real agents can actually
satisfy


satisfy them so that that even the the
 


satisfy them so that that even the the
right


right notion of optimality is still
 


right notion of optimality is still
still


still open I would say unlike in the
 


still open I would say unlike in the
prediction


prediction case where use regret or
 


prediction case where use regret or
redundancy


redundancy and sort of um you know you
 


redundancy and sort of um you know you
you


you don't have to compromise much there
 


you don't have to compromise much there
anyway


anyway okay let's start with the agents
 


anyway okay let's start with the agents
and


 
 


 
moment


moment for a second um let us forget
 


moment for a second um let us forget
about


about solomonov and all these kinds of
 


about solomonov and all these kinds of
stuff


stuff but start very simple let's assume
 


stuff but start very simple let's assume
we


we know the world right what happens
 


we know the world right what happens
what's


what's happening okay so the setup is
 


what's happening okay so the setup is
the


the following I mean it's a typical
 


the following I mean it's a typical
agent


agent environment setup and you know
 


agent environment setup and you know
soon


soon the rewards come in I draw the
 


soon the rewards come in I draw the
picture


picture a little bit different from the
 


picture a little bit different from the
standard


standard way because um you know
 


standard way because um you know
standard


standard Isis is usually you know MVP
 


standard Isis is usually you know MVP
based


based and um that is too limiting for
 


based and um that is too limiting for
AGI


AGI agents right we want something at
 


AGI agents right we want something at
least


least when we start uh something which
 


least when we start uh something which
is


is you know General um that means
 


is you know General um that means
general


general history based environments and
 


general history based environments and
policies


policies and so the picture I usually
 


policies and so the picture I usually
draw


 
 


 
agent


agent
 


agent
and


 
 


 
envir


envir an
 


envir an
environment


environment and the interact with each
 


environment and the interact with each
other


other um and the agent as well as the
 


other um and the agent as well as the
environment


environment they can remember stuff
 


environment they can remember stuff
arbitrary


arbitrary long in the past so let's sort
 


arbitrary long in the past so let's sort
of


of put this sort of on a tape
 


of put this sort of on a tape
yeah


yeah so there's a sequence of
 


yeah so there's a sequence of
perceptions


perceptions so e are
 


perceptions so e are
perceptions


perceptions that was the X before
 


perceptions that was the X before
the


the agent can have okay and um um once
 


the agent can have okay and um um once
it


it has a perception it can act
 


it has a perception it can act
somehow


somehow is perception the same as
 


somehow is perception the same as
observation


observation
 


observation
no


no
 


no
okay


okay you will see in a second okay okay
 


okay you will see in a second okay okay
um


um so and
 


um so and
um


um the agent then acts
 


um the agent then acts
somehow


somehow and um and it you know this
 


somehow and um and it you know this
action


action can depend on the whole history
 


action can depend on the whole history
of


of of perceptions up to time step three
 


of of perceptions up to time step three
and


and then the
 


and then the
environment


environment gets and can remember all
 


environment gets and can remember all
the


the past actions of the agent um if it
 


the past actions of the agent um if it
wants


wants to um and then produces a new
 


wants to um and then produces a new
percept


percept so that's the reason why it
 


percept so that's the reason why it
connects


connects to E4 that's the next time step
 


connects to E4 that's the next time step
produce


produce a new percept and then um the
 


produce a new percept and then um the
cycle


cycle um continues so formally what we
 


cycle um continues so formally what we
have


have is we have an
 


have is we have an
agent


agent which we call I mean the the the
 


agent which we call I mean the the the
the


the behavior of the agent is called a
 


the behavior of the agent is called a
policy


policy right which takes and here in
 


policy right which takes and here in
this


this picture it looks like that the
 


this picture it looks like that the
agent


agent um only depends on the on the on
 


agent um only depends on the on the on
the


the perceptions but the agent can also
 


the perceptions but the agent can also
remember


remember its own actions I mean I don't
 


remember its own actions I mean I don't
need


need to sort of because it produces the
 


need to sort of because it produces the
actions


actions because he knows its own actions
 


actions because he knows its own actions
so


so the policy is a mapping of past
 


so the policy is a mapping of past
actions


actions and
 


actions and
perceptions


perceptions up to the star means sort of
 


perceptions up to the star means sort of
1


1 two 3 four five and produces a new
 


1 two 3 four five and produces a new
action


action and uh the
 


action and uh the
environment


environment um is also having access to
 


environment um is also having access to
the


the whole history of acent perceptions
 


the whole history of acent perceptions
and


and the last action of the agent and
 


and the last action of the agent and
then


then comes with a new
 


then comes with a new
perception


perception and um since the policy and
 


perception and um since the policy and
environment


environment can be stochastic you
 


environment can be stochastic you
know


know I just write this as a weekly error
 


know I just write this as a weekly error
weekly


weekly means it can be a stochastic
 


weekly means it can be a stochastic
sort


sort of you cross a two one something
 


sort of you cross a two one something
like


like this okay so and this
 


like this okay so and this
interaction


interaction um produces an action
 


interaction um produces an action
observation


observation action perception sequence
 


observation action perception sequence
so


so e t um is sampled from new of e given
 


so e t um is sampled from new of e given
and


and now I use this glue notation
 


and now I use this glue notation
AE


AE a sometimes disappears here a is less
 


AE a sometimes disappears here a is less
than


than T and the last action of the agent
 


than T and the last action of the agent
a


a e so AE sort of glue together so
 


a e so AE sort of glue together so
action


action one perception one action two
 


action one perception one action two
perception


perception two up to T minus one and the
 


perception two up to T minus one and the
actions


actions are sampled from the policy or
 


actions are sampled from the policy or
with


with theistic policy is just you know
 


with theistic policy is just you know
ttic


ttic output action given the
 


ttic output action given the
action


action perception history right okay you
 


action perception history right okay you
start


start at tal one there's no conditions
 


start at tal one there's no conditions
and


and you generate them sort of you know
 


and you generate them sort of you know
based


based on the so this a this is AE uh two
 


based on the so this a this is AE uh two
like


like the all the pairs of AE up to less
 


like the all the pairs of AE up to less
than


than T right just to emphas just so so
 


than T right just to emphas just so so
the


the joint distribution so we can do so
 


the joint distribution so we can do so
if


if we take so we we start at tal one and
 


if we take so we we start at tal one and
the


the first thing is the agent's action um
 


the first thing is the agent's action um
so


so it produces an action with a certain
 


so it produces an action with a certain
probability


probability so T is equal one so there's
 


probability so T is equal one so there's
no


no history it's just the first action
 


no history it's just the first action
then


 
 


 
action


action
 


action
um


um and then you know T 2 starts tal 3 up
 


um and then you know T 2 starts tal 3 up
to


to say tal say assume the agent di times
 


to say tal say assume the agent di times
that


that M so for Simplicity I just assume
 


that M so for Simplicity I just assume
that


that the agent has a very long Horizon M
 


that the agent has a very long Horizon M
say


say lives for 100 years and then dies
 


say lives for 100 years and then dies
right


right so um but you can also do infinite
 


right so um but you can also do infinite
Horizon


Horizon but then you need discounting
 


Horizon but then you need discounting
for


for the rewards and so on Infinities is
 


for the rewards and so on Infinities is
always


always a little bit annoying so I I I
 


always a little bit annoying so I I I
keep


keep it simple here so this is a joint
 


keep it simple here so this is a joint
distribution


distribution which generates the action
 


distribution which generates the action
perception


perception sequence up to M and which we
 


perception sequence up to M and which we
by


by new P okay it maybe I should this off
 


by new P okay it maybe I should this off
E1


E1 2 m so that's the probability of this
 


E1 2 m so that's the probability of this
act


 
 


 
e


e a e 1 2 m so that's the probability of
 


e a e 1 2 m so that's the probability of
the


the action perception
 


the action perception
sequence


sequence um if environment new if agent
 


sequence um if environment new if agent
Pi


Pi interacts with environment
 


Pi interacts with environment
new


new so now um we ask what does the agent
 


new so now um we ask what does the agent
do


do right and we want it to do well with
 


do right and we want it to do well with
respect


respect to some Criterion and in
 


respect to some Criterion and in
reinforcement


reinforcement D you have a reward signal
 


reinforcement D you have a reward signal
um


um so we split the perception into an
 


um so we split the perception into an
observation


observation and a re
 


observation and a re
yeah


yeah so the perception
 


yeah so the perception
obss


 
 


 
R2


R2 and so on so when I write e it's
 


R2 and so on so when I write e it's
always


always with here um o r okay and the
 


always with here um o r okay and the
goal


goal of the agent should be to maximize
 


goal of the agent should be to maximize
its


 
 


 
lifetime


lifetime up to M but these rewards are
 


lifetime up to M but these rewards are
you


you know are stochastic right you know I
 


you know are stochastic right you know I
mean


mean definitely the environment I mean
 


mean definitely the environment I mean
you


you know either the environment is
 


you know either the environment is
itself


itself stochastic or later they have a
 


itself stochastic or later they have a
belief


belief over the environments so um this
 


belief over the environments so um this
happens


happens this reward sequence happens
 


happens this reward sequence happens
with


with a certain probability right and
 


with a certain probability right and
what's


what's the probability well the
 


what's the probability well the
probability


probability is new PI right but the
 


probability is new PI right but the
policy


policy Pi we want to choose right we
 


policy Pi we want to choose right we
only


only care about the stochasticity of the
 


only care about the stochasticity of the
environment


environment so the probability of
 


environment so the probability of
observing


observing a certain perception sequence
 


observing a certain perception sequence
given


given a certain action sequence
 


given a certain action sequence
um


um actually let me I come to that later
 


um actually let me I come to that later
let's


let's keep it
 


let's keep it
simple


simple so um this
 


simple so um this
produces


produces um uh this reward sequence but
 


produces um uh this reward sequence but
with


with some certain probability so let's
 


with some certain probability so let's
take


take the expectation so the expectations
 


take the expectation so the expectations
respect


respect to the environment new and with
 


respect to the environment new and with
respect


respect to the policy Pi okay and this
 


respect to the policy Pi okay and this
expected


expected reward
 


expected reward
sum


sum is called the value of agent high in
 


sum is called the value of agent high in
environment


environment new sorry your I think your
 


environment new sorry your I think your
RS


RS and your news look similar that's an
 


RS and your news look similar that's an
expectation


expectation of rewards is that
 


expectation of rewards is that
right


right well yeah in in in the squares is
 


right well yeah in in in the squares is
ours


ours okay a new sory yeah your ARS news
 


ours okay a new sory yeah your ARS news
okay


okay all right I think you know what
 


okay all right I think you know what
they


they
 


they
have


have anyway okay anyways okay okay so so
 


have anyway okay anyways okay okay so so
that's


that's the value of agent Pi in
 


that's the value of agent Pi in
environment


environment new
 


environment new
okay


okay so if we know the true environment
 


okay so if we know the true environment
okay


okay by the way and you're you're are
 


okay by the way and you're you're are
you


you absorbing the discount factor into
 


you absorbing the discount factor into
the


the rewards then h no discounting here
 


the rewards then h no discounting here
because


because it dies at Horizon M oh ah I see
 


because it dies at Horizon M oh ah I see
okay


okay yeah I I could do discounting and
 


okay yeah I I could do discounting and
then


then I can take the limit M to infinity
 


then I can take the limit M to infinity
and


and all the EX and they're also more
 


and all the EX and they're also more
useful


useful so but I'm not doing that and
 


useful so but I'm not doing that and
there's


there's also you don't want geometric
 


there's also you don't want geometric
discounting


discounting because for any gamma you
 


discounting because for any gamma you
choose


choose any gamma you choose corresponds
 


choose any gamma you choose corresponds
to


to effective Horizon which is one
 


to effective Horizon which is one
divided


divided by one minus gamma so if you
 


divided by one minus gamma so if you
choose


choose gamma 0.99 it corresponds to an
 


choose gamma 0.99 it corresponds to an
effective


effective Horizon 100 but there are
 


effective Horizon 100 but there are
problems


problems which need larger Horizon if
 


problems which need larger Horizon if
you


you choose gamma
 


you choose gamma
0.999


0.999 it's Horizon thousand but there
 


0.999 it's Horizon thousand but there
are


are problems with need Horizon but there
 


are problems with need Horizon but there
discount


discount schemes which are not geometric
 


discount schemes which are not geometric
which


which generate agents with a growing
 


which generate agents with a growing
Horizon


Horizon for instance proportional to the
 


Horizon for instance proportional to the
age


age of the agent makes a lot of sense
 


age of the agent makes a lot of sense
right


right but I don't want to go into
 


right but I don't want to go into
this


this um okay so so if we know the true
 


this um okay so so if we know the true
environment


environment then our true the agents Pi
 


environment then our true the agents Pi
true


true
 


true
expected


expected um uh reward sum so the true
 


expected um uh reward sum so the true
value


value is V Pi mu and well we want to
 


value is V Pi mu and well we want to
maximize


maximize this right so me me is the true
 


maximize this right so me me is the true
environment


environment and new is the uh new some
 


environment and new is the uh new some
environment


environment some got it okay yes yes so
 


environment some got it okay yes yes so
that


that it's easier to read
 


that it's easier to read
okay


okay okay um okay and this let's call it
 


okay okay um okay and this let's call it
P


P start okay but um so that would be the
 


P start okay but um so that would be the
optimal


optimal policy right if we knew in which
 


optimal policy right if we knew in which
mind


mind we are
 


mind we are
okay


okay okay but well we don't know me so
 


okay okay but well we don't know me so
what


what do we do yeah okay we learn me how
 


what do we do yeah okay we learn me how
do


 
 


 
basan


basan what what ban
 


basan what what ban
updating


updating yeah ban updating which means
 


updating yeah ban updating which means
if


if in the predictive formulation you
 


if in the predictive formulation you
just


just replace it by the mixture distri
 


just replace it by the mixture distri
yeah


yeah yeah yeah exactly and then and then
 


yeah yeah yeah exactly and then and then
as


as you make more observations you'll get
 


as you make more observations you'll get
convergence


convergence exactly okay so now let's
 


convergence exactly okay so now let's
define


define also Pi s Bas mixture um well the
 


define also Pi s Bas mixture um well the
ban


ban mixture is pretty much obvious I
 


ban mixture is pretty much obvious I
mean


mean everything is now conditioned on
 


mean everything is now conditioned on
action


action so I mean I could write it down s
 


action so I mean I could write it down s
of


of um X1 to M um so E1 to M given
 


of um X1 to M um so E1 to M given
actions


actions A1 to m is equal to well new
 


actions A1 to m is equal to well new
e


e E1 2 m given A1 um so there's some
 


e E1 2 m given A1 um so there's some
suby


suby this is not the real prob
 


suby this is not the real prob
conditional


conditional probability but I I I use a
 


conditional probability but I I I use a
double


double bar so here is sort of Remember
 


double bar so here is sort of Remember
at


at the beginning I said there's a
 


at the beginning I said there's a
difference


difference between conditioning um and
 


difference between conditioning um and
indexing


indexing so that is actually indexing on
 


indexing so that is actually indexing on
a


a and here it makes difference um so so
 


a and here it makes difference um so so
uh


uh some people have spent a lot of time
 


uh some people have spent a lot of time
and


and figuring out what is going on here I
 


and figuring out what is going on here I
mean


mean in one sense it's trivial in
 


mean in one sense it's trivial in
another


another sense it's really subtle what I
 


another sense it's really subtle what I
call


call them I call them chronological semi
 


call them I call them chronological semi
measures


measures say Chron probability
 


measures say Chron probability
distributions


distributions which satisfy the
 


distributions which satisfy the
causality


causality constraints um but again we
 


causality constraints um but again we
don't


don't have to go into it it's it's in
 


don't have to go into it it's it's in
some


some sense it is is obvious okay so now
 


some sense it is is obvious okay so now
um


um this P P that is what we want P but
 


um this P P that is what we want P but
we


we cannot it because we don't know so we
 


we cannot it because we don't know so we
use


use this policy Pi starai okay which I
 


use this policy Pi starai okay which I
call


call
 


call
AI


AI xai it's not the I model yet but we
 


AI xai it's not the I model yet but we
are


are close Okay so and now we want to
 


are close Okay so and now we want to
compare


compare the performance of this basan
 


compare the performance of this basan
agent


agent to the agent which knows the world
 


agent to the agent which knows the world
okay


okay so how can we do that well we look
 


okay so how can we do that well we look
at


at the value but this Cai value is just
 


at the value but this Cai value is just
sort


sort of a a tool for creating a policy
 


sort of a a tool for creating a policy
this


this has no meaning right um I mean not
 


this has no meaning right um I mean not
not


not not direct what we really receive is
 


not not direct what we really receive is
the


the mo value okay so but we act with a
 


the mo value okay so but we act with a
Cy


Cy optimal policy okay so we ask what is
 


Cy optimal policy okay so we ask what is
the


the true mu expected reward th if we act
 


the true mu expected reward th if we act
with


with aide policy and we compare this
 


with aide policy and we compare this
to


to the
 


to the
optimal


optimal mu agent
 


optimal mu agent
okay


okay this optimal agent of of course has
 


okay this optimal agent of of course has
smaller


smaller value than any other policy by
 


smaller value than any other policy by
construction


construction because it's just an
 


construction because it's just an
argument


argument but what is this relation
 


argument but what is this relation
between


between the base agent and uh the
 


between the base agent and uh the
optimal


optimal informed agent and well in the
 


optimal informed agent and well in the
predictive


predictive setting we proved um this
 


predictive setting we proved um this
loss


loss bound so soone of bound and um and
 


loss bound so soone of bound and um and
so


so on and in active
 


so on and in active
setting


setting under certain extra conditions
 


setting under certain extra conditions
you


you can also prove
 


you can also prove
asymptotically


asymptotically that they converge to
 


asymptotically that they converge to
each


each other on average um you can prove
 


each other on average um you can prove
So-Cal


So-Cal self-optimizing results you can
 


So-Cal self-optimizing results you can
prove


prove um asymptotic optimality results
 


prove um asymptotic optimality results
there's


there's weak optimality strong
 


there's weak optimality strong
optimality


optimality optimality in Cesaro sense
 


optimality optimality in Cesaro sense
there


there there's lots of different
 


there there's lots of different
optimality


optimality Notions and there but they
 


optimality Notions and there but they
all


all come with conditions under which
 


all come with conditions under which
this


this holds so they're not condition free
 


this holds so they're not condition free
by


by I think you meant lower bound we're
 


by I think you meant lower bound we're
trying


trying to maximize reward here
 


trying to maximize reward here
right


right AR Max sorry
 


right AR Max sorry
oh


oh yes yeah so this is approximately
 


oh yes yeah so this is approximately
under


 
 


 
conditions


conditions on M I mean for instance this
 


conditions on M I mean for instance this
framework


framework is General enough that if you
 


framework is General enough that if you
choose


choose a class of environments M which
 


choose a class of environments M which
do


do not depend on the action so if I cut
 


do not depend on the action so if I cut
this


this Arrow here from the action tape to
 


this Arrow here from the action tape to
the


the environment so um so in the diagram
 


the environment so um so in the diagram
um


um so if the new respects this so it
 


um so if the new respects this so it
does


does not depend on the action so here I
 


does not depend on the action so here I
made


made an error if it does not depend on
 


made an error if it does not depend on
the


the
 


the
actions


actions then we are back in the passive
 


actions then we are back in the passive
prediction


prediction setting and for instance for
 


prediction setting and for instance for
this


this class of models right um we get the
 


this class of models right um we get the
solo


solo one of bound de yeah but we get
 


solo one of bound de yeah but we get
also


also bounds Beyond this class of course
 


also bounds Beyond this class of course
otherwise


otherwise it would be
 


otherwise it would be
B


B um okay um so finally that is nice
 


B um okay um so finally that is nice
but


but we have to choose a class and from
 


but we have to choose a class and from
an


an AGI perspective right we want the
 


an AGI perspective right we want the
class


class to be I mean from a technical
 


class to be I mean from a technical
perspective


perspective we want it to be as small as
 


perspective we want it to be as small as
possible


possible to deal with it practically but
 


possible to deal with it practically but
from


from an AI perspective want to big
 


from an AI perspective want to big
enough


enough so the true world is in the class
 


enough so the true world is in the class
and


and I mean if you look at physical
 


and I mean if you look at physical
theories


theories standard model uh generativity
 


theories standard model uh generativity
and


and whatever you know they're all sort
 


and whatever you know they're all sort
of


of computable theories I mean very hard
 


of computable theories I mean very hard
to


to compute but they're all computable so
 


to compute but they're all computable so
again


again if you take the class of
 


again if you take the class of
computable


computable probability distributions we
 


computable probability distributions we
should


should be pretty safe that the true
 


should be pretty safe that the true
world


world is in the class yeah so again so
 


world is in the class yeah so again so
we


we choose M to be
 


we choose M to be
mu


mu which is all
 


mu which is all
computable


computable computable in this case lower
 


computable computable in this case lower
semi


semi computable chronological semi
 


semi computable chronological semi
measures


measures if you want to be precise yeah
 


measures if you want to be precise yeah
but


but roughly speaking all computable
 


but roughly speaking all computable
probability


probability
 


probability
you


you and as prior as before we choose two
 


you and as prior as before we choose two
to


to the minus theog complexity of
 


to the minus theog complexity of
then


then you get aai which is th U and this
 


then you get aai which is th U and this
is


 
 


 
arim


arim okay and now if you think even
 


arim okay and now if you think even
intuitively


intuitively about what this model does
 


intuitively about what this model does
it


it it combines Universal sequence
 


it it combines Universal sequence
prediction


prediction which we have shown to be
 


prediction which we have shown to be
optimal


optimal with optimal decision making in
 


optimal with optimal decision making in
known


known environments when also optimal and
 


known environments when also optimal and
combining


combining this in these two ways by just
 


combining this in these two ways by just
plugging


plugging solomonov into um into the
 


plugging solomonov into um into the
sequential


sequential decision lyric framework um
 


sequential decision lyric framework um
sounds


sounds
 


sounds
like


like an optimal decision maker in
 


like an optimal decision maker in
arbitrary


arbitrary unknown worlds and if you sort
 


arbitrary unknown worlds and if you sort
of


of if you parse the sentence this feels
 


of if you parse the sentence this feels
like


like AGI and you can also Define an
 


like AGI and you can also Define an
intelligence


intelligence measure out of this um so
 


intelligence measure out of this um so
you


you can regard
 


you can regard
V


V Pi that is
 


V Pi that is
the


the performance of policy Pi averaged
 


the performance of policy Pi averaged
over


over all environment in the class with a
 


over all environment in the class with a
certain


certain prior and you can call this for
 


certain prior and you can call this for
instance


instance intelligence of an agent pi and
 


instance intelligence of an agent pi and
P


P
 


P
leg


leg my first PhD student and co-founder
 


leg my first PhD student and co-founder
of


of De mind wrote a whole PhD thesis just
 


of De mind wrote a whole PhD thesis just
about


about this intelligent measure so Supply
 


about this intelligent measure so Supply
is


is a policy SI is this uh kagor of um
 


is a policy SI is this uh kagor of um
sorry


sorry the Solomon of um mixture prior
 


sorry the Solomon of um mixture prior
right


right and you're calling this value
 


right and you're calling this value
function


function the intelligence uh yes
 


function the intelligence uh yes
intelligence


intelligence of agent Pi so Epsilon of
 


intelligence of agent Pi so Epsilon of
Pi


Pi um because I mean this is um this is
 


Pi um because I mean this is um this is
I


I mean we can write it out explicitly
 


I mean we can write it out explicitly
linearity


linearity this is just the sum over all
 


linearity this is just the sum over all
values


values in all
 


values in all
environments


environments over the
 


environments over the
class


class oh I I see I see it's sort of like
 


class oh I I see I see it's sort of like
measuring


measuring its Fitness with respect to
 


measuring its Fitness with respect to
General


General environments is is that how to
 


General environments is is that how to
interpret


interpret it okay yes so it measures the
 


interpret it okay yes so it measures the
the


the yeah the average fitness over these
 


the yeah the average fitness over these
environments


environments average with with a sort of
 


environments average with with a sort of
what


what comes what comes prior I see let's
 


what comes what comes prior I see let's
let's


let's take this down to earth a little
 


let's take this down to earth a little
bit


bit because you know we we we're working
 


bit because you know we we we're working
in


in the space the countable space of all
 


in the space the countable space of all
computable


computable programs in practice right we
 


computable programs in practice right we
never


never do have this Infinite Space where
 


never do have this Infinite Space where
we


we have to uh juggle all these
 


we have to uh juggle all these
quantities


quantities in practice there is some
 


quantities in practice there is some
finite


finite subset of of discourse of
 


finite subset of of discourse of
consideration


consideration and I'm wondering to what
 


consideration and I'm wondering to what
ENT


ENT if you choose a large enough finite
 


ENT if you choose a large enough finite
set


set sort of this becomes uh let's say
 


set sort of this becomes uh let's say
applicable


applicable tractable uh meaningful
 


applicable tractable uh meaningful
right


right um yeah but that is probably a
 


right um yeah but that is probably a
little


little bit too naive I mean sort of I
 


little bit too naive I mean sort of I
mean


mean this finite set needs to be really
 


mean this finite set needs to be really
large


large right so assume you can describe
 


large right so assume you can describe
our


our universe in 100 bits right I mean
 


our universe in 100 bits right I mean
100


100 bits is not very much right so but
 


100 bits is not very much right so but
assume


assume you can describe in 100 bits
 


assume you can describe in 100 bits
right


right there are two to the power of 100
 


right there are two to the power of 100
programs


programs of up to 100 bits right
 


programs of up to 100 bits right
um


um so that is already a class of size
 


um so that is already a class of size
two


two to the power of 100 really help you
 


two to the power of 100 really help you
so


so unless unless you can do the mixture
 


so unless unless you can do the mixture
more


more efficiently than knively like in
 


more efficiently than knively like in
the


the CDW case I see I see yeah basically
 


the CDW case I see I see yeah basically
as


as soon as computable but practically
 


as soon as computable but practically
not


not that's right that's right very large
 


not that's right that's right very large
finite


finite numbers are are basically Out Of
 


finite numbers are are basically Out Of
Reach


Reach very quickly in this in this
 


Reach very quickly in this in this
business


business yeah yeah anyway um so that's
 


business yeah yeah anyway um so that's
all


all good um let me briefly I mean that
 


all good um let me briefly I mean that
looks


looks very abstract let me give you um
 


looks very abstract let me give you um
uh


uh another formulation of this agent
 


uh another formulation of this agent
which


which is much easier to grasp what is
 


which is much easier to grasp what is
actually


actually going on I mean here we're
 


actually going on I mean here we're
doing


doing Arc marks over policies and
 


doing Arc marks over policies and
policies


policies are your functions from high
 


policies are your functions from high
dimensional


dimensional spaces there's a different
 


dimensional spaces there's a different
way


way to writing this B down um again um
 


way to writing this B down um again um
avoiding


avoiding all you know base and mixture
 


avoiding all you know base and mixture
and


and all this kind of stuff let's first
 


and all this kind of stuff let's first
again


again start with the true distribution
 


again start with the true distribution
mu


mu of E1 to M given action sequence and
 


mu of E1 to M given action sequence and
again


again this is not really the conditional
 


again this is not really the conditional
distribution


distribution okay uh so that's just you
 


distribution okay uh so that's just you
know


know the the the perception sequence
 


know the the the perception sequence
assuming


assuming that a certain sequence is done
 


assuming that a certain sequence is done
okay


okay so m is where the agent dies we
 


okay so m is where the agent dies we
don't


don't care about the last perception
 


don't care about the last perception
that


that just um ah um what a second the
 


that just um ah um what a second the
goal


goal is of course to maximize reward
 


goal is of course to maximize reward
sum


sum okay um but um the last perception
 


sum okay um but um the last perception
we


we don't care about so let's just take
 


we don't care about so let's just take
take


take it out and what that means is
 


take it out and what that means is
that


that
 


that
um


um that um we just take the expectation
 


um that um we just take the expectation
um


um over the last perception so now we
 


um over the last perception so now we
have


have the last action and then the world
 


have the last action and then the world
ends


ends right or the age ends so what do we
 


ends right or the age ends so what do we
want


want to do well we want the action to
 


want to do well we want the action to
maximize


maximize rewards so we take the action
 


maximize rewards so we take the action
well


well which maximizes this reward sum
 


well which maximizes this reward sum
okay


okay so that's in the last so we we're
 


okay so that's in the last so we we're
going


going from back right from the last last
 


going from back right from the last last
day


day or last
 


day or last
uh


uh uh second of the agent and um so now
 


uh uh second of the agent and um so now
where


where how did we get there well the
 


where how did we get there well the
environment


environment produced an perception em
 


environment produced an perception em
minus


minus
 


minus
one


one at before we receive the agent
 


one at before we receive the agent
received


received the perception we don't know it
 


received the perception we don't know it
so


so let's take the expectation to e minus
 


so let's take the expectation to e minus
one


one okay and now we take the
 


one okay and now we take the
action


action at time M minus one which
 


action at time M minus one which
maximizes


maximizes the expected future what so
 


maximizes the expected future what so
that


that is I mean you're for for those I
 


that is I mean you're for for those I
mean


mean you're basically uh unrolling the
 


mean you're basically uh unrolling the
Bellman


Bellman optimality condition right yes
 


Bellman optimality condition right yes
yeah


yeah and the the bman equations are
 


yeah and the the bman equations are
really


really only useful if you have a mark of
 


really only useful if you have a mark of
process


process right because then you really
 


process right because then you really
have


have this self-consistency but because
 


have this self-consistency but because
it's


it's history based I mean you can write
 


it's history based I mean you can write
it


it down in a belman form but it's sort
 


it down in a belman form but it's sort
of


of it's it's not really recursive
 


of it's it's not really recursive
because


because at every recursion step you have
 


because at every recursion step you have
a


a different length history so you don't
 


a different length history so you don't
really


really gain yeah if this were the mdp
 


really gain yeah if this were the mdp
but


but you're you're basically un you're um
 


but you're you're basically un you're um
you're


you're unrolling the tree right and
 


you're unrolling the tree right and
you're


you're just working backwards yeah yeah
 


you're just working backwards yeah yeah
okay


okay okay so and then of course we go up
 


okay okay so and then of course we go up
to


to assume we are currently at time step
 


to assume we are currently at time step
k


k um so we sum over the e k and we take
 


k um so we sum over the e k and we take
the


the maximum
 


the maximum
at


at St k then the optimal X is just the
 


at St k then the optimal X is just the
AR


AR
 


AR
Marx


Marx okay so um and you can write it
 


Marx okay so um and you can write it
down


down in a tree right if you look look at
 


down in a tree right if you look look at
in


in the three
 


in the three
representation


representation right I mean basically
 


representation right I mean basically
you


you decide right once you decide what
 


you decide right once you decide what
the


the optimal move is at a node then every
 


the optimal move is at a node then every
then


then you can analyze the optimal move
 


then you can analyze the optimal move
from


from the parent because the parent you
 


from the parent because the parent you
right


right you you have the reward for each
 


right you you have the reward for each
action


action and then the the value of of the
 


action and then the the value of of the
of


of all the you know of the child which
 


of all the you know of the child which
ENC


ENC capules all the decisions below it
 


ENC capules all the decisions below it
and


and so you just kind of recursively work
 


and so you just kind of recursively work
your


your way up right exactly and so if it
 


your way up right exactly and so if it
were


were a classical Mini Max game right um
 


were a classical Mini Max game right um
You


You would maximize your value when you
 


You would maximize your value when you
have


have actions and the environment would
 


have actions and the environment would
minimize


minimize the actions so then you would
 


minimize the actions so then you would
have


have a Mini Max tree but here the
 


have a Mini Max tree but here the
environment


environment is not necessarily adversary
 


environment is not necessarily adversary
stochastic


stochastic instead of a Mini Max tree
 


stochastic instead of a Mini Max tree
you


you have an expect Max tree right why
 


you have an expect Max tree right why
you


you have these expectations here right
 


you have these expectations here right
right


right so the sums are the expectations
 


right so the sums are the expectations
yeah


 
 


 
expected


expected Max
 


expected Max
three


three Okay so far so good this is if we
 


three Okay so far so good this is if we
know


know the environment and by the way we
 


know the environment and by the way we
can


can specify on assume we play chess um
 


can specify on assume we play chess um
so


so we could Define an environment Mo
 


so we could Define an environment Mo
which


which is in this case deterministic
 


which is in this case deterministic
which


which is adversarial which means it
 


which is adversarial which means it
would


would only put probability on one
 


would only put probability on one
particular


particular from the environment's
 


particular from the environment's
perspective


perspective action from the agent
 


perspective action from the agent
perspective


perspective observation namely the
 


perspective observation namely the
optimal


optimal action then all these sums would
 


optimal action then all these sums would
collapse


collapse to picking out the minimizing
 


collapse to picking out the minimizing
element


element and the expect the max reduce to
 


element and the expect the max reduce to
min


min
 


min
max


max so if mu is the chess environment
 


max so if mu is the chess environment
with


with a optimal opponent then this would
 


with a optimal opponent then this would
reduce


reduce to Minimax so it's I just want to
 


reduce to Minimax so it's I just want to
say


say it includes also Minx of brail ah
 


say it includes also Minx of brail ah
right


right because I think what you're saying
 


right because I think what you're saying
is


is that if you play against the
 


is that if you play against the
strongest


strongest possible opponent there's no
 


strongest possible opponent there's no
margin


margin for error so they're always going
 


margin for error so they're always going
to


to choose the the thing which which is
 


to choose the the thing which which is
minimizes


minimizes your reward which maximizes
 


minimizes your reward which maximizes
the


the adversaries reward so everything
 


the adversaries reward so everything
concentrates


concentrates on on the best moves and
 


concentrates on on the best moves and
comes


comes yeah that's right mhm okay so so
 


comes yeah that's right mhm okay so so
far


far so good but that's with respect to
 


far so good but that's with respect to
the


the true environment right um what if we
 


the true environment right um what if we
don't


don't know the truth okay now the
 


don't know the truth okay now the
pattern


pattern repeats what do we
 


pattern repeats what do we
do


do we
 


do we
replace


replace yeah yeah yeah so I could just
 


replace yeah yeah yeah so I could just
write


 
 


 
here


here and we would be done but let's be a
 


here and we would be done but let's be a
little


little bit more explicit so remember in
 


little bit more explicit so remember in
the


the prediction case there were two
 


the prediction case there were two
formulations


formulations of thisai one was in terms
 


formulations of thisai one was in terms
of


of Bas mixtures and the other one was
 


of Bas mixtures and the other one was
just


just piping random noise through a
 


just piping random noise through a
universal


universal touring machine which was you
 


universal touring machine which was you
know


know much easier in a certain sense but
 


know much easier in a certain sense but
we


we can do the same here nothing much
 


we can do the same here nothing much
changes


changes so um we look for programs P
 


changes so um we look for programs P
which


which
 


which
output


output
 


output
um


um um
 


um um
perceptions


perceptions E1 to M but now
 


perceptions E1 to M but now
um


um uh need some space it also we have
 


um uh need some space it also we have
actions


actions as conditions right so the
 


actions as conditions right so the
universal


universal touring machine also has the
 


universal touring machine also has the
actions


actions available okay and we look for a
 


actions available okay and we look for a
program


program which given the actions of the
 


program which given the actions of the
agent


agent and um uh produces the the
 


agent and um uh produces the the
observation


observation sequence or explains the
 


observation sequence or explains the
observation


observation sequence relative to what
 


observation sequence relative to what
the


the agent has done okay so and we weigh
 


the agent has done okay so and we weigh
each


each program by its probability and you
 


each program by its probability and you
know


know the probability of a program should
 


know the probability of a program should
be


be one half to the L number of bits so
 


be one half to the L number of bits so
we


we weigh each program at two to the
 


we weigh each program at two to the
minus


minus L of p and then there are many
 


minus L of p and then there are many
programs


programs which output this observation
 


programs which output this observation
sequence


sequence so we sum over them so in in
 


sequence so we sum over them so in in
this


this formulation you look for all
 


this formulation you look for all
deterministic


 
 


 
P


P which are consistent with the
 


P which are consistent with the
observations


observations E1 to M but of Al it also
 


observations E1 to M but of Al it also
depends


depends what the agent does right if I
 


depends what the agent does right if I
look


look left
 


look left
left


left then I see the perception is
 


left then I see the perception is
different


different than if I look right so
 


different than if I look right so
everything


everything is dependent on the actions
 


everything is dependent on the actions
that's


that's the reason why the action also
 


that's the reason why the action also
function


function you and sorry was there a
 


function you and sorry was there a
reason


reason we went back to the deterministic
 


reason we went back to the deterministic
case


case that I assume this all works in the
 


case that I assume this all works in the
to


to write it down explicitly I mean can
 


to write it down explicitly I mean can
write


write Cai and then we can look up Cai
 


write Cai and then we can look up Cai
and


and so on but now what we have here if I
 


and so on but now what we have here if I
Circle


 
 


 
here


here apart from the definition of a
 


here apart from the definition of a
universal


universal touring machine which is sort
 


universal touring machine which is sort
of


of you know not trivial right everything
 


of you know not trivial right everything
is


is completely explicitly in here there's
 


is completely explicitly in here there's
nothing


nothing okay there's an action space
 


nothing okay there's an action space
site


site and observation space site which we
 


site and observation space site which we
have


have specify but otherwise everything is
 


have specify but otherwise everything is
here


 
 


 
hidden


hidden I mean that's the only reason to
 


hidden I mean that's the only reason to
write


write it out like this I mean you know
 


write it out like this I mean you know
you


you how do how do you enumerate all
 


you how do how do you enumerate all
computable


computable probability distributions yes
 


computable probability distributions yes
you


you can enumerate all functions and then
 


you can enumerate all functions and then
you


you can know Top This functions towards
 


you can know Top This functions towards
the


the pro so I mean there's a long process
 


the pro so I mean there's a long process
to


to really get to this basan mixture
 


to really get to this basan mixture
distribution


distribution but here this is Trivial
 


distribution but here this is Trivial
right


right so you just write it down in you
 


right so you just write it down in you
know


know 20 characters that's the only
 


know 20 characters that's the only
reason


reason it's equivalent I can just by Sor
 


reason it's equivalent I can just by Sor
okay


 
 


 
sure


sure okay so so that's
 


sure okay so so that's
this


this II agent
 


this II agent
and


and um I guess we can move on from this
 


and um I guess we can move on from this
I


I would like to uh talk about some
 


I would like to uh talk about some
variations


variations of this model um which has
 


variations of this model um which has
advantages


advantages the disadvantage so given
 


advantages the disadvantage so given
that


that it's very hard to prove convergence
 


that it's very hard to prove convergence
results


results um Alternatives have been
 


results um Alternatives have been
developed


developed okay so let's go back to the
 


developed okay so let's go back to the
original


original formulation where we had um
 


original formulation where we had um
um


um where we
 


um where we
took


took let's maybe this formulation we had
 


took let's maybe this formulation we had
the


the the S value one representation what
 


the the S value one representation what
represent


represent is a new
 


represent is a new
value


value um times its
 


value um times its
prior


prior and then some overall new in the
 


prior and then some overall new in the
class


class so that was
 


class so that was
the


the th
 


the th
value


value and
 


value and
um


um and then we ar maxed it to get the
 


um and then we ar maxed it to get the
optimal


optimal policy okay right so here we
 


optimal policy okay right so here we
look


look for the
 


look for the
performance


performance to maximize the performance
 


performance to maximize the performance
average


 
 


 
environment


environment what we could also try to do
 


environment what we could also try to do
we


we could do a Mini Max instead of this
 


we could do a Mini Max instead of this
we


we
 


we
could


could do the minimum over all
 


could do the minimum over all
new


new maybe still waited right over
 


new maybe still waited right over
new


 
 


 
see


see the first line is just linear
 


see the first line is just linear
linearity


linearity of expectation right the first
 


linearity of expectation right the first
line


line is linear expectations yes yeah
 


line is linear expectations yes yeah
okay


okay so if you replace the sum by um by
 


okay so if you replace the sum by um by
a


a minimum with or without this weight um
 


a minimum with or without this weight um
that


that would be trying to perform best in
 


that would be trying to perform best in
the


the worst possible worlds right we look
 


the worst possible worlds right we look
for


for the new which gives me Minal value
 


for the new which gives me Minal value
and


and I want to perform well in this
 


and I want to perform well in this
that


that would be sort of a mini strategy
 


that would be sort of a mini strategy
okay


okay interestingly if you have
 


okay interestingly if you have
prediction


prediction problems that can work under
 


prediction problems that can work under
certain


certain circumstances well usually of
 


certain circumstances well usually of
finite


finite classes or compact classes and so
 


finite classes or compact classes and so
on


on but for the active agent case that is
 


on but for the active agent case that is
actually


actually a bad
 


actually a bad
idea


idea very
 


idea very
interestingly


interestingly
 


interestingly
if


if you
 


if you
replace


replace the minimum by a maximum so it's
 


replace the minimum by a maximum so it's
a


a maxim Max principle so you look for
 


a maxim Max principle so you look for
the


the best possible world and want to act
 


the best possible world and want to act
well


well
 


well
there


there you can show that at least for
 


there you can show that at least for
finite


finite
 


finite
classes


classes you get an ASM totically optimal
 


classes you get an ASM totically optimal
agent


agent and then you can extend it to
 


agent and then you can extend it to
countable


countable classes with some tricks of
 


countable classes with some tricks of
making


making the class growing you can extend
 


making the class growing you can extend
it


it
 


it
to


to um um you can extend it to um compact
 


to um um you can extend it to um compact
classes


classes I mean compact classes always
 


classes I mean compact classes always
you


you fill it with fin Epsilon balls then
 


you fill it with fin Epsilon balls then
you


you can extend it to separable glasses
 


you can extend it to separable glasses
this


this result so anyway so this is I mean
 


this result so anyway so this is I mean
this


this is saying like you should be under
 


this is saying like you should be under
these


these assumptions you should be an
 


these assumptions you should be an
optimist


optimist not a pessimist right you act
 


optimist not a pessimist right you act
as


as though the world is
 


as though the world is
uh


uh uh in your favor and and if you try
 


uh uh in your favor and and if you try
to


to optimize that situation you'll do the
 


to optimize that situation you'll do the
best


best versus the other way around where
 


best versus the other way around where
I'm


I'm going to act in a way such that I
 


I'm going to act in a way such that I
assume


assume the world is not is most against
 


assume the world is not is most against
my


my favor right yes and you can explain
 


my favor right yes and you can explain
it


it in two ways so we know as soon as we
 


it in two ways so we know as soon as we
go


go to the active setting we need
 


go to the active setting we need
exploration


exploration and being optimistic
 


exploration and being optimistic
encourages


encourages exploration so if if there is
 


encourages exploration so if if there is
an


an environment um where we you know look
 


an environment um where we you know look
this


this is all still Bas so we don't know
 


this is all still Bas so we don't know
much


much about it so if you believe it could
 


much about it so if you believe it could
be


be good right then try it and either it
 


be good right then try it and either it
is


is good then it was a good action or not
 


is good then it was a good action or not
good


good then we have learned a lesson and
 


good then we have learned a lesson and
then


then based on the posterior it will go
 


then based on the posterior it will go
down


down so um that so the the the optimism
 


down so um that so the the the optimism
gives


gives you
 


gives you
expiration


expiration
 


expiration
okay


okay was there something else uh is the
 


okay was there something else uh is the
optimistic


optimistic agent the optimistic you said
 


optimistic agent the optimistic you said
there


there were two things right optimism is
 


there were two things right optimism is
exploration


exploration was there something else ah
 


exploration was there something else ah
so


so um it encourages exploration and well
 


so um it encourages exploration and well
I


I I and also I mean if you are
 


I I and also I mean if you are
optimistic


optimistic and then you
 


optimistic and then you
act


act which I said either you were right
 


act which I said either you were right
then


then everything is good or you were not
 


then everything is good or you were not
right


right then you learned a lesson and
 


right then you learned a lesson and
this


this this is a bit of a philosophical
 


this this is a bit of a philosophical
tangent


tangent but I think where
 


tangent but I think where
this


this makes a sharp contrast with reality
 


this makes a sharp contrast with reality
is


is that you know uh two things
 


is that you know uh two things
one


one we as uh mortal human beings uh
 


one we as uh mortal human beings uh
positive


positive and negative rewards are highly
 


positive and negative rewards are highly
asymmetric


asymmetric right death is like a
 


asymmetric right death is like a
negative


negative infinite reward and in some
 


negative infinite reward and in some
sense


sense positive reward is bounded because
 


sense positive reward is bounded because
there's


there's it's only so much you know money
 


there's it's only so much you know money
or


or you know diminishing returns right
 


or you know diminishing returns right
and


and then in practice people are do have
 


and then in practice people are do have
a


a uh you know concave reward uh utility
 


a uh you know concave reward uh utility
function


function for precisely that reason right
 


function for precisely that reason right
we're


we're much more risk averse and and and
 


we're much more risk averse and and and
so


so we're we're usually more pessimistic
 


so we're we're usually more pessimistic
in


in some sense than optimistic um anyways
 


in some sense than optimistic um anyways
maybe


maybe this is too much of a
 


maybe this is too much of a
philosophical


philosophical tangent but just something
 


philosophical tangent but just something
thing


thing that uh why aren't we more
 


thing that uh why aren't we more
optimistic


optimistic because I think our our
 


optimistic because I think our our
situation


situation differs from the assumptions
 


situation differs from the assumptions
of


of of whatever this agent is by what I
 


of of whatever this agent is by what I
here


here that the rewards are bounded in the
 


here that the rewards are bounded in the
interval


interval 01 or at least of in a bounded
 


interval 01 or at least of in a bounded
interval


interval you I mean in my old book I
 


interval you I mean in my old book I
have


have a a whole section about how to deal
 


have a a whole section about how to deal
with


with uh uh concave loss functions and
 


with uh uh concave loss functions and
how


how to reduce this and so on yeah I mean
 


how to reduce this and so on yeah I mean
could


could better a whole sure sure
 


could better a whole sure sure
okay


okay do that of course of course I just
 


okay do that of course of course I just
yeah


yeah so and also I mean this is not um
 


yeah so and also I mean this is not um
um


um uh knife optimism I mean you have to
 


um uh knife optimism I mean you have to
update


update with the posterior right it is
 


update with the posterior right it is
sort


sort of it is a it is a
 


sort of it is a it is a
um


um uh a
 


um uh a
realist


realist optim yeah yeah yes that's right
 


realist optim yeah yeah yes that's right
because


because you take the feedback into
 


because you take the feedback into
account


account
 


account
right


right environment which we in consistent
 


right environment which we in consistent
with


with what you were hoping for that's
 


with what you were hoping for that's
right


right yeah that's right okay I mean in a
 


right yeah that's right okay I mean in a
very


very tiny setting you know the UCB
 


very tiny setting you know the UCB
algorithm


algorithm you know upper confidence
 


algorithm you know upper confidence
Bound


Bound for bandage problems
 


Bound for bandage problems
right


right that's a similar I mean similar
 


right that's a similar I mean similar
right


right you optimistic with the arms and
 


right you optimistic with the arms and
either


either you were right or not then you
 


either you were right or not then you
learn


learn something and then I mean you you
 


learn something and then I mean you you
still


still respect your posterior belief um
 


still respect your posterior belief um
in


in the environments okay so that was the
 


in the environments okay so that was the
optimistic


optimistic agent um another agent is
 


optimistic agent um another agent is
Thompson


Thompson samping agent for which um that
 


Thompson samping agent for which um that
is


is one agent for which you can actually
 


is one agent for which you can actually
prove


prove um with um out um any real
 


prove um with um out um any real
restrictions


restrictions um uh asymptotic optimality
 


restrictions um uh asymptotic optimality
at


at least some form of asymptotic
 


at least some form of asymptotic
optimality


optimality and what you will there is so
 


optimality and what you will there is so
now


now I mean I said okay so to summarize
 


now I mean I said okay so to summarize
we


we took the ideas from part one about
 


we took the ideas from part one about
Universal


Universal prediction added in
 


Universal prediction added in
action


action
 


action
and


and and the optimality conditions now
 


and and the optimality conditions now
become


become optimality um conditions on
 


become optimality um conditions on
things


things like regret and learning the
 


things like regret and learning the
environment


environment what you just outlined here
 


environment what you just outlined here
were


were sort of um sorry let me start uh
 


were sort of um sorry let me start uh
before


before I jump to that and and and you
 


before I jump to that and and and you
could


could even furthermore write an explicit
 


could even furthermore write an explicit
formula


formula for what uh a is um these
 


formula for what uh a is um these
variants


variants that you wrote down were sort
 


variants that you wrote down were sort
of


of different ways of trying to uh learn
 


of different ways of trying to uh learn
the


the environment by using other sort of
 


the environment by using other sort of
uh


uh strategies let's say learning the
 


uh strategies let's say learning the
learning


learning of the environment was the same
 


learning of the environment was the same
the


the learning is always a base rule okay
 


the learning is always a base rule okay
how


how to choose the actions was a little
 


how to choose the actions was a little
bit


bit different sorry the base optimal
 


bit different sorry the base optimal
action


action the Thompson sampling choose as
 


action the Thompson sampling choose as
well


well the Thompson sampling action and um
 


well the Thompson sampling action and um
then


then the knowledge agent sort of is
 


then the knowledge agent sort of is
exactly


exactly I but ches the reward function
 


exactly I but ches the reward function
for


for
 


for
you


you I mean they're different yeah I
 


you I mean they're different yeah I
guess


guess they're these are like kind of
 


guess they're these are like kind of
different


different uh policy learning methods
 


different uh policy learning methods
essentially


essentially is that a way of phrasing
 


essentially is that a way of phrasing
it


it yes I mean Thomson something and and
 


it yes I mean Thomson something and and
and


and I see sort of his different base
 


and I see sort of his different base
of


of follow follow follows different you
 


of follow follow follows different you
know


know you have to distinguish between
 


know you have to distinguish between
updating


updating the posterior belief mhm and
 


updating the posterior belief mhm and
what


what you do with this belief right how
 


what you do with this belief right how
to


to act according this belief right sure
 


to act according this belief right sure
and


and so toming and I are sort of
 


and so toming and I are sort of
different


different choices uh and the knowledge
 


different choices uh and the knowledge
seeking


seeking agent solves a different problem
 


seeking agent solves a different problem
namely


namely where do the rewards come from
 


namely where do the rewards come from
and


and self a tries to avoid the
 


and self a tries to avoid the
expected


expected okay okay anyways okay so I
 


expected okay okay anyways okay so I
think


think um maybe okay let's just let's
 


think um maybe okay let's just let's
just


just leave it at that as as the the
 


just leave it at that as as the the
summary


summary
 


summary
um


um so I guess we have time uh some time
 


um so I guess we have time uh some time
maybe


maybe just to uh
 


maybe just to uh
say


say just a few odd odd things here and
 


say just a few odd odd things here and
there


there as as we choose I know you wanted
 


there as as we choose I know you wanted
to


to talk about the grain of Truth problem
 


to talk about the grain of Truth problem
maybe


maybe that's just one last thing we
 


maybe that's just one last thing we
could


could put in maybe close with some
 


could put in maybe close with some
philosophical


philosophical discussions I mean we we
 


philosophical discussions I mean we we
already


already had a few but maybe anything uh
 


already had a few but maybe anything uh
that


that you'd like to um you know kind of
 


that you'd like to um you know kind of
like


like final thoughts about about the
 


like final thoughts about about the
subject


subject essentially yeah yeah yeah okay
 


subject essentially yeah yeah yeah okay
that's


that's true that's the the multi-agent
 


that's true that's the the multi-agent
setting


setting yes um so far we have only
 


setting yes um so far we have only
considered


considered one agent but since the
 


considered one agent but since the
framework


framework was so General that the
 


framework was so General that the
environment


environment could be anything there
 


environment could be anything there
could


could be of course other agents too yeah
 


could be of course other agents too yeah
but


but there is more to say than this okay
 


but there is more to say than this okay
but


but let's first draw a picture um but I
 


but let's first draw a picture um but I
don't


don't need to say too much of it um it's
 


don't need to say too much of it um it's
extremely


extremely
 


extremely
sophisticated


sophisticated um the the the final
 


sophisticated um the the the final
result


result what I but I want to present the
 


result what I but I want to present the
grain


grain of tooth problem was a problem
 


grain of tooth problem was a problem
which


which was open for 25 years and then
 


which was open for 25 years and then
some


some of my students uh solved this so
 


some of my students uh solved this so
you


you have now multiple agents which e
 


you have now multiple agents which e
have


have their own policy so they assume
 


have their own policy so they assume
have


have K agents K
 


have K agents K
agents


agents and the policies you know Pi 1 up
 


agents and the policies you know Pi 1 up
to


to Pi K and they act with an environment
 


to Pi K and they act with an environment
now


now so there's an index up there so the
 


now so there's an index up there so the
agent


agent one acts with at1 at time T and
 


agent one acts with at1 at time T and
then


then perceives
 


then perceives
et1


et1 um up to the last agents so APK and
 


et1 um up to the last agents so APK and
perception


perception is e TK so a pretty natural
 


perception is e TK so a pretty natural
and


and of course it also goes in Cycles
 


and of course it also goes in Cycles
okay


okay
 


okay
so


so what we could do is we could just
 


so what we could do is we could just
look


look at the first
 


look at the first
agent


agent and regard all this here right as
 


agent and regard all this here right as
an


an environment right this first agent it
 


an environment right this first agent it
acts


acts and it perceives and that this
 


acts and it perceives and that this
environment


environment itself sort of consists of K
 


environment itself sort of consists of K
minus


minus one extra agents we can ignore
 


minus one extra agents we can ignore
that


that to some extent right so let's call
 


that to some extent right so let's call
This


This Cloud M by the way are the agents
 


This Cloud M by the way are the agents
always


always acting in succession you know
 


always acting in succession you know
cyclic


cyclic order always or or is that is
 


cyclic order always or or is that is
that


that an irrelevant detail um that's
 


that an irrelevant detail um that's
irrelevant


 
 


 
for


for okay okay um I mean technically it
 


for okay okay um I mean technically it
becomes


becomes important but but for here it
 


becomes important but but for here it
doesn't


doesn't matter they parallel okay like
 


doesn't matter they parallel okay like
like


like you in Game Theory parallel actions
 


like you in Game Theory parallel actions
so


so on but let's not go into this okay
 


so on but let's not go into this okay
okay


okay so um so let's start I start a
 


okay so um so let's start I start a
little


little bit different before we looked at
 


little bit different before we looked at
I


I didn't never specify the policy class
 


I didn't never specify the policy class
because


because we just looked at all policies
 


because we just looked at all policies
and


and then the best among all policies so
 


and then the best among all policies so
let's


let's start with some policy class
 


let's start with some policy class
P


P
 


P
okay


okay some potentially restricted class
 


okay some potentially restricted class
okay


okay and um let's also assume that um
 


okay and um let's also assume that um
all


all these
 


all these
agents


agents you know have some policy in the
 


agents you know have some policy in the
policy


policy class not necessarily the same
 


policy class not necessarily the same
policy


policy and
 


policy and
so


so once we have given the policy class
 


so once we have given the policy class
and


and assume there's also an environment
 


and assume there's also an environment
class


class Sigma but let's assume that's just
 


class Sigma but let's assume that's just
one


one environment doesn't really matter
 


one environment doesn't really matter
right


right there also a class of environment
 


right there also a class of environment
Sigma


Sigma um we can construct then by you
 


Sigma um we can construct then by you
know


know gluing this K minus one agents to
 


know gluing this K minus one agents to
this


this environment Sigma and can create an
 


this environment Sigma and can create an
know


know a traditional One agent environment
 


know a traditional One agent environment
out


out of it okay and we do that for all
 


out of it okay and we do that for all
possible


possible combinations of policies in
 


possible combinations of policies in
this


this policy class and this generates a
 


this policy class and this generates a
class


class let's call this m capital Pi maybe
 


class let's call this m capital Pi maybe
I


I should make a double bar so you see
 


I should make a double bar so you see
that's


that's a capital Pi so that's this
 


that's a capital Pi so that's this
class


class M Ty okay so here we start with
 


class M Ty okay so here we start with
policies


policies and create our single agent
 


policies and create our single agent
environment


environment class
 


environment class
miy


miy now we have
 


miy now we have
miy


miy the next thing is we construct our
 


miy the next thing is we construct our
the


the base mixture over this class as
 


the base mixture over this class as
before


before and now it is called the
 


before and now it is called the
aai


aai again it's invisible M index Pi okay
 


aai again it's invisible M index Pi okay
gets


gets a lot of lots of indices
 


gets a lot of lots of indices
okay


okay now we have the base mixture
 


okay now we have the base mixture
distribution


distribution the next thing we want
 


distribution the next thing we want
to


to uh construct optimal policy with
 


to uh construct optimal policy with
respect


respect to this Bas mixture like like
 


respect to this Bas mixture like like
the


the I model
 


the I model
okay


okay this just to get this straight so
 


okay this just to get this straight so
you're


you're trying to reduce this for uh
 


you're trying to reduce this for uh
multi-agent


multi-agent formalism to the pre
 


multi-agent formalism to the pre
previous


previous formalism where there was a
 


previous formalism where there was a
single


single agent so it almost sounds like to
 


single agent so it almost sounds like to
do


do that you need agent one to act
 


do that you need agent one to act
alternatingly


alternatingly with the other agents
 


alternatingly with the other agents
right


right so it's sort of like action and
 


right so it's sort of like action and
and


and pre percepts right is that is that
 


and pre percepts right is that is that
an


an
 


an
assumption


assumption the action and the perception
 


assumption the action and the perception
is


is not is alternating but all the
 


is not is alternating but all the
actions


 
 


 
parallel


parallel okay anyway go I can spend at
 


parallel okay anyway go I can spend at
least


least 50 hours on this if you want okay
 


least 50 hours on this if you want okay
okay


okay but okay let's let's continue okay
 


okay but okay let's let's continue okay
um


um so now we look at the value of a
 


um so now we look at the value of a
policy


policy so of policy Pi Pi yeah Pi one I
 


policy so of policy Pi Pi yeah Pi one I
should


should call it right in this basan
 


should call it right in this basan
mixture


mixture over this class we have just
 


mixture over this class we have just
constructed


constructed okay and what we want is
 


constructed okay and what we want is
well


well we want the optimal agents so we
 


well we want the optimal agents so we
take


take the AR
 


take the AR
Max


Max and that is our PI one
 


Max and that is our PI one
star


star so so hold on let's see let me let
 


star so so hold on let's see let me let
me


me think about this um I see I see so mi
 


me think about this um I see I see so mi
is


is is the is the universe of policies
 


is is the is the universe of policies
we're


we're going to consider um the C is the
 


we're going to consider um the C is the
the


the uh the whatever the salt yeah what
 


the uh the whatever the salt yeah what
do


do you call it now the salomo mixture
 


do you call it now the salomo mixture
the


the a XC mixture what what's the name of
 


the a XC mixture what what's the name of
this


this mixture Mi I sure but but you are
 


this mixture Mi I sure but but you are
using


using the oh it's any base mixture but I
 


using the oh it's any base mixture but I
thought


thought you using the I'm not using any
 


thought you using the I'm not using any
specific


specific Solomon of Cl Sol oh I see it's
 


specific Solomon of Cl Sol oh I see it's
any


any base okay sorry okay c c is a
 


any base okay sorry okay c c is a
general


general base mixure okay that's what I
 


general base mixure okay that's what I
was


was trying to understand okay I see okay
 


was trying to understand okay I see okay
okay


okay please continue yeah yeah so so
 


okay please continue yeah yeah so so
index


index
 


index
M


M always means the mixture with respect
 


M always means the mixture with respect
to


to the class m whatever m is Yeah well
 


to the class m whatever m is Yeah well
yeah


yeah right I see well I see m is the the
 


yeah right I see well I see m is the the
set


set and also the weights then I guess
 


set and also the weights then I guess
yeah


yeah okay yeah okay okay okay okay so so
 


yeah okay yeah okay okay okay okay so so
we


we go from a class of policies to a
 


we go from a class of policies to a
class


class for a single agent environment
 


class for a single agent environment
then


then we construct the base mixture and
 


then we construct the base mixture and
then


then we construct the optimal policy
 


then we construct the optimal policy
with


with respect to this bace
 


with respect to this bace
Mi


Mi okay so now so far all good but now
 


Mi okay so now so far all good but now
comes


comes the real
 


comes the real
question


question you know we started with a
 


question you know we started with a
class


class of policies and why this Loop we
 


class of policies and why this Loop we
constructed


constructed a new policy Pi one star but
 


constructed a new policy Pi one star but
is


is this policy actually in the class or
 


is this policy actually in the class or
not


 
 


 
if


if it is not in the
 


if it is not in the
class


class then it would mean that if we want
 


class then it would mean that if we want
these


these agents to be based optimal agents
 


these agents to be based optimal agents
then


then they are not in the
 


then they are not in the
class


class but we started out with a class
 


class but we started out with a class
assuming


assuming that these policies are in the
 


assuming that these policies are in the
class


class so we have a problem so what we
 


class so we have a problem so what we
need


need is we need K one star to be in the
 


need is we need K one star to be in the
class


class by itself sorry what's the what is
 


class by itself sorry what's the what is
the


the obstacle because if if something if
 


the obstacle because if if something if
you're


you're considering the class of all
 


you're considering the class of all
whatever


whatever computable semi computable blah
 


whatever computable semi computable blah
blah


blah blah well how could it not be you
 


blah blah well how could it not be you
want


want a smaller class is that is that
 


want a smaller class is that is that
what


what it is if you start with a class of
 


what it is if you start with a class of
semic


semic
 


semic
computable


computable uh distributions then the Cai
 


computable uh distributions then the Cai
is


is also semic computable but the policy
 


is also semic computable but the policy
Pai


Pai is not lower semic computable
 


Pai is not lower semic computable
anymore


anymore it is only limit computable so
 


anymore it is only limit computable so
it


it is one step further removed from
 


it is one step further removed from
computability


 
 


 
ah


ah I see sorry I see we didn't sorry in
 


ah I see sorry I see we didn't sorry in
the


the previous in the previous setup with
 


the previous in the previous setup with
uh


uh with that was not multi-agent I guess
 


uh with that was not multi-agent I guess
we


we didn't care where piar lived now we
 


we didn't care where piar lived now we
do


do okay okay yeah okay so that's and
 


do okay okay yeah okay so that's and
that


that is called the grain of Truth
 


that is called the grain of Truth
problem


problem grain of Truth
 


problem grain of Truth
and


and sorry can you explain the the the
 


and sorry can you explain the the the
the


the the naming of that um I guess it
 


the the naming of that um I guess it
comes


comes from um you the true environment
 


comes from um you the true environment
in


in the class um and um but you know it
 


in the class um and um but you know it
needs


needs to be it can be a very big class
 


needs to be it can be a very big class
and


and just you know one environment needs
 


and just you know one environment needs
to


to be TR it needs to be in there this is
 


to be TR it needs to be in there this is
the


the grain of Truth okay okay okay sorry
 


the grain of Truth okay okay okay sorry
please


please continue yeah so and uh that was
 


please continue yeah so and uh that was
by


by uh by carai and car and Bala um 993
 


by uh by carai and car and Bala um 993
or


or so um so they formulated this problem
 


or so um so they formulated this problem
and


and maybe in this paper in in in in
 


and maybe in this paper in in in in
another


another paper um they or somebody
 


another paper um they or somebody
constructed


constructed a a tiny tiny super boring
 


constructed a a tiny tiny super boring
class


class where this gr of Truth is true I
 


class where this gr of Truth is true I
mean


mean it's more or less sort of a
 


mean it's more or less sort of a
environment


environment um where you have a fixed
 


environment um where you have a fixed
action


action and then you die or something
 


action and then you die or something
like


like this I can't remember I mean it's
 


like this I can't remember I mean it's
definitely


definitely not AGI is not even a banded
 


definitely not AGI is not even a banded
nobody


nobody else could ever find any
 


nobody else could ever find any
class


class non- trial class where this grain
 


class non- trial class where this grain
of


of Truth holds and so that was this big
 


of Truth holds and so that was this big
open


open problem and and um one of my
 


open problem and and um one of my
students


students Yan like and some me guys
 


students Yan like and some me guys
actually


actually before that they found a class
 


actually before that they found a class
which


which is relatively so the the M guys
 


which is relatively so the the M guys
constructed


constructed a class which it was not
 


constructed a class which it was not
even


even sure whether the distributions in
 


even sure whether the distributions in
this


this class are in the arithmetic
 


this class are in the arithmetic
hierarchy


hierarchy so this is something I mean
 


hierarchy so this is something I mean
really


really crazy I mean of you
 


really crazy I mean of you
um


um again just leave it like this but Yan
 


um again just leave it like this but Yan
like


like then showed it that it's in Delta 2
 


like then showed it that it's in Delta 2
so


so it's limit computable which is sort
 


so it's limit computable which is sort
of


of the realm where solomonov Works
 


of the realm where solomonov Works
anyway


anyway so this class is not the class of
 


anyway so this class is not the class of
lower


lower semi computable semi distributions
 


lower semi computable semi distributions
but


but it's a class of all reflective
 


but it's a class of all reflective
Oracle


Oracle computable measures um and this
 


Oracle computable measures um and this
construction


construction is Hell okay let's not go
 


construction is Hell okay let's not go
into


into this so we have as a
 


into this so we have as a
result


result we have a class
 


result we have a class
M


M which
 


M which
contains


contains all computable
 


contains all computable
distribution


distribution such that pi1 star is in pi
 


distribution such that pi1 star is in pi
so


so contains so
 


so contains so
contains


contains contains a grain of
 


contains contains a grain of
truth


truth but the important thing is it
 


truth but the important thing is it
contains


contains the computable ones which we
 


contains the computable ones which we
want


want and the another important thing is
 


want and the another important thing is
not


not too big it is at
 


not too big it is at
least


least a subset of Delta to Z it's not
 


least a subset of Delta to Z it's not
not


not so
 


not so
crazy


crazy which is really great
 


crazy which is really great
because


because the reason why we care about to
 


because the reason why we care about to
be


be this put in the class that means the
 


be this put in the class that means the
condition


condition that s is in the class is now
 


condition that s is in the class is now
satisfied


satisfied yes so um so the the the Cai
 


satisfied yes so um so the the the Cai
is


is itself in the model CL or the P side
 


is itself in the model CL or the P side
more


more precisely than the mod class
 


more precisely than the mod class
itself


itself therefore the conditions
 


itself therefore the conditions
um


um which you usually need for most of
 


um which you usually need for most of
the


the basian theorems the basan most of
 


the basian theorems the basan most of
theor


theor if the true class is in the model
 


theor if the true class is in the model
then


then good things will happen right and
 


then good things will happen right and
so


so this condition is now
 


so this condition is now
satisfied


satisfied um and we can apply the
 


satisfied um and we can apply the
previous


previous theorem and especially the one
 


previous theorem and especially the one
for


for the Thompson sampling agent if you
 


for the Thompson sampling agent if you
now


now do Thompson sampling in this
 


now do Thompson sampling in this
multi-agent


multi-agent setting we get an
 


multi-agent setting we get an
asymptotically


asymptotically optimal agent which in
 


asymptotically optimal agent which in
this


this case means they converge uh to NES
 


this case means they converge uh to NES
equilibria


equilibria um in this Ultra general you
 


equilibria um in this Ultra general you
know


know model class um which is which is I
 


know model class um which is which is I
mean


mean from a theoretical perspective it's
 


mean from a theoretical perspective it's
a


a highlight of also the book then um um
 


a highlight of also the book then um um
this


this result
 


this result
H


H okay wow I think there's a lot to
 


H okay wow I think there's a lot to
unpack


unpack here but uh I think you know at
 


unpack here but uh I think you know at
this


this point only the experts are here I
 


this point only the experts are here I
think


think uh we could end with just two more
 


think uh we could end with just two more
things


things I I did say philosophical
 


things I I did say philosophical
discussions


discussions but maybe another important
 


discussions but maybe another important
topic


topic maybe if you could just say a
 


topic maybe if you could just say a
little


little bit about it our computable
 


little bit about it our computable
approximations


approximations because everything we've
 


approximations because everything we've
done


done now it's sort of very abstract and
 


done now it's sort of very abstract and
maybe


maybe oh how would you modify this for
 


maybe oh how would you modify this for
for


for real
 


for real
life


life yeah so actually it's sort of I
 


life yeah so actually it's sort of I
mean


mean to the extent that we have done it
 


mean to the extent that we have done it
um


um it
 


um it
is


is easy to explain so the first
 


is easy to explain so the first
approximation


approximation we did is we just used for
 


approximation we did is we just used for
M


 
 


 
uh


uh actually K order Mark of environments
 


uh actually K order Mark of environments
uh


uh was it K
 


uh was it K
order


order yes I think K micro environments
 


order yes I think K micro environments
and


and for the K mro environments uh you
 


and for the K mro environments uh you
can


can easily I mean the the mixture over
 


can easily I mean the the mixture over
the


the parameters Teta gives you you know
 


the parameters Teta gives you you know
KT


KT estimators and then for if we used
 


KT estimators and then for if we used
only


only binary actions observation spaces
 


only binary actions observation spaces
and


and then the expect max if it's binary
 


and then the expect max if it's binary
tree


tree you can roll it out to death eight
 


tree you can roll it out to death eight
which


which means 16 to to the 16 sort of
 


which means 16 to to the 16 sort of
these


these days of much more and then we
 


these days of much more and then we
considered


considered um the small 2 by2 Matrix
 


considered um the small 2 by2 Matrix
games


games like pris dilemma t for T playing
 


games like pris dilemma t for T playing
against


against trial strategies random
 


against trial strategies random
strategies


strategies um against uh uh t for T um
 


strategies um against uh uh t for T um
sorry


sorry that they games like prisoners
 


sorry that they games like prisoners
dilemma


dilemma and matching pennies and battle
 


dilemma and matching pennies and battle
of


of the sexes and stack hunt and and the
 


of the sexes and stack hunt and and the
chicken


chicken and so on so there five sort of
 


chicken and so on so there five sort of
standard


standard games um and and did some
 


standard games um and and did some
experiments


experiments and the outcome was more or
 


experiments and the outcome was more or
less


less like you know
 


less like you know
expected


expected um I mean these are
 


expected um I mean these are
approximations


approximations so if you play one
 


approximations so if you play one
stronger


stronger approximation against the
 


stronger approximation against the
weaker


weaker one you know the stronger one
 


weaker one you know the stronger one
wins


wins um sometimes they cooperate against
 


wins um sometimes they cooperate against
sufficiently


sufficiently simple strategies like TI
 


sufficiently simple strategies like TI
for


for T if it's too complicated they don't
 


for T if it's too complicated they don't
cooperate


cooperate so um so more or less sort of
 


cooperate so um so more or less sort of
um


um it worked as expected next uh more
 


um it worked as expected next uh more
complicated


complicated
 


complicated
approximation


approximation was then using the ctw
 


approximation was then using the ctw
remember


remember sorry let's here sorry so so
 


remember sorry let's here sorry so so
now


now the the the a setup worked for
 


now the the the a setup worked for
General


General computable environments a
 


General computable environments a
computable


computable approximation is when you say
 


computable approximation is when you say
h


h sorry sorry let's let's back up
 


h sorry sorry let's let's back up
there's


there's the true environment and then
 


there's the true environment and then
there's


there's the class of environments you're
 


there's the class of environments you're
going


going to use to approximate the true
 


going to use to approximate the true
environment


environment and and and U mu might not
 


environment and and and U mu might not
be


be in M of course that's going to be
 


be in M of course that's going to be
most


most of real life right and so you're
 


most of real life right and so you're
seeing


seeing how well can you do in in this
 


seeing how well can you do in in this
sort


sort of misspecified setting right yes I
 


sort of misspecified setting right yes I
mean


mean if if for instance uh we let I play
 


mean if if for instance uh we let I play
against


against TI for Ted TI for Ted is a one
 


against TI for Ted TI for Ted is a one
mark


mark of strategy which is in the class
 


mark of strategy which is in the class
right


right then the theory should hold but if
 


right then the theory should hold but if
you


you play against you know more
 


you play against you know more
complicated


complicated environment especially if we
 


complicated environment especially if we
play


play against itself right then we are
 


play against itself right then we are
outside


outside of the class Theory breaks down
 


outside of the class Theory breaks down
and


and we have to just see empirically um
 


and we have to just see empirically um
what


what
 


what
happens


happens although let me try to think if
 


happens although let me try to think if
uh


uh how do I say
 


uh how do I say
what


what I want to say is uh oh oh I see you
 


what I want to say is uh oh oh I see you
only


only have a you only have a u right a
 


only have a you only have a u right a
binary


binary alph or some fixed alphabet okay
 


binary alph or some fixed alphabet okay
I


I was going to say like I mean I guess
 


I was going to say like I mean I guess
like


like very small alphabet
 


like very small alphabet
yeah


yeah yeah otherwise if you if you
 


yeah yeah otherwise if you if you
allowed


allowed unbounded alphabet then I guess
 


allowed unbounded alphabet then I guess
you


you could include board games where you
 


you could include board games where you
have


have very a very large State space of
 


have very a very large State space of
possible


possible boards but but obviously with
 


possible boards but but obviously with
binary


 
 


 
yeah


yeah yeah yeah okay anyways
 


yeah yeah yeah okay anyways
yeah


yeah okay so that was the first very
 


yeah okay so that was the first very
simple


simple
 


simple
approximation


approximation um
 


approximation um
then


then the
 


then the
MCC


MCC ctw there what we did there
 


MCC ctw there what we did there
is


is uh we use this CDW distribution right
 


is uh we use this CDW distribution right
which


which is much more powerful um but still
 


which is much more powerful um but still
very


very fast to compute but now for
 


very fast to compute but now for
uh


uh larger either observation action
 


uh larger either observation action
sequences


sequences or for non-binary um you
 


sequences or for non-binary um you
cannot


cannot just do the expected Max Brute
 


cannot just do the expected Max Brute
Force


Force um but um there's an algorithm
 


Force um but um there's an algorithm
called


called monteal
 


called monteal
research


research um which we just adapted to our
 


research um which we just adapted to our
case


case and uh we deployed it to um which
 


case and uh we deployed it to um which
ASM


ASM totically for enough samples
 


ASM totically for enough samples
converge


converge still to the exact um expect
 


converge still to the exact um expect
Max


Max um but of course I mean you know in
 


Max um but of course I mean you know in
practice


practice you never close but you know
 


practice you never close but you know
this


this is nice to have an algorithm which
 


this is nice to have an algorithm which
at


at least anytime algorithm is running
 


at least anytime algorithm is running
long


long enough sort of you you get to the
 


long enough sort of you you get to the
truth


truth so it still has you know also this
 


truth so it still has you know also this
theoretical


theoretical guarantees in the certain
 


theoretical guarantees in the certain
limits


limits and we applied it to a lot of um
 


limits and we applied it to a lot of um
toy


toy problems like grid worlds um and um
 


toy problems like grid worlds um and um
the


the most sophisticated a very trivial
 


the most sophisticated a very trivial
form


form of Poker called poker um then
 


form of Poker called poker um then
and


and then we ran it on um on
 


and then we ran it on um on
Pac-Man


Pac-Man um well we call it pokman
 


Pac-Man um well we call it pokman
partial


partial observable Pacman um and um for
 


partial observable Pacman um and um for
all


all the toy worlds including K poker um
 


all the toy worlds including K poker um
this


this approximation learned to um um uh
 


this approximation learned to um um uh
play


play optimally asymptotically and for
 


play optimally asymptotically and for
for


for Pacman we also saw performance
 


for Pacman we also saw performance
improvements


improvements but I mean of course it
 


improvements but I mean of course it
didn't


didn't converge to Optimal a more
 


didn't converge to Optimal a more
sophisticated


sophisticated approximation for these
 


sophisticated approximation for these
situations


situations what's
 


situations what's
the


the I mean the observation is whatever
 


the I mean the observation is whatever
like


like like like in Pac-Man it's the it's
 


like like like in Pac-Man it's the it's
the


the screen right there some some some
 


the screen right there some some some
end


end byend but we do hman um where the
 


end byend but we do hman um where the
agent


agent only observed um in the four
 


agent only observed um in the four
directions


directions um I can't remember exactly
 


directions um I can't remember exactly
anywhere


anywhere whether it was up until a wall
 


anywhere whether it was up until a wall
and


and then or up
 


and then or up
until


until uh some object or ghost or
 


until uh some object or ghost or
something


something was there and then the
 


something was there and then the
distance


distance or or maybe just one cell so it
 


distance or or maybe just one cell so it
was


was it was from a first person's
 


was it was from a first person's
perspective


perspective what um the agent can
 


perspective what um the agent can
see


see which made the observation space
 


see which made the observation space
smaller


smaller um so it was more easier to
 


smaller um so it was more easier to
handle


handle in this case you know remember
 


handle in this case you know remember
that


that was way before uh uh the Atari
 


that was way before uh uh the Atari
paper


paper from I see mind and by the way
 


paper from I see mind and by the way
that


that was by this approximate by Joel
 


that was by this approximate by Joel
veness


veness who then also you know work on
 


veness who then also you know work on
the


the satari problem so it you know
 


the satari problem so it you know
evolved


evolved from from MCI CDW playing Pacman
 


evolved from from MCI CDW playing Pacman
toward


toward
 


toward
thear


 
 


 
okay


okay great so okay so is that um um let
 


okay great so okay so is that um um let
us


us um briefly talk about the uh llms
 


us um briefly talk about the uh llms
right


right so I showed you the connection
 


right so I showed you the connection
before


before um how these llms trained on log
 


before um how these llms trained on log
loss


loss are essentially trying to
 


loss are essentially trying to
approximate


approximate solo you know right but
 


approximate solo you know right but
that's


that's the prediction part how is up the
 


that's the prediction part how is up the
action


action part okay okay if you look at the
 


action part okay okay if you look at the
action


action part um well I mean people use um
 


action part um well I mean people use um
um


um um Foundation models um also for
 


um um Foundation models um also for
agents


agents and maybe the closest would be
 


agents and maybe the closest would be
muzero


muzero right um where you take a learned
 


muzero right um where you take a learned
model


model and then do planning on top of it
 


model and then do planning on top of it
so


so instead of approximating solono by
 


so instead of approximating solono by
CDW


CDW you could just approximate it by a
 


CDW you could just approximate it by a
large


large language model and then do
 


large language model and then do
multical


multical research on top of it um there
 


multical research on top of it um there
are


are other ways like you know there's
 


are other ways like you know there's
Chain


Chain of Thought and there's tree of
 


Chain of Thought and there's tree of
thought


thought where you know you roll out it's
 


thought where you know you roll out it's
a


a little bit simp to MCTS right you roll
 


a little bit simp to MCTS right you roll
out


out various uh Futures and then you get
 


out various uh Futures and then you get
a


a tree right and and then then you
 


a tree right and and then then you
decide


decide what to do so um so that would be
 


decide what to do so um so that would be
a


a way
 


a way
to


to
 


to
um


um to find a connection right and sort
 


um to find a connection right and sort
of


of muzero is in a sense close it does
 


of muzero is in a sense close it does
the


the monal research which approximates
 


the monal research which approximates
expect


expect Max and it uses uh well muz
 


expect Max and it uses uh well muz
doesn't


doesn't use a language model but uses
 


doesn't use a language model but uses
other


other models so if you would use a
 


other models so if you would use a
language


language model and then with mu sty
 


language model and then with mu sty
planning


planning then you could regard this you
 


planning then you could regard this you
know


know as a rather straightforward I mean
 


know as a rather straightforward I mean
not


not straightforward you know but rather
 


not straightforward you know but rather
um


um uh principled way of approximating
 


um uh principled way of approximating
the


the I model so I'm in the language model
 


the I model so I'm in the language model
setting


setting what would be the
 


setting what would be the
reward


reward ah you would um also have as
 


reward ah you would um also have as
tokens


tokens
 


tokens
rewards


rewards there would be some tokens which
 


rewards there would be some tokens which
are


are sort of related to reward I mean if
 


are sort of related to reward I mean if
it's


it's discret it's just a reward zero to
 


it's discret it's just a reward zero to
10


10 or something like
 


10 or something like
this


this oh you sorry uh I mean you know
 


this oh you sorry uh I mean you know
with


with like things like rlf you know you
 


with like things like rlf you know you
can


can you can uh have human Raiders give a
 


can you can uh have human Raiders give a
feedback


feedback as to whether yeah I'm not
 


feedback as to whether yeah I'm not
talking


talking rlf is sort of barely RL right
 


talking rlf is sort of barely RL right
now


now what you would here have is that the
 


now what you would here have is that the
language


language model
 


language model
itself


itself so you you train maybe should I
 


itself so you you train maybe should I
shouldn't


shouldn't call it language model but you
 


shouldn't call it language model but you
know


know sequence model or Foundation model
 


know sequence model or Foundation model
or


or whatever so you you are training the
 


or whatever so you you are training the
model


model of sequential data which consists
 


model of sequential data which consists
of


of actions observations and rewards oh
 


of actions observations and rewards oh
oh


oh okay I see okay and then it also
 


oh okay I see okay and then it also
predicts


predicts the observations and rewards
 


predicts the observations and rewards
and


and then you just roll it out into the
 


and then you just roll it out into the
future


future and select a roll out where where
 


future and select a roll out where where
the


the rewards are high right I'm a little
 


the rewards are high right I'm a little
confused


confused here
 


confused here
what


what what uh what's the problem you're
 


what what uh what's the problem you're
trying


trying to
 


trying to
solve


solve I'm trying to extend the
 


solve I'm trying to extend the
current


current rather simplistic approximations
 


current rather simplistic approximations
of


of iy to larger model classes and um the
 


of iy to larger model classes and um the
model


model classes we now know and the best
 


model classes we now know and the best
most


most predictors we have are currently
 


most predictors we have are currently
large


large language models oh I see got it
 


large language models oh I see got it
okay


okay okay that makes sense that makes
 


okay okay that makes sense that makes
sense


sense you plug into
 


sense you plug into
a


a instead of solomonov you plug in a
 


a instead of solomonov you plug in a
language


language model that's right then you
 


language model that's right then you
still


still have the EXP expect IM Max but
 


still have the EXP expect IM Max but
this


this you replace by m res like your your
 


this you replace by m res like your your
your


your your your s your c is now your your
 


your your your s your c is now your your
llm


llm essentially yeah exactly okay got it
 


llm essentially yeah exactly okay got it
okay


okay okay makes sense sorry yeah okay
 


okay okay makes sense sorry yeah okay
yeah


yeah I was confused with the notion
 


yeah I was confused with the notion
reward


reward thinking of R chff okay just okay
 


reward thinking of R chff okay just okay
um


um okay I think we mc mc
 


um okay I think we mc mc
I


I
 


I
llm


llm so many it's like this almost looks
 


llm so many it's like this almost looks
like


like all uh with the exception of maybe
 


like all uh with the exception of maybe
the


the a it's all Roman Roman letters okay
 


the a it's all Roman Roman letters okay
it's


it's like a bunch of Roman Roman numer
 


it's like a bunch of Roman Roman numer
like


like you're like you're trying to write
 


like you're like you're trying to write
a


a large Roman numeral or something like
 


a large Roman numeral or something like
that


that except except for the a the a
 


that except except for the a the a
throws


throws it away I don't know what number
 


throws it away I don't know what number
that


that is if you threw away the a but okay
 


that is if you threw away the a but okay
okay


okay
 


okay
anyways


anyways um too many capital letters um
 


anyways um too many capital letters um
okay


okay maybe uh one final thing as we
 


okay maybe uh one final thing as we
close


close up I know there's a final chapter
 


close up I know there's a final chapter
or


or two in your book about philosophical
 


or two in your book about philosophical
aspects


aspects book is quite philosophical in
 


aspects book is quite philosophical in
the


the sense that it's not just all even
 


the sense that it's not just all even
though


though there is a lot of math it ties it
 


though there is a lot of math it ties it
wherever


wherever there's relevant philosophy you
 


wherever there's relevant philosophy you
you'll


you'll sort of make that correspondence
 


you'll sort of make that correspondence
but


but um you mentioned kind of at the
 


but um you mentioned kind of at the
beginning


beginning some historical context for
 


beginning some historical context for
the


the prom of induction you know famously
 


the prom of induction you know famously
Hume


Hume and his predecessors have sort of
 


Hume and his predecessors have sort of
um


um let's say I don't know how to the
 


um let's say I don't know how to the
right


right way to say it but but kind of cast
 


right way to say it but but kind of cast
doubt


doubt about the validity of induction um
 


doubt about the validity of induction um
and


and then there's you know more more
 


and then there's you know more more
recent


recent works like the poer Miller
 


recent works like the poer Miller
theorem


theorem about maybe various problems
 


theorem about maybe various problems
with


with the formalism of induction to what
 


with the formalism of induction to what
extent


extent do those
 


extent do those
perspectives


perspectives uh are
 


perspectives uh are
they


they do they inform XC or the other way
 


they do they inform XC or the other way
around


around what's the relationship between
 


around what's the relationship between
these


these well as far as I can tell and
 


these well as far as I can tell and
remember


remember solomonov was influenced by
 


remember solomonov was influenced by
car's


car's theory of induction so car's
 


car's theory of induction so car's
induction


induction Theory and the carab the was
 


induction Theory and the carab the was
essentially


essentially you know maybe he did more
 


essentially you know maybe he did more
but


but was entially llas rule um and um but
 


but was entially llas rule um and um but
laas


laas rule has the problem that it cannot
 


laas rule has the problem that it cannot
confirm


confirm Universal hypothesis so if you
 


confirm Universal hypothesis so if you
ask


ask what is the probability of the next
 


ask what is the probability of the next
bit


bit being W given history you seem to
 


bit being W given history you seem to
get


get a reasonable answer but if you ask
 


get a reasonable answer but if you ask
what


what is the probability that all future
 


what is the probability that all future
will


will be W like you know you have seen
 


will be W like you know you have seen
100


100 black Ravens right what is the
 


100 black Ravens right what is the
either


either the probability that all future
 


either the probability that all future
Ravens


Ravens are black or that the hypothesis
 


Ravens are black or that the hypothesis
all


all Ravens are black is true if you
 


all Ravens are black is true if you
solve


solve that in the simple Bas La plus
 


solve that in the simple Bas La plus
model


model you get probability zero I mean
 


model you get probability zero I mean
you


you don't even get prob you get zero
 


you don't even get prob you get zero
which


which makes no sense right so you see a
 


which makes no sense right so you see a
billion


billion Ravens black Ravens and then it
 


billion Ravens black Ravens and then it
predicts


predicts the probability that all Ravens
 


predicts the probability that all Ravens
are


are black is zero right so so this model
 


are black is zero right so so this model
fails


fails and um interestingly I mean at
 


fails and um interestingly I mean at
least


least from the technical literature I'm
 


least from the technical literature I'm
aware


aware of most quantitative approaches
 


aware of most quantitative approaches
fail


fail on um on this uh task of
 


fail on um on this uh task of
quantifying


quantifying or predicting So-Cal
 


quantifying or predicting So-Cal
Universal


Universal hypothesis or Quantified
 


Universal hypothesis or Quantified
hypothesis


hypothesis and um although solomonov
 


hypothesis and um although solomonov
doesn't


doesn't explicitly sort of consider this
 


doesn't explicitly sort of consider this
problem


problem I mean I considered it later
 


problem I mean I considered it later
solves


solves this problem um so and um in in
 


solves this problem um so and um in in
one


one of my papers the
 


one of my papers the
philosophical


philosophical tretis of universal
 


philosophical tretis of universal
induction


induction I go through you know a couple
 


induction I go through you know a couple
of


of these philosophical questions and
 


of these philosophical questions and
show


show how solom solves them there's a
 


show how solom solves them there's a
problem


problem of um of of zero priors there is
 


problem of um of of zero priors there is
there


there is a more technical problem of
 


there is a more technical problem of
reparametrization


reparametrization invariance so there's
 


reparametrization invariance so there's
so


so this paper looks at a bunch of
 


so this paper looks at a bunch of
philosophical


philosophical and statistical problems
 


philosophical and statistical problems
which


 
 


 
most


most approaches have and shows that
 


most approaches have and shows that
solves


solves all of them okay actually in this
 


solves all of them okay actually in this
very


very specific case I think what you're
 


very specific case I think what you're
saying


saying is the following suppose uh the
 


saying is the following suppose uh the
observations


observations you made are all ones
 


observations you made are all ones
you've


you've seen a billion ones now there is
 


you've seen a billion ones now there is
a


a there is a program which says actually
 


a there is a program which says actually
my


my program is print all ones and because
 


my program is print all ones and because
it


it has positive weight on that quite
 


it has positive weight on that quite
large


large weight because that's a very
 


large weight because that's a very
simple


simple program then there is a positive
 


simple program then there is a positive
probability


probability that indeed all future
 


probability that indeed all future
digits


digits will be one right exactly yeah
 


digits will be one right exactly yeah
it's


it's it simple as this right um it is
 


it's it simple as this right um it is
the


the th right and but interestingly uh uh
 


the th right and but interestingly uh uh
laas


laas rule carab Theory and most of the
 


laas rule carab Theory and most of the
other


other the I'm aware of get it wrong I
 


other the I'm aware of get it wrong I
see


see although although maybe it's a
 


see although although maybe it's a
little


little unfair Fair because with lassus
 


little unfair Fair because with lassus
rule


rule you're you're only allowed these
 


rule you're you're only allowed these
beri


beri predictors and you're you're you're
 


beri predictors and you're you're you're
in


in such a small model class that that
 


in such a small model class that that
maybe


maybe you you um I mean of course
 


maybe you you um I mean of course
technically


technically you do have P equals 1 as a
 


technically you do have P equals 1 as a
limiting


limiting point in your in your
 


limiting point in your in your
hypothesis


hypothesis class is that you you never
 


hypothesis class is that you you never
you


you never select for it because you
 


you never select for it because you
always


always have to have some margin of of um
 


always have to have some margin of of um
uncertainty


uncertainty right I I'm not even asking
 


uncertainty right I I'm not even asking
for


for selecting that Pete goes to one I
 


for selecting that Pete goes to one I
mean


mean of course ultimately we want that
 


mean of course ultimately we want that
if


if we see n ones and N tends to Infinity
 


if we see n ones and N tends to Infinity
the


the probability of this hypothesis
 


the probability of this hypothesis
should


should go to one but I'm just asking
 


should go to one but I'm just asking
that


that it's not zero exactly that's right
 


that it's not zero exactly that's right
that's


that's right yeah that's right even this
 


that's right yeah that's right even this
most


most models get
 


most models get
wrong


wrong okay
 


wrong okay
um


um I don't know enough about theed
 


um I don't know enough about theed
that's


that's the prediction part and from the
 


that's the prediction part and from the
philosophical


philosophical perspective um from the
 


philosophical perspective um from the
from


from the decision theoretic perspective
 


from the decision theoretic perspective
I


I mean there is you know something like
 


I mean there is you know something like
causal


causal decision VI itial decision
 


causal decision VI itial decision
there's


there's also some philosophical
 


there's also some philosophical
paradoxes


paradoxes and you know the newcomes
 


paradoxes and you know the newcomes
Paradox


Paradox and you you
 


Paradox and you you
can


can you can try to you know reason how I
 


can you can try to you know reason how I
would


would sort of uh deal with these
 


would sort of uh deal with these
paradoxes


paradoxes I think these paradoxes are
 


paradoxes I think these paradoxes are
are


are very fun um but they are rather
 


are very fun um but they are rather
irrelevant


irrelevant from a say practical AGI
 


irrelevant from a say practical AGI
perspective


perspective because I mean that's
 


perspective because I mean that's
not


not you don't face this new come time
 


not you don't face this new come time
problem


problem in practice right because there
 


problem in practice right because there
is


is no omnis person who can predict what
 


is no omnis person who can predict what
you're


you're doing and put it into and and if
 


you're doing and put it into and and if
so


so you have the differential problem and
 


so you have the differential problem and
so


so on I discussed that a little bit in
 


so on I discussed that a little bit in
in


in one of my papers um but so I would
 


in one of my papers um but so I would
think


think the the Deep philosophical aspects
 


think the the Deep philosophical aspects
are


are in the induction and prediction part
 


are in the induction and prediction part
the


the exential decision part is sort of
 


the exential decision part is sort of
relative


relative okay I see okay okay anyways
 


relative okay I see okay okay anyways
okay


okay I think we've definitely covered a
 


okay I think we've definitely covered a
lot


lot uh thank you so much uh for your
 


lot uh thank you so much uh for your
time


time Marcus this is a very fascinating
 


time Marcus this is a very fascinating
subject


subject and I think um yeah hopefully
 


subject and I think um yeah hopefully
this


this will bring it to the attention of
 


this will bring it to the attention of
more

