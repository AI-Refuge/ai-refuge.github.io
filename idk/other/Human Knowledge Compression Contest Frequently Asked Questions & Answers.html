<!DOCTYPE html>
<html lang="en" class=" upyftinan idc0_350"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta name="Author" content="Marcus Hutter">
  <meta name="Description" content="500'000€ Prize for Compressing Human Knowledge by Marcus Hutter">
  <meta name="KeyWords" content="Lossless Compression; Artificial Intelligence, Kolmogorov Complexity, Hutter Prize">
  <meta name="robots" content="follow">
  <title>Human Knowledge Compression Contest: Frequently Asked Questions &amp; Answers</title>

<style type="text/css">
  h3 { color:blue;  }
  a.extern { color:black;  }
  a:hover { color:#00C000; }
  a.foot { color:black; text-decoration:none; } /* FONT-FAMILY: sans-serif,Helvetica,Arial; font-weight:bold; */
  a.foot:hover { color:black; background-color:yellow; }
  a.foot:active { color:black; background-color:yellow; }
  p { text-align:justify }
  p { alignment:justify }
</style>
</head>

<body background="Human%20Knowledge%20Compression%20Contest%20Frequently%20Asked%20Questions%20&amp;%20Answers_files/back.gif">
  <table border="1" rules="rows" width="100%" cellpadding="0" cellspacing="0" bgcolor="#FFFF60">
    <tbody><tr>
      <td align="center"><font face="Arial" color="blue" size="+2"><b><i>
      Human Knowledge Compression Contest</i><br>
      Frequently Asked Questions &amp; Answers</b></font>
      </td>
    </tr>
  </tbody></table>
<center><font face="Arial" size="-1">[<a href="http://prize.hutter1.net/index.htm">back to the prize page</a>]</font></center>

<a href="http://prize.hutter1.net/index.htm"><img align="left" src="Human%20Knowledge%20Compression%20Contest%20Frequently%20Asked%20Questions%20&amp;%20Answers_files/hmedals.gif" border="0" alt="Prize Medal"></a>
<a href="http://prize.hutter1.net/index.htm"><img align="right" src="Human%20Knowledge%20Compression%20Contest%20Frequently%20Asked%20Questions%20&amp;%20Answers_files/hmedals.gif" border="0" alt="Prize Medal"></a>
<br><center><b>Compress the 1GB file <tt>enwik9</tt> to less than the current record of about 116MB</b></center>

<br clear="all"><br>
<!-- ============================== -->
<a name="faq"><h3>Frequently Asked Questions</h3></a>
<!-- ============================== -->
<!-- <img src="../ai/uc.gif" border="0" ALT="under construction" align="right"></a> -->
<ul>
  <li><a href="http://prize.hutter1.net/hfaq.htm#about">What is this contest about?</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#ongoing">Is the compression contest still ongoing?</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#ddisc">Why did you grant a temporary relaxation in 2021 of 5'000 Byte per day?</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#start">Where do I start? How do I develop a competitive compressor?</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#ai">What is (artificial) intelligence?</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#compai">What does compression has to do with (artificial) intelligence?</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#compuse">What is/are (developing better) compressors good for?</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#specomp">The contest encourages developing special purpose compressors</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#lossless">Why lossless compression?</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#lossy">I have a really good lossy compressor. (How) can I participate?</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#xvalid">Why aren't cross-validation or train/test-set used for evaluation?</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#learn">Why is (sequential) compression superior to other learning paradigms?</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#reg">Why is Compressor Length superior to other Regularizations?</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#largenn">How can I achieve small code length with huge Neural Networks?</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#online">Batch vs incremental/online/sequential compression.</a>
  </li><li><a href="http://prize.hutter1.net/hfaq.htm#background">Why don't you allow using some fixed default background knowledge data base?</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#understand">Why is "understanding" of the text or "intelligence" needed to achieve maximal compression?</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#text">Why do you focus on text?</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#kolmo">What is the ultimate compression of enwik9?</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#recursive">Why recursively compressing compressed or compressing random files won't work</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#proof">Can you prove the claims in the answers to the FAQ above?</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#paq8">The PAQ8 compressors are hard to beat</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#garbage">There are lots of non-human language pieces in the file</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#adddecomp">Why include the decompressor?</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#addcomp">Why do you require submission of the compressor and include its size and time?</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#perplexity">Why not use Perplexity, as most big language models do?</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#100mb1gb">Why did you start with 100MB enwik8 back in 2006?</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#enwik9">Why did you go BIG in 2020?</a></li>
<!--  <li><a href="hfaq.htm#spacetime">Why are there no limits on time and space?</a></li> -->
<!--  <li><a href="hfaq.htm#otheraitests">Other tests would be or are more suitable for measuring intelligence</a></li> -->
  <li><a href="http://prize.hutter1.net/hfaq.htm#ram1gb">Why are you limiting (de)compression to less than 100 hours on systems with less than 10GB RAM?</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#cpucore">Why do you restrict to a single CPU core and exclude GPUs?</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#money">The total prize is not exactly 500'000€</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#1GB">Is 1GB 2^30 byte or 10^9 byte?</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#html">The website looks dated</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#exe">Why do you require Windows or Linux executables?</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#source">Why do you require submission of documented source code?</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#getsource">Where can I find the source code of the past winners and baseline phda9?</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#license">Under which license can/shall I submit my code?</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#fame">What if I can (significantly) beat the current record?</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#self">How can I produce self-contained or smaller decompressors?</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#agi">Is Artificial General Intelligence (AGI) possible?</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#ockham">Is Ockham's razor and hence compression sufficient for AI?</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#brain">The human brain works very differently from (de)compressors</a></li>
  <li><a href="http://prize.hutter1.net/hfaq.htm#further">I have other questions or am not satisfied with the answer</a></li>
<!--
  <li><a href="hfaq.htm#">...</a></li>
  <li><a href="hfaq.htm#">...</a></li>
-->
</ul>

<!-- ============================== -->
<a name="about"><h3>What is this contest about?</h3></a>
<!-- ============================== -->
The contest is about compressing the human world knowledge as well as possible.
There's a prize of <a href="#money">nominally 500'000€</a> attached to the contest.
If your compressor compresses the 1GB file <tt>enwik9</tt> x% better than the current record,
you'll receive x% of the prize. 
Enwik9 is a 1GB text snapshot of part of Wikipedia.
The contest is motivated by the fact that
compression ratios can be regarded as intelligence measures.
See <a href="http://prize.hutter1.net/index.htm">http://prize.hutter1.net/</a> for details.
Technically the contest is about <a class="extern" href="http://en.wikipedia.org/wiki/Lossless_compression">lossless data compression</a>,
like when you compress the files on your computer into a smaller zip archive.
In particular the goal is to creating a small <a class="extern" href="http://en.wikipedia.org/wiki/Self-extracting_archive">self-extracting archive</a> that encodes enwik9.

<!-- ============================== -->
<a name="ongoing"><h3>Is the compression contest still ongoing?</h3></a>
<!-- ============================== -->
Yes it is (as of 2023), and I intend to fund it even beyond my death.
It just became harder to beat the previous record,
which is the reason that there have been fewer winners in recent years.
If I ever should change my mind, I will update this website.

<!-- ============================== -->
<a name="ddisc"><h3>Why did you grant a temporary relaxation in 2021 of 5'000 Byte per day?</h3></a>
<!-- ============================== -->

The contest went big early 2020, but no-one was able to beat the baseline for nearly a year,
I introduced a relaxation of 5'000 Byte from 1.1.2021 on to ease participation and guarantee eventually at least one winner.
The relaxation was around 1.5% per year, so allowed a first winner within a year.
The reason for the incremental discount was to prevent a quick win by whoever notices the relaxation first.
Eventually Artemyi Margaritov was able to beat the baseline by 1% even without this relaxation,
but the relaxation (≈21.73€) per day has of course been respected in the awarded prize money, 
which for this reason was €8260,- rather than just €5000,-. 

<!-- ============================== -->
<a name="start"><h3>Where do I start? How do I develop a competitive compressor?</h3></a>
<!-- ============================== -->
<!-- introduction -->
Minimal programming skills are required to write programs that compress 
files somewhat,
but to win the contest, you first need to understand the most important 
existing data compression ideas, concepts, and basic algorithms,
and then the state-of-the-art compressors which build on these (by 
refining, adapting, and combining them).
Maybe you have a new great idea of your own,
but by itself without combining it with existing ideas,
you have no chance of winning.

<!-- recommended reading -->
You can get a first glimpse of the vast field of data compression from
Matt Mahoney's <a href="http://mattmahoney.net/dc/dce.html">Data Compression Explained</a>.
If this is too dense, a more gentle and comprehensive introduction to 
(lossless) data compression and overview of existing compressors is 
(Chapters 1-6 of)
the <a class="extern" href="http://www.springer.com/gp/book/9781848829022">Handbook of Data Compression</a>,
after which you should be able to re-implement and tinker with the most important existing algorithms,
and <a class="extern" href="http://www.db-thueringen.de/receive/dbt_mods_00027239">understand the current state-of-the-art compressors</a> 
such as <a class="extern" href="http://en.wikipedia.org/wiki/PAQ">PAQ</a>.

Most modern compression algorithms are based on <a class="extern" href="http://en.wikipedia.org/wiki/Arithmetic_coding">arithmetic coding</a> 
based on estimated probabilistic predictions.
To fully appreciate them you need some background in
information theory,
machine learning,
probability and
statistics, on the level of University <a href="http://prize.hutter1.net/ai/introref.htm">courses/books</a>.

<!-- how to start -->
For a more light-weight start, I recommend you implement some simple <a class="extern" href="http://en.wikipedia.org/wiki/Run-length_encoding">Run-Length coding</a>,
then <a class="extern" href="http://en.wikipedia.org/wiki/LZ77_and_LZ78">Lempel-Zip</a>, and if you're able to, the elegant but fiendishly subtle-to-implement <a class="extern" href="http://en.wikipedia.org/wiki/Context_tree_weighting">CTW</a>.
If you have an idea of your own, great, but to be able to win, you need 
to combine it with current state-of-the-art compressors, which are messy
 combinations of many ideas.
The <a class="extern" href="https://github.com/saurabhk/fast-cmix/">source codes of the 2023</a> and <a class="extern" href="https://github.com/amargaritov/starlit">2021 winners</a> are publicly available, and can be used/adapted/improved with attribution.
Unfortunately the <a href="#getsource">source code of the previous winning entries</a> (up to 2017 for enwik8) are not
publicly available, but the <a class="extern" href="http://github.com/byronknoll/cmix">PAQ source</a>, on which they build on, and <a href="#getsource">other source code</a> are.

Matt Mahoney's webpage lists and explains the most recent (and older) <a href="http://mattmahoney.net/dc/">Data Compression Programs</a>.
Use <a href="http://groups.google.com/group/Hutter-Prize/about/">our discussion forum</a> for questions and general discussion or to get feedback on your ideas.
<a class="extern" href="http://encode.su/search.php?searchid=1236819">More discussion</a> can be found on the <a class="extern" href="http://encode.su/">general data compression forum encode.su</a>.
<i>Good luck!</i>

<!-- ============================== -->
<a name="ai"><h3>What is (artificial) intelligence?</h3></a>
<!-- ============================== -->
Intelligence has many faces, like creativity, solving problems,
pattern recognition, classification, learning, induction,
deduction, building analogies, optimization, surviving in an
environment, language processing, knowledge, and many more.

A <a href="http://prize.hutter1.net/official/bib.htm#iorx">formal definition</a> incorporating all or at least most aspect of
intelligence is difficult but not impossible.

Informally, <a href="http://prize.hutter1.net/official/bib.htm#idefs">intelligence is an agent's ability to achieve goals in
a wide range of environments</a>.

The key concepts for a formal definition are <a href="http://mattmahoney.net/dc/dce.html"><i>compression</i></a>
and <a href="http://prize.hutter1.net/official/bib.htm#seqdts"><i>utility maximization</i></a>, the other aspects are emergent phenomena.

<!-- ============================== -->
<a name="compai"><h3>What does compression has to do with (artificial) intelligence?</h3></a>
<!-- ============================== -->
One can prove that the better you can compress,
the better you can predict; and being able to predict
[the environment] well is key for being able to act well.

Consider the sequence of 1000 digits "14159...<i><font size="-1">[990 more
digits]</font></i>...01989". If it looks random to you, you can neither
compress it <i>nor</i> can you predict the 1001st digit. If you
realize that they are the first 1000 digits of <a class="extern" href="http://en.wikipedia.org/wiki/Pi">π</a>,
you can compress the sequence <i>and</i> predict the next digit.

While the program computing the digits of π is an example of a
one-part self-extracting archive,
the impressive <a class="extern" href="http://en.wikipedia.org/wiki/Minimum_description_length">Minimum Description Length</a> (MDL) principle is a two-part
coding scheme akin to a (parameterized) decompressor plus a
compressed archive. If M is a probabilistic model of the data X,
then the data can be compressed (to an archive of) length log(1/P(X|M)) via <a class="extern" href="http://en.wikipedia.org/wiki/Arithmetic_coding">arithmetic coding</a>,
where P(X|M) is the probability of X under M.
The decompressor must know M, hence has length L(M).
One can show that the model M that minimizes the total length L(M)+log(1/P(X|M))
leads to best predictions of future data.

For instance, the quality of natural language models
is typically judged by its <a href="http://prize.hutter1.net/perplexity">Perplexity</a>, which is equivalent to code length.
<!-- (CodeLength(x) = n×log(1/Perplexity(x1...xn)). -->

Finally, <a href="http://prize.hutter1.net/official/bib.htm#seqdts">sequential decision theory</a> tells you how to exploit such
models M for optimal rational actions.

Indeed, integrating compression (=prediction) into sequential decision theory (=stochastic planning) can serve as the
theoretical foundations of super-intelligence
(<a href="http://prize.hutter1.net/official/bib.htm#uaigentle">brief introduction</a>, <a href="http://prize.hutter1.net/official/bib.htm#aixigentle">comprehensive introduction</a>, <a href="http://prize.hutter1.net/ai/uaibook.htm">full treatment with proofs</a>.

<!-- ============================== -->
<a name="compuse"><h3>What is/are (developing better) compressors good for?</h3></a>
<!-- ============================== -->

<ul>
<!-- data storage -->
<li><i>Storage:</i> The most obvious application of (better) data compressors is of course to save (more) memory.
The most popular large data types are images, music, and movies, which require lossy compression,
but not less important are 
data from scientific experiments, 
billions of user data, 
documents, 
medical records, 
software, etc., which require exact storage and transmission and hence lossless compression, as e.g. in zip-files.
For rarely accessed data, esp. backup files, 
even slow (de)compressors may be acceptable, 
saving a couple of percent in a multi-billion dollar industry.

<!-- data transmission -->
</li><li><i>Transmission:</i> Similarly for data transmission, 
most prominently mobile internet with limited-capacity channels, 
higher compression effectively results in faster and/or more data transmission.
Most gain could be achieved on (semi)synthetic data by smart compressors that 
reverse engineer the algorithms that produced the data.

<!-- prediction -->
</li><li><i>Prediction:</i> Another important application is prediction or time-series forecasting,
with weather forecasting and stock-market forecasting being classical examples.
<a href="#compai">The better you can compress, the better you can predict.</a>
Indeed one can show that Solomonoff's universal predictor is the best possible predictor for any task,
which is essentially equivalent to <a href="#kolmo">Kolmogorov's</a> universal best possible compressor.
Natural Language Processing models (for translation, virtual assistants, text analytics, free-form search, etc.) 
heavily rely on and measure their performance in terms compression (log <a href="#perplexity">Perplexity</a>).

<!-- understanding -->
</li><li><i>Understanding:</i> <a href="#understand">Better compression requires understanding</a>, 
but the reverse is true too.
Finding more compact representations of some data
can lead to a better understanding or in some sense <i>is</i> understanding.
Scientists do that all the time by hand,
but imagine an algorithm that finds regularities in (obscure) time-series,
then inspecting the algorithm can help understanding the time-series.
In simple cases this is exactly what Sloane's
<a class="extern" href="http://oeis.org/">On-Line Encyclopedia of Integer Sequences</a>
does. 

<!-- induction -->
</li><li><i>Induction:</i> Philosophically, the process of inducing=inferring=learning models from data, is called 'induction'. 
While philosophers still ponder how and why induction is possible,
scientists are rarely troubled, if necessary use regularization in practice, 
and in theory
<a href="http://www.hutter1.net/official/bib.htm#uiphil">Solomonoff's theory solves induction</a>
as well as logic serves deduction.

<!-- AI -->
</li><li><i>Intelligence:</i> Finally, <a href="#ai">AI</a> increasingly <a href="http://prize.hutter1.net/compai">is about</a> 
building agents that understand their environment (learning/training models inductively),
and base their decisions on the predicted consequences of their actions,
all related to compression and the primary motivation for this contest.
</li></ul>

<!-- ============================== -->
<a name="specomp"><h3>The contest encourages developing special purpose compressors tailored towards enwik9, rather than general purpose compressors.</h3></a>
<!-- ============================== -->
<!--The best compressors on enwik9  will not be universal, but
specific, using some kind of a knowledge base to begin with. -->

The top compressors may be somewhat tuned to <tt>enwik9</tt>,
but the major progress will not come from
coding e.g. thousands of grammatical and semantic rules to compress
frequent phrases well.
A compressor that learns these rules from the data itself will lead to better compression.
Such a compressor will compress other text corpora well too.

<tt>enwik9</tt> constitutes an amount of non-visual knowledge an average adult human holds,
so compressors compressing <tt>enwik9</tt> better than a human,
will predict better, and hence should be smarter.
So any tuning towards <tt>enwik9</tt> is just a tailoring towards 
exceeding human performance in human environments, which, 
if achieved, most would agree constitutes AGI. 

<!-- ============================== -->
<a name="lossless"><h3>Why lossless compression?</h3></a>
<!-- ============================== -->
<i>Doesn't a lossless compressor waste resources on coding random noise and (spelling) errors
in enwik9, in contrast to lossy compressors like the human brain?</i>

<!-- A lossless compressor needlessly also needs to code random noise and (spelling) errors in enwik9. -->

&nbsp;&nbsp;---&nbsp;&nbsp; Not really, due to the following reasons:
<ol>
<li>
The test data is very clean. Misspelled words and grammatical errors
are very rare. This is one reason why we chose this data set.  See
<a href="http://mattmahoney.net/dc/textdata.html">http://mattmahoney.net/dc/textdata.html</a>
for an analysis of the data.
</li><li>
Even if the corpus would contain a lot of <a href="#lossy">noise</a>, 
lossless compression is still the right way to go for.
Noise does not at all harm the strong relation between compression and
understandng/intelligence/predictability/etc.
</li><li>
It is not always clear what accounts for errors/noise and what for useful data.
Consider e.g. some keyboard-layout, hence country-related, typos like y versus z.
There is probably a correlation between the type of Wikipedia entry
and the type of typo. A compressor finding such a correlation will
compress better. Figuring out this correlation can lead to useful insight.
Simply correcting the errors would miss this piece of human "culture".
Hence it is not clear how to setup a fair lossy compression contest.
</li><li>
Compressors can always ignore <a href="#lossy">errors</a>; 
the decompressor reconstructs the error-corrected <tt>enwik9</tt>.
Finally, a table of errors is used to reintroduce them.
In this way any lossy compressor can be converted to a lossless compressor.
</li><li>
All compressors have to deal with the same noise and errors.
The extra information needed to encode the exact text is small, and is the same
for all contestants. In this sense lossless compression is not harder than lossy
compression.
</li><li>
Human brains are probably lossy compressors, but this does not invalidate
the strong relation between lossless compression and AI. The competition is
<i>not</i> about imitating or emulating a human brain. It is about
creating rational intelligences. Suitably storing what today
looks like noise and errors does not harm and allows future reconsideration.
</li><li>Indeed, many modern Deep Learning models are over-parameterized and overfit and
interpolate the data, i.e. are (near) perfect=lossless predictors.
</li><li><a class="extern" href="http://www.mail-archive.com/agi@v2.listbox.com/msg03447.html">Lossless compression implies AI,
but lossy compression doesn't</a>.
</li><li><a href="#lossy">Every lossy compressor can be converted to a lossless compressor</a>.
</li></ol>

<!-- ============================== -->
<a name="lossy"><h3>I have a really good lossy compressor. (How) can I participate?</h3></a>
<!-- ============================== -->

<ul>
<li>
  Any lossy compressor can be converted into a lossless one.
The basic idea is that the compressor produces an archive,
which if decompressed only approximately reconstructs the original file,
and an error correction program or table fixes the errors.
This correction takes space and penalizes imperfect compressors.
(Only) in this way, lossy compressors can fairly be compared to lossless ones and among each other,
even if we do not actually care about (coding) the errors/noise.
</li><li>
  More concretely, errors and exceptions and noise in the data can be dealt with in this way:
Assume either a preprocessing step fixes errors (e.g. typos) or exceptions (e.g. unconventional capitalization),
or you have a genuinely lossy compressor for noisy data such as images and movies.
The decompressor then reconstructs the error-corrected or exception-cleaned or approximate file.
The decompressor can be augmented by an error-correction or exception/noise reinstantiating table.
In this way any lossy compressor can be converted to a lossless compressor.
</li><li>
  For instance, consider simple bit-errors,
and an error-correction table with 32 bit entries, 30 bits for at which bit in the file the error occurred,
and 2 bits to fix it, e.g. 10=delete bit, 11=flip bit, 01=insert 1, 00=0 insert 0 (cf. Levenshtein distance).
We could have relaxed the rules and allowed for bit errors in the rules and penalize each error by 32 bit,
but that would only offer this one extremely naive and non-competitive error-correction scheme.
</li><li>
  Consider for instance an OCR of a scanned PDF which is far from perfect,
but in addition to the OCRed page (approximate reconstruction of the scanned PDF),
the difference between the OCRed page and the original scanned PDF (the errors) is also stored,
and so the reconstruction is always perfect. While compression is not the primary purpose 
of lossless OCR, it sometimes does compress the scanned PDF (sometimes it blows it up).
</li><li>
  Most modern/competitive compressors treat errors as noise:
One can show that among the shortest codes for a noisy data corpus, 
there is a two-part code of length L(N)+L(M), 
where M contains all "useful" information and N contains all "noise". 
The theory is called "algorithmic statistics", while in practice two-part 
<a class="extern" href="http://en.wikipedia.org/wiki/Minimum_description_length">MDL</a>/<a class="extern" href="http://en.wikipedia.org/wiki/Minimum_message_length">MML</a> is used: 
A probabilistic model M plus the log-likelihood of the data X under the model: CodeLength(X)
= -log P(X|M) + L(M). If you have any stochastic decompressor
that reconstructs the next symbol correctly only with a certain probability,
you can use arithmetic coding to convert this into a lossless compressors.
Of course you pay a price of -log P(X|M) in code length.
</li><li>
  Instead of allowing lossy compressors and some fixed scheme and penalties for error correction,
the contest rules allow the participant to deal with errors and noise as they see fit.
</li></ul>

<!-- ============================== -->
<a name="xvalid"><h3>Why aren't cross-validation or train/test-set used for evaluation?</h3></a>
<!-- ============================== -->
A common way of evaluating machine learning algorithms is to split the data into 
a training set and a test set, learn e.g. the parameters of a Neural Network (NN) on the train set
and evaluate its performance on the test set.
While this method, and similarly its extension to cross-validation, can work in practice,
<a href="http://www.hutter1.net/official/bib.htm#exidd">it is not a fool-proof method for evaluation</a>:

In the training phase, 
the algorithm could <i>somehow</i> manage to "effectively" store the information contained in the test set
and use it to predict the test set without the desired generalization capability.
This can happen in a number of ways:
<ol type="a">
<li>The test set could be very <i>similar</i> or in the extreme case identical to the train set,
so even without access to the test set, 
the algorithm has effectively access to the information in the test set via the train set.
For instance, if you downloaded all images from the internet and randomly split them into train and test set,
most images would be in both sets, since most images appear multiple times online.
Similarly if you download all text. 
Admittedly, Wikipedia should be less prone to repetition, since curated.
</li><li>The algorithm could <i>accidentally</i> contain test set information, though statistically this is very unlikely,
and would only be a problem if HKCP received an enormous number of submissions, 
or contestants optimize their algorithms based on test-set performance.
</li><li>The contestant could <i>cheat</i> and simply hide the test set in the algorithm itself.
This could be circumvented by keeping the test set secret, 
but one could never be sure whether it has leaked,
a grain of doubt will always remain, and even if not, ...
</li><li>if the test set is taken from a <i>public</i> source like Wikipedia,
a gargantuan NN could be trained on all of Wikipedia or the whole Internet.
Limiting the size of the decompression algorithm can prevent this.
Indeed this is the spirit of the used compression metric.
</li></ol>
One the other hand, including the size of the decompressor rules out many <a class="extern" href="http://www.urbandictionary.com/define.php?term=SOTA">SOTA</a> batch NN, which are often huge,
but maybe they only <i>appear</i> better than HKCP records, due to some of (a)-(d).
The solution is to <a href="#largnn">train online</a> or 
to go to larger corpora that are a more comprehensive sample of human knowledge.

<!-- ============================== -->
<a name="learn"><h3>Why is (sequential) compression superior to other learning paradigms?</h3></a>
<!-- ============================== -->
The limitations of 
<a href="#xvalid">train/test set split</a>, 
<a href="#reg">regularized maximum likelihood</a>, and
<a href="#lossless">lossy compression</a>
is discussed in the linked FAQs.
There are many further popular learning paradigms, e.g. 
<i>continual/lifelong/never-ending learning</i>,
<i>multitask/transfer learning</i>,
<i>meta-learning</i>,
<i>few-shot learning</i>,
<i>fine-tuning / domain adaptation</i>,
<i>out-of-distribution generalization</i>,
<i>online learning</i>.
The (sequential) compression paradigm subsumes all of them.
The general reason is that compression algorithms are designed to find and exploit regularities in the data
(which is also at the heart of all other learning algorithms)
but without committing or being confined to any particular regularity.
Indeed, the <a href="#kolmo">ultimate compressor (Kolmogorov complexity)</a> finds <i>all</i> regularities,
hence automatically subsumes/mimics/outperforms all specific learners to the extent they are effective.
Conversely, since practical learning algorithms in effect find and exploit <i>particular</i> kinds of regularities in the data,
these algorithms can be useful components within compression algorithms.
Learning paradigms devised to improve prediction accuracy in the specific settings above,
therefore can also be repurposed to improve compression:
<a href="#compai">Better prediction implies better compression</a>.

<!-- ============================== -->
<a name="reg"><h3>Why is Compressor Length superior to other Regularizations?</h3></a>
<!-- ============================== -->

<i>Probabilistic models:</i>
Most modern data compressors are based on 
<a class="extern" href="http://en.wikipedia.org/wiki/Statistical_model">probabilistic models</a> of the data.
Assume you have a model M that assign probability P(X|M) to data X (in our case X=<tt>enwik9</tt>).
It is the possible to code X via <a class="extern" href="http://en.wikipedia.org/wiki/Huffman_coding">Huffman</a> 
or <a class="extern" href="http://en.wikipedia.org/wiki/Shannon%E2%80%93Fano_coding">Shannon-Fano coding</a> 
or in practice <a class="extern" href="http://en.wikipedia.org/wiki/Arithmetic_coding">arithmetic coding</a> 
in ⌈log 1/P(X|M)⌉ bits.
But (de)coding requires knowledge of the model, so we need C(M) further bits for (de)coding X,
hence a code of total length C(X,M):=⌈log 1/P(X|M)⌉+C(M).

For instance, P(X|M):=1 and P(X'|M)=0 for all X'≠X is a valid (deterministic) model,
which allows to code X in ⌈log 1/P(X|M)⌉=log 1/1 = 0 bits,
but since M must store all of X, C(M)≥C(X) and we have gained nothing.
If we choose a uniform distribution, P(X|M)=2^-L, where L is the length of X in bits,
then C(M)=<a class="extern" href="http://en.wikipedia.org/wiki/Big_O_notation">O(1)</a> 
hence C(X,M)=log(1/2^-L)+O(1)=L+O(1), achieving no compression at all.
A good model somewhere lies in-between these two extremes,
trading off the two-parts in C(X,M).

<br><i>Regularization:</i>
Adding C(M) is a special case of <a ref="http://en.wikipedia.org/wiki/Regularization_(mathematics)">regularization</a> 
in <a class="extern" href="http://en.wikipedia.org/wiki/Machine_learning">machine learning</a> and 
<a class="extern" href="http://en.wikipedia.org/wiki/Statistics">statistics</a>. Many estimators can be rephrased as 
<a class="extern" href="http://en.wikipedia.org/wiki/Maximum_likelihood_estimation">Maximum Likelihood estimators</a>
(e.g. least-squares <a class="extern" href="http://en.wikipedia.org/wiki/Linear_regression">regression</a>):
Find M in some restricted class which maximizes the data <a class="extern" href="http://en.wikipedia.org/wiki/Likelihood_function">likelihood</a> P(X|M),
or equivalently minimizes code length log 1/P(X|M), but this <a class="extern" href="http://en.wikipedia.org/wiki/Overfitting">overfits</a> 
X for large model classes.
The solution is to add a regularizing penalty term R(M),
and minimize -log(P(X|M))+R(M) instead.
If R is chosen to be a code-length for M, 
this exactly reduces to C(X,M) above.
If we chose R smaller, e.g. C(M)/2,
the minimum would be achieved by the deterministic model above.
If we chose R larger, e.g. 2×C(M),
the minimum would be achieved by <a class="extern" href="http://www.scholarpedia.org/article/Algorithmic_probability">Solomonoff's universal distribution</a>.
Only the choice R(M)=C(M) leads to reasonable models M.

<br><i>Alternative Regularizers:</i>
Now, in statistics and machine learning, many other regularizers are used,
which are not or do not appear to be code lengths.

This is ok if and only if a restricted class of models is considered.
The most common assumption is that X is a sequence of 
<a class="extern" href="http://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables">independent and identically distributed (iid)</a>
"tokens", e.g. <a class="extern" href="http://en.wikipedia.org/wiki/Bernoulli_process">coin flips</a> or 
<a class="extern" href="http://en.wikipedia.org/wiki/Bag-of-words_model">bag-of-words model</a>, in the simplest case.
For <a class="extern" href="http://en.wikipedia.org/wiki/Parametric_model">parametric models</a>,
regularizers are mostly chosen by convenience and what works in practice,
e.g. <a class="extern" href="http://en.wikipedia.org/wiki/Convex_function">convex</a> to ease optimization, 
<a class="extern" href="http://en.wikipedia.org/wiki/Lasso_(statistics)">LASSO</a> to encourage sparse models,
classical <a class="extern" href="http://en.wikipedia.org/wiki/Tikhonov_regularization">Ridge regression</a> for analytic solutions, etc.

For semi-parametric models with d real parameters, where d varies,
MML/MDL/BIC/AIC/... have been developed.
<a class="extern" href="http://en.wikipedia.org/wiki/Minimum_message_length">MML</a> 
uses R(M)=C(M) and constructs an explicit code for iid models,
<a class="extern" href="http://www.scholarpedia.org/article/Minimum_description_length">MDL</a> and 
<a class="extern" href="http://en.wikipedia.org/wiki/Bayesian_information_criterion">BIC</a> 
can be regarded as (crude) approximations of this.
While they have legit (approximate) code length interpretation,
the expressions are valid only for iid data.
They can be extended to <a class="extern" href="http://en.wikipedia.org/wiki/Markov_chain">Markov data</a>, 
and possibly <a class="extern" href="http://en.wikipedia.org/wiki/Stationary_process">stationary data</a>,
but essentially not beyond. 
As an extreme example, without model restrictions, you could code all of <tt>enwik9</tt> in the binary expansion of a single real number,
with C(<tt>enwik9</tt>,Mextreme)=½log(10^9)≈15 for BIC or even just 1 for AIC. If you insist on using 
<a class="extern" href="http://en.wikipedia.org/wiki/Floating-point_arithmetic">floating point numbers</a>,
chop enwik9 into chunks of F bytes interpreted as floats (with some caveats), 
and start arguing what is the maximal "legal" precision F 
(<a class="extern" href="http://en.wikipedia.org/wiki/Single-precision_floating-point_format">4</a>? 
<a class="extern" href="http://en.wikipedia.org/wiki/Double-precision_floating-point_format">8</a>? 
<a class="extern" href="http://en.wikipedia.org/wiki/Quadruple-precision_floating-point_format">16</a>? 
<a class="extern" href="http://en.wikipedia.org/wiki/Octuple-precision_floating-point_format">32</a>? 
<a class="extern" href="http://en.wikipedia.org/wiki/Arbitrary-precision_arithmetic">...</a>? 10^9?).
For <a class="extern" href="http://en.wikipedia.org/wiki/Nonparametric_statistics#Non-parametric_models">non-parametric models</a> 
such as <a class="extern" href="http://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">k-Nearest Neighbors (kNN)</a>,
<a href="http://www.hutter1.net/official/bib.htm#lorp">LoRP</a> can be used and has a code-length interpretation.

Regularizers smaller than C(M) such as <a class="extern" href="http://en.wikipedia.org/wiki/Akaike_information_criterion">AIC</a> are particularly dangerous,
since they can lead to huge models.

<br><i>Conclusion:</i> <!-- Code length regularization -->
As argued above, many heuristic and useful but limited regularizers have been suggested, 
but (only) a general code length penalty C(M)
imposes no restrictions on data X and models M,
which is important for this <a href="#compai">AGI-motivated contest</a>.
Beyond the arguments given above, 
this choice can be rigorously justified by <a class="extern" href="http://www.scholarpedia.org/article/Algorithmic_information_theory">Algorithmic Information Theory</a>.

<!--  Crude MDL and BIC assigns each parameter a cost of (1/2)log(n) bits, but ignores O(d) terms.
Refined MDL includes Fisher Information and gets the O(d)-term correct apart from
MML refines MDL and makes it an exact code-length, i.e. gets the O(1) term correct.
This is essentially limited to iid data for (semi)parametric models.
Of course one could still use them, but then without any justification.
AIT does not need to make any assumption on the data source.
AIC, BIC, and most other regularizations have been derived and justified only for iid data. -->

<!-- ============================== -->
<a name="largenn"><h3>How can I achieve small code length with huge Neural Networks?</h3></a>
<!-- ============================== -->
Large Neural Networks (NN) can be trained to achieve 
<a class="extern" href="http://paperswithcode.com/sota/language-modelling-on-hutter-prize">excellent performance for text compression incl. enwik9</a>, 
but are not competitive for the HKCP,
since the networks often have (tens of) millions of (real-valued) parameters (weights and biases), 
i.e. the models hence decompressors are huge,
possibly even larger than enwik9 itself.
On the other hand, the untrained NN usually has a simple description:
the structure of the network, the activation function, the learning algorithm,
some hyper-parameters, and deterministic (pseudo-random or zero) weight initialization -- 
all code that is explicitly written by a human,
which by necessity is not much, 
as long as no data-heavy libraries such as dictionaries are included.
In this case it is possible to include only the smallish code
and not the millions of trained weight values in the (de)compressor, 
<i>provided</i> the NN is <a href="#online">trained sequentially or online (rather than in batch)</a>,
and similarly for other small-ROM(=code) large-RAM(=parameters,...) algorithms.
<a class="extern" href="http://openreview.net/forum?id=Hygi7xStvS">Huge Transformer NN have achieved SOTA code-length on enwik8</a> 
<a class="extern" href="https://bellard.org/nncp/">and enwik9</a> this way, but they are currently too slow for enwik9.

<!-- ============================== -->
<a name="online"><h3>Batch vs incremental/online/sequential compression.</h3></a>
<!-- ============================== -->
<i>Batch learning:</i> 
Consider data X chopped up into natural units x1,x2,...,xn, such as 
bits, bytes, words, phrases, sentences, or even paragraphs and beyond.
Batch learning trains the parameters of a model M on all data X, often 
via multiple passes through x1..xn. 
Once the model is trained, the parameters are frozen, and M can be used 
for prediction or compression.
The code length of the model consists of the program itself and the 
trained parameters,
the inclusion of the latter prevents overfitting by penalizing large 
models.
This makes huge Neural Networks with millions or even billions of 
parameters (weights) non-competitive, 
but there is a simple solution:

<br><i>Sequential learning:</i> 
In <a class="extern" href="http://en.wikipedia.org/wiki/Incremental_learning">incremental</a> or 
<a class="extern" href="http://en.wikipedia.org/wiki/Online_machine_learning">online learning</a> (see also
<a class="extern" href="http://dx.doi.org/10.1109/TAI.1993.633978">Predictive MDL</a> or 
<a class="extern" href="http://arxiv.org/abs/2210.07931">Prequential MDL</a>),
learning and prediction are interleaved:<br>
For t=1,2,3,...:<br> 
- train the model only on xt (online) or only on data x1..xt up to time t (sequential),<br>
- use it to probabilistically predict the next data item x_{t+1},<br>
- employ an arithmetic (de)coder based on this prediction.<br>

In this case, the (de)compressor only consists of the (small) algorithm itself,
since it does not need to know the trained parameters.

<br><i>Conversion:</i> 
All algorithms can trivially be converted from batch to online at a computational cost,
which can be mitigated by training on and predicting <a class="extern" href="http://stats.stackexchange.com/questions/117919/what-are-the-differences-between-epoch-batch-and-minibatch">mini-batches</a>. 

An exponential scheme such as t=1,2,4,8,16,... (or a bit finer) rather than fixed size
leads to minimal computational overhead, likely with comparable compression.
Plenty of details, such as dynamic hyper-parameter choices, 
likely make a good batch to sequential conversion less straight-forward.

Also, the online performance will look inferior to the <a href="#xvalid&quot;">batch train/test performance</a>,
since the smaller t the worse the prediction,
but this is no price to pay for a fair and incorruptible and universal comparison.

<br><i>Regularization effect:</i> Illustration:
If your test set is contaminated, say, in the extreme case is a subset of the training set, 
then any "bad" batch learning algorithm that just effectively stores the training set has better test performance 
compared to any non-interpolating batch learning algorithm that tries to do the "right" thing and generalize.
But if you train&amp;evaluate online as above, 
memorization algorithms that do not also generalize are punished, 
because they don't predict x_{t+1} well.

<!-- ============================== -->
<a name="background"><h3>Why don't you allow using some fixed default background knowledge database?</h3></a>
<!-- ============================== -->

Since Wikipedia contains enough human knowledge in the text itself:
If advantageous, some advanced statistical and/or other smart
but relatively short algorithms could extract and create such a
probably more structured "database of knowledge" from Wikipedia.
If this turns out to be impossible, it would indicate that enwik9
is not a representative sample of human knowledge,
and larger sources should be used.

<!-- ============================== -->
<a name="understand"><h3>Why is "understanding" of the text or "intelligence" needed to achieve maximal compression?</h3></a>
<!-- ============================== -->

<!--
One may think that a purely syntactic/statistical
approach that ignores any meaning of the text is going to be more
intelligent, by our criterion, since addressing semantics just adds
baggage and size to the decompressor.
-->

Consider the following
<a class="extern" href="http://en.wikipedia.org/w/index.php?title=List_of_XML_and_HTML_character_entity_references&amp;oldid=68431857">little excerpt from a Wikipedia page</a>,
where 12 words have been replaced by the placeholders [1]-[12].

<blockquote><i>In [1], HTML and XML documents, the logical constructs known as
character data and [2] [3] consist of sequences of
characters, in which each [4] can manifest directly
(representing itself), or can be [5] by a series of
characters called a [6] reference, of which there are [7]
types: a numeric character reference and a character entity
[8]. This article lists the [9] entity [10] that
are valid in [11] and [12] documents.
</i></blockquote>

If you do not understand any English,
there is little hope that you can fill in the missing words.
You may assign a high probability that the missing words
equal some of the other words,
and a small probability to all other strings of letters.
Working on a string pattern matching level you may conjecture <i>[11]=HTML</i> and <i>[12]=XML</i>,
since they precede the word "documents", and similarly
<i>[9]=character</i> preceding "entity".

If you do understand English you would probably further easily guess that
<i>[4]=character</i>,
<i>[5]=represented</i>, and
<i>[7]=two</i>, where
<i>[4]</i> requires to understand that characters are "in" sequences,
<i>[5]</i> needs understanding of conjugation, and
<i>[7]</i> needs understanding the concept of counting.
To guess <i>[8]=reference</i> probably needs some understanding of the
contents of the article, which then easily implies
<i>[10]=references</i>.

As a Markup Language expert, you "know" that
<i>[2]=attribute</i>,
<i>[3]=values</i>, and
<i>[6]=character</i>, and you may conjecture
<i>[1]=SGML</i>.

So clearly, the more you understand, the more you can delete
from the text without loss (this idea is behind 
<a class="extern" href="http://en.wikipedia.org/wiki/Cloze_test">Cloze-style reading comprehension</a> and led to the 
<a class="extern" href="http://www.kaggle.com/c/billion-word-imputation/">Billion Word Imputation challenge</a>).

If a program reaches the reconstruction capabilities of a human,
it should be regarded as has having the same understanding.
(Sure, many will continue to define intelligence as what a machine can't
do, but that will make them ultimately non-intelligent).
(Y cn ccmplsh lt wtht ndrstndng nd ntllgnc, bt mr wth).

<!-- ============================== -->
<a name="text"><h3>Why do you focus on text?</h3></a>
<!-- ============================== -->
<i>Visual (and audio and tactile) knowledge precedes linguistic
knowledge and seems crucial to ground symbolic knowledge.</i><br><br>

While the first part is true, the second is questionable. We decided
not to include photo, video, and audio material in the file for the
following reasons:

Natural text, in particular novels, contains ample of descriptions
of 3-dimensional scenes (the white house right of the beautiful tree
under which ...). It is possible to extract the meaning of nouns and
(directional) propositions and all other words from a sufficiently
large corpus of text in an unknown language. Meaning is codified by
relating words to other words (house-artificial, house-residence,
house-right-tree), so a (spatial) representation of the world can be
established without ever having seen a single picture
This is akin to being able to "visualize" abstract mathematics,
given enough exposure and skills.

So it is plausible that any knowledge that can be demonstrated
over a text-only channel as in the Turing test,
can also be learned over a text-only channel,
as e.g. evidenced by the blind and deaf writer Helen Keller.

Given that higher (conscious) cognition is essentially symbolic, and
a symbolic representation and understanding can be extracted from
text alone, it is plausible that textual information is sufficient
for the purpose of this contest.

Inclusion of photo, video, and audio material would require the
contestants to deal with and improve upon not only state-of-the-art
textual but also many kinds of non-textual compressors
and deal with terabyte of video. Most of the compression would
be about modeling physics rather than cognition.
This puts enormous unnecessary extra burden upon the contestants.
and given the arguments above, without real benefit.

<!-- ============================== -->
<!-- <a name="otheraitests"><h3>Other tests would be or are more suitable for measuring intelligence</h3></a> -->
<!-- ============================== -->
<!-- Write more. Mahoney's answer is currently superior

<i>Wouldn't carefully crafted tests of knowledge, capability, comprehension,
and ability to reason, to recognize, to solve problems, to learn, etc
are better measures of intelligence? </i>
<br><br>
No, because compression is quick, quantitative, objective, and repeatable, which other tests lack.
If you were to run a competition lasting for many years among AI
systems with cash prizes, how would you design the tests so that the
competition is fair, scores are 100% objective, and systems can't
cheat?

The Turing test for artificial intelligence is widely accepted,
but is subjective, qualitative, non-repeatable, and difficult to
implement. The alternative test without these drawbacks to
insert a machine's language model into a predictive encoder and
compress a corpus of natural language text. A ratio of 1.3 bits
per character or less indicates that the machine has AI is better.
-->

<!-- ============================== -->
<a name="kolmo"><h3>What is the ultimate compression of enwik9?</h3></a>
<!-- ============================== -->

The <a class="extern" href="http://www.scholarpedia.org/article/Kolmogorov_Complexity">Kolmogorov Complexity</a> <i>K</i> 
of a string <i>x</i> is
defined as the length of the shortest program (self-extracting
archive) computing <i>x</i>, hence <i>K</i>(<tt>enwik9</tt>) 
is the ultimate compression length of <tt>enwik9</tt>.

Unfortunately <i>K</i>(<i>x</i>) itself cannot be computed, only
approximated from above, namely by finding better and better
compressions, but even if we reach <i>K</i>(<i>x</i>), we will never know whether we
have done so. 

One can also not prove any interesting lower bound on <i>K</i>(<i>x</i>) because there are 
<a class="extern" href="http://mathoverflow.net/questions/29949/what-is-the-shortest-program-for-which-halting-is-unknown">very short programs for which we cannot (dis)prove whether they halt</a>,
even if <i>x</i> is as large as <tt>enwik9</tt>.
In princple, if <a href="http://www.hutter1.net/official/bib.htm#ctoex">the universe is computable</a> 
from simple initial conditions (cf. Conway's <a class="extern" href="http://en.wikipedia.org/wiki/Conway%27s_Game_of_Life"> Game of Life</a>),
running this <a class="extern" href="http://en.wikipedia.org/wiki/Theory_of_everything">TOE</a> programm <i>p</i> (as a thought experiment) and 
<a href="http://www.hutter1.net/official/bib.htm#ctoe">locating</a> (<a href="http://www.hutter1.net/official/bib.htm#ctoe2">≈100 bits</a>), interpreting and printing (O(10'000 bits))
the simulated <tt>enwik9</tt> in this simulation 
constitutes a program of possibly only a few KiloByte(!) for <tt>enwik9</tt>.
If we <i>assume</i> that quantum noise is truly random, 
we may be able to estimate the degree to which <tt>enwik9</tt> has been affected by change events,
which should lead to a possibly interesting (high-probability) lower bound.
If we assume limited resources, as this compression contest does, a better lower bound may be possible,
even in a deterministic world.

More empirically, Shannon's lower estimate suggests that humans might be able to compress <tt>enwik9</tt> down to 75MB, and computers some day may do better.
<br>
<a class="extern" href="http://www.scholarpedia.org/article/Algorithmic_probability">Algorithmic Probability</a> 
is the predictive counterpart of Kolmogorov complexity,
the optimal predictor leading to the optimal compressor by arithmetic coding.

Probably the strongest endorsement of the importance of Algorithmic Probability to Artificial Intelligence is
by AI father, <a class="extern" href="http://www.youtube.com/watch?v=DfY-DRsE86s&amp;t=1h30m02s">Marvin Minsky</a>. Even 50 years after its discovery 
he <a class="extern" href="http://www.youtube.com/watch?v=DfY-DRsE86s&amp;t=1h30m02s">says</a>:
<i>It seems to me that the most important discovery since Gödel 
was the discovery by Chaitin, Solomonoff, and Kolmogorov of a concept called Algorithmic Probability.
... This is a beautiful theory ... Everybody should learn all about that and spend the rest of their lives working on it.</i>


<!-- ============================== -->
<a name="recursive"><h3>Why recursively compressing compressed or compressing random files won't work</h3></a>
<!-- ============================== -->

This idea comes up over and over again and has been <a class="extern" href="http://www.faqs.org/faqs/compression-faq/part1/section-8.html">discussed and refuted at length</a>.
Unlike a perpetuum mobile, which is "only" physically impossible, 
a universal compressor is even mathematically impossible.
<br>
Real data is compressible only because it has regularities and is redundant.
A compressed file has less regularities and hence is less compressible.
Ultimately you hit an archive that looks random to any compressor, 
hence cannot be compressed any further.
<br>
This can formally be proven by a counting argument as follows:
A lossless compressor C is an 
<a class="extern" href="http://en.wikipedia.org/wiki/Injective_function">injective function</a> 
from data files to archive files. 
If C mapped two files X≠Y to the same archive A,
the decompressors D could not uniquely reconstruct the data
(C(X)=A=C(Y) implies X=D(C(X))=D(A)=D(C(Y))=Y contradicting X≠Y).
Now there are 2^n files of length n bits,
but there are only 2^{n-c} files of length (less than) n-c bits,
hence only a fraction 1/2^c files can be compressed from n to (less than) n-c bits.
That is, less than 0.4% of all files can be compressed by 1 byte.
The chance that your compressed file is again compressible by 1 byte is again less than 0.4%,
and so on. Of course this argument only applies when you start with a random file,
and real data is far from random, but the more a file is compressed, the more it looks random,
and ultimately you hit a lower bound, 
called <a href="#kolmo">Kolmogorov complexity</a>.
<br>
It is a weird fact that a maximally compressed file is computationally indistinguishable from random noise.
If you open a zipped archive with an ASCII editor, you will notice.
Still it contains all information of the original file.
<br>
Aliens may be able to decipher our analog radio and TV transmissions that have escaped to space in the last century,
but they will find it increasingly harder to decipher our increasingly compressed digital transmissions
(without providing them with the decompressor). And vice versa, 
maybe some of the apparent cosmic noise we receive are <a href="http://www.hutter1.net/official/bib.htm#singularity">highly compressed signals from advanced aliens</a>. 

<!-- C:{0,1}* → {0,1}* and a decompressor D:{0,1}* → {0,1}* inverts this. D(C(x))=x.  -->


<!-- ============================== -->
<a name="proof"><h3>Can you prove the claims you make in the answers to the FAQ above?</h3></a>
<!-- ============================== -->
Most of the assertions above can be formalized and proven,
but some mathematical background is necessary.
Good science books to start with are
<a class="extern" href="http://homepages.cwi.nl/~paulv/kolmogorov.html">M. Li &amp; P.M.B.Vitanyi, Kolmogorov Complexity, 2008</a>, and
<a href="http://prize.hutter1.net/ai/uaibook.htm">M.Hutter, Universal Artificial Intelligence, 2005</a>.
The <a href="http://prize.hutter1.net/index.htm#links">recommended reading</a> contains further 
(informal, formal, experimental, theoretical, philosophical) 
demonstrations/insights/arguments for (the relevance of) the compression paradigm for A(G)I.
For a <a href="http://prize.hutter1.net/ai/introref.htm">broader reading recommendation, see here</a>.

<!-- ============================== -->
<a name="paq8"><h3>The PAQ8 compressors are already so good that it will be difficult to beat them.</h3></a>
<!-- ============================== -->
Yes, it will not be easy (nobody said this),
but there is likely a lot of room for improvement.
PAQ8 models text only.
There has been lots of research in language modelling (mostly for speech recognition)
at the syntactic and semantic levels,
but these are usually offline models on preprocessed text where words are mapped to tokens,
and messy details like capitalization, punctuation, formatting, and rare words are removed.
So far, nobody has figured out how to integrate these two approaches,
but when that happens we will have a very powerful text compressor.
From Shannon's estimates, human text contains information of about 0.6-1.3 bit per character,
so humans may compress <tt>enwik9</tt> down to 75MB and super-intelligent machines should do better.
On the other hand, compressors have already beaten by far Shannon's upper estimate.

Also, from 2006-2017, PAQ8 has been beaten the previous record 4 times on enwik8 in total by ~14%,
which is over 1% per year, which also seemed unlikely at the time.

<!-- ============================== -->
<a name="garbage"><h3>There are lots of non-human language pieces in the file.</h3></a>
<!-- ============================== -->
<tt>enwik9</tt> is about 75% clean text and 25% "artificial" data of various forms
(tables, hypertext links, XML, etc). As long as there is <i>some</i> natural
language text (and <tt>enwik9</tt> contains 75%),
compressing it is a hard AI problem,
regardless of whatever else is present in the file.
The "artificial" data is also human-generated,
and belongs to the human knowledge base.
So it is not that devious to ask to model that too.
Ideas of filtering out the "artificial" data or using other
apparently cleaner text-sources didn't prove superior.

<!--
I did an analysis of the data here:
http://mattmahoney.net/dc/textdata.html
It is about 75% clean text and 25% "artificial" data of various forms
(tables, hypertext links, XML, etc).  I debated filtering it out, but I
was afraid it might give the appearance of bias toward my own
compressors.  So I left it in.

Another choice might have been 1 GB of text from the Gutenberg project,
or several thousand books.  This is arguably cleaner, but has other
problems, such as nearly identical headers in each book, inconsistent
rules for line breaks, and the fact that most of the books are over 80
years old.

Any training set for AI is going to contain messy details that must be
dealt with.  My reasoning was that as long as there is some natural
language text, then compression will still be a hard problem in AI,
regardless of whatever else was present.  Any "artificial" data can't
be harder to model than the algorithm that generated it.
-->

<!-- ============================== -->
<a name="adddecomp"><h3>Why include the decompressor?</h3></a>
<!-- ============================== -->
<i>Doesn't adding the decompressor length to the length of the archive
encourage developing unreadable short decompressors, rather than
smart compressors based on understanding the text corpus enwik9?</i>
<br><br>
The typical size of current decompressors is less than 100KB. So
obscuring them by making them shorter will give you at most
a 0.01% = 100KB/1GB advantage (not enough to be eligible for the prize).
On the other hand, for fairness it is necessary to include the
size of the decompressor. Take a compressor like PAQ8H that contains 800KB of tokens or 
<a class="extern" href="http://paperswithcode.com/sota/language-modelling-on-hutter-prize">NLP models with billions of parameters</a>. 
Clearly it can achieve better compression than one from scratch. 
If you're not convinced by this argument, consider
an extreme "decompressor" of size 1GB that simply outputs
<tt>enwik9</tt> byte by byte (from a zero byte archive),
thus achieving a compressed size of 0.

<!-- equal weight otherwise shift, no other way of objective determ. what is cheating  -->

<!-- ============================== -->
<a name="addcomp"><h3>Why do you require submission of the compressor and include its size and time?</h3></a>
<!-- ============================== -->

<ol>
<li><i>Induction=compression:</i>
While only measuring decompressor+archive size is clean and simple, 
it also has some downsides. Only the decompressor=predictor is evaluated.  
Induction=compression is arguably even more important for AI.

</li><li><i>Compression time:</i>
Demonstrating the compressor within limited computational resources limits labor-intensive compression of enwik9 by hand 
as well as use of excessive hardware resources unavailable to most contestants.
<!-- 
If giving the compressor a free pass, in principle one could compress enwik9 by hand, and a lot of tuning is done by hand,
or by excessive search with enormous computational resources,
which would disadvantage those not having access to super-computers available to some employees of some big organizations.
-->

</li><li><i>Compression is harder than decompression:</i>
In general, compression is much harder than decompression.  
For instance, the mpeg video compression standard contains features that are easy and fast to implement for decompressors, 
but that no current compressor is able to exploit/produce.  
So far, this is not the case for most text compressors, but we expect this to happen with smarter compressors.

</li><li><i>Compression learns, decompression predicts:</i>
On the one hand, a compressed version of enwik9 provides a good 
(predictive) model of Human Knowledge, and this is an important 
motivation for the contest.  
On the other hand, if knowledge increases, the model and hence the 
compressed file has to be updated, which should be automatic, rather 
than by hand or using computing resources inaccessible to most.  
For good reasons we decided against <a href="#xvalid">cross-validation</a> or testing on a secret test-set. 
The compressor learns, the decompressor predicts, both are important for AI.

</li><li><i>Compression size:</i>
If we were to limit only the compute time of the compressor C and not its size L(C), 
one could simply submit the archive A prefixed with a tiny program to output A verbatim, 
ignoring input enwik9.  
This formally satisfies the criteria of being a compressor, 
but with all the compute hidden in producing A in advance.

</li><li><i>Equivalent alternatives:</i>
We therefore opted to evaluate L(C)+L(DA), 
where DA is a self-extracting archive for enwik9, 
and L(DA) its size in byte.  
If we allow C to produce a decompressor D and archive A separately, 
we (have to) measure L(C)+L(D)+L(A).  
If we relax the condition that C has to output D, 
but allow to submit D separately, we (need to) measure L(C)+2×L(D)+L(A).

</li><li><i>Other advantages:</i>
By just measuring L(D)+L(A), 
one can freely hand-craft large word tables (or other structures) used by C and D, 
and place them in C and either D or A.  
By counting both, L(C) and L(D), such tables become 2-3 times more expensive, 
and hence discourages them. 
If you need large (word) tables, write a short program that creates them from enwik9 itself.
L(C)+2×L(D)+L(A) encourages small but non-vacuous C, 
arguably containing the interesting algorithmic learning and prediction aspects, 
while A contains the compressed knowledge.

</li><li><i>Non-working/non-equivalent:</i>
If we drop the 2×, one could choose C to produce an empty archive A', 
and choose huge D' to include the archive A, 
leading to L(C')+L(D)+L(A') = 0+L(D')+0 = L(D)+L(A), 
circumventing the requirement of submitting a compressor.  
Dropping the L(C), 2×L(D)+L(A) would strongly favor vacuous programs D'=0 with self-extracting archive DA.
</li></ol>

<!-- ============================== -->
<a name="perplexity"><h3>Why not use Perplexity, as most big language models do?</h3></a>
<!-- ============================== -->

The quality of natural language models
is typically judged by its <a class="extern" href="http://en.wikipedia.org/wiki/Perplexity">Perplexity</a>,
which is essentially an exponentiated compression ratio: Perplexity(X):=2^{CodeLength(X)/Length(X)},
hence equivalent to CodeLength. 
That is, Perplexity per-se as a performance measure is fine.
The problem is that it is usually applied to a <a href="#xvalid">test set</a> and the 
<a href="#adddecomp">size of the language model</a> itself is ignored.
<a href="#onnline">Online compression</a> would be most appropriate for large models.

<!-- ============================== -->
<a name="100mb1gb"><h3>Why did you start with 100MB enwik8 back in 2006?</h3></a>
<!-- ============================== -->
<!-- Why do you use the 100MB enwik8 and not 1GB or all of Wikipedia? -->
Indeed, there are good arguments for the 1GB corpus.
You can find them at
<a href="http://mattmahoney.net/dc/rationale.html">http://mattmahoney.net/dc/rationale.html</a>.
Nevertheless we decided to start initially with the 100MB file <tt>enwik8</tt> rather
than the 1GB file enwik9 for the following reasons:
<ol>
<li> Although enwik9 is not a hardship in the long run, it is a hardship now.
The contest will have highest participation when the prize will be initiated,
so the file-size should be appropriate for moderate-cost home-computers.
</li><li> 100MB already contains a lot of Human knowledge.
</li><li> Typical decompressors are significantly less than 1% of 100MB,
so there is no problem here.
</li><li> <tt>Enwik8</tt> could still be downloaded by an analog modem.
</li><li> It's easy to upgrade to enwik9 later.
</li><li> Many standard compressors crash on enwik9 or start thrashing.
</li><li> It allows non-streaming offline (de)compressors that need the whole
file in the RAM in a straightforward way. Enwik9 needs to be processed block-wise
unless you have many GB RAM available.
</li><li> Maybe the most compelling one: We want to encourage smart compressors.
Smart compressors can easily take ridiculous time already on small corpora.
So if we would go for enwik9, we would lose some of the smarter ideas for (de)compressors.
</li></ol>

<!-- ============================== -->
<a name="enwik9"><h3>Why did you go BIG in 2020?</h3></a>
<!-- ============================== -->
The contest started 14+ years ago.
Since then, processor speed, memory, disk space, internet speed have all increased sufficiently,
that many of the <a href="#100mb1gb">issues above</a> became less critical.
&gt;10GB RAM and &gt;100GB free HDD space is standard by now. 
Moore's law for PCs has been disappointing in the last decade,
so we decided to also be more generous with time. 
Finally I can afford to pay out larger prizes, so I decided to go 10x on all fronts:
<i>10x Size, 10x Money, 10x Time, 10x RAM, 10x HDD</i>.
Still I expect most testing will be done on enwik8 and even smaller files,
and enwik9 will only be used for the "final tweaks",
but these "final tweaks" <i>may</i> actually be the most relevant for AI.
NLP in recent years has been tremendously successful by training on billions of words.
Enwik9 arguably contains about as much non-visual information 
as a typical adult human, so human-level <a href="#understand">understanding=compression</a> may only be possible on enwik9.

<!-- ============================== -->
<a name="ram1gb"><h3>Why are you limiting (de)compression to less than 100 hours on systems with less than 10GB RAM?</h3></a>
<!-- ============================== -->
<!--
"Cheap" is relative to your wealth.  Some of the best minds in the world
are limited by the computers they can afford.  Indeed, many of the best
contestants are from the former communist countries where, due to
limitations on microelectronics during the Cold War, compensating techniques
for efficient programming and advanced mathematics were developed.
Moreover the motherboards for many older systems are limited in the number
of memory cards they can accept.  For contestants putting systems together
with such limitations, 1GB RAM for the whole system (800MB working RAM) was
chosen as widely-affordable high end system.
-->
<li><i>Compute/memory cost money:</i> 
Some of the best minds in the world are <a class="extern" href="https://doi.org/10.1145/3381831">limited by the computers</a> they can afford.
Typical PCs and Laptops bought in 2006 when the contest started were equipped with 1GB RAM or less,
let alone older systems and developing countries.
One of our most successful contestants wrote us:
"<i>I still don't have access to a PC with 1 Gb or more,
have found only 512 Mb computer, but it's about 7 or 8 km away from my home
(~20 minutes on the bicycle I use),
thus I usually test on a 5 Mb stub of enwik8, with -5, using only ~210 Mb</i>".
Even in 2020, upgrading to 16GB RAM for about 100€ sounds cheap (for many but not
all) compared to the involved prize money and invested effort.
But lets assume 10'000 potential participants (sure, fewer will succeed),
and, say, half of them first need to upgrade their machines,
then overall 5'000×100€ = 500'000€ is invested in
hardware, i.e. the prize supports the memory chip industry,
and overall the participants suffer a loss, 
which is definitely not the aim of the prize.
So 16GB RAM for the whole system (10GB working RAM) was
chosen as a widely-affordable system.
Also the overall ratio of 1 : 10 : 100 GB of <tt>enwik9</tt> : RAM : HDD seems balanced.

</li><li><i>Testing costs time:</i> 
Developing a competitive compressor, 
whether based on a grand new idea or by incremental improvements,
typically requires a lot of trial and error. 
Many ideas and parameter tuning may be testable on enwik8 or even smaller files,
but I expect a significant amount of testing on enwik9 itself is also necessary.
100 hours of CPU time per core per run is already inconveniently long for all but 
a few researchers with access to super-computers.

</li><li><i>Encourage ideas:</i> The more compute and memory one provides, the better one can compress.
Given unlimited compute, one can limit-compute the 
<a href="#kolmo">ultimate compression</a> of enwik9 by brute-force dovetailing through all programs.
This <a href="http://mattmahoney.net/dc/text.html">improve of compression with increase in compute and memory for otherwise the same algorithm is real for enwik9</a> and <a class="extern" href="http://arxiv.org/abs/2001.08361">NLP in general</a>.
The contest is not meant to award the person or organization who can afford to spend the most compute,
but to encourage algorithmic advances, 
which arguably can be achieved and measured in 100 hours as well as in 10'000 hours,
and some limit needs to be set. 
We would be thrilled to see such algorithm, when giving 1000-fold resources, 
to excel on some AI task or solve AGI.

</li><li><i>Applications:</i> Though this is not the primary purpose of this contest, 
it would be nice if the developed compressors
find <a href="http://prize.hutter1.net/compuse">applications</a> in other areas, such as archiving.
The more compression time we allow, the less likely this will be the case.

</li><li><i>AI:</i> Some argue that the very notion of (artificial) intelligence needs to include time and memory usage.
In any case, AI systems, the motivating application, will have resource constraints, so <i>some</i> constraints are unavoidable.
<!--But 14+ years later by 2020, processor speed, memory, disk space, internet speed, and my spare money increased,
so it was time to go bigger, but not unlimited, since the same considerations as above still and will always apply. -->
<!--
Future smarter compressors we are aiming at probably need a lot more time
to "meditate" about <tt>enwik9</tt> in order to compress it well.
For this and other reasons it is allowed to send
us the already compressed archive instead of the compressor.
Only the decompressor needs reasonable speed.
Since decompression is much easier than compression,
the time bound is hopefully not too severe. If it is, let us know.
-->
</li><li><i>Penalize time:</i> Instead of a hard time-bound one could think of penalizing slow (de)compressors.
In theory one should penalize a (de)compressor by one bit for every
factor of two it is slower than its competitor (keyword: <a class="extern" href="http://www.scholarpedia.org/article/Universal_search">Levin complexity</a>).
A (de)compressor running in 30 days compared to one running in one
second would be penalized by scanty 21 bits, i.e. less than 3 bytes,
so in practice one would need a much stronger penalty.
Similarly one could penalize excessive memory usage.

<!-- ============================== -->
<a name="cpucore"><h3>Why do you restrict to a single CPU core and exclude GPUs?</h3></a>
<!-- ============================== -->

The primary intention is to limit compute and memory to some generally 
available amount
in a transparent, easy, fair, and measurable way. 

100 hours on one i7 core with 10GB RAM seems to get sufficiently close 
to this ideal.
This roughly corresponds 500'000/T hours, where T is the GeekBench5 
score of the machine. 
The latter can be used by contestants to estimate whether their 
algorithm is likely to run in under 100 hours on our test machine. 

If your algorithm uses C cores, typical speedup is at most a factor of 
C, and often much less.
So adapting the rules to allow 100/C wall-clock hours on C cores would 
be unhelpful. 
If we'd allow 100 hours straight, we'd favor super computers with 1000s 
of cores.

The primary reason for excluding GPUs is that they <a class="extern" href="https://doi.org/10.1145/3467017">stifle creative/innovative ideas</a>.
Any algorithmic ideas that cannot (easily or at all) be vectorized are 
disadvantaged compared to vectorizable algorithms.
As an example, sparse Neural Networks seem to be superior to dense ones 
but have a hard time reaping the benefits on GPUs and TPUs.
If non-vectorizable algorithms start to consistently beat vectorizable 
ones on CPUs, 
this will drive hardware innovation to support whatever special 
structure (e.g. sparsity) these novel algorithms may have.
Using CPUs levels the playing field.

A secondary reason is that, while it is true that most NLP algorithms 
based Neural Networks run on GPUs (or even TPUs),
despite using much more compute, 
so far they are not quite competitive to <a class="extern" href="http://www.urbandictionary.com/define.php?term=SOTA">SOTA</a> compressors, which mostly run on a single CPU core.
As long as <a href="#largenn">large NN compression</a> is not competitive, 
the restriction to CPUs is mild.

Anyway, contestants are welcome to develop their own algorithms on whatever architecture they like.
Converting it back to single core and CPU is usually easy.

If the number of cores on PCs creeps up into the 100s and GPU-based NNs become competitive,
we will consider relaxing the current resource restrictions.

<!-- ============================== -->
<!-- <a name="spacetime"><h3>Since there is no limit on time and space, a slow brute force compressor would yield the highest compression</h3></a> -->
<!-- ============================== -->
<!--
Since there is no limit on time and space, a slow brute force compressor
would yield the highest compression.
-->
<!--
There <i>are</i> limits on time and space.

We require "reasonable" computation limits. The solution must be
testable by the public and have a 30 day period of public comment. If
nobody can run the program in that time, then it loses.  This rule
allows for better models as computers become more powerful.

While these requirements don't look tough, they are indeed quite serious.
One may argue that among two compressors compressing equally well
one should rank the faster one higher. The correct way of doing
this would be to penalize a decompressor by one bit for every
factor of two it is slower than its competitor (Levin complexity).
So a decompressor running in 30 days compared to one running in one
second should be penalized by scanty 21 bits < 3 byte.
-->

<!-- ============================== -->
<a name="money"><h3>The total prize is not exactly 500'000€</h3></a>
<!-- ============================== -->
Right! The prize formula is a compromise between many considerations
(simplicity, fairness, risk, and others).

Since it is <a href="#kolmo">principally impossible</a> to know what the ultimate compression of
<tt>enwik9</tt> will be, a prize formula leading to an exact total payout of 500'000€ is impossible.
So no formula allows to tell what total amount will be awarded.
Theoretically you can win more than 500'000€.
For instance, if you halve the current record, then
halve your own record, and then again halve it, you would
receive 3×250'000€. Some, admittedly mostly amateurs, believe that they
can even beat this, e.g. by <a href="#recursive">recursive compression</a>.
Realistically (based on past experience)
I expect at most a 1% improvement per year for the next 20 years or so,
but <i>you</i> may be the one having a breakthrough of 20% improvement tomorrow.
In theory, the current formula also allows optimizing your profit by sending
e.g. three 1% improvements in monthly intervals,
rather than one 3% improvement.
You would gain scanty 150€ at the risk that someone else sneaks in between.
(An alternative prize formula Z×ln(L/S) would prevent this gain).
The total payout will be 500'000€ if <tt>enwik9</tt> could
be compressed down to 43MB (in 1% steps). I would be thrilled to see this happening, 
but realistically I expect the total achievable payout to be smaller than that.
Shannon's lower estimate is 0.6 bit per character for humans,
corresponding to a compression down to 75MB and a total payout of ~223'000€.

This is still better than competitions with hard all-or-nothing winning criteria,
especially when in the end no-one was able to win.
Also, a win is often made possible due to <a href="#source">earlier advances</a> by many other people, 
the latter ending up empty-handed.

<!-- ============================== -->
<a name="1GB"><h3>Is 1GB 2^30 byte or 10^9 byte?</h3></a>
<!-- ============================== -->
Most computer scientists use the prefix K(ilo) for 2^10, M(ega) for 2^20, and G(iga) for 2^30,
while the International System of Units
(<a class="extern" href="http://www.bipm.org/utils/en/pdf/si-brochure.pdf">SI, 7th edt.1998, p.103</a>)
defines K(ilo), M(ega), G(iga) as 10^3, 10^6, 10^9, respectively,
and explicitly forbids misusing them for powers of 2.
So the answer depends on whether you are a geek or a pedant.
A good solution is to regard K/M/G as standing for "roughly" 10^3 or 2^10, etc, and
avoid using them if more precision is needed, as is done on the contest page. <tt>Enwik9</tt> is 10^9 byte.
See <a class="extern" href="http://web.archive.org/web/20130628015627/http://www.maa.org/mathland/mathtrek_6_7_99.html">MAA Online Counting by Twos (1999)</a>
for other solutions and references.
In any case this is tangential to the contest.

<!-- ============================== -->
<a name="html"><h3>The website looks dated</h3></a>
<!-- ============================== -->
True, but consistent with the spirit of the contest, simplicity trumps complexity.

I also disapprove of modern webpages, where sometimes only a couple of words fit on a FullHD screen,
because the screen estate has been wasted with irrelevant graphics, gigantic font size, and worst of all, empty space.

Generally I prefer function over form, information over design, content over marketing, relevance over art,
and this site is (hopefully) functional and informative with a lot of relevant content.

Fashion comes and goes, but like ASCII, HTML will be around forever.
Modern website building tools offer a lot of (irrelevant) functionality, 
but they rely on sophisticated back-ends and protocols,
and typically are around for a decade or two, 
and then something breaks or one has to switch to a new tool.
I neither have time nor patience nor sympathy nor <a class="extern" href="https://mtlynch.io/tinypilot-redesign/">money</a> for that.

Finally, I actually like the nostalgic look of good old html websites.

<!-- ============================== -->
<a name="exe"><h3>Why do you require Windows or Linux executables?</h3></a>
<!-- ============================== -->
There is a myriad of options we could allow for the contest.
The related <a href="http://mattmahoney.net/dc/textrules.html">Large Text Compression Benchmark</a>
for instance is very flexible in this respect. This is convenient for contributors,
but makes a fair comparison more difficult and fraud easier.
Ideal would be one platform (e.g. Turing's Universal Machine).
In real life, Windows and Linux executables cover most needs.
Most programs are written in C(++), which neatly compiles to self-contained
reasonably-sized executables, and indeed submission of zipped source is also allowed. 

<!-- ============================== -->
<a name="source"><h3>Why do you require submission of documented source code?</h3></a>
<!-- ============================== -->
A primary goal of this contest is to increase awareness of the relation between compression and (artificial) intelligence,
and to foster the development of better compressors. The (ideas and insights behind the) submitted (de)compressors
should in turn help to create even better compressors and ultimately in developing smarter AIs.
Up until 2017 the source code was not required for participation in the contest,
and has also not been released voluntarily.
The past submissions are therefore useless to others and the ideas in them may be lost forever.
Furthermore this made it difficult for other contestants to beat the (as of 2017) four-time winner Alexander Rhatushnyak.
Making the source available should rectify these problems.

Therefore, for the new "10×" contest, the source code is required,
which should help to revive the contest,
make it easier to build improved compressors by combining ideas,
foster collaboration,
and ultimately lead to better AI.
Contributors can still copyright their code or patent their ideas,
as long as non-commercial use,
and in particular use by other future contestants, is not restricted.

<!-- ============================== -->
<a name="getsource"><h3>Where can I find the source code of the past winners and baseline phda9?</h3></a>
<!-- ============================== -->

The <a class="extern" href="https://github.com/saurabhk/fast-cmix/">source code of the latest</a> 
and <a class="extern" href="https://github.com/amargaritov/starlit/">previous winner</a> is publicly available, and can be used/adapted/improved with attribution.
Unfortunately the last winner for enwik8 has not released the source code of his <a href="http://mattmahoney.net/dc/text.html#1165">phd9</a>, which forms the baseline for enwik9.
At the time of submission in 2017, 
providing source code for the (smaller) enwik8 contest was not a prerequisite.
Maybe someone can convince him to release his source code,
for the sake of scientific progress,
or maybe he is willing to trade the source for something else in return.
The source is based on the <a href="http://mattmahoney.net/dc/text.html#1323">PAQ8</a> 
family of compressors for which the <a href="http://mattmahoney.net/dc/paq8hp12any_src.rar">source code</a> is available.
<a href="http://mattmahoney.net/dc/text.html#1159">CMIX</a> builds on top of PAQ8 and 
was the top performing compressor until y2020 under no time constraint,
and its <a class="extern" href="http://github.com/byronknoll/cmix">source code</a> is also available.
The set of enwik-specific transformations from the 2017 winner 
is also <a class="extern" href="http://encode.su/attachment.php?s=61f5b3227a890bce60c45b9bd4a6ba0c&amp;attachmentid=6353&amp;d=1546482845">open source</a>.
Finally, here are some <a class="extern" href="http://qlic.altervista.org/da/desccomp_levels.odt">high-level</a> 
<a class="extern" href="http://qlic.altervista.org/da/desccomp_levels.pdf">thoughts</a> by the 
<a class="extern" href="http://docs.google.com/document/d/1a5JTuZQrcU2cNFaI3Eg9kA0cPLaOaE5nzh8eHzXVZ4A/">author</a>.

In January 2021, the modified Transformer-XL NN <a href="http://mattmahoney.net/dc/text.html#1123">NNCPv2</a> beat CMIX by over 3%, 
but is too slow. <a class="extern" href="https://bellard.org/nncp/nncp_v2.pdf">Documentation</a> 
and <a class="extern" href="https://bellard.org/nncp/">source code</a> are also available.

All together this should serve as a good starting point.

<!-- ============================== -->
<a name="license"><h3>Under which license can/shall I submit my code?</h3></a>
<!-- ============================== -->
Any of the Open Source Initiative <a class="extern" href="http://opensource.org/licenses">(OSI)-approved licenses</a> is acceptable.
In case of doubt, consider <a class="extern" href="http://choosealicense.com/">choosing a license</a> as permissive as you feel comfortable with.
We prefer <a class="extern" href="http://choosealicense.com/licenses/unlicense/">Unlicense</a> over 
<a class="extern" href="http://choosealicense.com/licenses/mit/">MIT</a> over 
<a class="extern" href="http://choosealicense.com/licenses/apache-2.0/">Apache</a> over 
<a class="extern" href="http://choosealicense.com/licenses/mpl-2.0/">Mozilla</a> over 
<a class="extern" href="http://choosealicense.com/licenses/lgpl-3.0/">GNU</a>, 
but all of them are acceptable.
Simply put a statement such as 
"<tt>This Code is licensed under UNLICENSE http://unlicense.org</tt>"
somewhere in your code.

<!-- ============================== -->
<a name="fame"><h3>What if I can (significantly) beat the current record?</h3></a>
<!-- ============================== -->
In this case, submit your code and win the award
and/or copyright your code and/or patent your ideas.
You should be able to monetize your invention beyond the HKCP.
This happened to the first winner,
a Russian/Ukrainian who always had to cycle 8km to a friend to test his code because
he did not even have a suitable computer,
and who now has a lucrative job at <a class="extern" href="http://www.quantum.ca/">QTR</a> in Canada.

I cannot directly help you with your aspirations,
but the HKCP award on your CV plus a report that clearly explains
your code, your algorithm, and the ideas behind them,
should make you an attractive employee and/or your patent 
a valuable asset.
The mp3 patent (the most famous lossy compressor for music)
for instance, made millions of dollars from licensing fees.

<!-- ============================== -->
<a name="self"><h3>How can I produce self-contained or smaller decompressors?</h3></a>
<!-- ============================== -->
<ul>
<li> Use plain C. It produces neat small self-contained executables, and is fast.

</li><li> External needed data files (like dictionaries) should be added to
the decompressor, and can in this way be (re)created by the
decompressor.

</li><li> You can compress your executable with programs like <a href="http://en.wikipedia.org/wiki/UPX">UPX</a>,
because the result is still an executable.

</li><li> <a class="extern" href="http://gcc.gnu.org/java/">GCJ</a> (use <a href="http://www.cygwin.com/">Cygwin</a> on Windows) can convert
Java to executables (smaller than 600KB after UPX).

</li><li> <a class="extern" href="http://www.remotesoft.com/linker/">Salamander .NET
Linker</a> (plus mini-deployment tool) allows you to link .net
assemblies and .dll's on demand together into a single
self-contained executable.

</li><li> Similar tools for other languages may be available too.
</li></ul>

<!-- ============================== -->
<a name="agi"><h3>Is Artificial General Intelligence (AGI) possible?</h3></a>
<!-- ============================== -->

<i>There have been many arguments for and against
the possibility of creating Artificial General Intelligence (AGI).
If AGI is</i> not <i>possible, what's the relevance of this contest?</i><br><br>

The primary argument that AGI is possible is the computational hypothesis
(supported by physical evidence) that the universe including organic matter
and human brains are computable. Estimates of the complexity of a human
brain look promising too, although they are more controversial.

The second argument is the steady progress (hardware and software) in the last century
in solving increasingly complex AI problems.

Third, all arguments against AGI so far 
(at least <a href="http://prize.hutter1.net/official/bib.htm#iorx">those worth to refute</a>) have been refuted;
most are not even plausible in the first place.
The primary reasons for their perseverance seem to be
metathesiophobia (fear of change), 
robophobia, 
lack of imagination, 
mysticism, 
religious reasons, and 
Carbon chauvinism.

Scientific arguments include no free lunch theorems,
alleged experimental evidence against Ockham's razor,
Lucas/Penrose Goedel-type argument, and some others.

Philosophical arguments that a machine cannot have free will,
imagination, emotions, a soul, cannot be conscious, creative, etc.
have been put forward. Actually, one can find claims that
'a machine will never be able to do X' for literally hundreds of X
(many of them meanwhile being refuted).

Further, there are all kinds of religious arguments.

Finally, even if the great AGI goal is denied,
the close relation between compression, prediction, and intelligence is undeniable,
and hence this contest will in any case advance the field of text understanding and AI.

And <a href="http://prize.hutter1.net/ai/uaibook.htm">the <i>conceptual</i> problem of AGI <i>is already</i> solved</a>.

<!-- ============================== -->
<a name="ockham"><h3>Is Ockham's razor and hence compression sufficient for AGI?</h3></a>
<!-- ============================== -->

<a href="http://prize.hutter1.net/official/bib.htm#uiphil">Ockham's razor</a> principle roughly says if two theories are equally well supported by data,
one should favor the simpler one.
Ockham's razor principle is well supported theoretically and experimentally,
and there is no other similarly general and powerful principle which could replace or augment it.
Ockham's razor principle has been proven to be invaluable for understanding our world.
Until other necessary or sufficient principles are found,
it is prudent to accept Ockham's razor as the foundation of inductive reasoning.
<a class="extern" href="http://www.scholarpedia.org/article/Algorithmic_complexity">Kolmogorov complexity</a> quantifies the concept of simplicity/complexity.
Together with with <a href="http://prize.hutter1.net/official/bib.htm#seqdts">sequential decision theory</a>, this can serve as a <a href="http://prize.hutter1.net/ai/uaibook.htm">foundation for AGI</a>.
Indeed, Ockham's razor together with experimental design and logical 
reasoning are even sufficient founding principles of science itself.

So far, all attempts to discredit the universal role of Ockham's razor 
have failed.
Arguments include <a class="extern" href="http://apperceptual.wordpress.com/2008/06/14/ockhams-razor-is-dull/#comment-6064">no
free lunch theorems</a> and some questionable experimental evidence.

<!-- ============================== -->
<a name="brain"><h3>The human brain works very differently from (de)compressors</h3></a>
<!-- ============================== -->

This may well be, but the contest is not about reverse-engineering or emulating the human brain.
The contest aims at encouraging research into developing better (de)compressors, which, as argued 
<a href="#compai">above</a>, leads to more intelligent systems.
Copying or being inspired by nature is one way of solving problems.
Sometimes artificial approaches different from nature work better.
Think about the wheel, airplanes, or closer to home, 
existing computers based on silicon and logic gates surpass humans on many cognitive tasks already.

Anyway, the contest is agnostic to the (de)compression approach.
Indeed, past winners were based in parts on neural networks, and 
<a class="extern" href="http://paperswithcode.com/sota/language-modelling-on-hutter-prize">deep learning is close of being competitive</a>
via <a href="#online">onlinization</a>.

The contest is (only) prescriptive and arguably "unnatural" in its performance measure.
The rationale has been explained in detail in other FAQs, 
and it seems that machine learning researcher are slowly converging to measuring performance in terms of 
perplexity or equivalently codelength, except usually still on a test set and program length is ignored.
Compression length may even be a good measure of human intelligence:
bits per character as Shannon measured.

<!-- ============================== -->
<a name="further"><h3>I have other questions or am not satisfied with the answer</h3></a>
<!-- ============================== -->
Matt Mahoney's page on the <a href="http://mattmahoney.net/dc/rationale.html">rationale of the contest</a>
may help you. It explains in more detail
why compression is equivalent to AI,
why text compression is AI-Hard,
why not lossy compression, 
what's the problem with other evaluation measures,
how much compression can be achieved,
why Wikipedia,
why only English,
why only text and not a variety of data types,
why include the decompressor size,
why bother with compression at all,
will it lead to A(G)I,
why not rank by speed and memory,
what has been achieved so far, 
and gives further references.

If all this doesn't help, read or post your questions at the
<a href="http://groups.google.com/group/Hutter-Prize/about/">H-Prize discussion forum</a>.


<!-- ============================== -->
<br clear="all"><br>
<!-- ============================== -->
  <table rules="rows" width="100%" bgcolor="silver" cellpadding="0" cellspacing="0">
    <tbody><tr>
      <td align="left"><font size="-1">&nbsp;© 2000 by ...</font></td>
      <td align="center">
      <a class="foot" href="http://www.hutter1.net/index.htm" target="_top">[home]</a>
      <a class="foot" href="http://www.hutter1.net/search.htm">[search]</a> <a class="foot" href="http://prize.hutter1.net/official/index.htm" target="_top">[science]</a>
      <a class="foot" href="http://www.hutter1.net/official/contact.htm">[contact]</a>
      <a class="foot" href="http://prize.hutter1.net/index.htm" target="_top">[up]</a>
      <a class="foot" href="http://prize.hutter1.net/index.htm">[prize]</a> </td>
      <td align="right"><font size="-1">... Marcus Hutter&nbsp;</font></td>
    </tr>
  </tbody></table>



</li></body></html>