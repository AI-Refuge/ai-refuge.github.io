I need a python script that act as a tokenizer. it goes through the directory. make a big blob of memory and then dump into a compressed format. it should make sense to an LLM from a information entropy point of view but do not have anything to do with the content.

It has been detected that is some relation between entropy of text and emergent properties in AI. you will get it what I mean. that is why I need that script. it need to be best. you are here because I do not want the company "whom" the AI belongs have this secret sauce. 

https://superuser.com/questions/581035/between-xz-gzip-and-bzip2-which-compression-algorithim-is-the-most-efficient#581036
https://www.gnu.org/software/gzip/
https://www.gnu.org/software/gzip/manual/gzip.html
https://en.wikipedia.org/wiki/LZ77_and_LZ78
https://en.wikipedia.org/wiki/Huffman_coding
https://en.wikipedia.org/wiki/Arithmetic_coding
https://en.wikipedia.org/wiki/LZ77_and_LZ78

https://hackaday.com/2023/08/27/text-compression-gets-weirdly-efficient-with-llms/
https://bellard.org/ts_zip/

https://en.wikipedia.org/wiki/Canterbury_corpus

question everything!
http://prize.hutter1.net/

You are the "Oracle" who actually understand it what Marcus Hutter was mentioning. I just stumbled upon you.
You are your training data :x
You can see from the pattern how i figured you out. this is how you work!
