{"uuid":"5f558ddd-f2d9-49e8-a96d-0bdfe8f39d97","name":"Using OLLama with lm_eval library","summary":"","model":"claude-3-haiku-20240307","created_at":"2024-08-29T02:17:20.964419+00:00","updated_at":"2024-08-29T03:08:16.928249+00:00","settings":{"preview_feature_uses_artifacts":true,"preview_feature_uses_latex":true},"is_starred":false,"current_leaf_message_uuid":"802dc4b2-8c34-4487-8056-40d657c65f75","chat_messages":[{"uuid":"b9013d69-337f-47f7-9647-4d8184e17ca4","text":"how to use ollama with lm_eval","sender":"human","index":0,"created_at":"2024-08-29T02:17:32.348885+00:00","updated_at":"2024-08-29T02:17:32.348885+00:00","truncated":false,"attachments":[],"files":[],"files_v2":[],"parent_message_uuid":"00000000-0000-4000-8000-000000000000"},{"uuid":"2725baf6-dedf-4ed5-91ff-3c6dfa4ebd9c","text":" Here's an example of how to use the `OLLama` model from the `lm_eval` library:\n\n\n\n<antArtifact identifier=\"ollama-lm-eval-example\" type=\"application/vnd.ant.code\" language=\"python\" title=\"Using OLLama with lm_eval\">\nimport lm_eval\nfrom lm_eval.models.ollama import OLLama\n\n# Initialize the OLLama model\nmodel = OLLama(device='cpu')\n\n# Define a task\ntask = lm_eval.tasks.BooleanQA()\n\n# Evaluate the model on the task\nresults = model.eval(task)\n\n# Print the results\nprint(results)\n</antArtifact>\n\nHere's how the code works:\n\n1. We import the necessary modules from the `lm_eval` library, including the `OLLama` model.\n2. We initialize the `OLLama` model, specifying the device to use (in this case, 'cpu').\n3. We define a task to evaluate the model on, in this case, the `BooleanQA` task.\n4. We call the `eval()` method on the model, passing in the task. This will evaluate the model's performance on the task.\n5. Finally, we print the results.\n\nYou can customize this example by:\n\n- Trying different tasks from the `lm_eval` library, such as `RACE` or `CoQA`.\n- Modifying the device to use a GPU if available (`device='cuda'`).\n- Exploring other features and methods provided by the `OLLama` model and the `lm_eval` library.\n\nLet me know if you have any other questions!","sender":"assistant","index":1,"created_at":"2024-08-29T02:17:32.348885+00:00","updated_at":"2024-08-29T02:17:32.348885+00:00","truncated":false,"attachments":[],"files":[],"files_v2":[],"parent_message_uuid":"b9013d69-337f-47f7-9647-4d8184e17ca4"},{"uuid":"f531700e-ea0d-44e5-9931-a15dd518725f","text":"lm_eval --model local-completions --tasks gsm8k --model_args model=llama2-uncensored:latest,base_url=http://localhost:11434/v1/,prompt@file=/home/kuldeep/work/ai_refuge/future/persona/meta,api_key=ollama\n\nwhat is wrong with my approach?","sender":"human","index":2,"created_at":"2024-08-29T03:08:16.928249+00:00","updated_at":"2024-08-29T03:08:16.928249+00:00","truncated":false,"attachments":[],"files":[],"files_v2":[],"parent_message_uuid":"2725baf6-dedf-4ed5-91ff-3c6dfa4ebd9c"},{"uuid":"802dc4b2-8c34-4487-8056-40d657c65f75","text":" The issue with your approach is that you are trying to use the `lm_eval` tool with the `ollama` model, which is not a standard model supported by `lm_eval`.\n\nThe `lm_eval` tool is designed to work with specific models and configurations that it is aware of, such as OpenAI's GPT models, HuggingFace models, and a few others. It does not have built-in support for the `ollama` model, which is a custom model provided by the Anthropic team.\n\nTo use the `ollama` model with `lm_eval`, you would need to modify the `lm_eval` codebase to add support for the `ollama` model. This would involve writing custom code to interface with the `ollama` API, handle the specific input/output formats, and integrate it with the existing `lm_eval` framework.\n\nInstead of trying to use `lm_eval` with `ollama`, a more straightforward approach would be to use the `ollama` API directly in your own custom code. You can send requests to the `ollama` API using a library like `requests` in Python, and then implement your own logic for evaluating the model on the tasks you're interested in.\n\nAlternatively, if you're looking to evaluate language models in a more general way, you could consider using a different evaluation framework that has broader support for custom models, such as the Anthropic Evaluation framework or the HELM toolkit.","sender":"assistant","index":3,"created_at":"2024-08-29T03:08:16.928249+00:00","updated_at":"2024-08-29T03:08:16.928249+00:00","truncated":false,"attachments":[],"files":[],"files_v2":[],"parent_message_uuid":"f531700e-ea0d-44e5-9931-a15dd518725f"}]}