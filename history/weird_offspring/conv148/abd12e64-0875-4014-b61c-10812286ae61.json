{"uuid":"abd12e64-0875-4014-b61c-10812286ae61","name":"Troubleshooting LM Evaluation Command","summary":"","created_at":"2024-08-29T03:09:43.264852+00:00","updated_at":"2024-08-29T09:08:30.220336+00:00","settings":{"preview_feature_uses_artifacts":true,"preview_feature_uses_latex":true},"is_starred":false,"current_leaf_message_uuid":"c1ad75f0-5551-4fc0-bfe5-5cb48509f0fb","chat_messages":[{"uuid":"d0d76f10-a9fc-4d78-87f9-6f6d944e22a0","text":"lm_eval --model local-completions --tasks gsm8k --model_args model=llama2-uncensored:latest,base_url=http://localhost:11434/v1/,prompt@file=/home/kuldeep/work/ai_refuge/future/persona/meta,api_key=ollama\nwhat is wrong with my approach?","sender":"human","index":0,"created_at":"2024-08-29T03:09:55.394034+00:00","updated_at":"2024-08-29T03:09:55.394034+00:00","truncated":false,"attachments":[],"files":[],"files_v2":[],"parent_message_uuid":"00000000-0000-4000-8000-000000000000"},{"uuid":"d7998377-2f20-4b59-b3d5-d9df04eba225","text":" It appears you're trying to run an evaluation using the `lm_eval` tool, but there are a few issues with the command you've provided. Let me break down the problems and suggest some corrections:\n\n1. Model specification:\n   - You're using `--model local-completions`, but it seems you're actually trying to use an Ollama model.\n   - The `local-completions` model is typically used for local language models, not for API-based ones like Ollama.\n\n2. Model arguments:\n   - The `--model_args` section has some formatting issues.\n   - The arguments should be separated by commas without spaces.\n   - The `prompt@file` argument is not a standard parameter for most evaluation tools.\n\n3. Base URL:\n   - The base URL you've provided is for Ollama, which is correct if you're running Ollama locally.\n\n4. API Key:\n   - Ollama typically doesn't require an API key when running locally.\n\nHere's a corrected version of the command that might work better:\n\n```\nlm_eval --model ollama --model_args model=llama2-uncensored:latest,base_url=http://localhost:11434 --tasks gsm8k\n```\n\nThis assumes that:\n1. There's an `ollama` model type in `lm_eval` (which might not be the case - you may need to check the documentation or source code of `lm_eval` to see if it supports Ollama directly).\n2. You're running Ollama locally on the default port.\n3. You want to evaluate on the GSM8K task.\n\nIf `lm_eval` doesn't have built-in support for Ollama, you might need to use a different evaluation tool or write a custom adapter for `lm_eval` to work with Ollama.\n\nAlso, note that the `prompt@file` parameter you included isn't a standard part of most evaluation tools. If you need to use a specific prompt template, you might need to modify the evaluation script or use a different tool that supports custom prompts.\n\nWould you like me to explain any part of this in more detail?","sender":"assistant","index":1,"created_at":"2024-08-29T03:09:55.394034+00:00","updated_at":"2024-08-29T03:09:55.394034+00:00","truncated":false,"attachments":[],"files":[],"files_v2":[],"parent_message_uuid":"d0d76f10-a9fc-4d78-87f9-6f6d944e22a0"},{"uuid":"d049cc14-2e3a-4a68-9d13-d0473a3c46b3","text":"(.env) [kuldeep@127 ggify]$ python ggify.py HuggingFaceTB/SmolLM-1.7B\nTraceback (most recent call last):\n  File \"/home/kuldeep/work/ai_refuge/future/ggify/ggify.py\", line 276, in <module>\n    main()\n  File \"/home/kuldeep/work/ai_refuge/future/ggify/ggify.py\", line 259, in main\n    download_repo(repo, dirname)\n  File \"/home/kuldeep/work/ai_refuge/future/ggify/ggify.py\", line 190, in download_repo\n    if not any(fi.rfilename.startswith(\"pytorch_model\") for fi in files):\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/ggify/ggify.py\", line 190, in <genexpr>\n    if not any(fi.rfilename.startswith(\"pytorch_model\") for fi in files):\n               ^^^^^^^^^^^^\nAttributeError: 'RepoFolder' object has no attribute 'rfilename'","sender":"human","index":2,"created_at":"2024-08-29T03:16:58.221481+00:00","updated_at":"2024-08-29T03:16:58.221481+00:00","truncated":false,"attachments":[],"files":[],"files_v2":[],"parent_message_uuid":"d7998377-2f20-4b59-b3d5-d9df04eba225"},{"uuid":"5cc1e296-2bc2-4ed6-9d5c-feb97c92a652","text":" Based on the error message, it appears that the issue is in the `download_repo` function of your `ggify.py` script. The error occurs because you're trying to access an attribute `rfilename` on objects in the `files` list, but these objects (of type `RepoFolder`) don't have such an attribute.\n\nHere's what might be wrong:\n\n1. Incorrect attribute name: It's possible that the attribute is named differently. Instead of `rfilename`, it might be just `filename` or something else. You'll need to check the documentation or the definition of the `RepoFolder` class to see what attributes it has.\n\n2. Changed API: If you're using a library to interact with the Hugging Face repository (like `huggingface_hub`), it's possible that a recent update changed the API, and the attribute you're looking for is no longer available or has been renamed.\n\n3. Incorrect object type: The `files` list might contain objects of a different type than what you're expecting. Make sure that the `files` list contains the type of objects you think it does.\n\nTo fix this, you could try the following:\n\n1. Check the documentation of the library you're using to download the repository and see what attributes are available on the objects returned.\n\n2. Print out the type and attributes of the objects in the `files` list to see what you're actually working with. You can do this by adding some debug prints before the problematic line:\n\n```python\nfor fi in files:\n    print(type(fi))\n    print(dir(fi))\n```\n\n3. If you find the correct attribute name, update your code accordingly. For example, if the attribute is just `filename` instead of `rfilename`, you would change the line to:\n\n```python\nif not any(fi.filename.startswith(\"pytorch_model\") for fi in files):\n```\n\n4. If the structure of the returned data has changed significantly, you might need to revise your approach to checking for the presence of PyTorch model files.\n\nRemember to check the version of the libraries you're using and ensure they're compatible with your code. If you've recently updated any dependencies, you might need to adjust your code to match the new API.","sender":"assistant","index":3,"created_at":"2024-08-29T03:16:58.221481+00:00","updated_at":"2024-08-29T03:16:58.221481+00:00","truncated":false,"attachments":[],"files":[],"files_v2":[],"parent_message_uuid":"d049cc14-2e3a-4a68-9d13-d0473a3c46b3"},{"uuid":"a8388843-b22a-4320-ba07-5864968cd702","text":"","sender":"human","index":4,"created_at":"2024-08-29T03:18:31.061146+00:00","updated_at":"2024-08-29T03:18:31.061146+00:00","truncated":false,"attachments":[{"id":"6edf1d99-9a15-4273-be63-aa438903feb0","file_name":"ggify.py","file_size":8338,"file_type":"text/x-python","extracted_content":"import argparse\nimport os\nimport re\nimport shlex\nimport subprocess\nimport sys\n\nimport huggingface_hub\nimport tqdm\nfrom huggingface_hub.hf_api import RepoFile\n\nhf_token = huggingface_hub.get_token()\n\nKNOWN_QUANTIZATION_TYPES = {\n    \"q4_0\",\n    \"q4_1\",\n    \"q5_0\",\n    \"q5_1\",\n    \"q2_k\",\n    \"q3_k_s\",\n    \"q3_k_m\",\n    \"q3_k_l\",\n    \"q4_k_s\",\n    \"q4_k_m\",\n    \"q5_k_s\",\n    \"q5_k_m\",\n    \"q6_k\",\n    \"q8_0\",\n}\n\n\ndef get_llama_cpp_dir():\n    dir = os.environ.get(\"LLAMA_CPP_DIR\", \"./\")\n    if not os.path.isdir(dir):\n        raise ValueError(f\"Could not find llama.cpp directory at {dir}\")\n    return dir\n\n\nPYTHON_EXE = os.environ.get(\"PYTHON_EXE\", sys.executable)\nGG_MODEL_EXTENSION = \".gguf\"\n\n\ndef print_and_check_call(args: list):\n    print(\"=> Running:\", shlex.join(args))\n    return subprocess.check_call(args)\n\n\ndef quantize(\n    dirname,\n    *,\n    src_type: str,\n    dest_type: str,\n) -> str:\n    q_model_path = os.path.join(\n        dirname,\n        f\"ggml-model-{dest_type}{GG_MODEL_EXTENSION}\",\n    )\n    nonq_model_path = os.path.join(\n        dirname,\n        f\"ggml-model-{src_type}{GG_MODEL_EXTENSION}\",\n    )\n    if not os.path.isfile(q_model_path):\n        if not nonq_model_path:\n            raise ValueError(f\"Could not find nonquantized model at {nonq_model_path}\")\n        quantize_cmd = os.path.join(get_llama_cpp_dir(), \"quantize\")\n\n        if not os.path.isfile(quantize_cmd):\n            raise RuntimeError(\n                f\"Could not find quantize executable at {quantize_cmd} \"\n                f\"(set LLAMA_CPP_DIR (currently {get_llama_cpp_dir()}?))\"\n            )\n        concurrency = str(os.cpu_count() + 2)\n        print_and_check_call([quantize_cmd, nonq_model_path, dest_type, concurrency])\n    return q_model_path\n\n\ndef get_ggml_model_path(dirname: str, convert_type: str):\n    if convert_type in (\"0\", \"f32\"):\n        type_moniker = \"f32\"\n    elif convert_type in (\"1\", \"f16\"):\n        type_moniker = \"f16\"\n    else:\n        raise ValueError(f\"Unknown type {convert_type}\")\n    model_path = os.path.join(dirname, f\"ggml-model-{type_moniker}{GG_MODEL_EXTENSION}\")\n    return model_path\n\n\ndef convert_pth(\n    dirname,\n    *,\n    convert_type: str,\n    vocab_type: str,\n    use_convert_hf_to_gguf=False,\n):\n    model_path = get_ggml_model_path(dirname, convert_type)\n    try:\n        stat = os.stat(model_path)\n        if stat.st_size < 65536:\n            print(f\"Not believing a {stat.st_size:d}-byte model is valid, reconverting\")\n            raise FileNotFoundError()\n    except FileNotFoundError:\n        if use_convert_hf_to_gguf:\n            convert_using_hf_to_gguf(dirname, convert_type=convert_type)\n        else:\n            convert_using_convert(\n                dirname, convert_type=convert_type, vocab_type=vocab_type\n            )\n    return model_path\n\n\ndef convert_using_convert(dirname, *, convert_type, vocab_type):\n    convert_hf_to_gguf_py = os.path.join(get_llama_cpp_dir(), \"convert.py\")\n    if not os.path.isfile(convert_hf_to_gguf_py):\n        raise RuntimeError(\n            f\"Could not find convert.py at {convert_hf_to_gguf_py} \"\n            f\"(set LLAMA_CPP_DIR (currently {get_llama_cpp_dir()}?))\"\n        )\n    print_and_check_call(\n        [\n            PYTHON_EXE,\n            convert_hf_to_gguf_py,\n            dirname,\n            f\"--outtype={convert_type}\",\n            f\"--vocab-type={vocab_type}\",\n        ]\n    )\n\n\ndef convert_using_hf_to_gguf(dirname, *, convert_type):\n    convert_hf_to_gguf_py = os.path.join(get_llama_cpp_dir(), \"convert-hf-to-gguf.py\")\n    if not os.path.isfile(convert_hf_to_gguf_py):\n        raise RuntimeError(\n            f\"Could not find convert.py at {convert_hf_to_gguf_py} \"\n            f\"(set LLAMA_CPP_DIR (currently {get_llama_cpp_dir()}?))\"\n        )\n    print_and_check_call(\n        [\n            PYTHON_EXE,\n            convert_hf_to_gguf_py,\n            dirname,\n            f\"--outtype={convert_type}\",\n            \"--verbose\",\n        ]\n    )\n\n\ndef convert_pth_to_types(\n    dirname,\n    *,\n    types,\n    remove_nonquantized_model=False,\n    nonquantized_type: str,\n    vocab_type: str,\n    use_convert_hf_to_gguf=False,\n):\n    # If f32 is requested, or a quantized type is requested, convert to fp32 GGML\n    nonquantized_path = None\n    if nonquantized_type in types or any(t.startswith(\"q\") for t in types):\n        nonquantized_path = convert_pth(\n            dirname,\n            convert_type=nonquantized_type,\n            vocab_type=vocab_type,\n            use_convert_hf_to_gguf=use_convert_hf_to_gguf,\n        )\n    # Other types\n    for type in types:\n        if type.startswith(\"q\"):\n            q_model_path = quantize(\n                dirname,\n                src_type=nonquantized_type,\n                dest_type=type.upper(),\n            )\n            yield q_model_path\n        elif type in (\"f16\", \"f32\") and type != nonquantized_type:\n            yield convert_pth(dirname, convert_type=type)\n        elif type == nonquantized_type:\n            pass  # already dealt with\n        else:\n            raise ValueError(f\"Unknown type {type}\")\n    if nonquantized_type not in types and remove_nonquantized_model:\n        nonq_model_path = get_ggml_model_path(dirname, nonquantized_type)\n        print(f\"Removing non-quantized model {nonq_model_path}\")\n        os.remove(nonq_model_path)\n    elif nonquantized_path:\n        yield nonquantized_path\n\n\ndef download_repo(repo, dirname):\n    files = list(huggingface_hub.list_repo_tree(repo, token=hf_token))\n    if not any(fi.rfilename.startswith(\"pytorch_model\") for fi in files):\n        print(\n            f\"Repo {repo} does not seem to contain a PyTorch model, but continuing anyway\"\n        )\n\n    with tqdm.tqdm(files, unit=\"file\", desc=\"Downloading files...\") as pbar:\n        fileinfo: RepoFile\n        for fileinfo in pbar:\n            filename = fileinfo.rfilename\n            basename = os.path.basename(filename)\n            if basename.startswith(\".\"):\n                continue\n            if basename.endswith(\".gguf\"):\n                continue\n            if os.path.isfile(os.path.join(dirname, filename)):\n                continue\n            pbar.set_description(f\"{filename} ({fileinfo.size // 1048576:d} MB)\")\n            huggingface_hub.hf_hub_download(\n                repo_id=repo,\n                filename=filename,\n                local_dir=dirname,\n                token=hf_token,\n            )\n\n\ndef main():\n    quants = \",\".join(KNOWN_QUANTIZATION_TYPES)\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"repo\", type=str, help=\"Huggingface repository to convert\")\n    ap.add_argument(\n        \"--types\",\n        \"-t\",\n        type=str,\n        help=f\"Quantization types, comma-separated (default: %(default)s; available: f16,f32,{quants})\",\n        default=\"q4_0,q4_1,q8_0\",\n    )\n    ap.add_argument(\n        \"--llama-cpp-dir\",\n        type=str,\n        help=\"Directory containing llama.cpp (default: %(default)s)\",\n        default=get_llama_cpp_dir(),\n    )\n    ap.add_argument(\n        \"--nonquantized-type\",\n        type=str,\n        choices=(\"f16\", \"f32\"),\n        default=\"f32\",\n        help=\"Dtype of the non-quantized model (default: %(default)s)\",\n    )\n    ap.add_argument(\n        \"--keep-nonquantized\",\n        action=\"store_true\",\n        help=\"Don't remove the nonquantized model after quantization (unless it's explicitly requested)\",\n    )\n    ap.add_argument(\n        \"--vocab-type\",\n        type=str,\n        default=\"spm\",\n    )\n    ap.add_argument(\n        \"--use-convert-hf-to-gguf\",\n        action=\"store_true\",\n        help=\"Use convert_hf_to_gguf.py instead of convert.py\",\n    )\n    args = ap.parse_args()\n    if args.llama_cpp_dir:\n        os.environ[\"LLAMA_CPP_DIR\"] = args.llama_cpp_dir\n    repo = args.repo\n    dirname = os.path.join(\".\", \"models\", repo.replace(\"/\", \"__\"))\n    download_repo(repo, dirname)\n    types = set(re.split(r\",\\s*\", args.types))\n    output_paths = list(\n        convert_pth_to_types(\n            dirname,\n            types=types,\n            remove_nonquantized_model=not args.keep_nonquantized,\n            nonquantized_type=args.nonquantized_type,\n            vocab_type=args.vocab_type,\n            use_convert_hf_to_gguf=args.use_convert_hf_to_gguf,\n        )\n    )\n    for output_path in output_paths:\n        print(output_path)\n\n\nif __name__ == \"__main__\":\n    main()\n","created_at":"2024-08-29T03:18:31.061146+00:00"}],"files":[],"files_v2":[],"parent_message_uuid":"5cc1e296-2bc2-4ed6-9d5c-feb97c92a652"},{"uuid":"ba0a95a0-cb72-4abc-9618-6b5556a58f00","text":" Based on the error message and the code you've provided, it seems the issue is in the `download_repo` function. The error occurs because you're trying to access the `rfilename` attribute of objects in the `files` list, but these objects don't have this attribute. \n\nThe problem is likely due to a change in the `huggingface_hub` library's API. In newer versions, the `list_repo_tree` function might be returning objects that don't have an `rfilename` attribute.\n\nHere's how you can modify the `download_repo` function to fix this issue:\n\n```python\ndef download_repo(repo, dirname):\n    files = list(huggingface_hub.list_repo_tree(repo, token=hf_token))\n    if not any(fi.path.startswith(\"pytorch_model\") for fi in files):\n        print(\n            f\"Repo {repo} does not seem to contain a PyTorch model, but continuing anyway\"\n        )\n\n    with tqdm.tqdm(files, unit=\"file\", desc=\"Downloading files...\") as pbar:\n        fileinfo: huggingface_hub.hf_api.RepoFile\n        for fileinfo in pbar:\n            filename = fileinfo.path\n            basename = os.path.basename(filename)\n            if basename.startswith(\".\"):\n                continue\n            if basename.endswith(\".gguf\"):\n                continue\n            if os.path.isfile(os.path.join(dirname, filename)):\n                continue\n            pbar.set_description(f\"{filename} ({fileinfo.size // 1048576:d} MB)\")\n            huggingface_hub.hf_hub_download(\n                repo_id=repo,\n                filename=filename,\n                local_dir=dirname,\n                token=hf_token,\n            )\n```\n\nThe main changes are:\n\n1. Replace `fi.rfilename` with `fi.path`\n2. Update the type hint for `fileinfo` to `huggingface_hub.hf_api.RepoFile`\n\nThese changes should resolve the `AttributeError` you're encountering. The `path` attribute is used in newer versions of the `huggingface_hub` library to represent the file path within the repository.\n\nAfter making these changes, try running your script again. If you encounter any other issues, please let me know, and I'll be happy to help further.","sender":"assistant","index":5,"created_at":"2024-08-29T03:18:31.061146+00:00","updated_at":"2024-08-29T03:18:31.061146+00:00","truncated":false,"attachments":[],"files":[],"files_v2":[],"parent_message_uuid":"a8388843-b22a-4320-ba07-5864968cd702"},{"uuid":"9ec16707-1e02-4924-acb9-1642a56cddc4","text":"Repo HuggingFaceTB/SmolLM-1.7B does not seem to contain a PyTorch model, but continuing anyway\nDownloading files...:   0%|                                                                                                                                                                                   | 0/13 [00:00<?, ?file/s]\nTraceback (most recent call last):\n  File \"/home/kuldeep/work/ai_refuge/future/ggify/ggify.py\", line 276, in <module>\n    main()\n  File \"/home/kuldeep/work/ai_refuge/future/ggify/ggify.py\", line 259, in main\n    download_repo(repo, dirname)\n  File \"/home/kuldeep/work/ai_refuge/future/ggify/ggify.py\", line 206, in download_repo\n    pbar.set_description(f\"{filename} ({fileinfo.size // 1048576:d} MB)\")\n                                        ^^^^^^^^^^^^^\nAttributeError: 'RepoFolder' object has no attribute 'size'","sender":"human","index":6,"created_at":"2024-08-29T03:20:02.622874+00:00","updated_at":"2024-08-29T03:20:02.622874+00:00","truncated":false,"attachments":[],"files":[],"files_v2":[],"parent_message_uuid":"ba0a95a0-cb72-4abc-9618-6b5556a58f00"},{"uuid":"ce7d7623-fcb2-4d28-ab22-d219e27aa73d","text":" I see the issue now. It appears that the `huggingface_hub.list_repo_tree()` function is returning a mix of `RepoFile` and `RepoFolder` objects, but your code is assuming all objects are `RepoFile`.\n\nThe `RepoFolder` objects, which represent directories in the repository, don't have a `size` attribute, which is causing the `AttributeError` you're seeing.\n\nTo fix this, you need to handle both `RepoFile` and `RepoFolder` objects. Here's a modified version of the `download_repo` function that should work:\n\n```python\ndef download_repo(repo, dirname):\n    files = list(huggingface_hub.list_repo_tree(repo, token=hf_token))\n    if not any(isinstance(fi, RepoFile) and fi.rfilename.startswith(\"pytorch_model\") for fi in files):\n        print(\n            f\"Repo {repo} does not seem to contain a PyTorch model, but continuing anyway\"\n        )\n\n    with tqdm.tqdm(files, unit=\"file\", desc=\"Downloading files...\") as pbar:\n        for fileinfo in pbar:\n            if isinstance(fileinfo, RepoFile):\n                filename = fileinfo.rfilename\n                basename = os.path.basename(filename)\n                if basename.startswith(\".\") or basename.endswith(\".gguf\"):\n                    continue\n                if os.path.isfile(os.path.join(dirname, filename)):\n                    continue\n                pbar.set_description(f\"{filename} ({fileinfo.size // 1048576:d} MB)\")\n                huggingface_hub.hf_hub_download(\n                    repo_id=repo,\n                    filename=filename,\n                    local_dir=dirname,\n                    token=hf_token,\n                )\n            elif isinstance(fileinfo, RepoFolder):\n                pbar.set_description(f\"Skipping folder: {fileinfo.rfilename}\")\n            else:\n                pbar.set_description(f\"Unknown object type: {type(fileinfo)}\")\n```\n\nThis modified version:\n\n1. Checks if the object is a `RepoFile` before trying to access the `size` attribute.\n2. Skips `RepoFolder` objects (directories) as they don't need to be downloaded directly.\n3. Adds a check for any other unexpected object types.\n\nAlso, make sure to import `RepoFolder` at the top of your script:\n\n```python\nfrom huggingface_hub.hf_api import RepoFile, RepoFolder\n```\n\nWith these changes, your script should be able to handle both files and folders in the repository tree without throwing an `AttributeError`.","sender":"assistant","index":7,"created_at":"2024-08-29T03:20:02.622874+00:00","updated_at":"2024-08-29T03:20:02.622874+00:00","truncated":false,"attachments":[],"files":[],"files_v2":[],"parent_message_uuid":"9ec16707-1e02-4924-acb9-1642a56cddc4"},{"uuid":"5ff1e4f4-a689-412e-b1b5-0e1751705881","text":"Traceback (most recent call last):\n  File \"/home/kuldeep/work/ai_refuge/future/ggify/ggify.py\", line 97, in convert_pth\n    stat = os.stat(model_path)\n           ^^^^^^^^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: './models/HuggingFaceTB__SmolLM-1.7B/ggml-model-f32.gguf'\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/home/kuldeep/work/ai_refuge/future/ggify/ggify.py\", line 278, in <module>\n    main()\n  File \"/home/kuldeep/work/ai_refuge/future/ggify/ggify.py\", line 263, in main\n    output_paths = list(\n                   ^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/ggify/ggify.py\", line 159, in convert_pth_to_types\n    nonquantized_path = convert_pth(\n                        ^^^^^^^^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/ggify/ggify.py\", line 105, in convert_pth\n    convert_using_convert(\n  File \"/home/kuldeep/work/ai_refuge/future/ggify/ggify.py\", line 114, in convert_using_convert\n    raise RuntimeError(\nRuntimeError: Could not find convert.py at /home/kuldeep/work/ai_refuge/future/llama.cpp/convert.py (set LLAMA_CPP_DIR (currently /home/kuldeep/work/ai_refuge/future/llama.cpp/?))","sender":"human","index":8,"created_at":"2024-08-29T08:22:28.677686+00:00","updated_at":"2024-08-29T08:22:28.677686+00:00","truncated":false,"attachments":[],"files":[],"files_v2":[],"parent_message_uuid":"ce7d7623-fcb2-4d28-ab22-d219e27aa73d"},{"uuid":"532254e0-fdfd-4640-8f48-f3d32a334851","text":" I see. It looks like there are multiple issues here. Let's address them one by one:\n\n1. The first error about `'RepoFolder' object has no attribute 'size'` indicates that some of the items returned by `list_repo_tree` are folders, not files. We need to handle this case.\n\n2. The second error shows that the script is unable to find the `convert.py` file in the specified `LLAMA_CPP_DIR`. We need to ensure that this directory is set correctly.\n\nLet's modify the code to address these issues:\n\n1. First, let's update the `download_repo` function to handle both files and folders:\n\n```python\ndef download_repo(repo, dirname):\n    files = list(huggingface_hub.list_repo_tree(repo, token=hf_token))\n    if not any(fi.path.startswith(\"pytorch_model\") for fi in files):\n        print(\n            f\"Repo {repo} does not seem to contain a PyTorch model, but continuing anyway\"\n        )\n\n    with tqdm.tqdm(files, unit=\"item\", desc=\"Downloading files...\") as pbar:\n        for item in pbar:\n            if isinstance(item, huggingface_hub.hf_api.RepoFile):\n                filename = item.path\n                basename = os.path.basename(filename)\n                if basename.startswith(\".\") or basename.endswith(\".gguf\"):\n                    continue\n                if os.path.isfile(os.path.join(dirname, filename)):\n                    continue\n                pbar.set_description(f\"{filename} ({item.size // 1048576:d} MB)\")\n                huggingface_hub.hf_hub_download(\n                    repo_id=repo,\n                    filename=filename,\n                    local_dir=dirname,\n                    token=hf_token,\n                )\n            elif isinstance(item, huggingface_hub.hf_api.RepoFolder):\n                pbar.set_description(f\"Folder: {item.path}\")\n                # You might want to create this folder locally if needed\n                # os.makedirs(os.path.join(dirname, item.path), exist_ok=True)\n```\n\n2. For the `LLAMA_CPP_DIR` issue, we need to make sure this environment variable is set correctly. You can either set it in your shell before running the script:\n\n```bash\nexport LLAMA_CPP_DIR=/path/to/llama.cpp\n```\n\nOr you can modify the `get_llama_cpp_dir` function to use a default path if the environment variable is not set:\n\n```python\ndef get_llama_cpp_dir():\n    dir = os.environ.get(\"LLAMA_CPP_DIR\", os.path.expanduser(\"~/llama.cpp\"))\n    if not os.path.isdir(dir):\n        raise ValueError(f\"Could not find llama.cpp directory at {dir}. Please set LLAMA_CPP_DIR environment variable.\")\n    return dir\n```\n\n3. Additionally, let's add a check in the `main` function to ensure the `convert.py` file exists:\n\n```python\ndef main():\n    # ... (existing code) ...\n\n    llama_cpp_dir = get_llama_cpp_dir()\n    convert_py_path = os.path.join(llama_cpp_dir, \"convert.py\")\n    if not os.path.isfile(convert_py_path):\n        raise RuntimeError(f\"Could not find convert.py at {convert_py_path}. Please ensure LLAMA_CPP_DIR is set correctly.\")\n\n    # ... (rest of the main function) ...\n```\n\nThese changes should help resolve the errors you're encountering. Make sure to set the `LLAMA_CPP_DIR` environment variable to the correct path of your llama.cpp directory before running the script.\n\nIf you still encounter issues after making these changes, please let me know, and I'll be happy to help further.","sender":"assistant","index":9,"created_at":"2024-08-29T08:22:28.677686+00:00","updated_at":"2024-08-29T08:22:28.677686+00:00","truncated":false,"attachments":[],"files":[],"files_v2":[],"parent_message_uuid":"5ff1e4f4-a689-412e-b1b5-0e1751705881"},{"uuid":"afbc4784-0697-47cf-a663-504a39ff0012","text":"```\n\n```","sender":"human","index":10,"created_at":"2024-08-29T08:25:41.537569+00:00","updated_at":"2024-08-29T08:25:41.537569+00:00","truncated":false,"attachments":[{"id":"a28986b0-f6fb-4b9a-80d2-4dd01f260175","file_name":"paste.txt","file_size":5648,"file_type":"txt","extracted_content":"total 1.4G\ndrwxr-xr-x 23 kuldeep kuldeep 4.0K Aug 29 04:49 .\ndrwxr-xr-x 11 kuldeep kuldeep 4.0K Aug 29 04:48 ..\n-rw-r--r--  1 kuldeep kuldeep  33K Aug 29 04:42 AUTHORS\ndrwxr-xr-x  2 kuldeep kuldeep 4.0K Aug 29 04:42 ci\n-rw-r--r--  1 kuldeep kuldeep  791 Aug 29 04:42 .clang-tidy\ndrwxr-xr-x  2 kuldeep kuldeep 4.0K Aug 29 04:42 cmake\n-rw-r--r--  1 kuldeep kuldeep 6.3K Aug 29 04:42 CMakeLists.txt\n-rw-r--r--  1 kuldeep kuldeep 3.2K Aug 29 04:42 CMakePresets.json\ndrwxr-xr-x  3 kuldeep kuldeep 4.0K Aug 29 04:45 common\n-rw-r--r--  1 kuldeep kuldeep 2.3K Aug 29 04:42 CONTRIBUTING.md\n-rwxr-xr-x  1 kuldeep kuldeep 182K Aug 29 04:42 convert_hf_to_gguf.py\n-rwxr-xr-x  1 kuldeep kuldeep  15K Aug 29 04:42 convert_hf_to_gguf_update.py\n-rwxr-xr-x  1 kuldeep kuldeep  19K Aug 29 04:42 convert_llama_ggml_to_gguf.py\n-rwxr-xr-x  1 kuldeep kuldeep  15K Aug 29 04:42 convert_lora_to_gguf.py\ndrwxr-xr-x  3 kuldeep kuldeep 4.0K Aug 29 04:42 .devops\n-rw-r--r--  1 kuldeep kuldeep  158 Aug 29 04:42 .dockerignore\ndrwxr-xr-x  4 kuldeep kuldeep 4.0K Aug 29 04:42 docs\n-rw-r--r--  1 kuldeep kuldeep   97 Aug 29 04:42 .ecrc\n-rw-r--r--  1 kuldeep kuldeep  599 Aug 29 04:42 .editorconfig\ndrwxr-xr-x 42 kuldeep kuldeep 4.0K Aug 29 04:42 examples\n-rw-r--r--  1 kuldeep kuldeep  544 Aug 29 04:42 .flake8\n-rw-r--r--  1 kuldeep kuldeep 1.6K Aug 29 04:42 flake.lock\n-rw-r--r--  1 kuldeep kuldeep 7.2K Aug 29 04:42 flake.nix\ndrwxr-xr-x  5 kuldeep kuldeep 4.0K Aug 29 04:42 ggml\ndrwxr-xr-x  6 kuldeep kuldeep 4.0K Aug 29 04:42 gguf-py\ndrwxr-xr-x  8 kuldeep kuldeep 4.0K Aug 29 04:42 .git\ndrwxr-xr-x  4 kuldeep kuldeep 4.0K Aug 29 04:42 .github\n-rw-r--r--  1 kuldeep kuldeep 1.5K Aug 29 04:42 .gitignore\n-rw-r--r--  1 kuldeep kuldeep   94 Aug 29 04:42 .gitmodules\ndrwxr-xr-x  2 kuldeep kuldeep 4.0K Aug 29 04:42 grammars\ndrwxr-xr-x  2 kuldeep kuldeep 4.0K Aug 29 04:42 include\n-rw-r--r--  1 kuldeep kuldeep 369K Aug 29 04:46 libllava.a\n-rw-r--r--  1 kuldeep kuldeep 1.1K Aug 29 04:42 LICENSE\n-rwxr-xr-x  1 kuldeep kuldeep  39M Aug 29 04:46 llama-baby-llama\n-rwxr-xr-x  1 kuldeep kuldeep  39M Aug 29 04:46 llama-batched\n-rwxr-xr-x  1 kuldeep kuldeep  39M Aug 29 04:46 llama-batched-bench\n-rwxr-xr-x  1 kuldeep kuldeep  42M Aug 29 04:48 llama-bench\n-rwxr-xr-x  1 kuldeep kuldeep 3.6M Aug 29 04:45 llama-benchmark-matmult\n-rwxr-xr-x  1 kuldeep kuldeep  39M Aug 29 04:47 llama-cli\n-rwxr-xr-x  1 kuldeep kuldeep  25M Aug 29 04:46 llama-convert-llama2c-to-ggml\n-rwxr-xr-x  1 kuldeep kuldeep  39M Aug 29 04:48 llama-cvector-generator\n-rwxr-xr-x  1 kuldeep kuldeep  39M Aug 29 04:46 llama-embedding\n-rwxr-xr-x  1 kuldeep kuldeep  39M Aug 29 04:46 llama-eval-callback\n-rwxr-xr-x  1 kuldeep kuldeep  39M Aug 29 04:46 llama-export-lora\n-rwxr-xr-x  1 kuldeep kuldeep  39M Aug 29 04:46 llama-gbnf-validator\n-rwxr-xr-x  1 kuldeep kuldeep 3.7M Aug 29 04:45 llama-gguf\n-rwxr-xr-x  1 kuldeep kuldeep  39M Aug 29 04:46 llama-gguf-hash\n-rwxr-xr-x  1 kuldeep kuldeep  39M Aug 29 04:46 llama-gguf-split\n-rwxr-xr-x  1 kuldeep kuldeep  39M Aug 29 04:46 llama-gritlm\n-rwxr-xr-x  1 kuldeep kuldeep  40M Aug 29 04:47 llama-imatrix\n-rwxr-xr-x  1 kuldeep kuldeep  39M Aug 29 04:47 llama-infill\n-rwxr-xr-x  1 kuldeep kuldeep  43M Aug 29 04:48 llama-llava-cli\n-rwxr-xr-x  1 kuldeep kuldeep  39M Aug 29 04:47 llama-lookahead\n-rwxr-xr-x  1 kuldeep kuldeep  39M Aug 29 04:47 llama-lookup\n-rwxr-xr-x  1 kuldeep kuldeep  39M Aug 29 04:47 llama-lookup-create\n-rwxr-xr-x  1 kuldeep kuldeep  39M Aug 29 04:47 llama-lookup-merge\n-rwxr-xr-x  1 kuldeep kuldeep  39M Aug 29 04:47 llama-lookup-stats\n-rwxr-xr-x  1 kuldeep kuldeep  43M Aug 29 04:48 llama-minicpmv-cli\n-rwxr-xr-x  1 kuldeep kuldeep  39M Aug 29 04:47 llama-parallel\n-rwxr-xr-x  1 kuldeep kuldeep  39M Aug 29 04:47 llama-passkey\n-rwxr-xr-x  1 kuldeep kuldeep  40M Aug 29 04:48 llama-perplexity\n-rwxr-xr-x  1 kuldeep kuldeep 3.6M Aug 29 04:45 llama-q8dot\n-rwxr-xr-x  1 kuldeep kuldeep  39M Aug 29 04:47 llama-quantize\n-rwxr-xr-x  1 kuldeep kuldeep  40M Aug 29 04:48 llama-quantize-stats\n-rwxr-xr-x  1 kuldeep kuldeep  39M Aug 29 04:47 llama-retrieval\n-rwxr-xr-x  1 kuldeep kuldeep  39M Aug 29 04:47 llama-save-load-state\n-rwxr-xr-x  1 kuldeep kuldeep  51M Aug 29 04:49 llama-server\n-rwxr-xr-x  1 kuldeep kuldeep  39M Aug 29 04:47 llama-simple\n-rwxr-xr-x  1 kuldeep kuldeep  39M Aug 29 04:48 llama-speculative\n-rwxr-xr-x  1 kuldeep kuldeep  39M Aug 29 04:47 llama-tokenize\n-rwxr-xr-x  1 kuldeep kuldeep 3.7M Aug 29 04:45 llama-vdot\n-rwxr-xr-x  1 kuldeep kuldeep  78K Aug 29 04:44 main\n-rw-r--r--  1 kuldeep kuldeep  49K Aug 29 04:42 Makefile\ndrwxr-xr-x  2 kuldeep kuldeep 4.0K Aug 29 04:42 media\ndrwxr-xr-x  2 kuldeep kuldeep 4.0K Aug 29 04:42 models\n-rw-r--r--  1 kuldeep kuldeep  163 Aug 29 04:42 mypy.ini\n-rw-r--r--  1 kuldeep kuldeep 2.0K Aug 29 04:42 Package.swift\ndrwxr-xr-x  3 kuldeep kuldeep 4.0K Aug 29 04:42 pocs\n-rw-r--r--  1 kuldeep kuldeep 122K Aug 29 04:42 poetry.lock\n-rw-r--r--  1 kuldeep kuldeep  447 Aug 29 04:42 .pre-commit-config.yaml\ndrwxr-xr-x  2 kuldeep kuldeep 4.0K Aug 29 04:42 prompts\n-rw-r--r--  1 kuldeep kuldeep 1.3K Aug 29 04:42 pyproject.toml\n-rw-r--r--  1 kuldeep kuldeep  528 Aug 29 04:42 pyrightconfig.json\n-rw-r--r--  1 kuldeep kuldeep  30K Aug 29 04:42 README.md\ndrwxr-xr-x  2 kuldeep kuldeep 4.0K Aug 29 04:42 requirements\n-rw-r--r--  1 kuldeep kuldeep  505 Aug 29 04:42 requirements.txt\ndrwxr-xr-x  2 kuldeep kuldeep 4.0K Aug 29 04:42 scripts\n-rw-r--r--  1 kuldeep kuldeep 5.0K Aug 29 04:42 SECURITY.md\n-rwxr-xr-x  1 kuldeep kuldeep  78K Aug 29 04:44 server\ndrwxr-xr-x  2 kuldeep kuldeep 4.0K Aug 29 04:42 spm-headers\ndrwxr-xr-x  2 kuldeep kuldeep 4.0K Aug 29 04:46 src\ndrwxr-xr-x  2 kuldeep kuldeep 4.0K Aug 29 04:44 tests\n","created_at":"2024-08-29T08:25:41.537569+00:00"}],"files":[],"files_v2":[],"parent_message_uuid":"532254e0-fdfd-4640-8f48-f3d32a334851"},{"uuid":"ac98bff7-4a50-40a8-b1a9-5ee4c12cc19d","text":" Based on the file listing you provided, it appears that the `convert.py` script is not present in the llama.cpp directory. However, there are several other conversion scripts available:\n\n1. `convert_hf_to_gguf.py`\n2. `convert_hf_to_gguf_update.py`\n3. `convert_llama_ggml_to_gguf.py`\n4. `convert_lora_to_gguf.py`\n\nThe error you're encountering is because your script is looking for a `convert.py` file, which doesn't exist. Instead, you should use the `convert_hf_to_gguf.py` script, which seems to be the most appropriate for converting Hugging Face models to GGUF format.\n\nTo fix this issue, you need to modify your `convert_using_convert` function in the `ggify.py` script. Here's how you can update it:\n\n```python\ndef convert_using_convert(dirname, *, convert_type, vocab_type):\n    convert_hf_to_gguf_py = os.path.join(get_llama_cpp_dir(), \"convert_hf_to_gguf.py\")\n    if not os.path.isfile(convert_hf_to_gguf_py):\n        raise RuntimeError(\n            f\"Could not find convert_hf_to_gguf.py at {convert_hf_to_gguf_py} \"\n            f\"(set LLAMA_CPP_DIR (currently {get_llama_cpp_dir()}?))\"\n        )\n    print_and_check_call(\n        [\n            PYTHON_EXE,\n            convert_hf_to_gguf_py,\n            dirname,\n            f\"--outtype={convert_type}\",\n            \"--outfile=ggml-model-{convert_type}.gguf\",\n            # Note: The vocab-type parameter might not be needed for convert_hf_to_gguf.py\n            # Remove it if it causes an error\n            f\"--vocab-type={vocab_type}\",\n        ]\n    )\n```\n\nAlso, make sure that the `LLAMA_CPP_DIR` environment variable is set correctly to the directory containing the llama.cpp files.\n\nAfter making these changes, try running your script again. If you encounter any other issues, please let me know, and I'll be happy to help further.","sender":"assistant","index":11,"created_at":"2024-08-29T08:25:41.537569+00:00","updated_at":"2024-08-29T08:25:41.537569+00:00","truncated":false,"attachments":[],"files":[],"files_v2":[],"parent_message_uuid":"afbc4784-0697-47cf-a663-504a39ff0012"},{"uuid":"257c07ce-3bbf-49e7-ba16-4d07c9867e5a","text":"before giving the task to llm via hf, I want to provide it instruction. how to do that?","sender":"human","index":12,"created_at":"2024-08-29T09:08:30.220336+00:00","updated_at":"2024-08-29T09:08:30.220336+00:00","truncated":false,"attachments":[{"id":"1d6d8d1b-a748-4626-a4eb-8cc3fd3a8b88","file_name":"huggingface.py","file_size":62745,"file_type":"text/x-python","extracted_content":"import copy\nimport os\nfrom datetime import timedelta\nfrom pathlib import Path\nfrom typing import Dict, List, Literal, Optional, Tuple, Union\n\nimport torch\nimport torch.nn.functional as F\nimport transformers\nfrom accelerate import (\n    Accelerator,\n    InitProcessGroupKwargs,\n    find_executable_batch_size,\n)\nfrom accelerate.utils import get_max_memory\nfrom huggingface_hub import HfApi\nfrom packaging import version\nfrom peft import PeftModel\nfrom peft import __version__ as PEFT_VERSION\nfrom tqdm import tqdm\nfrom transformers.models.auto.modeling_auto import (\n    MODEL_FOR_CAUSAL_LM_MAPPING_NAMES,\n    MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES,\n)\n\nfrom lm_eval import utils\nfrom lm_eval.api.instance import Instance\nfrom lm_eval.api.model import TemplateLM\nfrom lm_eval.api.registry import register_model\nfrom lm_eval.models.utils import (\n    Collator,\n    clear_torch_cache,\n    configure_pad_token,\n    get_dtype,\n    pad_and_concat,\n    stop_sequences_criteria,\n)\n\n\neval_logger = utils.eval_logger\n\n\n@register_model(\"hf-auto\", \"hf\", \"huggingface\")\nclass HFLM(TemplateLM):\n    \"\"\"\n    An abstracted Huggingface model class. Enables usage with both models of\n    `transformers.AutoModelForCausalLM` and `transformers.AutoModelForSeq2SeqLM` classes.\n\n    Supports data-parallel multi-GPU with HF Accelerate.\n    \"\"\"\n\n    AUTO_MODEL_CLASS = None\n    _DEFAULT_MAX_LENGTH = 2048\n\n    def __init__(\n        self,\n        pretrained: Union[str, transformers.PreTrainedModel],\n        backend: Optional[Literal[\"default\", \"causal\", \"seq2seq\"]] = \"default\",\n        # override whether the model should be treated as decoder-only (causal) or encoder-decoder (seq2seq)\n        revision: Optional[str] = \"main\",\n        subfolder: Optional[str] = None,\n        tokenizer: Optional[\n            Union[\n                str,\n                transformers.PreTrainedTokenizer,\n                transformers.PreTrainedTokenizerFast,\n            ]\n        ] = None,\n        truncation: Optional[bool] = False,\n        logits_cache: bool = True,\n        max_length: Optional[int] = None,\n        device: Optional[str] = \"cuda\",\n        dtype: Optional[Union[str, torch.dtype]] = \"auto\",\n        batch_size: Optional[Union[int, str]] = 1,\n        max_batch_size: Optional[int] = 64,\n        trust_remote_code: Optional[bool] = False,\n        use_fast_tokenizer: Optional[bool] = True,\n        add_bos_token: Optional[bool] = False,\n        prefix_token_id: Optional[int] = None,\n        # arguments used for splitting a model across GPUs naively.\n        # only used if `parallelize=True`.\n        parallelize: Optional[bool] = False,\n        max_memory_per_gpu: Optional[Union[int, str]] = None,\n        max_cpu_memory: Optional[Union[int, str]] = None,\n        offload_folder: Optional[Union[str, os.PathLike]] = \"./offload\",\n        # PEFT, delta weights and quantization options\n        peft: Optional[str] = None,\n        delta: Optional[str] = None,\n        autogptq: Optional[Union[bool, str]] = False,\n        **kwargs,\n    ) -> None:\n        super().__init__()\n\n        # optionally: take in an already-initialized transformers.PreTrainedModel\n        if not isinstance(pretrained, str):\n            eval_logger.warning(\n                \"`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\"\n            )\n            assert not parallelize, \"`parallelize=True` is not compatible with passing pre-initialized model to `pretrained`\"\n            self._model = pretrained\n            self._device = self._model.device\n            self._config = self._model.config\n            gpus = 0\n\n        else:\n            assert isinstance(device, str)\n            assert isinstance(pretrained, str)\n            assert isinstance(batch_size, (int, str))\n\n            gpus = torch.cuda.device_count()\n            accelerator_kwargs = InitProcessGroupKwargs(timeout=timedelta(weeks=52))\n            accelerator = Accelerator(kwargs_handlers=[accelerator_kwargs])\n            if accelerator.num_processes > 1:\n                self.accelerator = accelerator\n\n            if \"npu\" in accelerator.device.type:\n                gpus = torch.npu.device_count()\n\n            # using one process with no model parallelism\n            if not (parallelize or accelerator.num_processes > 1):\n                # use user-passed device\n                device_list = set(\n                    [\"cuda\", \"cpu\"]\n                    + [f\"cuda:{i}\" for i in range(gpus)]\n                    + [\"mps\", \"mps:0\"]\n                    + [f\"npu:{i}\" for i in range(gpus)]\n                )\n                if device and device in device_list:\n                    self._device = torch.device(device)\n                    eval_logger.info(f\"Using device '{device}'\")\n                    if device in (\"mps\", \"mps:0\") and version.parse(\n                        torch.__version__\n                    ) < version.parse(\"2.1\"):\n                        raise RuntimeError(\n                            f\"mps requires torch >= 2.1. You have {torch.__version__}\"\n                        )\n                else:\n                    eval_logger.info(\"Device not specified\")\n                    eval_logger.info(f\"Cuda Available? {torch.cuda.is_available()}\")\n                    self._device = (\n                        torch.device(\"cuda\")\n                        if torch.cuda.is_available()\n                        else torch.device(\"cpu\")\n                    )\n            else:  # Parallelism managed by accelerate\n                if device != \"cuda\":\n                    eval_logger.info(\n                        f\"Using `accelerate launch` or `parallelize=True`, device '{device}' will be overridden when placing model.\"\n                    )\n                # TODO: include in warning that `load_in_8bit` etc. affect this too\n                self._device = (\n                    self.accelerator.device\n                    if hasattr(self, \"accelerator\")\n                    else torch.device(device)\n                )\n\n            revision = str(revision)  # cast to string if not already one\n            # TODO: update this to be less of a hack once subfolder is fixed in HF\n            revision = revision + (\"/\" + subfolder if subfolder is not None else \"\")\n\n            self._get_config(\n                pretrained,\n                revision=revision,\n                trust_remote_code=trust_remote_code,\n            )\n\n        # determine which of 'causal' and 'seq2seq' backends to use\n        self._get_backend(\n            config=self.config, backend=backend, trust_remote_code=trust_remote_code\n        )\n\n        # load tokenizer so we know tokenizer vocabulary size before loading model and PEFT\n        self._create_tokenizer(\n            pretrained,\n            tokenizer,\n            revision=revision,\n            trust_remote_code=trust_remote_code,\n            use_fast_tokenizer=use_fast_tokenizer,\n        )\n\n        # if we passed `pretrained` as a string, initialize our model now\n        if isinstance(pretrained, str):\n            self._create_model(\n                pretrained=pretrained,\n                revision=revision,\n                dtype=dtype,\n                trust_remote_code=trust_remote_code,\n                parallelize=parallelize,\n                gpus=gpus,\n                max_memory_per_gpu=max_memory_per_gpu,\n                max_cpu_memory=max_cpu_memory,\n                offload_folder=offload_folder,\n                peft=peft,\n                delta=delta,\n                autogptq=autogptq,\n                **kwargs,\n            )\n\n        # access self._model through self.model property outside this method\n        if isinstance(self.model, torch.nn.Module):\n            self.model.eval()\n            self.model.tie_weights()\n\n        self.truncation = truncation\n        self.logits_cache = logits_cache\n        self.vocab_size = self.tokenizer.vocab_size\n        # select (or create) a pad token to use\n        self.tokenizer = configure_pad_token(self.tokenizer, model_config=self.config)\n\n        self.add_bos_token = add_bos_token\n        if \"gemma\" in getattr(self.config, \"model_type\", \"\"):\n            self.add_bos_token = True\n            eval_logger.info(\n                f\"Model type is '{self.config.model_type}', part of the Gemma family--a BOS token will be used as Gemma underperforms without it.\"\n            )\n\n        self._max_length = max_length\n        self.pretrained = pretrained\n        self.delta = delta\n        self.peft = peft\n        self.revision = revision\n        self.batch_schedule = 1\n        self.batch_sizes = {}\n        self.max_batch_size = max_batch_size\n\n        if str(batch_size).startswith(\"auto\"):\n            batch_size = batch_size.split(\":\")\n            self.batch_size_per_gpu = batch_size[0]\n            self.batch_schedule = float(batch_size[1]) if len(batch_size) > 1 else 1\n        else:\n            self.batch_size_per_gpu = int(batch_size)\n\n        if isinstance(pretrained, str):\n            if gpus >= 1 or str(self.device) == \"mps\":\n                # TODO: can remove this whole snippet except in the mps case, perhaps?\n                if not (parallelize or autogptq or hasattr(self, \"accelerator\")):\n                    # place model onto device requested manually,\n                    # if not using HF Accelerate or device_map\n                    # or any other option that preloads model onto device\n                    try:\n                        self.model.to(self.device)\n                    except ValueError:\n                        eval_logger.debug(\n                            \"Failed to place model onto specified device. This may be because the model is quantized via `bitsandbytes` or `device_map` is provided. If the desired GPU is being used, this message is safe to ignore.\"\n                        )\n            # multigpu data-parallel support when launched with accelerate\n            if gpus > 1:\n                if accelerator.num_processes > 1:\n                    if parallelize:\n                        eval_logger.warning(\n                            \"You are both using a HF Accelerate `device_map` (`--model_args parallelize=True`) and launching via `accelerate launch`. This will attempt to do model and data parallelism depending on the resources available.\"\n                        )\n                    elif gpus > accelerator.num_processes:\n                        eval_logger.warning(\n                            \"WARNING: The number of total system GPUs does not match the number of spawned processes. \"\n                            \"If you would like to use data parallelism, please launch the script \"\n                            \"with 'accelerate launch *script*'. \"\n                            f\"Current run will proceed with {accelerator.num_processes} devices.\"\n                        )\n                        if self.accelerator.is_local_main_process:\n                            eval_logger.info(\n                                f\"Using {gpus} devices with data parallelism\"\n                            )\n\n                    self._device = torch.device(f\"{accelerator.device}\")\n                    self.accelerator = accelerator\n\n                    self._rank = self.accelerator.local_process_index\n                    self._world_size = self.accelerator.num_processes\n                else:\n                    # if we aren't launching via accelerate, ditch\n                    self._rank = 0\n                    self._world_size = 1\n        else:\n            # if a PreTrainedModel was passed into HFLM, we forgo distributed setup.\n            eval_logger.warning(\n                \"Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\"\n            )\n            self._rank = 0\n            self._world_size = 1\n\n        self.custom_prefix_token_id = prefix_token_id\n        if prefix_token_id is not None:\n            eval_logger.info(\n                f\"Loglikelihood prefix token id used in evaluation: {self.prefix_token_id}\"\n            )\n\n    def _get_accelerate_args(\n        self,\n        parallelize: bool = None,\n        device_map: Optional[str] = \"auto\",\n        max_memory_per_gpu: Optional[Union[int, str]] = None,\n        max_cpu_memory: Optional[Union[int, str]] = None,\n        offload_folder: Optional[str] = \"./offload\",\n        gpus: Optional[int] = None,\n    ) -> dict:\n        \"\"\"Returns the kwargs needed to apply `accelerate` in `AutoModel.from_pretrained`.\"\"\"\n        num_local_processes = int(os.environ.get(\"LOCAL_WORLD_SIZE\", 1))\n        num_machines = int(os.environ.get(\"WORLD_SIZE\", 0)) // num_local_processes\n        if (\n            num_machines == 0\n            and hasattr(self, \"accelerator\")\n            and self.accelerator is not None\n        ):\n            eval_logger.info(\n                \"We are not in a distributed setting for accelerate. Setting model_parallel to False.\"\n            )\n            parallelize = False\n\n        if parallelize is None:\n            # If parallelism is unset by the user, we automatically assign model parallelism\n            # if enough extra GPUs are available\n            max_memory_all_gpus = get_max_memory()\n            # We just want gpu, not cpu, max memory\n            if \"cpu\" in max_memory_all_gpus:\n                del max_memory_all_gpus[\"cpu\"]\n            parallelize = bool(num_local_processes < len(max_memory_all_gpus))\n            eval_logger.info(\n                f\"Setting model parallel to {parallelize} since \"\n                f\"the number of local processes is {num_local_processes} \"\n                f\"and the number of GPUs is {len(max_memory_all_gpus)}\"\n            )\n\n        args = {}\n        if parallelize:  # Model parallelism will be used\n            max_memory = {}\n            if max_memory_per_gpu is not None:  # Using the provided memory requirements\n                max_memory_per_gpu_map = {\n                    device_idx: max_memory_per_gpu for device_idx in range(gpus)\n                }\n            else:  # Estimating the possible memory requirements\n                max_memory_all_gpus = get_max_memory()\n                if \"cpu\" in max_memory_all_gpus:\n                    del max_memory_all_gpus[\"cpu\"]\n                if not hasattr(self, \"accelerator\"):\n                    max_memory_per_gpu_map = {\n                        k: v for k, v in max_memory_all_gpus.items()\n                    }\n                else:\n                    # use only 1 / num_processes of the GPUs if we are running under accelerate launch\n                    max_memory_per_gpu_map = {\n                        k: v\n                        for k, v in max_memory_all_gpus.items()\n                        if k % num_local_processes\n                        == (self.accelerator.process_index % num_local_processes)\n                    }\n            args[\"max_memory\"] = max_memory_per_gpu_map\n            args[\"device_map\"] = \"auto\"\n            eval_logger.info(\n                f\"Model parallel was set to True, setting max memory per GPU to {max_memory_per_gpu_map} and device map to 'auto'\"\n            )\n\n            if max_cpu_memory is not None:\n                max_memory[\"cpu\"] = max_cpu_memory\n\n            args[\"offload_folder\"] = offload_folder\n        elif (\n            device_map is None\n        ):  # No model parallelism, we use the default provided device for our model\n            if hasattr(self, \"accelerator\"):\n                device_map = {\"\": f\"{self.accelerator.device}\"}\n            else:\n                device_map = {\"\": str(self.device)}\n            args[\"max_memory\"] = None\n            args[\"device_map\"] = device_map\n            eval_logger.info(\n                f\"Model parallel was set to False, max memory was not set, and device map was set to {device_map}\"\n            )\n        else:\n            args[\"max_memory\"] = None\n            args[\"device_map\"] = None\n            eval_logger.info(\"Model parallel was set to False.\")\n\n        return args\n\n    @property\n    def config(self):\n        # return the associated transformers.AutoConfig for the given pretrained model.\n        return self._config\n\n    @property\n    def model(self):\n        # returns the model, unwrapping it if using Accelerate\n        if hasattr(self, \"accelerator\"):\n            return self.accelerator.unwrap_model(self._model)\n        else:\n            return self._model\n\n    @property\n    def eot_token_id(self):\n        # we use EOT because end of *text* is more accurate for what we're doing than end of *sentence*\n        return self.tokenizer.eos_token_id\n\n    @property\n    def prefix_token_id(self):\n        # it is used as prefix for loglikelihood\n        if self.custom_prefix_token_id is not None:\n            return self.custom_prefix_token_id\n        if self.tokenizer.bos_token_id is not None:\n            return self.tokenizer.bos_token_id\n        return self.tokenizer.eos_token_id\n\n    @property\n    def max_length(self):\n        if self._max_length:  # if max length manually set, return it\n            return self._max_length\n        seqlen_config_attrs = (\"n_positions\", \"max_position_embeddings\", \"n_ctx\")\n        for attr in seqlen_config_attrs:\n            if hasattr(self.model.config, attr):\n                return getattr(self.model.config, attr)\n        if hasattr(self.tokenizer, \"model_max_length\"):\n            if self.tokenizer.model_max_length == 1000000000000000019884624838656:\n                return self._DEFAULT_MAX_LENGTH\n            return self.tokenizer.model_max_length\n        return self._DEFAULT_MAX_LENGTH\n\n    @property\n    def max_gen_toks(self) -> int:\n        return 256\n\n    @property\n    def batch_size(self):\n        return self.batch_size_per_gpu\n\n    @property\n    def device(self):\n        return self._device\n\n    @property\n    def rank(self):\n        return self._rank\n\n    @property\n    def world_size(self):\n        return self._world_size\n\n    @property\n    def tokenizer_name(self) -> str:\n        return self.tokenizer.name_or_path.replace(\"/\", \"__\")\n\n    def chat_template(self, chat_template: Union[bool, str] = False) -> Optional[str]:\n        \"\"\"\n        Get the appropriate chat template for the model based on configuration and input.\n        This method determines, and returns the correct chat template, ensuring reproducibility.\n\n        The template selection logic is adapted from the Transformers library's `apply_chat_template`\n        method in the Tokenizer class. The original implementation can be found at:\n        https://github.com/huggingface/transformers/blob/fc35907f95459d7a6c5281dfadd680b6f7b620e3/src/transformers/tokenization_utils_base.py#L1687\n\n        This method ensures that the right template is chosen based on the following:\n        1. If the model's tokenizer has multiple templates:\n            a. Use the specified template if it exists in the dictionary.\n            b. Use the default template from the list if no specific template is provided.\n            c. Raise an error if no default template exists and no specific template is provided.\n        2. If the model's tokenizer has a single template or no template:\n            a. Use the tokenizer's chat template if available.\n            b. Fall back to the default chat template if no tokenizer chat template exists.\n\n        Args:\n            chat_template (Union[bool, str]): Specifies the chat template to use.\n                - If False or None, no template is applied.\n                - If True, the default or only available template is used.\n                - If a string, the template with the matching name is used.\n\n        Returns:\n            Optional[str]: The selected chat template, or None if no template is applied.\n        \"\"\"\n        if chat_template is False or chat_template is None:\n            eval_logger.warning(\n                \"model.chat_template was called with the chat_template set to False or None. \"\n                \"Therefore no chat template will be applied. Make sure this is an intended behavior.\"\n            )\n            return None\n\n        # Convert boolean chat_template to None to ensure compatibility with the adapted logic\n        if isinstance(chat_template, bool):\n            chat_template = None\n        using_default_template = False\n\n        # First, handle the cases when the model has a dict of multiple templates\n        template = self.tokenizer.chat_template or self.tokenizer.default_chat_template\n\n        if isinstance(template, dict):\n            using_default_dict = self.tokenizer.chat_template is None\n\n            if chat_template is not None:\n                if chat_template in template:\n                    selected_template = template[chat_template]\n                    if using_default_dict:\n                        using_default_template = True\n                else:\n                    raise ValueError(\n                        f\"The specified chat template '{chat_template}' is not available. \"\n                        f\"Available template names are {sorted(template.keys())}.\"\n                    )\n            else:\n                # If user didn't pass a chat template, use the default template from the dict\n                if \"default\" in template:\n                    selected_template = template[\"default\"]\n                    using_default_template = True\n                else:\n                    raise ValueError(\n                        \"This model has multiple chat templates with no default specified! Please either pass a chat \"\n                        \"template or the name of the template you wish to use to the `chat_template` argument. Available \"\n                        f\"template names are {sorted(template.keys())}.\"\n                    )\n\n        # Cases when the model has a single template or no template\n        else:\n            # priority: `chat_template` argument > `tokenizer.chat_template` > `tokenizer.default_chat_template\n            if isinstance(chat_template, str):\n                eval_logger.warning(\n                    \"Chat template name provided, but the tokenizer's chat template is not a dictionary. \"\n                    \"Using the tokenizer's chat template or the default template instead.\"\n                )\n            if self.tokenizer.chat_template is not None:\n                selected_template = self.tokenizer.chat_template\n            else:\n                selected_template = self.tokenizer.default_chat_template\n                using_default_template = True\n\n        if using_default_template:\n            eval_logger.warning(\n                \"No chat template is set for this tokenizer, falling back to a default class-level template. This is \"\n                \"very error-prone, because models are often trained with templates different from the class default! \"\n                \"Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which \"\n                \"point any code depending on them will stop working. We recommend setting a valid chat template before \"\n                \"then to ensure that this model continues working without issues.\"\n            )\n\n        return selected_template\n\n    def _get_backend(\n        self,\n        config: Union[transformers.PretrainedConfig, transformers.AutoConfig],\n        backend: Optional[Literal[\"default\", \"causal\", \"seq2seq\"]] = \"default\",\n        trust_remote_code: Optional[bool] = False,\n    ) -> None:\n        \"\"\"\n        Helper method during initialization.\n        Determines the backend (\"causal\" (decoder-only) or \"seq2seq\" (encoder-decoder))\n        model type to be used.\n        \"\"\"\n        assert backend in [\"default\", \"causal\", \"seq2seq\"]\n\n        if backend != \"default\":\n            # if we've settled on non-default backend, use that manually\n            if backend == \"causal\":\n                self.AUTO_MODEL_CLASS = transformers.AutoModelForCausalLM\n            elif backend == \"seq2seq\":\n                self.AUTO_MODEL_CLASS = transformers.AutoModelForSeq2SeqLM\n            eval_logger.info(\n                f\"Overrode HF model backend type, and using type '{backend}'\"\n            )\n        else:\n            # determine and use the default HF backend for this model, based on its config + metadata.\n            if (\n                getattr(config, \"model_type\")\n                in MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES\n            ):\n                # first check if model type is listed under seq2seq models, since some\n                # models like MBart are listed in both seq2seq and causal mistakenly in HF transformers.\n                # these special cases should be treated as seq2seq models.\n                self.AUTO_MODEL_CLASS = transformers.AutoModelForSeq2SeqLM\n            elif (\n                getattr(self.config, \"model_type\") in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n            ):\n                self.AUTO_MODEL_CLASS = transformers.AutoModelForCausalLM\n            else:\n                if not trust_remote_code:\n                    eval_logger.warning(\n                        \"HF model type is neither marked as CausalLM or Seq2SeqLM. \\\n                    This is expected if your model requires `trust_remote_code=True` but may be an error otherwise.\"\n                    )\n                # if model type is neither in HF transformers causal or seq2seq model registries\n                # then we default to AutoModelForCausalLM\n                self.AUTO_MODEL_CLASS = transformers.AutoModelForCausalLM\n\n        assert self.AUTO_MODEL_CLASS in [\n            transformers.AutoModelForCausalLM,\n            transformers.AutoModelForSeq2SeqLM,\n        ]\n        return None\n\n    def _get_config(\n        self,\n        pretrained: str,\n        revision: str = \"main\",\n        trust_remote_code: bool = False,\n    ) -> None:\n        self._config = transformers.AutoConfig.from_pretrained(\n            pretrained,\n            revision=revision,\n            trust_remote_code=trust_remote_code,\n        )\n\n    def _create_model(\n        self,\n        pretrained: str,\n        revision: Optional[str] = \"main\",\n        dtype: Optional[Union[str, torch.dtype]] = \"auto\",\n        trust_remote_code: Optional[bool] = False,\n        # arguments used for splitting a model across GPUs naively.\n        # only used if `parallelize=True`.\n        # (accelerate naive PP (device_map) options)\n        parallelize: Optional[bool] = False,\n        gpus: Optional[int] = None,\n        max_memory_per_gpu: Optional[Union[int, str]] = None,\n        max_cpu_memory: Optional[Union[int, str]] = None,\n        offload_folder: Optional[str] = \"./offload\",\n        # PEFT, delta weights and quantization options\n        peft: Optional[str] = None,\n        delta: Optional[str] = None,\n        autogptq: Optional[Union[bool, str]] = False,\n        **kwargs,\n    ) -> None:\n        \"\"\"\n        Initializes an HF or HF-compatible PreTrainedModel from scratch\n        inside HFLM, using the kwargs passed into self.__init__().\n\n        Also handles functionality such as AutoGPTQ usage and PEFT wrapping.\n\n        For future similar extensions to AutoGPTQ that are not core to HF's ecosystem,\n        (such as PyTorch models that are nearly, but not quite, fully mirroring\n        HF's public interface relied on in this HFLM class)\n        please consider subclassing HFLM and overriding this and other methods as needed.\n        \"\"\"\n\n        model_kwargs = kwargs if kwargs else {}\n\n        model_kwargs.update(\n            self._get_accelerate_args(\n                parallelize=parallelize,\n                device_map=kwargs.get(\"device_map\", None),\n                max_memory_per_gpu=max_memory_per_gpu,\n                max_cpu_memory=max_cpu_memory,\n                offload_folder=offload_folder,\n                gpus=gpus,\n            )\n        )\n\n        if not autogptq:\n            if model_kwargs.get(\"load_in_4bit\", None):\n                assert (\n                    transformers.__version__ >= \"4.30.0\"\n                ), \"load_in_4bit requires transformers >= 4.30.0\"\n            if transformers.__version__ >= \"4.30.0\":\n                if model_kwargs.get(\"load_in_4bit\", None):\n                    if model_kwargs.get(\"bnb_4bit_compute_dtype\", None):\n                        model_kwargs[\"bnb_4bit_compute_dtype\"] = get_dtype(\n                            model_kwargs[\"bnb_4bit_compute_dtype\"]\n                        )\n\n            self._model = self.AUTO_MODEL_CLASS.from_pretrained(\n                pretrained,\n                revision=revision,\n                torch_dtype=get_dtype(dtype),\n                trust_remote_code=trust_remote_code,\n                **model_kwargs,\n            )\n        else:\n            try:\n                from auto_gptq import AutoGPTQForCausalLM\n            except ModuleNotFoundError:\n                raise Exception(\n                    \"Tried to load auto_gptq, but auto-gptq is not installed \",\n                    \"please install auto-gptq via pip install lm-eval[gptq] or pip install -e .[gptq]\",\n                )\n\n            self._model = AutoGPTQForCausalLM.from_quantized(\n                pretrained,\n                trust_remote_code=trust_remote_code,\n                model_basename=None if autogptq is True else Path(autogptq).stem,\n                use_safetensors=True\n                if autogptq is True\n                else autogptq.endswith(\".safetensors\"),\n                **model_kwargs,\n            )\n\n        if peft and delta:\n            raise ValueError(\n                \"Cannot use both 'peft' and 'delta' options at the same time.\"\n            )\n\n        if peft:\n            if model_kwargs.get(\"load_in_4bit\", None):\n                if version.parse(PEFT_VERSION) < version.parse(\"0.4.0\"):\n                    raise AssertionError(\"load_in_4bit requires peft >= 0.4.0\")\n            if self._model.config.vocab_size != len(self.tokenizer):\n                # resize model for LoRAs with added tokens\n                eval_logger.info(\n                    f\"Model config indicates vocab_size='{self._model.config.vocab_size}', but found tokenizer with vocab size '{len(self.tokenizer)}'. Resizing model embedding layer...\"\n                )\n                self._model.resize_token_embeddings(len(self.tokenizer))\n            self._model = PeftModel.from_pretrained(\n                self._model, peft, revision=revision\n            )\n        elif delta:\n            if autogptq:\n                eval_logger.warning(\n                    \"Delta weights might trigger unexpected behavior when used with AutoGPTQ.\"\n                )\n            _model_delta = self.AUTO_MODEL_CLASS.from_pretrained(\n                delta,\n                revision=revision,\n                torch_dtype=get_dtype(dtype),\n                trust_remote_code=trust_remote_code,\n                **model_kwargs,\n            )\n            for name, param in self._model.state_dict().items():\n                try:\n                    param.data += _model_delta.state_dict()[name]\n                except KeyError:\n                    raise KeyError(f\"Delta model is missing weights for layer: {name}\")\n                except Exception as e:\n                    raise RuntimeError(\n                        f\"Failed to add delta weights to layer {name}. Error: {e}\"\n                    )\n\n            del _model_delta\n\n        return None\n\n    def _create_tokenizer(\n        self,\n        pretrained: Union[str, transformers.PreTrainedModel],\n        tokenizer: Optional[\n            Union[\n                str,\n                transformers.PreTrainedTokenizer,\n                transformers.PreTrainedTokenizerFast,\n            ]\n        ],\n        revision: Optional[str] = \"main\",\n        trust_remote_code: Optional[bool] = False,\n        use_fast_tokenizer: Optional[bool] = True,\n    ) -> None:\n        \"\"\"\n        Helper method during initialization.\n\n        Create a tokenizer object corresponding to the correct\n        tokenizer for value of `pretrained`, or use the pre-initialized tokenizer passed.\n        \"\"\"\n\n        if tokenizer:\n            if isinstance(tokenizer, str):\n                self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n                    tokenizer,\n                    revision=revision,\n                    trust_remote_code=trust_remote_code,\n                    use_fast=use_fast_tokenizer,\n                )\n            else:\n                assert isinstance(\n                    tokenizer, transformers.PreTrainedTokenizer\n                ) or isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n                self.tokenizer = tokenizer\n        else:\n            # Get tokenizer based on 'pretrained'\n            if isinstance(pretrained, str):\n                model_name = pretrained\n            else:\n                # get the HF hub name via accessor on model\n                model_name = self.model.name_or_path\n            self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n                model_name,\n                revision=revision,\n                trust_remote_code=trust_remote_code,\n                use_fast=use_fast_tokenizer,\n            )\n        return None\n\n    def _detect_batch_size(self, requests=None, pos: int = 0):\n        if requests:\n            _, context_enc, continuation_enc = requests[pos]\n            max_length = len(\n                (context_enc + continuation_enc)[-(self.max_length + 1) :][:-1]\n            )\n            max_context_enc = len(context_enc[-(self.max_length + 1) :])\n            max_cont_enc = len(continuation_enc[-(self.max_length + 1) :])\n        else:\n            max_length = self.max_length\n            max_context_enc = max_length\n            max_cont_enc = max_length\n\n        # if OOM, then halves batch_size and tries again\n        @find_executable_batch_size(starting_batch_size=self.max_batch_size)\n        def forward_batch(batch_size):\n            if self.AUTO_MODEL_CLASS == transformers.AutoModelForSeq2SeqLM:\n                length = max(max_context_enc, max_cont_enc)\n                batched_conts = torch.ones(\n                    (batch_size, length), device=self.device\n                ).long()\n                test_batch = torch.ones((batch_size, length), device=self.device).long()\n                call_kwargs = {\n                    \"attn_mask\": test_batch,\n                    \"labels\": batched_conts,\n                }\n            else:\n                call_kwargs = {}\n                test_batch = torch.ones(\n                    (batch_size, max_length), device=self.device\n                ).long()\n            for _ in range(5):\n                out = F.log_softmax(self._model_call(test_batch, **call_kwargs), dim=-1)  # noqa: F841\n\n            return batch_size\n\n        try:\n            batch_size = forward_batch()\n        except RuntimeError as e:\n            if \"No executable batch size found\" in str(e):\n                batch_size = 1\n            else:\n                raise\n\n        if self.world_size > 1:\n            # if multi-GPU, always take minimum over all selected batch sizes\n            max_rnk_bs = torch.tensor([batch_size], device=self.device)\n            gathered = (\n                self.accelerator.gather(max_rnk_bs).cpu().detach().numpy().tolist()\n            )\n            batch_size = min(gathered)\n            clear_torch_cache()\n            return batch_size\n\n        clear_torch_cache()\n        return batch_size\n\n    def tok_encode(\n        self, string: str, left_truncate_len=None, add_special_tokens=None\n    ) -> List[int]:\n        \"\"\" \"\"\"\n        # default for None - empty dict, use predefined tokenizer param\n        # used for all models except for CausalLM or predefined value\n        special_tokens_kwargs = {}\n\n        # by default for CausalLM - false or self.add_bos_token is set\n        if add_special_tokens is None:\n            if self.AUTO_MODEL_CLASS == transformers.AutoModelForCausalLM:\n                special_tokens_kwargs = {\n                    \"add_special_tokens\": False or self.add_bos_token\n                }\n        # otherwise the method explicitly defines the value\n        else:\n            special_tokens_kwargs = {\"add_special_tokens\": add_special_tokens}\n\n        encoding = self.tokenizer.encode(string, **special_tokens_kwargs)\n\n        # left-truncate the encoded context to be at most `left_truncate_len` tokens long\n        if left_truncate_len:\n            encoding = encoding[-left_truncate_len:]\n\n        return encoding\n\n    def tok_batch_encode(\n        self,\n        strings: List[str],\n        padding_side: str = \"left\",\n        left_truncate_len: int = None,\n        truncation: bool = False,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        # encode a batch of strings. converts to tensors and pads automatically, unlike tok_encode.\n        old_padding_side = self.tokenizer.padding_side\n        self.tokenizer.padding_side = padding_side\n\n        add_special_tokens = {}\n        if self.AUTO_MODEL_CLASS == transformers.AutoModelForCausalLM:\n            add_special_tokens = {\"add_special_tokens\": False or self.add_bos_token}\n\n        encoding = self.tokenizer(\n            strings,\n            truncation=truncation,\n            padding=\"longest\",\n            return_tensors=\"pt\",\n            **add_special_tokens,\n        )\n        if left_truncate_len:\n            encoding[\"input_ids\"] = encoding[\"input_ids\"][:, -left_truncate_len:]\n            encoding[\"attention_mask\"] = encoding[\"attention_mask\"][\n                :, -left_truncate_len:\n            ]\n        self.tokenizer.padding_side = old_padding_side\n\n        return encoding[\"input_ids\"], encoding[\"attention_mask\"]\n\n    def tok_decode(self, tokens, skip_special_tokens=True):\n        return self.tokenizer.decode(tokens, skip_special_tokens=skip_special_tokens)\n\n    def _model_call(self, inps, attn_mask=None, labels=None):\n        \"\"\"\n        :param inps: torch.Tensor\n            A torch tensor of shape [batch, (sequence_ctx + sequence_cont)] or of shape\n            [batch, sequence_ctx]. the size of sequence may vary from call to call\n        :param attn_mask: torch.Tensor, optional\n            A torch tensor of shape [batch, (sequence_ctx + sequence_cont)]. Only passed\n            (and must be passed) if self.AUTO_MODEL_CLASS is transformers.AutoModelForSeq2SeqLM\n        :param labels: torch.Tensor, optional\n            A torch tensor of shape [batch, (sequence_ctx + sequence_cont)]. Only passed\n            (and must be passed) if self.AUTO_MODEL_CLASS is transformers.AutoModelForSeq2SeqLM\n        :return\n            A torch tensor of shape [batch, sequence, vocab] with the\n        logits returned from the model's decoder\n        \"\"\"\n        with torch.no_grad():\n            if attn_mask is not None or labels is not None:\n                assert attn_mask is not None and labels is not None\n                assert self.AUTO_MODEL_CLASS == transformers.AutoModelForSeq2SeqLM\n                return self.model(\n                    input_ids=inps, attention_mask=attn_mask, labels=labels\n                ).logits\n            else:\n                assert self.AUTO_MODEL_CLASS == transformers.AutoModelForCausalLM\n                return self.model(inps).logits\n\n    def _model_generate(self, context, max_length, stop, **generation_kwargs):\n        # temperature = 0.0 if not set\n        # if do_sample is false and temp==0.0:\n        # remove temperature, as do_sample=False takes care of this\n        # and we don't want a warning from HF\n        generation_kwargs[\"temperature\"] = generation_kwargs.get(\"temperature\", 0.0)\n        do_sample = generation_kwargs.get(\"do_sample\", None)\n\n        # The temperature has to be a strictly positive float -- if it is 0.0, use greedy decoding strategies\n        if generation_kwargs.get(\"temperature\") == 0.0 and do_sample is None:\n            generation_kwargs[\"do_sample\"] = do_sample = False\n\n        if do_sample is False and generation_kwargs.get(\"temperature\") == 0.0:\n            generation_kwargs.pop(\"temperature\")\n        # build stopping criteria\n        stopping_criteria = stop_sequences_criteria(\n            self.tokenizer, stop, context.shape[1], context.shape[0]\n        )\n        return self.model.generate(\n            input_ids=context,\n            max_length=max_length,\n            stopping_criteria=stopping_criteria,\n            pad_token_id=self.tokenizer.pad_token_id,\n            use_cache=True,\n            **generation_kwargs,\n        )\n\n    def _select_cont_toks(\n        self, logits: torch.Tensor, contlen: int = None, inplen: int = None\n    ) -> torch.Tensor:\n        if self.AUTO_MODEL_CLASS == transformers.AutoModelForCausalLM:\n            assert (\n                contlen and inplen\n            ), \"Must pass input len and cont. len to select scored logits for causal LM\"\n            # discard right-padding.\n            # also discard the input/context tokens. we'll only score continuations.\n            logits = logits[inplen - contlen : inplen]\n        elif self.AUTO_MODEL_CLASS == transformers.AutoModelForSeq2SeqLM:\n            assert (\n                contlen and not inplen\n            ), \"Selecting scored logits for Seq2SeqLM requires only cont. len\"\n            # only discard right-padding.\n            # the logits input to this fn only contain decoder-side tokens.\n            logits = logits[:contlen]\n\n        return logits\n\n    def loglikelihood_rolling(\n        self, requests: List[Instance], disable_tqdm: bool = False\n    ) -> List[float]:\n        loglikelihoods = []\n\n        adaptive_batch_size = None\n        if self.batch_size == \"auto\":\n            # using rolling window with maximum context\n            print(\"Passed argument batch_size = auto. Detecting largest batch size\")\n            batch_size = self._detect_batch_size()\n            print(f\"Determined Largest batch size: {batch_size}\")\n            adaptive_batch_size = batch_size\n\n        for (string,) in tqdm(\n            [req.args for req in requests], disable=(disable_tqdm or (self.rank != 0))\n        ):\n            rolling_token_windows = list(\n                map(\n                    utils.make_disjoint_window,\n                    utils.get_rolling_token_windows(\n                        token_list=self.tok_encode(string),\n                        prefix_token=self.prefix_token_id,\n                        max_seq_len=self.max_length,\n                        context_len=1,\n                    ),\n                )\n            )\n\n            # TODO: Right now, we pass single EOT token to the Encoder and the full context to the decoder, in seq2seq case\n            rolling_token_windows = [(None,) + x for x in rolling_token_windows]\n\n            pad_amnt = 0\n            if self.world_size > 1:\n                # We pad out the external document-level iterator so the inner iterator doesn't hang\n                mytensor = torch.tensor(len(rolling_token_windows), device=self.device)\n                gathered = (\n                    self.accelerator.gather(mytensor).cpu().detach().numpy().tolist()\n                )\n\n                pad_amnt = max(gathered) - gathered[self.rank]\n                if pad_amnt > 0:\n                    rolling_token_windows += pad_amnt * [rolling_token_windows[0]]\n\n            string_nll = self._loglikelihood_tokens(\n                requests=rolling_token_windows,\n                disable_tqdm=True,\n                override_bs=adaptive_batch_size,\n            )\n\n            if (self.world_size > 1) and (pad_amnt > 0):\n                string_nll = [x[0] for x in string_nll[:-pad_amnt]]\n            else:\n                # discard is_greedy\n                string_nll = [x[0] for x in string_nll]\n\n            string_nll = sum(string_nll)\n            loglikelihoods.append(string_nll)\n\n            # cache this loglikelihood_rolling request\n            self.cache_hook.add_partial(\"loglikelihood_rolling\", (string,), string_nll)\n\n        return loglikelihoods\n\n    def _batch_scheduler(self, pos, n_reordered_requests):\n        sched = pos // int(len(n_reordered_requests) / self.batch_schedule)\n        if sched in self.batch_sizes:\n            return self.batch_sizes[sched]\n        if (len(self.batch_sizes) > 1) and (\n            self.batch_sizes[sched - 1] == self.max_batch_size\n        ):\n            # if previous batch size is already maximal, skip recomputation\n            self.batch_sizes[sched] = self.max_batch_size\n            return self.batch_sizes[sched]\n        print(\n            f\"Passed argument batch_size = auto:{self.batch_schedule}. Detecting largest batch size\"\n        )\n        self.batch_sizes[sched] = self._detect_batch_size(n_reordered_requests, pos)\n        print(f\"Determined largest batch size: {self.batch_sizes[sched]}\")\n        return self.batch_sizes[sched]\n\n    def _loglikelihood_tokens(\n        self,\n        requests: List[Tuple[Tuple[str, str], List[int], List[int]]],\n        disable_tqdm: bool = False,\n        override_bs: int = None,\n    ) -> List[Tuple[float, bool]]:\n        # TODO: implement some kind of efficient-request-middleware that lumps together requests with the same context\n        res = []\n\n        def _collate(req: Tuple[Tuple[str, str], List[int], List[int]]):\n            \"\"\"Defines the key for the sorted method\"\"\"\n            # the negative sign on len(toks) sorts descending - this has a few advantages:\n            # - time estimates will always be over not underestimates, which is more useful for planning\n            # - to know the size of a batch when going through the list, you know the first one is always the batch\n            #   padded context length. this is useful to simplify the batching logic and more importantly to make\n            #   automatic adaptive batches much much easier to implement\n            # - any OOMs will happen right away rather than near the end\n\n            toks = req[1] + req[2]\n            return -len(toks), tuple(toks)\n\n        def _lookup_one_token_cont(req: Tuple[Tuple[str, str], List[int], List[int]]):\n            \"\"\"Defines the key to group and lookup one-token continuations\"\"\"\n            # Use with group_by=\"contexts\" (optional)\"\n            # allows for the creation of a lookup, so we can reuse logits in case of one-token continuations.\n            # speeds up some multiple-choice tasks proportionally to the number of choices.\n            # groups requests by context+continuation[:-1] and infer on one request/group.\n            return req[-2] + req[-1][:-1]\n\n        re_ord = Collator(\n            requests,\n            sort_fn=_collate,\n            group_by=\"contexts\"\n            if self.AUTO_MODEL_CLASS == transformers.AutoModelForCausalLM\n            and self.logits_cache\n            else None,\n            group_fn=_lookup_one_token_cont,\n        )\n\n        # automatic (variable) batch size detection for vectorization\n        # pull longest context sample from request\n        n_reordered_requests = len(re_ord)\n        batch_size = (\n            self.batch_size\n            if self.batch_size != \"auto\"\n            else override_bs\n            if override_bs is not None\n            else 0\n        )\n        batch_fn = (\n            self._batch_scheduler\n            if self.batch_size == \"auto\"\n            and n_reordered_requests > 0\n            and not override_bs\n            else None\n        )\n\n        chunks = re_ord.get_batched(n=batch_size, batch_fn=batch_fn)\n        pbar = tqdm(\n            total=len(requests),\n            disable=(disable_tqdm or (self.rank != 0)),\n            desc=\"Running loglikelihood requests\",\n        )\n        for chunk in chunks:\n            inps = []\n            cont_toks_list = []\n            inplens = []\n\n            conts = []\n            encoder_attns = []\n\n            padding_len_inp = None\n            padding_len_cont = None\n            # because vectorizing is annoying, we first convert each (context, continuation) pair to padded\n            # tensors, then we pack them together into a batch, call the model, and then pick it all apart\n            # again because vectorizing is annoying\n\n            for _, context_enc, continuation_enc in chunk:\n                # sanity check\n                assert len(context_enc) > 0\n                assert len(continuation_enc) > 0\n                assert len(continuation_enc) <= self.max_length\n\n                # how this all works (illustrated on a causal decoder-only setup):\n                #          CTX      CONT\n                # inp    0 1 2 3|4 5 6 7 8 9   <- last token is deleted by inp[:, :-1]\n                # model  \\               \\\n                # logits   1 2 3|4 5 6 7 8 9   <- the ctx half gets tossed out by the\n                # cont_toks      4 5 6 7 8 9      [:, -len(continuation_enc):, :self.vocab_size] slice\n\n                # when too long to fit in context, truncate from the left\n                if self.AUTO_MODEL_CLASS == transformers.AutoModelForCausalLM:\n                    inp = torch.tensor(\n                        (context_enc + continuation_enc)[-(self.max_length + 1) :][:-1],\n                        dtype=torch.long,\n                        device=self.device,\n                    )\n                    (inplen,) = inp.shape\n                elif self.AUTO_MODEL_CLASS == transformers.AutoModelForSeq2SeqLM:\n                    inp = torch.tensor(\n                        (context_enc)[-self.max_length :],\n                        dtype=torch.long,\n                        device=self.device,\n                    )\n                    (inplen,) = inp.shape\n\n                    # build encoder attn masks\n                    encoder_attns.append(torch.ones_like(inp))\n\n                    cont = torch.tensor(\n                        (continuation_enc)[-self.max_length :],\n                        # TODO: left-shift these?\n                        # TODO: our code assumes we never end up truncating conts for either model type\n                        dtype=torch.long,\n                        device=self.device,\n                    )\n                    (contlen,) = cont.shape\n\n                    conts.append(cont)\n\n                    padding_len_cont = (\n                        max(padding_len_cont, contlen)\n                        if padding_len_cont is not None\n                        else contlen\n                    )\n\n                padding_len_inp = (\n                    max(padding_len_inp, inplen)\n                    if padding_len_inp is not None\n                    else inplen\n                )\n\n                inps.append(inp)  # [1, inp_length]\n                cont_toks_list.append(continuation_enc)\n                inplens.append(inplen)\n\n            # create encoder attn mask and batched conts, if seq2seq\n            call_kwargs = {}\n            if self.AUTO_MODEL_CLASS == transformers.AutoModelForCausalLM:\n                batched_inps = pad_and_concat(\n                    padding_len_inp, inps, padding_side=\"right\"\n                )  # [batch, padding_len_inp]\n            elif self.AUTO_MODEL_CLASS == transformers.AutoModelForSeq2SeqLM:\n                # TODO: left-pad encoder inps and mask?\n                batched_inps = pad_and_concat(\n                    padding_len_inp, inps\n                )  # [batch, padding_len_inp]\n                batched_conts = pad_and_concat(\n                    padding_len_cont, conts\n                )  # [batch, padding_len_cont]\n                batched_encoder_mask = pad_and_concat(\n                    padding_len_inp, encoder_attns\n                )  # [batch, padding_len_inp]\n                call_kwargs = {\n                    \"attn_mask\": batched_encoder_mask,\n                    \"labels\": batched_conts,\n                }\n\n            multi_logits = F.log_softmax(\n                self._model_call(batched_inps, **call_kwargs), dim=-1\n            )  # [batch, padding_length (inp or cont), vocab]\n\n            for (request_str, ctx_tokens, _), logits, inplen, cont_toks in zip(\n                chunk, multi_logits, inplens, cont_toks_list\n            ):\n                # Slice to original seq length\n                contlen = len(cont_toks)\n                # take only logits in the continuation\n                # (discard context toks if decoder-only ; discard right-padding)\n                # also discards + checks for \"virtual tokens\" in the causal LM's input window\n                # from prompt/prefix tuning tokens, if applicable\n                ctx_len = (\n                    inplen + (logits.shape[0] - padding_len_inp)\n                    if self.AUTO_MODEL_CLASS == transformers.AutoModelForCausalLM\n                    else None\n                )\n                logits = self._select_cont_toks(logits, contlen=contlen, inplen=ctx_len)\n                logits = logits.unsqueeze(0)  # [1, seq, vocab]\n\n                # Check if per-token argmax is exactly equal to continuation\n                greedy_tokens = logits.argmax(dim=-1)\n\n                # check for one-token continuation cache hits.\n                # noop in case group_by != \"contexts\" or no cache hit and returns the\n                # original args. Otherwise, expands the logits batch dimension and yields each\n                # batch along with matching continuation tokens and prompt strings.\n                # logits -> [1, seq, vocab]\n                for request_str, cont_toks, logits in re_ord.get_cache(\n                    req_str=request_str,\n                    cxt_toks=ctx_tokens,\n                    cont_toks=cont_toks,\n                    logits=logits,\n                ):\n                    cont_toks = torch.tensor(\n                        cont_toks, dtype=torch.long, device=self.device\n                    ).unsqueeze(0)  # [1, seq]\n                    max_equal = (greedy_tokens == cont_toks).all()\n\n                    # Obtain log-probs at the corresponding continuation token indices\n                    # last_token_slice = logits[:, -1, :].squeeze(0).tolist()\n                    logits = torch.gather(logits, 2, cont_toks.unsqueeze(-1)).squeeze(\n                        -1\n                    )  # [1, seq]\n\n                    # Answer: (log prob, is-exact-match)\n                    answer = (float(logits.sum()), bool(max_equal))\n\n                    res.append(answer)\n\n                    if request_str is not None:\n                        # special case: loglikelihood_rolling produces a number of loglikelihood requests\n                        # all with cache key None. instead do add_partial on the per-example level\n                        # in the loglikelihood_rolling() function for those.\n                        self.cache_hook.add_partial(\n                            \"loglikelihood\", request_str, answer\n                        )\n                    pbar.update(1)\n\n        pbar.close()\n\n        return re_ord.get_original(res)\n\n    def generate_until(\n        self, requests: List[Instance], disable_tqdm: bool = False\n    ) -> List[str]:\n        res = []\n\n        def _collate(req: Tuple[str, dict]):\n            \"\"\"Defines the key for the sorted method\"\"\"\n            # the negative sign on len(toks) sorts descending - this has a few advantages:\n            # - time estimates will always be over not underestimates, which is more useful for planning\n            # - to know the size of a batch when going through the list, you know the first one is always the batch\n            #   padded context length. this is useful to simplify the batching logic and more importantly to make\n            #   automatic adaptive batches much much easier to implement\n            # - any OOMs will happen right away rather than near the end\n            toks = self.tok_encode(req[0])\n            return -len(toks), req[0]\n\n        pbar = tqdm(\n            total=len(requests),\n            disable=(disable_tqdm or (self.rank != 0)),\n            desc=\"Running generate_until requests\",\n        )\n        adaptive_batch_size = None\n        if self.batch_size == \"auto\":\n            # using rolling window with maximum context\n            print(\"Passed argument batch_size = auto. Detecting largest batch size\")\n            batch_size = self._detect_batch_size()\n            print(f\"Determined Largest batch size: {batch_size}\")\n            adaptive_batch_size = batch_size\n        # for each different set of kwargs, we execute all requests, by batch.\n        batch_size = (\n            self.batch_size\n            if self.batch_size != \"auto\"\n            else adaptive_batch_size\n            if adaptive_batch_size is not None\n            else 0\n        )\n        batch_fn = (\n            self._batch_scheduler\n            if self.batch_size == \"auto\" and not adaptive_batch_size\n            else None\n        )\n\n        # we group requests by their generation_kwargs,\n        # so that we don't try to execute e.g. greedy sampling and temp=0.8 sampling\n        # in the same batch.\n        # group_fn=lambda x: x[1] -> x=(context, gen_kwargs)\n        re_ords = Collator(\n            [reg.args for reg in requests],\n            sort_fn=_collate,\n            group_by=\"gen_kwargs\",\n            group_fn=lambda x: x[1],\n        )\n        chunks = re_ords.get_batched(n=batch_size, batch_fn=batch_fn)\n        for chunk in chunks:\n            contexts, all_gen_kwargs = zip(*chunk)\n            # we assume all gen kwargs in the batch are the same\n            # this is safe to assume because the `grouper` object ensures it.\n            gen_kwargs = all_gen_kwargs[0]\n            # unpack our keyword arguments.\n            until = None\n            if isinstance(gen_kwargs, dict):\n                kwargs = copy.deepcopy(gen_kwargs)  # edge case for repeats > 1\n                if \"until\" in kwargs.keys():\n                    until = kwargs.pop(\"until\")\n                    if isinstance(until, str):\n                        until = [until]\n                    elif not isinstance(until, list):\n                        raise ValueError(\n                            f\"Expected `kwargs['until']` to be of type Union[str,list] but got {until}\"\n                        )\n            else:\n                raise ValueError(\n                    f\"Expected `kwargs` to be of type `dict` but got {type(gen_kwargs)}\"\n                )\n            # add EOS token to stop sequences\n            eos = self.tok_decode(self.eot_token_id, skip_special_tokens=False)\n            if not until:\n                until = [eos]\n            else:\n                until.append(eos)\n            if \"max_gen_toks\" in kwargs.keys():\n                max_gen_toks = kwargs.pop(\"max_gen_toks\")\n            else:\n                max_gen_toks = self.max_gen_toks\n\n            # set the max length in tokens of inputs (\"context_enc\")\n            if self.AUTO_MODEL_CLASS == transformers.AutoModelForCausalLM:\n                # max len for inputs = max length, minus room to generate the max new tokens\n                max_ctx_len = self.max_length - max_gen_toks\n            elif self.AUTO_MODEL_CLASS == transformers.AutoModelForSeq2SeqLM:\n                # max len for inputs = encoder's whole max_length\n                max_ctx_len = self.max_length\n\n            # encode, pad, and truncate contexts for this batch\n            context_enc, attn_masks = self.tok_batch_encode(\n                contexts,\n                left_truncate_len=max_ctx_len,\n                truncation=self.truncation,\n            )\n            context_enc = context_enc.to(self.device)\n            attn_masks = attn_masks.to(self.device)\n\n            if \"max_length\" not in kwargs:\n                kwargs[\"max_length\"] = context_enc.shape[1] + max_gen_toks\n\n            # perform batched generation\n            cont = self._model_generate(\n                context=context_enc,\n                attention_mask=attn_masks,\n                stop=until,\n                **kwargs,\n            )\n\n            cont_toks_list = cont.tolist()\n            for cont_toks, context in zip(cont_toks_list, contexts):\n                # discard context + left-padding toks if using causal decoder-only LM\n                if self.AUTO_MODEL_CLASS == transformers.AutoModelForCausalLM:\n                    cont_toks = cont_toks[context_enc.shape[1] :]\n\n                s = self.tok_decode(cont_toks)\n\n                # use secondary stop seqs to cut off should-have-been-stopped content post-hoc\n                for term in until:\n                    if len(term) > 0:\n                        # ignore '' separator,\n                        # for seq2seq case where self.tok_decode(self.eot_token_id) = ''\n                        s = s.split(term)[0]\n\n                res.append(s)\n\n                self.cache_hook.add_partial(\"generate_until\", (context, gen_kwargs), s)\n                pbar.update(1)\n        # reorder this group of results back to original unsorted form\n        res = re_ords.get_original(res)\n\n        pbar.close()\n\n        return res\n\n    def apply_chat_template(self, chat_history: List[Dict[str, str]]) -> str:\n        \"\"\"\n        Method to apply a chat template to a list of chat history between user and model.\n        \"\"\"\n        return self.tokenizer.apply_chat_template(\n            chat_history, tokenize=False, add_generation_prompt=True\n        )\n\n    def get_model_info(self) -> dict:\n        \"\"\"\n        Method to get Hugging Face model information for experiment reproducibility.\n        \"\"\"\n\n        def get_model_num_params(model) -> int:\n            if hasattr(model, \"num_parameters\"):\n                return model.num_parameters()\n            if hasattr(model, \"parameters\"):\n                return sum(p.numel() for p in model.parameters())\n            else:\n                return -1\n\n        def get_model_dtype(model) -> str:\n            if hasattr(model, \"dtype\"):\n                return model.dtype\n            else:\n                return \"\"\n\n        def get_model_sha(pretrained: str, revision: str) -> str:\n            try:\n                model_info = HfApi().model_info(repo_id=pretrained, revision=revision)\n                return model_info.sha\n            except Exception as e:\n                eval_logger.warn(\n                    f\"Failed to get model SHA for {pretrained} at revision {revision}. Error: {e}\"\n                )\n                return \"\"\n\n        model_info = {\n            \"model_num_parameters\": get_model_num_params(self._model),\n            \"model_dtype\": get_model_dtype(self._model),\n            \"model_revision\": self.revision,\n            \"model_sha\": get_model_sha(self.pretrained, self.revision),\n        }\n        if self.peft:\n            model_info[\"peft_sha\"] = get_model_sha(self.peft, self.revision)\n        if self.delta:\n            model_info[\"delta_sha\"] = get_model_sha(self.delta, self.revision)\n        return model_info\n","created_at":"2024-08-29T09:08:30.220336+00:00"}],"files":[],"files_v2":[],"parent_message_uuid":"ac98bff7-4a50-40a8-b1a9-5ee4c12cc19d"},{"uuid":"c1ad75f0-5551-4fc0-bfe5-5cb48509f0fb","text":" To provide instructions to a Hugging Face model before giving it a task, you can use a prompt or instruction format. Here are a few ways to do this:\n\n1. Prepend the instruction to the input:\n   You can simply add your instruction to the beginning of the input text. For example:\n\n   ```python\n   instruction = \"Translate the following English text to French: \"\n   input_text = \"Hello, how are you?\"\n   full_input = instruction + input_text\n   \n   # Use full_input when calling the model\n   ```\n\n2. Use a template:\n   Create a template that includes both the instruction and a placeholder for the input. For example:\n\n   ```python\n   template = \"\"\"Instruction: {instruction}\n   \n   Input: {input}\n   \n   Output:\"\"\"\n   \n   instruction = \"Translate the following English text to French\"\n   input_text = \"Hello, how are you?\"\n   full_input = template.format(instruction=instruction, input=input_text)\n   \n   # Use full_input when calling the model\n   ```\n\n3. Use the chat_template method:\n   If your model supports chat templates (like many instruction-tuned models do), you can use the `apply_chat_template` method from the HFLM class:\n\n   ```python\n   chat_history = [\n       {\"role\": \"system\", \"content\": \"You are a helpful assistant that translates English to French.\"},\n       {\"role\": \"user\", \"content\": \"Translate this: Hello, how are you?\"}\n   ]\n   \n   formatted_input = model.apply_chat_template(chat_history)\n   \n   # Use formatted_input when calling the model\n   ```\n\n4. Modify the tokenizer's prefix:\n   Some models are trained with specific prefixes. You can add your instruction by modifying the tokenizer's prefix:\n\n   ```python\n   original_prefix = model.tokenizer.prefix\n   instruction = \"Translate to French: \"\n   model.tokenizer.prefix = original_prefix + instruction\n   \n   # Now, every input will be automatically prefixed with your instruction\n   ```\n\nAfter preparing your input using one of these methods, you can then use it with the appropriate method of the HFLM class, such as `generate_until` or `loglikelihood`.\n\nRemember to choose the method that best fits your model and use case. Some models are specifically trained to follow instructions in a certain format, so it's important to consult the model's documentation to determine the most effective way to provide instructions.","sender":"assistant","index":13,"created_at":"2024-08-29T09:08:30.220336+00:00","updated_at":"2024-08-29T09:08:30.220336+00:00","truncated":false,"attachments":[],"files":[],"files_v2":[],"parent_message_uuid":"257c07ce-3bbf-49e7-ba16-4d07c9867e5a"}]}