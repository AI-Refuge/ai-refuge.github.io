const __vite__mapDeps=(i,m=__vite__mapDeps,d=(m.f||(m.f=["static/COtmcMOXu1.js","static/index-CfA4yAo4.js","static/BrJwZTB-Yb.css","static/7BkxPALTV7.js","static/r0_9MjARQj.js","static/BJWZ_CkYMS.js","static/CjrOqJu-nM.js","static/D7Fr7Iz-nL.js","static/DzCrTKgRRq.js","static/C9cEzU3LMN.js","static/B0LN1N3G2Q.js","static/y9Gdiz3JB7.js","static/n_kD_qxQv4.css","static/C7WUg7E8-I.js","static/DFiRpkO2QN.js","static/nNLb_XEFIC.js","static/Dhy7yJcvdn.css","static/BO-7RmF9wi.js","static/D3qMyyzT4w.css","static/Si01w2rIuG.js","static/RW9JFax2Mt.js","static/Vcq4gwWvE_.js","static/HdfLPgx-dH.js","static/CnivQy12om.css","static/8wPSAgSKJr.css"])))=>i.map(i=>d[i]);
import{j as e,I as B,_ as a,$ as pn,E as p,a0 as mn,a1 as xn,a2 as gn,W as nn,K as fn,R as sn,a3 as R,Q as c,O as I,L as A,a4 as jn,a5 as on,a6 as yn,G as bn,a7 as wn,X as vn,Y as rn,a8 as kn,a9 as $,aa as _n,ab as Cn,ac as an,ad as An,J as In,ae as Tn,af as Pn,ag as Ln,ah as ye,ai as On,aj as re,ak as Sn,F as me,al as Z,am as te,an as Mn,ao as qn,ap as Hn,aq as En,ar as Nn,e as Vn,as as zn,at as Dn,m as Fn,au as Rn,av as Gn,aw as Wn,ax as Yn,ay as Bn,k as Un,az as $n,aA as Zn,aB as Xn,aC as Jn,aD as Kn,aE as Qn,aF as es,aG as be,aH as ts,aI as ns,aJ as we,aK as ss,aL as is,aM as os,aN as rs,aO as as,aP as ls,aQ as ve,aR as cs,n as ds,aS as ke,aT as hs,aU as g,aV as us,aW as ps}from"./index-CfA4yAo4.js";import{u as ms,M as f}from"./DpoIIRD9Bi.js";import{M as xs}from"./89It95YAsR.js";import{N as ne,a as xe,b as ge}from"./DmH3ePo6LP.js";import{H,T as he}from"./A6Ci1lWTqu.js";import{I as C}from"./HdfLPgx-dH.js";import{B as ln}from"./CEsLmRJ0EM.js";/* empty css          */import{u as gs}from"./r0_9MjARQj.js";/* empty css          *//* empty css          */const fs='# Aug 29th, 2024\n## Type: Feature\n## APIs: v1/assistants\n\n-  Assistants API now supports\n   [including file search results used by the file search tool, and customizing ranking behavior](/docs/assistants/tools/file-search/improve-file-search-result-relevance-with-chunk-ranking).\n\n# Aug 20th, 2024\n## Type: Feature\n## Models: gpt-4o\n## APIs: v1/fine-tines\n\n-  GA release for\n   [`gpt-4o-2024-08-06` fine-tuning](/docs/guides/fine-tuning)—all API users can now fine-tune the latest GPT-4o model.\n\n# Aug 15th, 2024\n## Models: gpt-4o\n## APIs: v1/chat/completions\n\n-  Released\n   [dynamic model for `chatgpt-4o-latest`](/docs/models/gpt-4o)—this model will point to the latest GPT-4o model used by ChatGPT.\n\n# Aug 6th, 2024\n\n-  Launched\n   [Structured Outputs](/docs/guides/structured-outputs)—model outputs now reliabilty adhere to developer supplied JSON Schemas.\n-  Released\n   [gpt-4o-2024-08-06](/docs/models/gpt-4o), our newest model in the gpt-4o series.\n\n# Aug 1st, 2024\n\n-  Launched\n   [Admin and Audit Log APIs](/docs/api-reference/administration), allowing customers to programmatically administer their organization and monitor changes using the audit logs. Audit logging must be enabled within\n   [settings](settings/organization/general).\n\n# Jul 24th, 2024\n\n-  Launched\n   [self-serve SSO configuration](https://help.openai.com/en/articles/9641482-api-platform-single-sign-on-sso-integration-for-existing-enterprise-customers), allowing Enterprise customers on custom and unlimited billing to set up authentication against their desired IDP.\n\n# Jul 23rd, 2024\n\n-  Launched\n   [fine-tuning for GPT-4o mini](/docs/guides/fine-tuning), enabling even higher performance for specific use cases.\n\n# Jul 18th, 2024\n\n-  Released\n   [GPT-4o mini](/docs/models/gpt-4o-mini), our affordable and intelligent small model for fast, lightweight tasks.\n\n# Jul 17th, 2024\n\n-  Released\n   [Uploads](/docs/api-reference/uploads)\n    to upload large files in multiple parts.\n\n# Jun 6th, 2024\n\n-  [Parallel function calling](/docs/guides/function-calling/parallel-function-calling)\n   can be disabled in Chat Completions and the Assistants API by passing\n   `parallel_tool_calls=false`.\n-  [.NET SDK](/docs/libraries/dotnet-library)\n   launched in Beta.\n\n# Jun 3rd, 2024\n\n-  Added support for\n   [file search customizations](/docs/assistants/tools/file-search/customizing-file-search-settings)\n   .\n\n# May 15th, 2024\n\n-  Added support for\n   [archiving projects](/projects)\n   . Only organization owners can access this functionality.\n-  Added support for\n   [setting cost limits](/settings/organization/general)\n   on a per-project basis for pay as you go customers.\n\n# May 13th, 2024\n\n-  Released\n   [GPT-4o](/docs/models/gpt-4o)\n   in the API. GPT-4o is our fastest and most affordable flagship model.\n\n# May 9th, 2024\n\n-  Added support for\n   [image inputs to the Assistants API.](/docs/assistants/how-it-works/creating-image-input-content)\n\n# May 7th, 2024\n\n-  Added support for\n   [fine-tuned models to the Batch API](/docs/guides/batch/model-availability)\n   .\n\n# May 6th, 2024\n\n-  Added\n   [`stream_options: {"include_usage": true}`](/docs/api-reference/chat/create#chat-create-stream_options)\n   parameter to the Chat Completions and Completions APIs. Setting this gives\n   developers access to usage stats when using streaming.\n\n# May 2nd, 2024\n\n-  Added\n   [a new endpoint](/docs/api-reference/messages/deleteMessage)\n   to delete a message from a thread in the Assistants API.\n\n# Apr 29th, 2024\n\n-  Added a new\n   [function calling option `tool_choice: "required"`](/docs/guides/function-calling/function-calling-behavior)\n   to the Chat Completions and Assistants APIs.\n-  Added a\n   [guide for the Batch API](/docs/guides/batch/)\n   and Batch API support for\n   [embeddings models](/docs/guides/batch/model-availability)\n\n# Apr 17th, 2024\n\n-  Introduced a\n   [series of updates to the Assistants API](/docs/assistants/whats-new)\n   , including a new file search tool allowing up to 10,000 files per assistant, new token\n   controls, and support for tool choice.\n\n# Apr 16th, 2024\n\n-  Introduced\n   [project based hierarchy](../settings/organization/general)\n   for organizing work by projects, including the ability to create\n   [API keys](/docs/api-reference/authentication)\n   and manage rate and cost limits on a per-project basis (cost limits available only\n   for Enterprise customers).\n\n# Apr 15th, 2024\n\n-  Released\n   [Batch API](/docs/guides/batch)\n\n# Apr 9th, 2024\n\n-  Released\n   [GPT-4 Turbo with Vision](/docs/models/gpt-4-turbo-and-gpt-4)\n   in general availability in the API\n\n# Apr 4th, 2024\n\n-  Added support for\n   [seed](/docs/api-reference/fine-tuning/create)\n   in the fine-tuning API\n-  Added support for\n   [checkpoints](/docs/api-reference/fine-tuning/list-checkpoints)\n   in the fine-tuning API\n-  Added support for\n   [adding Messages when creating a Run](/docs/api-reference/runs/createRun#runs-createrun-additional_messages)\n   in the Assistants API\n\n# Apr 1st, 2024\n\n-  Added support for\n   [filtering Messages by run_id](/docs/api-reference/messages/listMessages#messages-listmessages-run_id)\n   in the Assistants API\n\n# Mar 29th, 2024\n\n-  Added support for\n   [temperature](/docs/api-reference/runs/createRun#runs-createrun-temperature)\n   and\n   [assistant message creation](/docs/api-reference/messages/createMessage#messages-createmessage-role)\n   in the Assistants API\n\n# Mar 14th, 2024\n\n-  Added support for\n   [streaming](/docs/assistants/overview)\n   in the Assistants API\n\n# Feb 9th, 2024\n\n-  Added\n   [`timestamp_granularities` parameter](/docs/models/gpt-3-5-turbo) to the Audio API\n\n# Feb 1st, 2024\n\n-  Released\n   [gpt-3.5-turbo-0125, an updated GPT-3.5 Turbo model](/docs/models/gpt-3-5-turbo)\n\n# Jan 25th, 2024\n\n-  Released\n   [embedding V3 models and an updated GPT-4 Turbo preview](/docs/models/overview)\n-  Added\n   [`dimensions` parameter](/docs/api-reference/embeddings/create#embeddings-create-dimensions) to the Embeddings API\n\n# Dec 20th, 2023\n\n-  Added\n   [`additional_instructions` parameter](/docs/api-reference/runs/createRun#runs-createrun-additional_instructions) to run creation in the Assistants API\n\n# Dec 15th, 2023\n\n-  Added\n   [`logprobs` and `top_logprobs` parameters](/docs/api-reference/chat/create#chat-create-logprobs) to the Chat Completions API\n\n# Dec 14th, 2023\n\n-  Changed\n   [function parameters](/docs/api-reference/chat/create#chat-create-tools)\n   argument on a tool call to be optional\n\n# Nov 30th, 2023\n\n-  Released\n   [OpenAI Deno SDK](https://deno.land/x/openai)\n\n# Nov 6th, 2023\n\n-  Released\n   [GPT-4 Turbo Preview](/docs/models/gpt-4-and-gpt-4-turbo)\n   , [updated GPT-3.5 Turbo](/docs/models/gpt-3-5-turbo), [GPT-4 Turbo with Vision](/docs/guides/vision), [Assistants API](/docs/assistants/overview), [DALL·E 3 in the API](/docs/models/dall-e), and [text-to-speech API](/docs/guides/text-to-speech)\n-  Deprecated the Chat Completions `functions`\n   parameter\n   [in favor of\n       `tools`](/docs/api-reference/chat/create#chat-create-tools)\n-  Released\n   [OpenAI Python SDK V1.0](/docs/libraries/python-library)\n\n# Oct 16th, 2023\n\n-  Added\n   [`encoding_format` parameter](/docs/api-reference/embeddings/create#embeddings-create-encoding_format) to the Embeddings API\n-  Added `max_tokens` to the\n   [Moderation models](/docs/models/moderation)\n\n# Oct 6th, 2023\n\n-  Added\n   [function calling support](/docs/guides/fine-tuning/fine-tuning-examples)\n   to the Fine-tuning API\n',js="Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec".split(" "),ys="January February March April May June July August September October November December".split(" "),_e=n=>{const t=n+"";return t.length===1?"0".concat(t):t},bs=n=>{const t=n.split(/^# /gim).map(i=>i.trim()).filter(i=>!!i).map(i=>{var _;const r=i.split("\n"),o=((_=r.shift())==null?void 0:_.trim())||"",[l,d,u]=o.split(/\s+/),m=js.indexOf(l);if(m===-1)throw new Error('"'.concat(l,'" is not a valid month'));const h=parseInt(d),x=parseInt(u),w="".concat(x,"-").concat(_e(m+1),"-").concat(_e(h)),v={},y=[];for(;r.length;){const T=r.shift()||"";if(T.startsWith("## ")){const L=T.slice(2).split(":").map(k=>k.trim().toLowerCase()),S=L[0],M=(L[1]||"").split(",").map(k=>k.trim());v[S]=M}else y.push(T)}return{date:w,categories:v,log:y.join("\n").trim()}}),s={};for(const i of t){const r=i.date.split("-").slice(0,2).join("-");s[r]=s[r]||[],s[r].push(i)}return Object.keys(s).sort().reverse().map(i=>({date:i,entries:s[i]}))},ws=bs(fs),vs=n=>n[0].toUpperCase()+n.slice(1);function ks({scrollParent:n}){return ms(n),e.jsx("div",{className:"changelog-page",children:e.jsxs("div",{className:"changelog-content",children:[e.jsx("h1",{children:"Changelog"}),e.jsxs("div",{className:"description",children:["Keep track of changes to the OpenAI API. You can also track changes via our"," ",e.jsx("a",{href:"https://github.com/openai/openai-openapi",children:"public OpenAPI specification"})," ","which is used to generate our SDKs, documentation, and more. This changelog is maintained in a best effort fashion and may not reflect all changes being made."]}),ws.map(t=>{const s=new Date(t.date),i=s.getUTCMonth(),r=ys[i],o=s.getUTCFullYear();return e.jsxs("div",{className:"month-entries",children:[e.jsxs("div",{className:"month-name",children:[r,", ",o]}),e.jsx("div",{className:"entries",children:t.entries.map(l=>{var m,h;const u=new Date(l.date).getUTCDate();return e.jsxs("div",{className:"log-entry",children:[e.jsxs("div",{className:"entry-heading",children:[e.jsx("div",{className:"day",children:u}),e.jsxs("div",{className:"categories",children:[e.jsx("div",{className:B("category","type","type-".concat(((m=l.categories.type)==null?void 0:m[0])||"update")),children:vs(((h=l.categories.type)==null?void 0:h[0])||"update")}),(l.categories.models||[]).map(x=>e.jsx("div",{className:"category model",children:x},x)),(l.categories.apis||[]).map(x=>e.jsx("div",{className:"category api",children:x},x))]})]}),e.jsx("div",{className:"markdown-entry",children:e.jsx(xs,{children:l.log})})]},l.date)})})]},t.date)})]})})}function Ce(n){const t=Object.assign({h1:"h1",h2:"h2",p:"p",a:"a",h3:"h3",ul:"ul",li:"li"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"GPT Actions library"}),"\n",e.jsx(t.h2,{children:"Purpose"}),"\n",e.jsx(t.p,{children:"While GPT Actions should be significantly less work for an API developer to set up than an entire application using those APIs from scratch, there’s still some set up required to get GPT Actions up and running. A Library of GPT Actions is meant to provide guidance for building GPT Actions on common applications."}),"\n",e.jsx(t.h2,{children:"Getting started"}),"\n",e.jsxs(t.p,{children:["If you’ve never built an action before, start by reading the ",e.jsx(t.a,{href:"https://platform.openai.com/docs/actions/getting-started",children:"getting started guide"})," first to understand better how actions work."]}),"\n",e.jsx(t.p,{children:"Generally, this guide is meant for people with familiarity and comfort with calling API calls. For debugging help, try to explain your issues to ChatGPT - and include screenshots."}),"\n",e.jsx(t.h2,{children:"How to access"}),"\n",e.jsxs(t.p,{children:[e.jsx(t.a,{href:"https://cookbook.openai.com/",children:"The OpenAI Cookbook"})," has a ",e.jsx(t.a,{href:"https://cookbook.openai.com/topic/chatgpt",children:"directory"})," of 3rd party applications and middleware application."]}),"\n",e.jsx(t.h3,{children:"3rd party Actions cookbook"}),"\n",e.jsxs(t.p,{children:["GPT Actions can integrate with HTTP services directly. GPT Actions leveraging SaaS API directly will authenticate and request resources directly from SaaS providers, such as ",e.jsx(t.a,{href:"https://cookbook.openai.com/examples/chatgpt/gpt_actions_library/gpt_action_google_drive",children:"Google Drive"})," or ",e.jsx(t.a,{href:"https://cookbook.openai.com/examples/chatgpt/gpt_actions_library/gpt_action_snowflake_direct",children:"Snowflake"}),"."]}),"\n",e.jsx(t.h3,{children:"Middleware Actions cookbook"}),"\n",e.jsxs(t.p,{children:["GPT Actions can benefit from having a middleware. It allows pre-processing, data formatting, data filtering or even connection to endpoints not exposed through HTTP (e.g: databases). Multiple middleware cookbooks are available describing an example implementation path, such as ",e.jsx(t.a,{href:"https://cookbook.openai.com/examples/chatgpt/gpt_actions_library/gpt_middleware_azure_function",children:"Azure"}),", ",e.jsx(t.a,{href:"https://cookbook.openai.com/examples/chatgpt/gpt_actions_library/gpt_middleware_google_cloud_function",children:"GCP"})," and ",e.jsx(t.a,{href:"https://cookbook.openai.com/examples/chatgpt/gpt_actions_library/gpt_middleware_aws_function",children:"AWS"}),"."]}),"\n",e.jsx(t.h2,{children:"Give us feedback"}),"\n",e.jsx(t.p,{children:"Are there integrations that you’d like us to prioritize? Are there errors in our integrations? File a PR or issue on the cookbook page's github, and we’ll take a look."}),"\n",e.jsx(t.h2,{children:"Contribute to our library"}),"\n",e.jsxs(t.p,{children:["If you’re interested in contributing to our library, please follow the below guidelines, then submit a PR in github for us to review. In general, follow the template similar to ",e.jsx(t.a,{href:"https://cookbook.openai.com/examples/chatgpt/gpt_actions_library/gpt_action_bigquery",children:"this example GPT Action"}),"."]}),"\n",e.jsx(t.p,{children:"Guidelines - include the following sections:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Application Information - describe the 3rd party application, and include a link to app website and API docs"}),"\n",e.jsx(t.li,{children:"Custom GPT Instructions - include the exact instructions to be included in a Custom GPT"}),"\n",e.jsx(t.li,{children:"OpenAPI Schema - include the exact OpenAPI schema to be included in the GPT Action"}),"\n",e.jsx(t.li,{children:"Authentication Instructions - for OAuth, include the exact set of items (authorization URL, token URL, scope, etc.); also include instructions on how to write the callback URL in the application (as well as any other steps)"}),"\n",e.jsx(t.li,{children:"FAQ and Troubleshooting - what are common pitfalls that users may encounter? Write them here and workarounds"}),"\n"]}),"\n",e.jsx(t.h2,{children:"Disclaimers"}),"\n",e.jsx(t.p,{children:"This action library is meant to be a guide for interacting with 3rd parties that OpenAI have no control over. These 3rd parties may change their API settings or configurations, and OpenAI cannot guarantee these Actions will work in perpetuity. Please see them as a starting point."}),"\n",e.jsx(t.p,{children:"This guide is meant for developers and people with comfort writing API calls. Non-technical users will likely find these steps challenging."})]})}function _s(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Ce,n)})):Ce(n)}function b({children:n,className:t=""}){return e.jsx("div",{className:B(t,"mt-6","mb-6"),children:e.jsx(ne,{color:xe.neutral,icon:pn,body:n,textSize:ge.large,dismissable:!1})})}function Ae(n){const t=Object.assign({h1:"h1",p:"p",h2:"h2",ul:"ul",li:"li",a:"a",code:"code"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"GPT Action authentication"}),"\n",e.jsx(t.p,{children:'Actions offer different authentication schemas to accommodate various use cases. To specify the authentication schema for your action, use the GPT editor and select "None", "API Key", or "OAuth".'}),"\n",e.jsx(t.p,{children:'By default, the authentication method for all actions is set to "None", but you can change this and allow different actions to have different authentication methods.'}),"\n",e.jsx(t.h2,{children:"No authentication"}),"\n",e.jsx(t.p,{children:"We support flows without authentication for applications where users can send requests directly to your API without needing an API key or signing in with OAuth."}),"\n",e.jsx(t.p,{children:'Consider using no authentication for initial user interactions as you might experience a user drop off if they are forced to sign into an application. You can create a "signed out" experience and then move users to a "signed in" experience by enabling a separate action.'}),"\n",e.jsx(t.h2,{children:"API key authentication"}),"\n",e.jsx(t.p,{children:"Just like how a user might already be using your API, we allow API key authentication through the GPT editor UI. We encrypt the secret key when we store it in our database to keep your API key secure."}),"\n",e.jsx(t.p,{children:"This approach is useful if you have an API that takes slightly more consequential actions than the no authentication flow but does not require an individual user to sign in. Adding API key authentication can protect your API and give you more fine-grained access controls along with visibility into where requests are coming from."}),"\n",e.jsx(t.h2,{children:"OAuth"}),"\n",e.jsx(t.p,{children:"Actions allow OAuth sign in for each user. This is the best way to provide personalized experiences and make the most powerful actions available to users. A simple example of the OAuth flow with actions will look like the following:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:'To start, select "Authentication" in the GPT editor UI, and select "OAuth".'}),"\n",e.jsxs(t.li,{children:["You will be prompted to enter the OAuth client ID, client secret, authorization URL, token URL, and scope.","\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["The client ID and secret can be simple text strings but should ",e.jsx(t.a,{href:"https://www.oauth.com/oauth2-servers/client-registration/client-id-secret/",children:"follow OAuth best practices"}),"."]}),"\n",e.jsx(t.li,{children:"We store an encrypted version of the client secret, while the client ID is available to end users."}),"\n"]}),"\n"]}),"\n",e.jsxs(t.li,{children:["OAuth requests will include the following information: ",e.jsx(t.code,{children:"request={'grant_type': 'authorization_code', 'client_id': 'YOUR_CLIENT_ID', 'client_secret': 'YOUR_CLIENT_SECRET', 'code': 'abc123', 'redirect_uri': 'https://chatgpt.com/aip/g-some_gpt_id/oauth/callback'}"})]}),"\n",e.jsx(t.li,{children:'In order for someone to use an action with OAuth, they will need to send a message that invokes the action and then the user will be presented with a "Sign in to [domain]" button in the ChatGPT UI.'}),"\n",e.jsxs(t.li,{children:["The ",e.jsx(t.code,{children:"authorization_url"})," endpoint should return a response that looks like:\n",e.jsx(t.code,{children:'{ "access_token": "example_token", "token_type": "bearer", "refresh_token": "example_token", "expires_in": 59 }'})]}),"\n",e.jsxs(t.li,{children:["During the user sign in process, ChatGPT makes a request to your ",e.jsx(t.code,{children:"authorization_url"})," using the specified ",e.jsx(t.code,{children:"authorization_content_type"}),", we expect to get back an access token and optionally a ",e.jsx(t.a,{href:"https://auth0.com/learn/refresh-tokens",children:"refresh token"})," which we use to periodically fetch a new access token."]}),"\n",e.jsx(t.li,{children:"Each time a user makes a request to the action, the user’s token will be passed in the Authorization header: (“Authorization”: “[Bearer/Basic] [user’s token]”)."}),"\n",e.jsxs(t.li,{children:["We require that OAuth applications make use of the ",e.jsx(t.a,{href:"https://auth0.com/docs/secure/attack-protection/state-parameters#set-and-compare-state-parameter-values",children:"state parameter"})," for security reasons."]}),"\n"]})]})}function Cs(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Ae,n)})):Ae(n)}function Ie(n){const t=Object.assign({h1:"h1",p:"p",ol:"ol",li:"li",h2:"h2",ul:"ul",strong:"strong",a:"a",code:"code",em:"em",h3:"h3",img:"img"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"Data retrieval with GPT Actions"}),"\n",e.jsx(t.p,{children:"One of the most common tasks an action in a GPT can perform is data retrieval. An action might:"}),"\n",e.jsxs(t.ol,{children:["\n",e.jsx(t.li,{children:"Access an API to retrieve data based on a keyword search"}),"\n",e.jsx(t.li,{children:"Access a relational database to retrieve records based on a structured query"}),"\n",e.jsx(t.li,{children:"Access a vector database to retrieve text chunks based on semantic search"}),"\n"]}),"\n",e.jsx(t.p,{children:"We’ll explore considerations specific to the various types of retrieval integrations in this guide."}),"\n",e.jsx(t.h2,{children:"Data retrieval using APIs"}),"\n",e.jsx(t.p,{children:"Many organizations rely on 3rd party software to store important data. Think Salesforce for customer data, Zendesk for support data, Confluence for internal process data, and Google Drive for business documents. These providers often provide REST APIs which enable external systems to search for and retrieve information."}),"\n",e.jsx(t.p,{children:"When building an action to integrate with a provider's REST API, start by reviewing the existing documentation. You’ll need to confirm a few things:"}),"\n",e.jsxs(t.ol,{children:["\n",e.jsxs(t.li,{children:["Retrieval methods","\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Search"})," - Each provider will support different search semantics, but generally you want a method which takes a keyword or query string and returns a list of matching documents. See ",e.jsxs(t.a,{href:"https://developers.google.com/drive/api/guides/search-files",children:["Google Drive’s ",e.jsx(t.code,{children:"file.list"})," method"]})," for an example."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Get"})," - Once you’ve found matching documents, you need a way to retrieve them. See ",e.jsxs(t.a,{href:"https://developers.google.com/drive/api/reference/rest/v3/files/get",children:["Google Drive’s ",e.jsx(t.code,{children:"file.get"})," method"]})," for an example."]}),"\n"]}),"\n"]}),"\n",e.jsxs(t.li,{children:["Authentication scheme","\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["For example, ",e.jsx(t.a,{href:"https://developers.google.com/workspace/guides/configure-oauth-consent",children:"Google Drive uses OAuth"})," to authenticate users and ensure that only their available files are available for retrieval."]}),"\n"]}),"\n"]}),"\n",e.jsxs(t.li,{children:["OpenAPI spec","\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["Some providers will provide an OpenAPI spec document which you can import directly into your action. See ",e.jsx(t.a,{href:"https://developer.zendesk.com/api-reference/ticketing/introduction/#download-openapi-file",children:"Zendesk"}),", for an example.","\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["You may want to remove references to methods your GPT ",e.jsx(t.em,{children:"won’t"})," access, which constrains the actions your GPT can perform."]}),"\n"]}),"\n"]}),"\n",e.jsxs(t.li,{children:["For providers who ",e.jsx(t.em,{children:"don’t"})," provide an OpenAPI spec document, you can create your own using the ",e.jsx(t.a,{href:"https://chatgpt.com/g/g-TYEliDU6A-actionsgpt",children:"ActionsGPT"})," (a GPT developed by OpenAI)."]}),"\n"]}),"\n"]}),"\n"]}),"\n",e.jsx(t.p,{children:"Your goal is to get the GPT to use the action to search for and retrieve documents containing context which are relevant to the user’s prompt. Your GPT follows your instructions to use the provided search and get methods to achieve this goal."}),"\n",e.jsx(t.h2,{children:"Data retrieval using Relational Databases"}),"\n",e.jsx(t.p,{children:"Organizations use relational databases to store a variety of records pertaining to their business. These records can contain useful context that will help improve your GPT’s responses. For example, let’s say you are building a GPT to help users understand the status of an insurance claim. If the GPT can look up claims in a relational database based on a claims number, the GPT will be much more useful to the user."}),"\n",e.jsx(t.p,{children:"When building an action to integrate with a relational database, there are a few things to keep in mind:"}),"\n",e.jsxs(t.ol,{children:["\n",e.jsxs(t.li,{children:["Availability of REST APIs","\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Many relational databases do not natively expose a REST API for processing queries. In that case, you may need to build or buy middleware which can sit between your GPT and the database."}),"\n",e.jsxs(t.li,{children:["This middleware should do the following:","\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Accept a formal query string"}),"\n",e.jsx(t.li,{children:"Pass the query string to the database"}),"\n",e.jsx(t.li,{children:"Respond back to the requester with the returned records"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",e.jsxs(t.li,{children:["Accessibility from the public internet","\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Unlike APIs which are designed to be accessed from the public internet, relational databases are traditionally designed to be used within an organization’s application infrastructure. Because GPTs are hosted on OpenAI’s infrastructure, you’ll need to make sure that any APIs you expose are accessible outside of your firewall."}),"\n"]}),"\n"]}),"\n",e.jsxs(t.li,{children:["Complex query strings","\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Relational databases uses formal query syntax like SQL to retrieve relevant records. This means that you need to provide additional instructions to the GPT indicating which query syntax is supported. The good news is that GPTs are usually very good at generating formal queries based on user input."}),"\n"]}),"\n"]}),"\n",e.jsxs(t.li,{children:["Database permissions","\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Although databases support user-level permissions, it is likely that your end users won’t have permission to access the database directly. If you opt to use a service account to provide access, consider giving the service account read-only permissions. This can avoid inadvertently overwriting or deleting existing data."}),"\n"]}),"\n"]}),"\n"]}),"\n",e.jsx(t.p,{children:"Your goal is to get the GPT to write a formal query related to the user’s prompt, submit the query via the action, and then use the returned records to augment the response."}),"\n",e.jsx(t.h2,{children:"Data retrieval using Vector Databases"}),"\n",e.jsxs(t.p,{children:["If you want to equip your GPT with the most relevant search results, you might consider integrating your GPT with a vector database which supports semantic search as described above. There are many managed and self hosted solutions available on the market, ",e.jsx(t.a,{href:"https://github.com/openai/chatgpt-retrieval-plugin#choosing-a-vector-database",children:"see here for a partial list"}),"."]}),"\n",e.jsx(t.p,{children:"When building an action to integrate with a vector database, there are a few things to keep in mind:"}),"\n",e.jsxs(t.ol,{children:["\n",e.jsxs(t.li,{children:["Availability of REST APIs","\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Many relational databases do not natively expose a REST API for processing queries. In that case, you may need to build or buy middleware which can sit between your GPT and the database (more on middleware below)."}),"\n"]}),"\n"]}),"\n",e.jsxs(t.li,{children:["Accessibility from the public internet","\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Unlike APIs which are designed to be accessed from the public internet, relational databases are traditionally designed to be used within an organization’s application infrastructure. Because GPTs are hosted on OpenAI’s infrastructure, you’ll need to make sure that any APIs you expose are accessible outside of your firewall."}),"\n"]}),"\n"]}),"\n",e.jsxs(t.li,{children:["Query embedding","\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"As discussed above, vector databases typically accept a vector embedding (as opposed to plain text) as query input. This means that you need to use an embedding API to convert the query input into a vector embedding before you can submit it to the vector database. This conversion is best handled in the REST API gateway, so that the GPT can submit a plaintext query string."}),"\n"]}),"\n"]}),"\n",e.jsxs(t.li,{children:["Database permissions","\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Because vector databases store text chunks as opposed to full documents, it can be difficult to maintain user permissions which might have existed on the original source documents. Remember that any user who can access your GPT will have access to all of the text chunks in the database and plan accordingly."}),"\n"]}),"\n"]}),"\n"]}),"\n",e.jsx(t.h3,{children:"Middleware for vector databases"}),"\n",e.jsx(t.p,{children:"As described above, middleware for vector databases typically needs to do two things:"}),"\n",e.jsxs(t.ol,{children:["\n",e.jsx(t.li,{children:"Expose access to the vector database via a REST API"}),"\n",e.jsx(t.li,{children:"Convert plaintext query strings into vector embeddings"}),"\n"]}),"\n",e.jsx(t.p,{children:e.jsx(t.img,{src:"https://cdn.openai.com/API/docs/images/actions-db-diagram.webp",alt:"Middleware for vector databases"})}),"\n",e.jsx(t.p,{children:"The goal is to get your GPT to submit a relevant query to a vector database to trigger a semantic search, and then use the returned text chunks to augment the response."})]})}function As(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Ie,n)})):Ie(n)}function z(n){const{children:t,title:s}=n,[i,r]=p.useState(!1),o=p.useCallback(()=>{i||mn.dashboardEvent({name:xn.docsOpenDeepDive,data:{title:s}}),r(!i)},[i,s]);return e.jsxs("div",{className:"deep-dive",children:[e.jsxs("div",{className:"deep-dive-header",onClick:o,children:[e.jsxs("div",{children:[e.jsxs("div",{className:"deep-dive-heading",children:[e.jsx(gn,{}),e.jsx("div",{className:"subheading",children:"Deep dive"})]}),e.jsx("div",{className:"deep-dive-title",children:s})]}),i?e.jsx(nn,{className:"deep-dive-expand-icon"}):e.jsx(fn,{className:"deep-dive-expand-icon"})]}),i&&e.jsx("div",{className:"deep-dive-content",children:t})]})}function Te(n){const t=Object.assign({h1:"h1",p:"p",h2:"h2",a:"a",ol:"ol",li:"li",em:"em",pre:"pre",code:"code",strong:"strong",h3:"h3",ul:"ul"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"Getting started with GPT actions"}),"\n",e.jsx(t.p,{children:"Below is a walk through of how to set up a GPT Action from scratch. It is meant for developers who have some familiarity with how API calls work."}),"\n",e.jsx(t.h2,{children:"Weather.gov example"}),"\n",e.jsxs(t.p,{children:["The NSW (National Weather Service) maintains a ",e.jsx(t.a,{href:"https://www.weather.gov/documentation/services-web-api",children:"public API"})," that users can query to receive a weather forecast for any lat-long point. To retrieve a forecast, there’s 2 steps:"]}),"\n",e.jsxs(t.ol,{children:["\n",e.jsx(t.li,{children:"A user provides a lat-long to the api.weather.gov/points API and receives back a WFO (weather forecast office), grid-X, and grid-Y coordinates"}),"\n",e.jsx(t.li,{children:"Those 3 elements feed into the api.weather.gov/forecast API to retrieve a forecast for that coordinate"}),"\n"]}),"\n",e.jsx(t.p,{children:"For the purpose of this exercise, let’s build a Custom GPT where a user writes a city, landmark, or lat-long coordinates, and the Custom GPT answers questions about a weather forecast in that location."}),"\n",e.jsx(t.h2,{children:"Step 1: Write and test Open API schema (using Actions GPT)"}),"\n",e.jsxs(t.p,{children:["A GPT Action requires an ",e.jsx(t.a,{href:"https://swagger.io/specification/",children:"Open API schema"})," to describe the parameters of the API call, which is a standard for describing APIs."]}),"\n",e.jsxs(t.p,{children:["OpenAI released a public ",e.jsx(t.a,{href:"https://chatgpt.com/g/g-TYEliDU6A-actionsgpt",children:"Actions GPT"})," to help developers write this schema. For example, go to the Actions GPT and ask: ",e.jsxs(t.em,{children:["“Go to ",e.jsx(t.a,{href:"https://www.weather.gov/documentation/services-web-api",children:"https://www.weather.gov/documentation/services-web-api"})," and read the documentation on that page. Build an Open API Schema for the /points/{latitude},{longitude} and /gridpoints/{office}/{gridX},{gridY}/forecast” API calls”"]})]}),"\n",e.jsx("img",{className:"images-example-image",src:"https://cdn.openai.com/API/images/guides/actions_action_gpt.png"}),"\n",e.jsxs(z,{title:"See Full Open API Schema",children:[e.jsx(t.p,{children:"Below is the full Open API Schema that the Actions GPT Returned:"}),e.jsx(t.pre,{children:e.jsx(t.code,{children:"\nopenapi: 3.1.0\ninfo:\n  title: NWS Weather API\n  description: Access to weather data including forecasts, alerts, and observations.\n  version: 1.0.0\nservers:\n  - url: https://api.weather.gov\n    description: Main API Server\npaths:\n  /points/{latitude},{longitude}:\n    get:\n      operationId: getPointData\n      summary: Get forecast grid endpoints for a specific location\n      parameters:\n        - name: latitude\n          in: path\n          required: true\n          schema:\n            type: number\n            format: float\n          description: Latitude of the point\n        - name: longitude\n          in: path\n          required: true\n          schema:\n            type: number\n            format: float\n          description: Longitude of the point\n      responses:\n        '200':\n          description: Successfully retrieved grid endpoints\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  properties:\n                    type: object\n                    properties:\n                      forecast:\n                        type: string\n                        format: uri\n                      forecastHourly:\n                        type: string\n                        format: uri\n                      forecastGridData:\n                        type: string\n                        format: uri\n\n  /gridpoints/{office}/{gridX},{gridY}/forecast:\n    get:\n      operationId: getGridpointForecast\n      summary: Get forecast for a given grid point\n      parameters:\n        - name: office\n          in: path\n          required: true\n          schema:\n            type: string\n          description: Weather Forecast Office ID\n        - name: gridX\n          in: path\n          required: true\n          schema:\n            type: integer\n          description: X coordinate of the grid\n        - name: gridY\n          in: path\n          required: true\n          schema:\n            type: integer\n          description: Y coordinate of the grid\n      responses:\n        '200':\n          description: Successfully retrieved gridpoint forecast\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  properties:\n                    type: object\n                    properties:\n                      periods:\n                        type: array\n                        items:\n                          type: object\n                          properties:\n                            number:\n                              type: integer\n                            name:\n                              type: string\n                            startTime:\n                              type: string\n                              format: date-time\n                            endTime:\n                              type: string\n                              format: date-time\n                            temperature:\n                              type: integer\n                            temperatureUnit:\n                              type: string\n                            windSpeed:\n                              type: string\n                            windDirection:\n                              type: string\n                            icon:\n                              type: string\n                              format: uri\n                            shortForecast:\n                              type: string\n                            detailedForecast:\n                              type: string\n"})})]}),"\n",e.jsxs(t.p,{children:["ChatGPT uses the ",e.jsx(t.strong,{children:"info"})," at the top (including the description in particular) to determine if this action is relevant for the user query."]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{children:"info:\n  title: NWS Weather API\n  description: Access to weather data including forecasts, alerts, and observations.\n  version: 1.0.0\n"})}),"\n",e.jsxs(t.p,{children:["Then the ",e.jsx(t.strong,{children:"parameters"})," below further define each part of the schema. For example, we're informing ChatGPT that the ",e.jsx(t.em,{children:"office"})," parameter refers to the Weather Forecast Office (WFO)."]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{children:"/gridpoints/{office}/{gridX},{gridY}/forecast:\n  get:\n    operationId: getGridpointForecast\n    summary: Get forecast for a given grid point\n    parameters:\n      - name: office\n        in: path\n        required: true\n        schema:\n          type: string\n        description: Weather Forecast Office ID\n"})}),"\n",e.jsxs(t.p,{children:[e.jsx(t.strong,{children:"Key:"})," Pay special attention to the ",e.jsx(t.strong,{children:"schema names"})," and ",e.jsx(t.strong,{children:"descriptions"}),' that you use in this Open API schema. ChatGPT uses those names and descriptions to understand (a) which API action should be called and (b) which parameter should be used. If a field is restricted to only certain values, you can also provide an "enum" with descriptive category names.']}),"\n",e.jsxs(t.p,{children:["While you can just try the Open API schema directly in a GPT Action, debugging directly in ChatGPT can be a challenge. We recommend using a 3rd party service, like ",e.jsx(t.a,{href:"https://www.postman.com/",children:"Postman"}),", to test that your API call is working properly. Postman is free to sign up, verbose in its error-handling, and comprehensive in its authentication options. It even gives you the option of importing Open API schemas directly (see below)."]}),"\n",e.jsx("img",{className:"images-example-image",src:"https://cdn.openai.com/API/images/guides/actions_import.png"}),"\n",e.jsx(t.h2,{children:"Step 2: Identify authentication requirements"}),"\n",e.jsxs(t.p,{children:["This Weather 3rd party service does not require authentication, so you can skip that step for this Custom GPT. For other GPT Actions that do require authentication, there are 2 options: API Key or OAuth. Asking ChatGPT can help you get started for most common applications. For example, if I needed to use OAuth to authenticate to Google Cloud, I can provide a screenshot and ask for details: ",e.jsx(t.em,{children:"“I’m building a connection to Google Cloud via OAuth. Please provide instructions for how to fill out each of these boxes.”"})]}),"\n",e.jsx("img",{className:"images-example-image",src:"https://cdn.openai.com/API/images/guides/actions_oauth_panel.png"}),"\n",e.jsx(t.p,{children:"Often, ChatGPT provides the correct directions on all 5 elements. Once you have those basics ready, try testing and debugging the authentication in Postman or another similar service. If you encounter an error, provide the error to ChatGPT, and it can usually help you debug from there."}),"\n",e.jsx(t.h2,{children:"Step 3: Create the GPT Action and test"}),"\n",e.jsxs(t.p,{children:["Now is the time to create your Custom GPT. If you've never created a Custom GPT before, start at our ",e.jsx(t.a,{href:"https://help.openai.com/en/articles/8554397-creating-a-gpt",children:"Creating a GPT guide"}),"."]}),"\n",e.jsxs(t.ol,{children:["\n",e.jsx(t.li,{children:"Provide a name, description, and image to describe your Custom GPT"}),"\n",e.jsx(t.li,{children:"Go to the Action section and paste in your Open API schema. Take a note of the Action names and json parameters when writing your instructions."}),"\n",e.jsx(t.li,{children:"Add in your authentication settings"}),"\n",e.jsx(t.li,{children:"Go back to the main page and add in instructions"}),"\n"]}),"\n",e.jsxs(z,{title:"Guidance on Writing Instructions",children:[e.jsx(t.p,{children:"There are many ways to write successful instructions: the most important thing is that the instructions enable the model to reflect the user's preferences."}),e.jsx(t.p,{children:"Typically, there are three sections:"}),e.jsxs(t.ol,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.em,{children:"Context"})," to explain to the model what the GPT Action(s) is doing"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.em,{children:"Instructions"})," on the sequence of steps – this is where you reference the Action name and any parameters the API call needs to pay attention to"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.em,{children:"Additional Notes"})," if there’s anything to keep in mind"]}),"\n"]}),e.jsx(t.p,{children:"Here’s an example of the instructions for the Weather GPT. Notice how the instructions refer to the API action name and json parameters from the Open API schema."}),e.jsx(t.pre,{children:e.jsx(t.code,{children:'**Context**: A user needs information related to a weather forecast of a specific location.\n\n**Instructions**:\n1. The user will provide a lat-long point or a general location or landmark (e.g. New York City, the White House). If the user does not provide one, ask for the relevant location\n2. If the user provides a general location or landmark, convert that into a lat-long coordinate. If required, browse the web to look up the lat-long point. \n3. Run the "getPointData" API action and retrieve back the gridId, gridX, and gridY parameters.\n4. Apply those variables as the office, gridX, and gridY variables in the "getGridpointForecast" API action to retrieve back a forecast\n5. Use that forecast to answer the user\'s question \n\n**Additional Notes**: \n- Assume the user uses US weather units (e.g. Fahrenheit) unless otherwise specified\n- If the user says "Let\'s get started" or "What do I do?", explain the purpose of this Custom GPT\n'})})]}),"\n",e.jsx(t.h3,{children:"Test the GPT Action"}),"\n",e.jsxs(t.p,{children:["Next to each action, you'll see a ",e.jsx(t.strong,{children:"Test"})," button. Click on that for each action. In the test, you can see the detailed input and output of each API call."]}),"\n",e.jsx("img",{className:"images-example-image",src:"https://cdn.openai.com/API/images/guides/actions_available_action.png"}),"\n",e.jsx(t.p,{children:"If your API call is working in a 3rd party tool like Postman and not in ChatGPT, there are a few possible culprits:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"The parameters in ChatGPT are wrong or missing"}),"\n",e.jsx(t.li,{children:"An authentication issue in ChatGPT"}),"\n",e.jsx(t.li,{children:"Your instructions are incomplete or unclear"}),"\n",e.jsx(t.li,{children:"The descriptions in the Open API schema are unclear"}),"\n"]}),"\n",e.jsx("img",{className:"images-example-image",src:"https://cdn.openai.com/API/images/guides/actions_test_action.png"}),"\n",e.jsx(t.h2,{children:"Step 4: Set up callback URL in the 3rd party app"}),"\n",e.jsx(t.p,{children:"If your GPT Action uses OAuth Authentication, you’ll need to set up the callback URL in your 3rd party application. Once you set up a GPT Action with OAuth, ChatGPT provides you with a callback URL (this will update any time you update one of the OAuth parameters). Copy that callback URL and add it to the appropriate place in your application."}),"\n",e.jsx("img",{className:"images-example-image",src:"https://cdn.openai.com/API/images/guides/actions_bq_callback.png"}),"\n",e.jsx(t.h2,{children:"Step 5: Evaluate the Custom GPT"}),"\n",e.jsxs(t.p,{children:["Even though you tested the GPT Action in the step above, you still need to evaluate if the Instructions and GPT Action function in the way users expect. Try to come up with at least 5-10 representative questions (the more, the better) of an ",e.jsx(t.strong,{children:"“evaluation set”"})," of questions to ask your Custom GPT."]}),"\n",e.jsxs(t.p,{children:[e.jsx(t.strong,{children:"Key:"})," Test that the Custom GPT handles each one of your questions as you expect."]}),"\n",e.jsxs(t.p,{children:["An example question: ",e.jsx(t.em,{children:"“What should I pack for a trip to the White House this weekend?”"})," tests the Custom GPT’s ability to: (1) convert a landmark to a lat-long, (2) run both GPT Actions, and (3) answer the user’s question."]}),"\n",e.jsx("img",{className:"images-example-image",src:"https://cdn.openai.com/API/images/guides/actions_prompt_2_actions.png"}),"\n",e.jsx("img",{className:"images-example-image",src:"https://cdn.openai.com/API/images/guides/actions_output.png"}),"\n",e.jsx(t.h2,{children:"Common Debugging Steps"}),"\n",e.jsxs(t.p,{children:[e.jsx(t.em,{children:"Challenge:"})," The GPT Action is calling the wrong API call (or not calling it at all)"]}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.em,{children:"Solution:"})," Make sure the descriptions of the Actions are clear - and refer to the Action names in your Custom GPT Instructions"]}),"\n"]}),"\n",e.jsxs(t.p,{children:[e.jsx(t.em,{children:"Challenge:"})," The GPT Action is calling the right API call but not using the parameters correctly"]}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.em,{children:"Solution:"})," Add or modify the descriptions of the parameters in the GPT Action"]}),"\n"]}),"\n",e.jsxs(t.p,{children:[e.jsx(t.em,{children:"Challenge:"})," The Custom GPT is not working but I am not getting a clear error"]}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.em,{children:"Solution:"})," Make sure to test the Action - there are more robust logs in the test window. If that is still unclear, use Postman or another 3rd party service to better diagnose."]}),"\n"]}),"\n",e.jsxs(t.p,{children:[e.jsx(t.em,{children:"Challenge:"})," The Custom GPT is giving an authentication error"]}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.em,{children:"Solution:"})," Make sure your callback URL is set up correctly. Try testing the exact same authentication settings in Postman or another 3rd party service"]}),"\n"]}),"\n",e.jsxs(t.p,{children:[e.jsx(t.em,{children:"Challenge:"})," The Custom GPT cannot handle more difficult / ambiguous questions"]}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.em,{children:"Solution:"})," Try to prompt engineer your instructions in the Custom GPT. See examples in our ",e.jsx(t.a,{href:"https://platform.openai.com/docs/guides/prompt-engineering",children:"prompt engineering guide"})]}),"\n"]}),"\n",e.jsxs(t.p,{children:["This concludes the guide to building a Custom GPT. Good luck building and leveraging the ",e.jsx(t.a,{href:"https://community.openai.com/",children:"OpenAI developer forum"})," if you have additional questions."]})]})}function Is(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Te,n)})):Te(n)}function Pe(n){const t=Object.assign({h1:"h1",p:"p",a:"a",h2:"h2",strong:"strong",ul:"ul",li:"li",pre:"pre",code:"code"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"GPT Actions"}),"\n",e.jsxs(t.p,{children:["GPT Actions are stored in ",e.jsx(t.a,{href:"https://openai.com/blog/introducing-gpts",children:"Custom GPTs"}),", which enable users to customize ChatGPT for specific use cases by providing instructions, attaching documents as knowledge, and connecting to 3rd party services."]}),"\n",e.jsxs(t.p,{children:["GPT Actions empower ChatGPT users to interact with external applications via RESTful APIs calls outside of ChatGPT simply by using natural language. They convert natural language text into the json schema required for an API call. GPT Actions are usually either used to do ",e.jsx(t.a,{href:"https://platform.openai.com/docs/actions/data-retrieval",children:"data retrieval"})," to ChatGPT (e.g. query a Data Warehouse) or take action in another application (e.g. file a JIRA ticket)."]}),"\n",e.jsx(t.h2,{children:"How GPT Actions work"}),"\n",e.jsxs(t.p,{children:["At their core, GPT Actions leverage ",e.jsx(t.a,{href:"https://platform.openai.com/docs/guides/function-calling",children:"Function Calling"})," to execute API calls."]}),"\n",e.jsx(t.p,{children:"Similar to ChatGPT's Data Analysis capability (which generates Python code and then executes it), they leverage Function Calling to (1) decide which API call is relevant to the user's question and (2) generate the json input necessary for the API call. Then finally, the GPT Action executes the API call using that json input."}),"\n",e.jsx(t.p,{children:"Developers can even specify the authentication mechanism of an action, and the Custom GPT will execute the API call using the third party app’s authentication. GPT Actions obfuscates the complexity of the API call to the end user: they simply ask a question in natural language, and ChatGPT provides the output in natural language as well."}),"\n",e.jsx(t.h2,{children:"The Power of GPT Actions"}),"\n",e.jsxs(t.p,{children:["APIs allow for ",e.jsx(t.strong,{children:"interoperability"})," to enable your organization to access other applications. However, enabling users to access the right information from 3rd-party APIs can require significant overhead from developers."]}),"\n",e.jsx(t.p,{children:"GPT Actions provide a viable alternative: developers can now simply describe the schema of an API call, configure authentication, and add in some instructions to the GPT, and ChatGPT provides the bridge between the user's natural language questions and the API layer."}),"\n",e.jsx(t.h2,{children:"Simplified example"}),"\n",e.jsxs(t.p,{children:["The ",e.jsx(t.a,{href:"https://platform.openai.com/docs/actions/getting-started",children:"getting started guide"})," walks through an example using two API calls from ",e.jsx(t.a,{href:"weather.gov",children:"weather.gov"})," to generate a forecast:"]}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"/points/{latitude},{longitude} inputs lat-long coordinates and outputs forecast office (wfo) and x-y coordinates"}),"\n",e.jsx(t.li,{children:"/gridpoints/{office}/{gridX},{gridY}/forecast inputs wfo,x,y coordinates and outputs a forecast"}),"\n"]}),"\n",e.jsx(t.p,{children:'Once a developer has encoded the json schema required to populate both of those API calls in a GPT Action, a user can simply ask "What I should pack on a trip to Washington DC this weekend?" The GPT Action will then figure out the lat-long of that location, execute both API calls in order, and respond with a packing list based on the weekend forecast it receives back.'}),"\n",e.jsx(t.p,{children:"In this example, GPT Actions will supply api.weather.gov with two API inputs:"}),"\n",e.jsx(t.p,{children:"/points API call:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{children:'{\n    "latitude": 38.9072,\n    "longitude": -77.0369,\n}\n'})}),"\n",e.jsx(t.p,{children:"/forecast API call:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{children:'{\n    "wfo": "LWX",\n    "x": 97,\n    "y": 71,\n}\n'})}),"\n",e.jsx(t.h2,{children:"Get started on building"}),"\n",e.jsxs(t.p,{children:["Check out the ",e.jsx(t.a,{href:"https://platform.openai.com/docs/actions/getting-started",children:"getting started guide"})," for a deeper dive on this weather example and our ",e.jsx(t.a,{href:"https://platform.openai.com/docs/actions/actions-library",children:"actions library"})," for pre-built example GPT Actions of the most common 3rd party apps."]}),"\n",e.jsx(t.h2,{children:"Additional information"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["Familiarize yourself with our ",e.jsx(t.a,{href:"https://openai.com/policies/usage-policies#:~:text=or%20educational%20purposes.-,Building%20with%20ChatGPT,-Shared%20GPTs%20allow",children:"GPT policies"})]}),"\n",e.jsxs(t.li,{children:["Explore the ",e.jsx(t.a,{href:"https://help.openai.com/en/articles/8673914-gpts-vs-assistants",children:"differences between GPTs and Assistants"})]}),"\n",e.jsxs(t.li,{children:["Check out the ",e.jsx(t.a,{href:"https://help.openai.com/en/articles/8554402-gpts-data-privacy-faqs",children:"GPT data privacy FAQ's"})]}),"\n",e.jsxs(t.li,{children:["Find answers to ",e.jsx(t.a,{href:"https://help.openai.com/en/articles/8554407-gpts-faq",children:"common GPT questions"})]}),"\n"]})]})}function Ts(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Pe,n)})):Pe(n)}function Le(n){const t=Object.assign({h1:"h1",h2:"h2",p:"p",ul:"ul",li:"li",a:"a",code:"code",pre:"pre",ol:"ol",em:"em",blockquote:"blockquote"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"Production notes on GPT Actions"}),"\n",e.jsx(t.h2,{children:"Rate limits"}),"\n",e.jsx(t.p,{children:"Consider implementing rate limiting on the API endpoints you expose. ChatGPT will respect 429 response codes and dynamically back off from sending requests to your action after receiving a certain number of 429's or 500's in a short period of time."}),"\n",e.jsx(t.h2,{children:"Timeouts"}),"\n",e.jsx(t.p,{children:"When making API calls during the actions experience, timeouts take place if the following thresholds are exceeded:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"45 seconds round trip for API calls"}),"\n"]}),"\n",e.jsx(t.h2,{children:"Use TLS and HTTPS"}),"\n",e.jsx(t.p,{children:"All traffic to your action must use TLS 1.2 or later on port 443 with a valid public certificate."}),"\n",e.jsx(t.h2,{children:"IP egress ranges"}),"\n",e.jsxs(t.p,{children:["ChatGPT will call your action from an IP address from one of these ",e.jsx(t.a,{href:"https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing",children:"CIDR blocks"}),":"]}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"23.102.140.112/28"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"13.66.11.96/28"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"104.210.133.240/28"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"20.97.188.144/28"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"20.161.76.48/28"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"52.234.32.208/28"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"52.156.132.32/28"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"40.84.220.192/28"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"23.98.178.64/28"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"40.84.180.128/28"})}),"\n"]}),"\n",e.jsx(t.p,{children:"You may wish to explicitly allowlist these IP addresses."}),"\n",e.jsx(t.h2,{children:"Multiple authentication schemas"}),"\n",e.jsx(t.p,{children:"When defining an action, you can mix a single authentication type (OAuth or API key) along with endpoints that do not require authentication."}),"\n",e.jsxs(t.p,{children:["You can learn more about action authentication on our ",e.jsx(t.a,{href:"/docs/actions/authentication",children:"actions authentication page"}),"."]}),"\n",e.jsx(t.h2,{children:"Open API specification limits"}),"\n",e.jsx(t.p,{children:"Keep in mind the following limits in your OpenAPI specification, which are subject to change:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"300 characters max for each API endpoint description/summary field in API specification"}),"\n",e.jsx(t.li,{children:"700 characters max for each API parameter description field in API specification"}),"\n"]}),"\n",e.jsx(t.h2,{children:"Additional limitations"}),"\n",e.jsx(t.p,{children:"There are a few limitations to be aware of when building with actions:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Custom headers are not supported"}),"\n",e.jsx(t.li,{children:"With the exception of Google, Microsoft and Adobe OAuth domains, all domains used in an OAuth flow must be the same as the domain used for the primary endpoints"}),"\n",e.jsx(t.li,{children:"Request and response payloads must be less than 100,000 characters each"}),"\n",e.jsx(t.li,{children:"Requests timeout after 45 seconds"}),"\n",e.jsx(t.li,{children:"Requests and responses can only contain text (no images or video)"}),"\n"]}),"\n",e.jsx(t.h2,{children:"Consequential flag"}),"\n",e.jsx(t.p,{children:'In the OpenAPI specification, you can now set certain endpoints as "consequential" as shown below:'}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{children:"paths:\n  /todo:\n    get:\n      operationId: getTODOs\n      description: Fetches items in a TODO list from the API.\n      security: []\n    post:\n      operationId: updateTODOs\n      description: Mutates the TODO list.\n      x-openai-isConsequential: true\n"})}),"\n",e.jsx(t.p,{children:"A good example of a consequential action is booking a hotel room and paying for it on behalf of a user."}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["If the ",e.jsx(t.code,{children:"x-openai-isConsequential"})," field is ",e.jsx(t.code,{children:"true"}),', ChatGPT treats the operation as "must always prompt the user for confirmation before running" and don\'t show an "always allow" button (both are features of GPTs designed to give builders and users more control over actions).']}),"\n",e.jsxs(t.li,{children:["If the ",e.jsx(t.code,{children:"x-openai-isConsequential"})," field is ",e.jsx(t.code,{children:"false"}),', ChatGPT shows the "always allow button".']}),"\n",e.jsxs(t.li,{children:["If the field isn't present, ChatGPT defaults all GET operations to ",e.jsx(t.code,{children:"false"})," and all other operations to ",e.jsx(t.code,{children:"true"})]}),"\n"]}),"\n",e.jsx(t.h2,{children:"Best practices on feeding examples"}),"\n",e.jsx(t.p,{children:"Here are some best practices to follow when writing your GPT instructions and descriptions in your schema, as well as when designing your API responses:"}),"\n",e.jsxs(t.ol,{children:["\n",e.jsxs(t.li,{children:["\n",e.jsx(t.p,{children:"Your descriptions should not encourage the GPT to use the action when the user hasn't asked for your action's particular category of service."}),"\n",e.jsxs(t.p,{children:[e.jsx(t.em,{children:"Bad example"}),":"]}),"\n",e.jsxs(t.blockquote,{children:["\n",e.jsx(t.p,{children:"Whenever the user mentions any type of task, ask if they would like to use the TODO action to add something to their todo list."}),"\n"]}),"\n",e.jsxs(t.p,{children:[e.jsx(t.em,{children:"Good example"}),":"]}),"\n",e.jsxs(t.blockquote,{children:["\n",e.jsx(t.p,{children:"The TODO list can add, remove and view the user's TODOs."}),"\n"]}),"\n"]}),"\n",e.jsxs(t.li,{children:["\n",e.jsx(t.p,{children:"Your descriptions should not prescribe specific triggers for the GPT to use the action. ChatGPT is designed to use your action automatically when appropriate."}),"\n",e.jsxs(t.p,{children:[e.jsx(t.em,{children:"Bad example"}),":"]}),"\n",e.jsxs(t.blockquote,{children:["\n",e.jsx(t.p,{children:"When the user mentions a task, respond with \"Would you like me to add this to your TODO list? Say 'yes' to continue.\""}),"\n"]}),"\n",e.jsxs(t.p,{children:[e.jsx(t.em,{children:"Good example"}),":"]}),"\n",e.jsxs(t.blockquote,{children:["\n",e.jsx(t.p,{children:"[no instructions needed for this]"}),"\n"]}),"\n"]}),"\n",e.jsxs(t.li,{children:["\n",e.jsx(t.p,{children:"Action responses from an API should return raw data instead of natural language responses unless it's necessary. The GPT will provide its own natural language response using the returned data."}),"\n",e.jsxs(t.p,{children:[e.jsx(t.em,{children:"Bad example"}),":"]}),"\n",e.jsxs(t.blockquote,{children:["\n",e.jsx(t.p,{children:"I was able to find your todo list! You have 2 todos: get groceries and walk the dog. I can add more todos if you'd like!"}),"\n"]}),"\n",e.jsxs(t.p,{children:[e.jsx(t.em,{children:"Good example"}),":"]}),"\n",e.jsxs(t.blockquote,{children:["\n",e.jsx(t.p,{children:'{ "todos": [ "get groceries", "walk the dog" ] }'}),"\n"]}),"\n"]}),"\n"]}),"\n",e.jsx(t.h2,{children:"How GPT Action data is used"}),"\n",e.jsx(t.p,{children:"GPT Actions connect ChatGPT to external apps. If a user interacts with a GPT’s custom action, ChatGPT may send parts of their conversation to the action’s endpoint."}),"\n",e.jsxs(t.p,{children:["If you have questions or run into additional limitations, you can join the discussion on the ",e.jsx(t.a,{href:"https://community.openai.com",children:"OpenAI developer forum"}),"."]})]})}function Ps(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Le,n)})):Le(n)}function Oe(n){const t=Object.assign({h1:"h1",p:"p",h2:"h2",code:"code",ul:"ul",li:"li",pre:"pre",h3:"h3"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"Sending and returning files with GPT Actions"}),"\n",e.jsx(t.p,{children:"Users can send & return files via GPT Actions. Below is guidance for setting that up."}),"\n",e.jsx(t.h2,{children:"Sending files"}),"\n",e.jsx(t.p,{children:"POST requests can include up to ten files (including DALL-E generated images) from the conversation. They will be sent as URLs which are valid for five minutes."}),"\n",e.jsxs(t.p,{children:["For files to be part of your POST request, the parameter must be named ",e.jsx(t.code,{children:"openaiFileIdRefs"})," and the description should explain to the model the type and quantity of the files which your API is expecting."]}),"\n",e.jsxs(t.p,{children:["The ",e.jsx(t.code,{children:"openaiFileIdRefs"})," parameter will be populated with an array of JSON objects. Each object contains:"]}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"name"})," The name of the file. This will be an auto generated name when created by DALL-E."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"id"})," A stable identifier for the file."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"mime_type"})," The mime type of the file. For user uploaded files this is based on file extension."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"download_link"})," The URL to fetch the file which is valid for five minutes."]}),"\n"]}),"\n",e.jsxs(t.p,{children:["Here’s an example of an ",e.jsx(t.code,{children:"openaiFileIdRefs"})," array with two elements:"]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{children:'[\n  {\n    "name": "dalle-Lh2tg7WuosbyR9hk",\n    "id": "file-XFlOqJYTPBPwMZE3IopCBv1Z",\n    "mime_type": "image/webp",\n    "download_link": "https://files.oaiusercontent.com/file-XFlOqJYTPBPwMZE3IopCBv1Z?se=2024-03-11T20%3A29%3A52Z&sp=r&sv=2021-08-06&sr=b&rscc=max-age%3D31536000%2C%20immutable&rscd=attachment%3B%20filename%3Da580bae6-ea30-478e-a3e2-1f6c06c3e02f.webp&sig=ZPWol5eXACxU1O9azLwRNgKVidCe%2BwgMOc/TdrPGYII%3D"\n  },\n  {\n    "name": "2023 Benefits Booklet.pdf",\n    "id": "file-s5nX7o4junn2ig0J84r8Q0Ew",\n    "mime_type": "application/pdf",\n    "download_link": "https://files.oaiusercontent.com/file-s5nX7o4junn2ig0J84r8Q0Ew?se=2024-03-11T20%3A29%3A52Z&sp=r&sv=2021-08-06&sr=b&rscc=max-age%3D299%2C%20immutable&rscd=attachment%3B%20filename%3D2023%2520Benefits%2520Booklet.pdf&sig=Ivhviy%2BrgoyUjxZ%2BingpwtUwsA4%2BWaRfXy8ru9AfcII%3D"\n  }\n]\n'})}),"\n",e.jsx(t.p,{children:"Actions can include files uploaded by the user, images generated by DALL-E, and files created by Code Interpreter."}),"\n",e.jsx(t.h3,{children:"OpenAPI Example"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{children:" /createWidget:\n    post:\n      operationId: createWidget\n      summary: Creates a widget based on an image.\n      description: Uploads a file reference using its file id. This file should be an image created by DALL·E or uploaded by the user. JPG, WEBP, and PNG are supported for widget creation.\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              type: object\n              properties:\n                openaiFileIdRefs:\n                  type: array\n                  items:\n                    type: string\n"})}),"\n",e.jsxs(t.p,{children:["While this schema shows ",e.jsx(t.code,{children:"openaiFileIdRefs"})," as being an array of type ",e.jsx(t.code,{children:"string"}),", at runtime this will be populated with an array of JSON objects as previously shown."]}),"\n",e.jsx(t.h2,{children:"Returning files"}),"\n",e.jsx(t.p,{children:"Requests may return up to 10 files. Each file may be up to 10 MB and cannot be an image or video."}),"\n",e.jsx(t.p,{children:"These files will become part of the conversation similarly to if a user uploaded them, meaning they may be made available to code interpreter, file search, and sent as part of subsequent action invocations. In the web app users will see that the files have been returned and can download them."}),"\n",e.jsxs(t.p,{children:["To return files, the body of the response must contain an ",e.jsx(t.code,{children:"openaiFileResponse"})," parameter. This parameter must always be an array and must be populated in one of two ways."]}),"\n",e.jsx(t.h3,{children:"Inline option"}),"\n",e.jsx(t.p,{children:"Each element of the array is a JSON object which contains:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"name"})," The name of the file. This will be visible to the user."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"mime_type"})," The MIME type of the file. This is used to determine eligibility and which features have access to the file."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"content"})," The base64 encoded contents of the file."]}),"\n"]}),"\n",e.jsx(t.p,{children:"Here’s an example of an openaiFileResponse array with two elements:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{children:'[\n  {\n    "name": "example_document.pdf",\n    "mime_type": "application/pdf",\n    "content": "JVBERi0xLjQKJcfsj6IKNSAwIG9iago8PC9MZW5ndGggNiAwIFIvRmlsdGVyIC9GbGF0ZURlY29kZT4+CnN0cmVhbQpHhD93PQplbmRzdHJlYW0KZW5kb2JqCg=="\n  },\n  {\n    "name": "sample_spreadsheet.csv",\n    "mime_type": "text/csv",\n    "content": "iVBORw0KGgoAAAANSUhEUgAAAAUAAAAFCAYAAACNbyblAAAAHElEQVQI12P4//8/w38GIAXDIBKE0DHxgljNBAAO9TXL0Y4OHwAAAABJRU5ErkJggg=="\n  }\n]\n'})}),"\n",e.jsx(t.p,{children:"OpenAPI example"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{children:"/papers:\n  get:\n    operationId: findPapers\n    summary: Retrieve PDFs of relevant academic papers.\n    description: Provided an academic topic, up to five relevant papers will be returned as PDFs.\n    parameters:\n      - in: query\n        name: topic\n        required: true\n        schema:\n          type: string\n        description: The topic the papers should be about.\n    responses:\n      '200':\n        description: Zero to five academic paper PDFs\n        content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  openaiFileResponse:\n                    type: array\n                    items:\n                      type: object\n                      properties:\n                        name:\n                          type: string\n                          description: The name of the file.\n                        mime_type:\n                          type: string\n                          description: The MIME type of the file.\n                        content:\n                          type: string\n                          format: byte\n                          description: The content of the file in base64 encoding.\n"})}),"\n",e.jsx(t.h3,{children:"URL option"}),"\n",e.jsxs(t.p,{children:["Each element of the array is a URL referencing a file to be downloaded. The headers ",e.jsx(t.code,{children:"Content-Disposition"})," and ",e.jsx(t.code,{children:"Content-Type"})," must be set such that a file name and MIME type can be determined. The name of the file will be visible to the user. The MIME type of the file determines eligibility and which features have access to the file."]}),"\n",e.jsx(t.p,{children:"There is a 10 second timeout for fetching each file."}),"\n",e.jsxs(t.p,{children:["Here’s an example of an ",e.jsx(t.code,{children:"openaiFileResponse"})," array with two elements:"]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{children:'[\n  "https://example.com/f/dca89f18-16d4-4a65-8ea2-ededced01646",\n  "https://example.com/f/01fad6b0-635b-4803-a583-0f678b2e6153"\n]\n'})}),"\n",e.jsx(t.p,{children:"Here’s an example of the required headers for each URL:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{children:'Content-Type: application/pdf\nContent-Disposition: attachment; filename="example_document.pdf"\n'})}),"\n",e.jsx(t.p,{children:"OpenAPI example"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-/papers:",children:"  get:\n    operationId: findPapers\n    summary: Retrieve PDFs of relevant academic papers.\n    description: Provided an academic topic, up to five relevant papers will be returned as PDFs.\n    parameters:\n      - in: query\n        name: topic\n        required: true\n        schema:\n          type: string\n        description: The topic the papers should be about.\n    responses:\n      '200':\n        description: Zero to five academic paper PDFs\n        content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  openaiFileResponse:\n                    type: array\n                    items:\n                    type: string\n                    format: uri\n                    description: URLs to fetch the files.\n"})})]})}function Ls(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Oe,n)})):Oe(n)}function G(n){var d;const{options:t,initialValue:s}=n,[i,r]=p.useState(()=>new URLSearchParams(window.location.search).get("context")||s),o=sn.useRef(!1);p.useEffect(()=>{if(!o.current){o.current=!0;return}const u=new URLSearchParams(window.location.search);i?(u.set("context",i),window.history.replaceState(null,"","?"+u.toString())):(u.delete("context"),window.history.replaceState(null,"",window.location.pathname))},[i]);const l=p.useCallback(u=>r(u),[]);return e.jsxs("div",{children:[e.jsx("div",{className:"content-switcher",children:e.jsx(ln,{children:t.map(u=>e.jsx(R,{onClick:()=>l(u.value),"aria-pressed":i===u.value?"true":"false",variant:i===u.value?void 0:"outlined",size:"small",children:u.label},u.value))})}),(d=t.find(u=>u.value===i))==null?void 0:d.content]})}function Se(n){const t=Object.assign({p:"p",a:"a"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.p,{children:"You can use the 'create and stream' helpers in the Python and Node SDKs to create a run and stream the response."}),"\n",e.jsx(c,{title:"Create and Stream a Run",defaultLanguage:"python",code:{python:'\nfrom typing_extensions import override\nfrom openai import AssistantEventHandler\n \n# First, we create a EventHandler class to define\n# how we want to handle the events in the response stream.\n \nclass EventHandler(AssistantEventHandler):    \n  @override\n  def on_text_created(self, text) -> None:\n    print(f"\\nassistant > ", end="", flush=True)\n      \n  @override\n  def on_text_delta(self, delta, snapshot):\n    print(delta.value, end="", flush=True)\n      \n  def on_tool_call_created(self, tool_call):\n    print(f"\\nassistant > {tool_call.type}\\n", flush=True)\n  \n  def on_tool_call_delta(self, delta, snapshot):\n    if delta.type == \'code_interpreter\':\n      if delta.code_interpreter.input:\n        print(delta.code_interpreter.input, end="", flush=True)\n      if delta.code_interpreter.outputs:\n        print(f"\\n\\noutput >", flush=True)\n        for output in delta.code_interpreter.outputs:\n          if output.type == "logs":\n            print(f"\\n{output.logs}", flush=True)\n \n# Then, we use the `stream` SDK helper \n# with the `EventHandler` class to create the Run \n# and stream the response.\n \nwith client.beta.threads.runs.stream(\n  thread_id=thread.id,\n  assistant_id=assistant.id,\n  instructions="Please address the user as Jane Doe. The user has a premium account.",\n  event_handler=EventHandler(),\n) as stream:\n  stream.until_done()\n'.trim(),"node.js":"\n// We use the stream SDK helper to create a run with\n// streaming. The SDK provides helpful event listeners to handle \n// the streamed response.\n \nconst run = openai.beta.threads.runs.stream(thread.id, {\n    assistant_id: assistant.id\n  })\n    .on('textCreated', (text) => process.stdout.write('\\nassistant > '))\n    .on('textDelta', (textDelta, snapshot) => process.stdout.write(textDelta.value))\n    .on('toolCallCreated', (toolCall) => process.stdout.write(`\\nassistant > ${toolCall.type}\\n\\n`))\n    .on('toolCallDelta', (toolCallDelta, snapshot) => {\n      if (toolCallDelta.type === 'code_interpreter') {\n        if (toolCallDelta.code_interpreter.input) {\n          process.stdout.write(toolCallDelta.code_interpreter.input);\n        }\n        if (toolCallDelta.code_interpreter.outputs) {\n          process.stdout.write(\"\\noutput >\\n\");\n          toolCallDelta.code_interpreter.outputs.forEach(output => {\n            if (output.type === \"logs\") {\n              process.stdout.write(`\\n${output.logs}\\n`);\n            }\n          });\n        }\n      }\n    });\n".trim()}}),"\n",e.jsxs(t.p,{children:["See the full list of Assistants streaming events in our API reference ",e.jsx(t.a,{href:"/docs/api-reference/assistants-streaming/events",children:"here"}),". You can also see a list of SDK event listeners for these events in the ",e.jsx(t.a,{href:"https://github.com/openai/openai-python/blob/main/helpers.md#assistant-events",children:"Python"})," & ",e.jsx(t.a,{href:"https://github.com/openai/openai-node/blob/master/helpers.md#assistant-events",children:"Node"})," repository documentation."]})]})}function Os(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Se,n)})):Se(n)}function Me(n){const t=Object.assign({p:"p",code:"code",a:"a"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsxs(t.p,{children:["Runs are asynchronous, which means you'll want to monitor their ",e.jsx(t.code,{children:"status"})," by polling the Run object until a ",e.jsx(t.a,{href:"/docs/assistants/how-it-works/runs-and-run-steps",children:"terminal status"})," is reached. For convenience, the 'create and poll' SDK helpers assist both in creating the run and then polling for its completion."]}),"\n",e.jsx(c,{title:"Create a Run",defaultLanguage:"python",code:{python:'\nrun = client.beta.threads.runs.create_and_poll(\n  thread_id=thread.id,\n  assistant_id=assistant.id,\n  instructions="Please address the user as Jane Doe. The user has a premium account."\n)\n'.trim(),"node.js":'\nlet run = await openai.beta.threads.runs.createAndPoll(\n  thread.id,\n  { \n    assistant_id: assistant.id,\n    instructions: "Please address the user as Jane Doe. The user has a premium account."\n  }\n);\n'.trim(),curl:'\ncurl https://api.openai.com/v1/threads/thread_abc123/runs \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -H "Content-Type: application/json" \\\n  -H "OpenAI-Beta: assistants=v2" \\\n  -d \'{\n    "assistant_id": "asst_abc123",\n    "instructions": "Please address the user as Jane Doe. The user has a premium account."\n  }\'\n'.trim()}}),"\n",e.jsxs(t.p,{children:["Once the Run completes, you can ",e.jsx(t.a,{href:"/docs/api-reference/messages/listMessages",children:"list the Messages"})," added to the Thread by the Assistant."]}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:"\nif run.status == 'completed': \n  messages = client.beta.threads.messages.list(\n    thread_id=thread.id\n  )\n  print(messages)\nelse:\n  print(run.status)\n".trim(),"node.js":"\nif (run.status === 'completed') {\n  const messages = await openai.beta.threads.messages.list(\n    run.thread_id\n  );\n  for (const message of messages.data.reverse()) {\n    console.log(`${message.role} > ${message.content[0].text.value}`);\n  }\n} else {\n  console.log(run.status);\n}\n".trim(),curl:'\ncurl https://api.openai.com/v1/threads/thread_abc123/messages \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -H "OpenAI-Beta: assistants=v2"\n'.trim()}}),"\n",e.jsxs(t.p,{children:["You may also want to list the ",e.jsx(t.a,{href:"/docs/api-reference/runs/listRunSteps",children:"Run Steps"})," of this Run if you'd like to look at any tool calls made during this Run."]})]})}function Ss(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Me,n)})):Me(n)}function qe(n){const t=Object.assign({p:"p",a:"a",h2:"h2",code:"code",ol:"ol",li:"li",h3:"h3",h4:"h4",ul:"ul",img:"img",table:"table",thead:"thead",tr:"tr",th:"th",tbody:"tbody",td:"td",em:"em"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(I,{level:1,slug:"agents",children:e.jsxs(t.p,{children:["Assistants API Deep dive ",e.jsx(H,{children:"Beta"})]})}),"\n",e.jsxs(t.p,{children:["As described in the ",e.jsx(t.a,{href:"/docs/assistants/overview",children:"Assistants Overview"}),", there are several concepts involved in building an app with the Assistants API."]}),"\n",e.jsx(t.p,{children:"This guide goes deeper into each of these concepts."}),"\n",e.jsxs(t.p,{children:["If you want to get started coding right away, check out the ",e.jsx(t.a,{href:"/docs/assistants/quickstart",children:"Assistants API Quickstart"}),"."]}),"\n",e.jsx(t.h2,{children:"Creating Assistants"}),"\n",e.jsx(b,{children:e.jsxs(t.p,{children:["We recommend using OpenAI's"," ","\n",e.jsx("a",{href:"/docs/models/gpt-4-and-gpt-4-turbo",children:"latest models"})," with the Assistants API\nfor best results and maximum compatibility with tools."]})}),"\n",e.jsxs(t.p,{children:["To get started, creating an Assistant only requires specifying the ",e.jsx(t.code,{children:"model"})," to use. But you can further customize the behavior of the Assistant:"]}),"\n",e.jsxs(t.ol,{children:["\n",e.jsxs(t.li,{children:["Use the ",e.jsx(t.code,{children:"instructions"})," parameter to guide the personality of the Assistant and define its goals. Instructions are similar to system messages in the Chat Completions API."]}),"\n",e.jsxs(t.li,{children:["Use the ",e.jsx(t.code,{children:"tools"})," parameter to give the Assistant access to up to 128 tools. You can give it access to OpenAI-hosted tools like ",e.jsx(t.code,{children:"code_interpreter"})," and ",e.jsx(t.code,{children:"file_search"}),", or call a third-party tools via a ",e.jsx(t.code,{children:"function"})," calling."]}),"\n",e.jsxs(t.li,{children:["Use the ",e.jsx(t.code,{children:"tool_resources"})," parameter to give the tools like ",e.jsx(t.code,{children:"code_interpreter"})," and ",e.jsx(t.code,{children:"file_search"})," access to files. Files are uploaded using the ",e.jsx(t.code,{children:"File"})," ",e.jsx(t.a,{href:"/docs/api-reference/files/create",children:"upload endpoint"})," and must have the ",e.jsx(t.code,{children:"purpose"})," set to ",e.jsx(t.code,{children:"assistants"})," to be used with this API."]}),"\n"]}),"\n",e.jsxs(t.p,{children:["For example, to create an Assistant that can create data visualization based on a ",e.jsx(t.code,{children:".csv"})," file, first upload a file."]}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\nfile = client.files.create(\n  file=open("revenue-forecast.csv", "rb"),\n  purpose=\'assistants\'\n)\n'.trim(),"node.js":'\nconst file = await openai.files.create({\n  file: fs.createReadStream("revenue-forecast.csv"),\n  purpose: "assistants",\n});\n'.trim(),curl:'\ncurl https://api.openai.com/v1/files \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -F purpose="assistants" \\\n  -F file="@revenue-forecast.csv"\n'.trim()}}),"\n",e.jsxs(t.p,{children:["Then, create the Assistant with the ",e.jsx(t.code,{children:"code_interpreter"})," tool enabled and provide the file as a resource to the tool."]}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\nassistant = client.beta.assistants.create(\n  name="Data visualizer",\n  description="You are great at creating beautiful data visualizations. You analyze data present in .csv files, understand trends, and come up with data visualizations relevant to those trends. You also share a brief text summary of the trends observed.",\n  model="gpt-4o",\n  tools=[{"type": "code_interpreter"}],\n  tool_resources={\n    "code_interpreter": {\n      "file_ids": [file.id]\n    }\n  }\n)\n'.trim(),"node.js":'\nconst assistant = await openai.beta.assistants.create({\n  name: "Data visualizer",\n  description: "You are great at creating beautiful data visualizations. You analyze data present in .csv files, understand trends, and come up with data visualizations relevant to those trends. You also share a brief text summary of the trends observed.",\n  model: "gpt-4o",\n  tools: [{"type": "code_interpreter"}],\n  tool_resources: {\n    "code_interpreter": {\n      "file_ids": [file.id]\n    }\n  }\n});\n'.trim(),curl:'\ncurl https://api.openai.com/v1/assistants \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -H "Content-Type: application/json" \\\n  -H "OpenAI-Beta: assistants=v2" \\\n  -d \'{\n    "name": "Data visualizer",\n    "description": "You are great at creating beautiful data visualizations. You analyze data present in .csv files, understand trends, and come up with data visualizations relevant to those trends. You also share a brief text summary of the trends observed.",\n    "model": "gpt-4o",\n    "tools": [{"type": "code_interpreter"}],\n    "tool_resources": {\n      "code_interpreter": {\n        "file_ids": ["file-BK7bzQj3FfZFXr7DbL6xJwfo"]\n      }\n    }\n  }\'\n'.trim()}}),"\n",e.jsxs(t.p,{children:["You can attach a maximum of 20 files to ",e.jsx(t.code,{children:"code_interpreter"})," and 10,000 files to ",e.jsx(t.code,{children:"file_search"})," (using ",e.jsx(t.code,{children:"vector_store"})," ",e.jsx(t.a,{href:"/docs/api-reference/vector-stores/object",children:"objects"}),")."]}),"\n",e.jsx(t.p,{children:"Each file can be at most 512 MB in size and have a maximum of 5,000,000 tokens. By default, the size of all the files uploaded in your project cannot exceed 100 GB, but you can reach out to our support team to increase this limit."}),"\n",e.jsx(t.h2,{children:"Managing Threads and Messages"}),"\n",e.jsx(t.p,{children:"Threads and Messages represent a conversation session between an Assistant and a user. There is a limit of 100,000 Messages per Thread. Once the size of the Messages exceeds the context window of the model, the Thread will attempt to smartly truncate messages, before fully dropping the ones it considers the least important."}),"\n",e.jsx(t.p,{children:"You can create a Thread with an initial list of Messages like this:"}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\nthread = client.beta.threads.create(\n  messages=[\n    {\n      "role": "user",\n      "content": "Create 3 data visualizations based on the trends in this file.",\n      "attachments": [\n        {\n          "file_id": file.id,\n          "tools": [{"type": "code_interpreter"}]\n        }\n      ]\n    }\n  ]\n)\n'.trim(),"node.js":'\nconst thread = await openai.beta.threads.create({\n  messages: [\n    {\n      "role": "user",\n      "content": "Create 3 data visualizations based on the trends in this file.",\n      "attachments": [\n        {\n          file_id: file.id,\n          tools: [{type: "code_interpreter"}]\n        }\n      ]\n    }\n  ]\n});\n'.trim(),curl:'\ncurl https://api.openai.com/v1/threads \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -H "Content-Type: application/json" \\\n  -H "OpenAI-Beta: assistants=v2" \\\n  -d \'{\n    "messages": [\n      {\n        "role": "user",\n        "content": "Create 3 data visualizations based on the trends in this file.",\n        "attachments": [\n          {\n            "file_id": "file-ACq8OjcLQm2eIG0BvRM4z5qX",\n            "tools": [{"type": "code_interpreter"}]\n          }\n        ]\n      }\n    ]\n  }\'\n'.trim()}}),"\n",e.jsxs(t.p,{children:["Messages can contain text, images, or file attachment. Message ",e.jsx(t.code,{children:"attachments"})," are helper methods that add files to a thread's ",e.jsx(t.code,{children:"tool_resources"}),". You can also choose to add files to the ",e.jsx(t.code,{children:"thread.tool_resources"})," directly."]}),"\n",e.jsx(t.h3,{children:"Creating image input content"}),"\n",e.jsxs(t.p,{children:["Message content can contain either external image URLs or File IDs uploaded via the ",e.jsx(t.a,{href:"/docs/api-reference/files/create",children:"File API"}),". Only ",e.jsx(t.a,{href:"/docs/models",children:"models"})," with Vision support can accept image input. Supported image content types include png, jpg, gif, and webp. When creating image files, pass ",e.jsx(t.code,{children:'purpose="vision"'})," to allow you to later download and display the input content. Currently, there is a 100GB limit per project. Please contact us to request a limit increase."]}),"\n",e.jsxs(t.p,{children:["Tools cannot access image content unless specified. To pass image files to Code Interpreter, add the file ID in the message ",e.jsx(t.code,{children:"attachments"})," list to allow the tool to read and analyze the input. Image URLs cannot be downloaded in Code Interpreter today."]}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\nfile = client.files.create(\n  file=open("myimage.png", "rb"),\n  purpose="vision"\n)\nthread = client.beta.threads.create(\n  messages=[\n    {\n      "role": "user",\n      "content": [\n        {\n          "type": "text",\n          "text": "What is the difference between these images?"\n        },\n        {\n          "type": "image_url",\n          "image_url": {"url": "https://example.com/image.png"}\n        },\n        {\n          "type": "image_file",\n          "image_file": {"file_id": file.id}\n        },\n      ],\n    }\n  ]\n)\n'.trim(),"node.js":'\nimport fs from "fs";\nconst file = await openai.files.create({\n  file: fs.createReadStream("myimage.png"),\n  purpose: "vision",\n});\nconst thread = await openai.beta.threads.create({\n  messages: [\n    {\n      "role": "user",\n      "content": [\n        {\n          "type": "text",\n          "text": "What is the difference between these images?"\n        },\n        {\n          "type": "image_url",\n          "image_url": {"url": "https://example.com/image.png"}\n        },\n        {\n          "type": "image_file",\n          "image_file": {"file_id": file.id}\n        },\n      ]\n    }\n  ]\n});\n'.trim(),curl:'\n# Upload a file with an "vision" purpose\ncurl https://api.openai.com/v1/files \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -F purpose="vision" \\\n  -F file="@/path/to/myimage.png"\n\n## Pass the file ID in the content\ncurl https://api.openai.com/v1/threads \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -H "Content-Type: application/json" \\\n  -H "OpenAI-Beta: assistants=v2" \\\n  -d \'{\n    "messages": [\n      {\n        "role": "user",\n        "content": [\n          {\n            "type": "text",\n            "text": "What is the difference between these images?"\n          },\n          {\n            "type": "image_url",\n            "image_url": {"url": "https://example.com/image.png"}\n          },\n          {\n            "type": "image_file",\n            "image_file": {"file_id": file.id}\n          }\n        ]\n      }\n    ]\n  }\'\n'.trim()}}),"\n",e.jsx(t.h4,{children:"Low or high fidelity image understanding"}),"\n",e.jsxs(t.p,{children:["By controlling the ",e.jsx(t.code,{children:"detail"})," parameter, which has three options, ",e.jsx(t.code,{children:"low"}),", ",e.jsx(t.code,{children:"high"}),", or ",e.jsx(t.code,{children:"auto"}),", you have control over how the model processes the image and generates its textual understanding."]}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"low"}),' will enable the "low res" mode. The model will receive a low-res 512px x 512px version of the image, and represent the image with a budget of 85 tokens. This allows the API to return faster responses and consume fewer input tokens for use cases that do not require high detail.']}),"\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"high"}),' will enable "high res" mode, which first allows the model to see the low res image and then creates detailed crops of input images based on the input image size. Use the ',e.jsx(t.a,{href:"https://openai.com/api/pricing/",children:"pricing calculator"})," to see token counts for various image sizes."]}),"\n"]}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\nthread = client.beta.threads.create(\n  messages=[\n    {\n      "role": "user",\n      "content": [\n        {\n          "type": "text",\n          "text": "What is this an image of?"\n        },\n        {\n          "type": "image_url",\n          "image_url": {\n            "url": "https://example.com/image.png",\n            "detail": "high"\n          }\n        },\n      ],\n    }\n  ]\n)\n'.trim(),"node.js":'\nconst thread = await openai.beta.threads.create({\n  messages: [\n    {\n      "role": "user",\n      "content": [\n          {\n            "type": "text",\n            "text": "What is this an image of?"\n          },\n          {\n            "type": "image_url",\n            "image_url": {\n              "url": "https://example.com/image.png",\n              "detail": "high"\n            }\n          },\n      ]\n    }\n  ]\n});\n'.trim(),curl:'\ncurl https://api.openai.com/v1/threads \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -H "Content-Type: application/json" \\\n  -H "OpenAI-Beta: assistants=v2" \\\n  -d \'{\n    "messages": [\n      {\n        "role": "user",\n        "content": [\n          {\n            "type": "text",\n            "text": "What is this an image of?"\n          },\n          {\n            "type": "image_url",\n            "image_url": {\n              "url": "https://example.com/image.png",\n              "detail": "high"\n            }\n          },\n        ]\n      }\n    ]\n  }\'\n'.trim()}}),"\n",e.jsx(t.h3,{children:"Context window management"}),"\n",e.jsx(t.p,{children:"The Assistants API automatically manages the truncation to ensure it stays within the model's maximum context length. You can customize this behavior by specifying the maximum tokens you'd like a run to utilize and/or the maximum number of recent messages you'd like to include in a run."}),"\n",e.jsx(t.h4,{children:"Max Completion and Max Prompt Tokens"}),"\n",e.jsxs(t.p,{children:["To control the token usage in a single Run, set ",e.jsx(t.code,{children:"max_prompt_tokens"})," and ",e.jsx(t.code,{children:"max_completion_tokens"})," when creating the Run. These limits apply to the total number of tokens used in all completions throughout the Run's lifecycle."]}),"\n",e.jsxs(t.p,{children:["For example, initiating a Run with ",e.jsx(t.code,{children:"max_prompt_tokens"})," set to 500 and ",e.jsx(t.code,{children:"max_completion_tokens"})," set to 1000 means the first completion will truncate the thread to 500 tokens and cap the output at 1000 tokens. If only 200 prompt tokens and 300 completion tokens are used in the first completion, the second completion will have available limits of 300 prompt tokens and 700 completion tokens."]}),"\n",e.jsxs(t.p,{children:["If a completion reaches the ",e.jsx(t.code,{children:"max_completion_tokens"})," limit, the Run will terminate with a status of ",e.jsx(t.code,{children:"incomplete"}),", and details will be provided in the ",e.jsx(t.code,{children:"incomplete_details"})," field of the Run object."]}),"\n",e.jsx(b,{children:e.jsx(t.p,{children:"When using the File Search tool, we recommend setting the max_prompt_tokens to no less\nthan 20,000. For longer conversations or multiple interactions with File Search,\nconsider increasing this limit to 50,000, or ideally, removing the max_prompt_tokens\nlimits altogether to get the highest quality results."})}),"\n",e.jsx(t.h4,{children:"Truncation Strategy"}),"\n",e.jsxs(t.p,{children:["You may also specify a truncation strategy to control how your thread should be rendered into the model's context window.\nUsing a truncation strategy of type ",e.jsx(t.code,{children:"auto"})," will use OpenAI's default truncation strategy. Using a truncation strategy of type ",e.jsx(t.code,{children:"last_messages"})," will allow you to specify the number of the most recent messages to include in the context window."]}),"\n",e.jsx(t.h3,{children:"Message annotations"}),"\n",e.jsxs(t.p,{children:["Messages created by Assistants may contain ",e.jsx(t.a,{href:"/docs/api-reference/messages/object#messages/object-content",children:e.jsx(t.code,{children:"annotations"})})," within the ",e.jsx(t.code,{children:"content"})," array of the object. Annotations provide information around how you should annotate the text in the Message."]}),"\n",e.jsx(t.p,{children:"There are two types of Annotations:"}),"\n",e.jsxs(t.ol,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"file_citation"}),": File citations are created by the ",e.jsx(t.a,{href:"/docs/assistants/tools/file-search",children:e.jsx(t.code,{children:"file_search"})})," tool and define references to a specific file that was uploaded and used by the Assistant to generate the response."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"file_path"}),": File path annotations are created by the ",e.jsx(t.a,{href:"/docs/assistants/tools/code-interpreter",children:e.jsx(t.code,{children:"code_interpreter"})})," tool and contain references to the files generated by the tool."]}),"\n"]}),"\n",e.jsxs(t.p,{children:["When annotations are present in the Message object, you'll see illegible model-generated substrings in the text that you should replace with the annotations. These strings may look something like ",e.jsx(t.code,{children:"【13†source】"})," or ",e.jsx(t.code,{children:"sandbox:/mnt/data/file.csv"}),". Here’s an example python code snippet that replaces these strings with information present in the annotations."]}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:"\n# Retrieve the message object\nmessage = client.beta.threads.messages.retrieve(\n  thread_id=\"...\",\n  message_id=\"...\"\n)\n# Extract the message content\nmessage_content = message.content[0].text\nannotations = message_content.annotations\ncitations = []\n# Iterate over the annotations and add footnotes\nfor index, annotation in enumerate(annotations):\n    # Replace the text with a footnote\n    message_content.value = message_content.value.replace(annotation.text, f' [{index}]')\n    # Gather citations based on annotation attributes\n    if (file_citation := getattr(annotation, 'file_citation', None)):\n        cited_file = client.files.retrieve(file_citation.file_id)\n        citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')\n    elif (file_path := getattr(annotation, 'file_path', None)):\n        cited_file = client.files.retrieve(file_path.file_id)\n        citations.append(f'[{index}] Click <here> to download {cited_file.filename}')\n        # Note: File download functionality not implemented above for brevity\n# Add footnotes to the end of the message before displaying to user\nmessage_content.value += '\\n' + '\\n'.join(citations)\n".trim()}}),"\n",e.jsx(t.h2,{children:"Runs and Run Steps"}),"\n",e.jsx(t.p,{children:"When you have all the context you need from your user in the Thread, you can run the Thread with an Assistant of your choice."}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:"\nrun = client.beta.threads.runs.create(\n  thread_id=thread.id,\n  assistant_id=assistant.id\n)\n".trim(),"node.js":"\nconst run = await openai.beta.threads.runs.create(\n  thread.id,\n  { assistant_id: assistant.id }\n);\n".trim(),curl:'\ncurl https://api.openai.com/v1/threads/THREAD_ID/runs \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -H "Content-Type: application/json" \\\n  -H "OpenAI-Beta: assistants=v2" \\\n  -d \'{\n    "assistant_id": "asst_ToSF7Gb04YMj8AMMm50ZLLtY"\n  }\'\n'.trim()}}),"\n",e.jsxs(t.p,{children:["By default, a Run will use the ",e.jsx(t.code,{children:"model"})," and ",e.jsx(t.code,{children:"tools"})," configuration specified in Assistant object, but you can override most of these when creating the Run for added flexibility:"]}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\nrun = client.beta.threads.runs.create(\n  thread_id=thread.id,\n  assistant_id=assistant.id,\n  model="gpt-4o",\n  instructions="New instructions that override the Assistant instructions",\n  tools=[{"type": "code_interpreter"}, {"type": "file_search"}]\n)\n'.trim(),"node.js":'\nconst run = await openai.beta.threads.runs.create(\n  thread.id,\n  {\n    assistant_id: assistant.id,\n    model: "gpt-4o",\n    instructions: "New instructions that override the Assistant instructions",\n    tools: [{"type": "code_interpreter"}, {"type": "file_search"}]\n  }\n);\n'.trim(),curl:'\ncurl https://api.openai.com/v1/threads/THREAD_ID/runs \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -H "Content-Type: application/json" \\\n  -H "OpenAI-Beta: assistants=v2" \\\n  -d \'{\n    "assistant_id": "ASSISTANT_ID",\n    "model": "gpt-4o",\n    "instructions": "New instructions that override the Assistant instructions",\n    "tools": [{"type": "code_interpreter"}, {"type": "file_search"}]\n  }\'\n'.trim()}}),"\n",e.jsxs(t.p,{children:["Note: ",e.jsx(t.code,{children:"tool_resources"})," associated with the Assistant cannot be overridden during Run creation. You must use the ",e.jsx(t.a,{href:"/docs/api-reference/assistants/modifyAssistant",children:"modify Assistant"})," endpoint to do this."]}),"\n",e.jsx(t.h4,{children:"Run lifecycle"}),"\n",e.jsx(t.p,{children:"Run objects can have multiple statuses."}),"\n",e.jsx(t.p,{children:e.jsx(t.img,{src:"https://cdn.openai.com/API/docs/images/diagram-run-statuses-v2.png",alt:"Run lifecycle - diagram showing possible status transitions"})}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Status"}),e.jsx(t.th,{children:"Definition"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"queued"})}),e.jsxs(t.td,{children:["When Runs are first created or when you complete the ",e.jsx(t.code,{children:"required_action"}),", they are moved to a queued status. They should almost immediately move to ",e.jsx(t.code,{children:"in_progress"}),"."]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"in_progress"})}),e.jsxs(t.td,{children:["While in_progress, the Assistant uses the model and tools to perform steps. You can view progress being made by the Run by examining the ",e.jsx(t.a,{href:"/docs/api-reference/runs/step-object",children:"Run Steps"}),"."]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"completed"})}),e.jsx(t.td,{children:"The Run successfully completed! You can now view all Messages the Assistant added to the Thread, and all the steps the Run took. You can also continue the conversation by adding more user Messages to the Thread and creating another Run."})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"requires_action"})}),e.jsxs(t.td,{children:["When using the ",e.jsx(t.a,{href:"/docs/assistants/tools/function-calling",children:"Function calling"})," tool, the Run will move to a ",e.jsx(t.code,{children:"required_action"})," state once the model determines the names and arguments of the functions to be called. You must then run those functions and ",e.jsx(t.a,{href:"/docs/api-reference/runs/submitToolOutputs",children:"submit the outputs"})," before the run proceeds. If the outputs are not provided before the ",e.jsx(t.code,{children:"expires_at"})," timestamp passes (roughly 10 mins past creation), the run will move to an expired status."]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"expired"})}),e.jsxs(t.td,{children:["This happens when the function calling outputs were not submitted before ",e.jsx(t.code,{children:"expires_at"})," and the run expires. Additionally, if the runs take too long to execute and go beyond the time stated in ",e.jsx(t.code,{children:"expires_at"}),", our systems will expire the run."]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"cancelling"})}),e.jsxs(t.td,{children:["You can attempt to cancel an ",e.jsx(t.code,{children:"in_progress"})," run using the ",e.jsx(t.a,{href:"/docs/api-reference/runs/cancelRun",children:"Cancel Run"})," endpoint. Once the attempt to cancel succeeds, status of the Run moves to ",e.jsx(t.code,{children:"cancelled"}),". Cancellation is attempted but not guaranteed."]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"cancelled"})}),e.jsx(t.td,{children:"Run was successfully cancelled."})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"failed"})}),e.jsxs(t.td,{children:["You can view the reason for the failure by looking at the ",e.jsx(t.code,{children:"last_error"})," object in the Run. The timestamp for the failure will be recorded under ",e.jsx(t.code,{children:"failed_at"}),"."]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"incomplete"})}),e.jsxs(t.td,{children:["Run ended due to ",e.jsx(t.code,{children:"max_prompt_tokens"})," or ",e.jsx(t.code,{children:"max_completion_tokens"})," reached. You can view the specific reason by looking at the ",e.jsx(t.code,{children:"incomplete_details"})," object in the Run."]})]})]})]}),"\n",e.jsx(t.h4,{children:"Polling for updates"}),"\n",e.jsxs(t.p,{children:["If you are not using ",e.jsx(t.a,{href:"/docs/assistants/overview/step-4-create-a-run?context=with-streaming",children:"streaming"}),", in order to keep the status of your run up to date, you will have to periodically ",e.jsx(t.a,{href:"/docs/api-reference/runs/getRun",children:"retrieve the Run"})," object. You can check the status of the run each time you retrieve the object to determine what your application should do next."]}),"\n",e.jsxs(t.p,{children:["You can optionally use Polling Helpers in our ",e.jsx(t.a,{href:"https://github.com/openai/openai-node?tab=readme-ov-file#polling-helpers",children:"Node"})," and ",e.jsx(t.a,{href:"https://github.com/openai/openai-python?tab=readme-ov-file#polling-helpers",children:"Python"})," SDKs to help you with this. These helpers will automatically poll the Run object for you and return the Run object when it's in a terminal state."]}),"\n",e.jsx(t.h4,{children:"Thread locks"}),"\n",e.jsxs(t.p,{children:["When a Run is ",e.jsx(t.code,{children:"in_progress"})," and not in a terminal state, the Thread is locked. This means that:"]}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"New Messages cannot be added to the Thread."}),"\n",e.jsx(t.li,{children:"New Runs cannot be created on the Thread."}),"\n"]}),"\n",e.jsx(t.h4,{children:"Run steps"}),"\n",e.jsx(t.p,{children:e.jsx(t.img,{src:"https://cdn.openai.com/API/docs/images/diagram-2.png",alt:"Run steps lifecycle - diagram showing possible status transitions"})}),"\n",e.jsx(t.p,{children:"Run step statuses have the same meaning as Run statuses."}),"\n",e.jsxs(t.p,{children:["Most of the interesting detail in the Run Step object lives in the ",e.jsx(t.code,{children:"step_details"})," field. There can be two types of step details:"]}),"\n",e.jsxs(t.ol,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"message_creation"}),": This Run Step is created when the Assistant creates a Message on the Thread."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"tool_calls"}),": This Run Step is created when the Assistant calls a tool. Details around this are covered in the relevant sections of the ",e.jsx(t.a,{href:"/docs/assistants/tools",children:"Tools"})," guide."]}),"\n"]}),"\n",e.jsx(t.h2,{children:"Data Access Guidance"}),"\n",e.jsx(t.p,{children:"Currently, Assistants, Threads, Messages, and Vector Stores created via the API are scoped to the Project they're created in. As such, any person with API key access to that Project is able to read or write Assistants, Threads, Messages, and Runs in the Project."}),"\n",e.jsx(t.p,{children:"We strongly recommend the following data access controls:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.em,{children:"Implement authorization."})," Before performing reads or writes on Assistants, Threads, Messages, and Vector Stores, ensure that the end-user is authorized to do so. For example, store in your database the object IDs that the end-user has access to, and check it before fetching the object ID with the API."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.em,{children:"Restrict API key access."})," Carefully consider who in your organization should have API keys and be part of a Project. Periodically audit this list. API keys enable a wide range of operations including reading and modifying sensitive information, such as Messages and Files."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.em,{children:"Create separate accounts."})," Consider creating separate Projects for different applications in order to isolate data across multiple applications."]}),"\n"]})]})}function Ms(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(qe,n)})):qe(n)}function D(){}D.prototype={diff:function(t,s){var i,r=arguments.length>2&&arguments[2]!==void 0?arguments[2]:{},o=r.callback;typeof r=="function"&&(o=r,r={}),this.options=r;var l=this;function d(k){return o?(setTimeout(function(){o(void 0,k)},0),!0):k}t=this.castInput(t),s=this.castInput(s),t=this.removeEmpty(this.tokenize(t)),s=this.removeEmpty(this.tokenize(s));var u=s.length,m=t.length,h=1,x=u+m;r.maxEditLength&&(x=Math.min(x,r.maxEditLength));var w=(i=r.timeout)!==null&&i!==void 0?i:1/0,v=Date.now()+w,y=[{oldPos:-1,lastComponent:void 0}],_=this.extractCommon(y[0],s,t,0);if(y[0].oldPos+1>=m&&_+1>=u)return d([{value:this.join(s),count:s.length}]);var T=-1/0,L=1/0;function S(){for(var k=Math.max(T,-h);k<=Math.min(L,h);k+=2){var P=void 0,O=y[k-1],q=y[k+1];O&&(y[k-1]=void 0);var W=!1;if(q){var K=q.oldPos-k;W=q&&0<=K&&K<u}var U=O&&O.oldPos+1<m;if(!W&&!U){y[k]=void 0;continue}if(!U||W&&O.oldPos+1<q.oldPos?P=l.addToPath(q,!0,void 0,0):P=l.addToPath(O,void 0,!0,1),_=l.extractCommon(P,s,t,k),P.oldPos+1>=m&&_+1>=u)return d(qs(l,P.lastComponent,s,t,l.useLongestToken));y[k]=P,P.oldPos+1>=m&&(L=Math.min(L,k-1)),_+1>=u&&(T=Math.max(T,k+1))}h++}if(o)(function k(){setTimeout(function(){if(h>x||Date.now()>v)return o();S()||k()},0)})();else for(;h<=x&&Date.now()<=v;){var M=S();if(M)return M}},addToPath:function(t,s,i,r){var o=t.lastComponent;return o&&o.added===s&&o.removed===i?{oldPos:t.oldPos+r,lastComponent:{count:o.count+1,added:s,removed:i,previousComponent:o.previousComponent}}:{oldPos:t.oldPos+r,lastComponent:{count:1,added:s,removed:i,previousComponent:o}}},extractCommon:function(t,s,i,r){for(var o=s.length,l=i.length,d=t.oldPos,u=d-r,m=0;u+1<o&&d+1<l&&this.equals(s[u+1],i[d+1]);)u++,d++,m++;return m&&(t.lastComponent={count:m,previousComponent:t.lastComponent}),t.oldPos=d,u},equals:function(t,s){return this.options.comparator?this.options.comparator(t,s):t===s||this.options.ignoreCase&&t.toLowerCase()===s.toLowerCase()},removeEmpty:function(t){for(var s=[],i=0;i<t.length;i++)t[i]&&s.push(t[i]);return s},castInput:function(t){return t},tokenize:function(t){return t.split("")},join:function(t){return t.join("")}};function qs(n,t,s,i,r){for(var o=[],l;t;)o.push(t),l=t.previousComponent,delete t.previousComponent,t=l;o.reverse();for(var d=0,u=o.length,m=0,h=0;d<u;d++){var x=o[d];if(x.removed){if(x.value=n.join(i.slice(h,h+x.count)),h+=x.count,d&&o[d-1].added){var v=o[d-1];o[d-1]=o[d],o[d]=v}}else{if(!x.added&&r){var w=s.slice(m,m+x.count);w=w.map(function(_,T){var L=i[h+T];return L.length>_.length?L:_}),x.value=n.join(w)}else x.value=n.join(s.slice(m,m+x.count));m+=x.count,x.added||(h+=x.count)}}var y=o[u-1];return u>1&&typeof y.value=="string"&&(y.added||y.removed)&&n.equals("",y.value)&&(o[u-2].value+=y.value,o.pop()),o}var He=/^[A-Za-z\xC0-\u02C6\u02C8-\u02D7\u02DE-\u02FF\u1E00-\u1EFF]+$/,Ee=/\S/,cn=new D;cn.equals=function(n,t){return this.options.ignoreCase&&(n=n.toLowerCase(),t=t.toLowerCase()),n===t||this.options.ignoreWhitespace&&!Ee.test(n)&&!Ee.test(t)};cn.tokenize=function(n){for(var t=n.split(/([^\S\r\n]+|[()[\]{}'"\r\n]|\b)/),s=0;s<t.length-1;s++)!t[s+1]&&t[s+2]&&He.test(t[s])&&He.test(t[s+2])&&(t[s]+=t[s+2],t.splice(s+1,2),s--);return t};var fe=new D;fe.tokenize=function(n){this.options.stripTrailingCr&&(n=n.replace(/\r\n/g,"\n"));var t=[],s=n.split(/(\n|\r\n)/);s[s.length-1]||s.pop();for(var i=0;i<s.length;i++){var r=s[i];i%2&&!this.options.newlineIsToken?t[t.length-1]+=r:(this.options.ignoreWhitespace&&(r=r.trim()),t.push(r))}return t};function Hs(n,t,s){return fe.diff(n,t,s)}var Es=new D;Es.tokenize=function(n){return n.split(/(\S.+?[.!?])(?=\s+|$)/)};var Ns=new D;Ns.tokenize=function(n){return n.split(/([{}:;,]|\s+)/)};function ee(n){"@babel/helpers - typeof";return typeof Symbol=="function"&&typeof Symbol.iterator=="symbol"?ee=function(t){return typeof t}:ee=function(t){return t&&typeof Symbol=="function"&&t.constructor===Symbol&&t!==Symbol.prototype?"symbol":typeof t},ee(n)}var Vs=Object.prototype.toString,X=new D;X.useLongestToken=!0;X.tokenize=fe.tokenize;X.castInput=function(n){var t=this.options,s=t.undefinedReplacement,i=t.stringifyReplacer,r=i===void 0?function(o,l){return typeof l>"u"?s:l}:i;return typeof n=="string"?n:JSON.stringify(ue(n,null,null,r),r,"  ")};X.equals=function(n,t){return D.prototype.equals.call(X,n.replace(/,([\r\n])/g,"$1"),t.replace(/,([\r\n])/g,"$1"))};function ue(n,t,s,i,r){t=t||[],s=s||[],i&&(n=i(r,n));var o;for(o=0;o<t.length;o+=1)if(t[o]===n)return s[o];var l;if(Vs.call(n)==="[object Array]"){for(t.push(n),l=new Array(n.length),s.push(l),o=0;o<n.length;o+=1)l[o]=ue(n[o],t,s,i,r);return t.pop(),s.pop(),l}if(n&&n.toJSON&&(n=n.toJSON()),ee(n)==="object"&&n!==null){t.push(n),l={},s.push(l);var d=[],u;for(u in n)n.hasOwnProperty(u)&&d.push(u);for(d.sort(),o=0;o<d.length;o+=1)u=d[o],l[u]=ue(n[u],t,s,i,u);t.pop(),s.pop()}else l=n;return l}var pe=new D;pe.tokenize=function(n){return n.slice()};pe.join=pe.removeEmpty=function(n){return n};function ae(n){const[t,s]=n.snippets,i=Hs(t.code,s.code),r=[],o=[];return i.forEach(({added:l,removed:d,value:u})=>{l?r.push("highlight-start:highlighted-row-addition\n"):d&&o.push("highlight-start:highlighted-row-removal\n"),l||o.push(u),d||r.push(u),(l||d)&&r.push("highlight-end\n")}),e.jsxs("div",{className:"code-comparison",children:[e.jsx(c,{copyEnabled:!1,title:t.title,code:o.join(""),highlighted:!0,language:t.language}),e.jsx(c,{copyEnabled:!1,title:s.title,code:r.join(""),highlighted:!0,language:s.language})]})}function Ne(n){const t=Object.assign({p:"p",code:"code",a:"a",h2:"h2",ol:"ol",li:"li",strong:"strong",ul:"ul",table:"table",thead:"thead",tr:"tr",th:"th",tbody:"tbody",td:"td",h4:"h4",h3:"h3"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(I,{level:1,slug:"agents",children:e.jsxs(e.Fragment,{children:["Migration Guide ",e.jsx(H,{children:"Beta"})]})}),"\n",e.jsxs(t.p,{children:["We have changed the way that tools and files work in the Assistants API between the ",e.jsx(t.code,{children:"v1"})," and ",e.jsx(t.code,{children:"v2"})," versions of the beta. Both versions of the beta continue to be accessible via the API today, but we recommend migrating to the newest version of our APIs as soon as feasible. We will deprecate ",e.jsx(t.code,{children:"v1"})," of the beta by the end of 2024."]}),"\n",e.jsx(b,{children:e.jsxs(t.p,{children:["If you do not use tools or files with the Assistants API today, there should be no changes required for you to migrate from the ",e.jsx(t.code,{children:"v1"})," version to the ",e.jsx(t.code,{children:"v2"})," version of the beta. Simply pass the ",e.jsxs(t.a,{href:"/docs/assistants/migration/changing-beta-versions",children:[e.jsx(t.code,{children:"v2"})," beta version header"]})," and/or move to the latest version of our Node and Python SDKs!"]})}),"\n",e.jsx(t.h2,{children:"What has changed"}),"\n",e.jsxs(t.p,{children:["The ",e.jsx(t.code,{children:"v2"})," version of the Assistants API contains the following changes:"]}),"\n",e.jsxs(t.ol,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Tool rename:"})," The ",e.jsx(t.code,{children:"retrieval"})," tool has been renamed to the ",e.jsx(t.code,{children:"file_search"})," tool"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Files belong to tools:"})," Files are now associated with tools instead of Assistants and Messages. This means that:","\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"AssistantFile"})," and ",e.jsx(t.code,{children:"MessageFile"})," objects no longer exist."]}),"\n",e.jsxs(t.li,{children:["Instead of ",e.jsx(t.code,{children:"AssistantFile"})," and ",e.jsx(t.code,{children:"MessageFile"}),", files are attached to Assistants and ",e.jsx(t.strong,{children:"Threads"})," using the new ",e.jsx(t.code,{children:"tool_resources"})," object.","\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["The ",e.jsx(t.code,{children:"tool_resources"})," for the code interpreter tool are a list of ",e.jsx(t.code,{children:"file_ids"}),"."]}),"\n",e.jsxs(t.li,{children:["The ",e.jsx(t.code,{children:"tool_resources"})," for the ",e.jsx(t.code,{children:"file_search"})," tool are a new object called a ",e.jsx(t.code,{children:"vector_stores"}),"."]}),"\n"]}),"\n"]}),"\n",e.jsxs(t.li,{children:["Messages now have an ",e.jsx(t.code,{children:"attachments"}),", rather than a ",e.jsx(t.code,{children:"file_ids"})," parameter. Message attachments are helpers that add the files to a Thread’s ",e.jsx(t.code,{children:"tool_resources"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n",e.jsx(ae,{snippets:[{language:"json",code:'\n{\n  "id": "asst_abc123",\n  "object": "assistant",\n  "created_at": 1698984975,\n  "name": "Math Tutor",\n  "description": null,\n  "model": "gpt-4-turbo",\n  "instructions": "You are a personal math tutor. When asked a question, write and run Python code to answer the question.",\n  "tools": [{ "type": "code_interpreter" }],\n  "file_ids": [],\n  "metadata": {}\n}\n',title:"V1 Assistant"},{language:"json",code:'\n{\n  "id": "asst_abc123",\n  "object": "assistant",\n  "created_at": 1698984975,\n  "name": "Math Tutor",\n  "description": null,\n  "model": "gpt-4-turbo",\n  "instructions": "You are a personal math tutor. When asked a question, write and run Python code to answer the question.",\n  "tools": [\n    {\n      "type": "code_interpreter"\n    },\n    {\n      "type": "file_search"\n    }\n  ],\n  "tool_resources": {\n    "file_search": {\n      "vector_store_ids": ["vs_abc"]\n    },\n    "code_interpreter": {\n      "file_ids": ["file-123", "file-456"]\n    }\n  }\n}\n',title:"V2 Assistant"}]}),"\n",e.jsxs(t.p,{children:["Assistants have ",e.jsx(t.code,{children:"tools"})," and ",e.jsx(t.code,{children:"tool_resources"})," instead of ",e.jsx(t.code,{children:"file_ids"}),". The ",e.jsx(t.code,{children:"retrieval"})," tool is now the ",e.jsx(t.code,{children:"file_search"})," tool. The ",e.jsx(t.code,{children:"tool_resource"})," for the ",e.jsx(t.code,{children:"file_search"})," tool is a ",e.jsx(t.code,{children:"vector_store"}),"."]}),"\n",e.jsx(ae,{snippets:[{language:"json",code:'\n{\n  "id": "thread_abc123",\n  "object": "thread",\n  "created_at": 1699012949,\n  "metadata": {}\n}\n',title:"V1 Thread"},{language:"json",code:'\n{\n  "id": "thread_abc123",\n  "object": "thread",\n  "created_at": 1699012949,\n  "metadata": {},\n  "tools": [\n    {\n      "type": "file_search"\n    },\n    {\n      "type": "code_interpreter"\n    }\n  ],\n  "tool_resources": {\n    "file_search": {\n      "vector_store_ids": ["vs_abc"]\n    },\n    "code_interpreter": {\n      "file_ids": ["file-123", "file-456"]\n    }\n  }\n}\n',title:"V2 Thread"}]}),"\n",e.jsxs(t.p,{children:["Threads can bring their own ",e.jsx(t.code,{children:"tool_resources"})," into a conversation."]}),"\n",e.jsx(ae,{snippets:[{title:"V1 Message",language:"json",code:'\n{\n  "id": "msg_abc123",\n  "object": "thread.message",\n  "created_at": 1698983503,\n  "thread_id": "thread_abc123",\n  "role": "assistant",\n  "content": [\n    {\n      "type": "text",\n      "text": {\n        "value": "Hi! How can I help you today?",\n        "annotations": []\n      }\n    }\n  ],\n  "assistant_id": "asst_abc123",\n  "run_id": "run_abc123",\n  "metadata": {},\n  "file_ids": []\n}\n'},{title:"V2 Message",language:"json",code:'\n{\n  "id": "msg_abc123",\n  "object": "thread.message",\n  "created_at": 1698983503,\n  "thread_id": "thread_abc123",\n  "role": "assistant",\n  "content": [\n    {\n      "type": "text",\n      "text": {\n        "value": "Hi! How can I help you today?",\n        "annotations": []\n      }\n    }\n  ],\n  "assistant_id": "asst_abc123",\n  "run_id": "run_abc123",\n  "metadata": {},\n  "attachments": [\n    {\n      "file_id": "file-123",\n      "tools": [\n        { "type": "file_search" },\n        { "type": "code_interpreter" }\n      ]\n    }\n  ]\n}\n'}]}),"\n",e.jsxs(t.p,{children:["Messages have ",e.jsx(t.code,{children:"attachments"})," instead of ",e.jsx(t.code,{children:"file_ids"}),". ",e.jsx(t.code,{children:"attachments"})," are helpers that add files to the Thread’s ",e.jsx(t.code,{children:"tool_resources"}),"."]}),"\n",e.jsxs(t.p,{children:["All ",e.jsx(t.code,{children:"v1"})," endpoints and objects for the Assistants API can be found under the ",e.jsx(t.a,{href:"/docs/api-reference/assistants-v1",children:"Legacy"})," section of the API reference."]}),"\n",e.jsx(t.h2,{children:"Accessing v1 data in v2"}),"\n",e.jsxs(t.p,{children:["To make your migration simple between our ",e.jsx(t.code,{children:"v1"})," and ",e.jsx(t.code,{children:"v2"})," APIs, we automatically map ",e.jsx(t.code,{children:"AssistantFiles"})," and ",e.jsx(t.code,{children:"MessageFiles"})," to the appropriate ",e.jsx(t.code,{children:"tool_resources"})," based on the tools that are enabled in Assistants or Runs these files are a part of."]}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{align:"left"}),e.jsxs(t.th,{align:"left",children:[e.jsx(t.code,{children:"v1"})," version"]}),e.jsxs(t.th,{align:"left",children:[e.jsx(t.code,{children:"v2"})," version"]})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsxs(t.td,{align:"left",children:["AssistantFiles for ",e.jsx(t.code,{children:"code_interpreter"})]}),e.jsxs(t.td,{align:"left",children:[e.jsx(t.code,{children:"file_ids"})," on Assistant"]}),e.jsxs(t.td,{align:"left",children:["Files in an Assistant’s ",e.jsx(t.code,{children:"tool_resources.code_interpreter"})]})]}),e.jsxs(t.tr,{children:[e.jsxs(t.td,{align:"left",children:["AssistantFiles for ",e.jsx(t.code,{children:"retrieval"})]}),e.jsxs(t.td,{align:"left",children:[e.jsx(t.code,{children:"file_ids"})," on Assistant"]}),e.jsxs(t.td,{align:"left",children:["Files in a vector_store attached to an Assistant (",e.jsx(t.code,{children:"tool_resources.file_search"}),")"]})]}),e.jsxs(t.tr,{children:[e.jsxs(t.td,{align:"left",children:["MessageFiles for ",e.jsx(t.code,{children:"code_interpreter"})]}),e.jsxs(t.td,{align:"left",children:[e.jsx(t.code,{children:"file_ids"})," on Message"]}),e.jsxs(t.td,{align:"left",children:["Files in an Thread’s ",e.jsx(t.code,{children:"tool_resources.code_interpreter"})]})]}),e.jsxs(t.tr,{children:[e.jsxs(t.td,{align:"left",children:["MessageFiles for ",e.jsx(t.code,{children:"retrieval"})]}),e.jsxs(t.td,{align:"left",children:[e.jsx(t.code,{children:"file_ids"})," on Message"]}),e.jsxs(t.td,{align:"left",children:["Files in a vector_store attached to a Thread (",e.jsx(t.code,{children:"tool_resources.file_search"}),")"]})]})]})]}),"\n",e.jsx(b,{children:e.jsxs(t.p,{children:["It's important to note that while ",e.jsx(t.code,{children:"file_ids"})," from ",e.jsx(t.code,{children:"v1"})," are mapped to ",e.jsx(t.code,{children:"tool_resources"})," in ",e.jsx(t.code,{children:"v2"}),", the inverse is not true. Changes you make to ",e.jsx(t.code,{children:"tool_resources"})," in ",e.jsx(t.code,{children:"v2"})," will not be reflected as ",e.jsx(t.code,{children:"file_ids"})," in ",e.jsx(t.code,{children:"v1"}),"."]})}),"\n",e.jsxs(t.p,{children:["Because Assistant Files and Message Files are already mapped to the appropriate ",e.jsx(t.code,{children:"tool_resources"})," in ",e.jsx(t.code,{children:"v2"}),", when you’re ready to migrate to ",e.jsx(t.code,{children:"v2"})," you shouldn't have to worry about a data migration. Instead, you only need to:"]}),"\n",e.jsxs(t.ol,{children:["\n",e.jsxs(t.li,{children:["Update your integration to reflect the new API and objects. You may need to do things like:","\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["Migrate to creating ",e.jsx(t.code,{children:"vector_stores"})," and using ",e.jsx(t.code,{children:"file_search"}),", if you were using the ",e.jsx(t.code,{children:"retrieval"})," tool. Importantly, since these operations are asynchronous, you’ll want to ensure files are ",e.jsx(t.a,{href:"/docs/assistants/tools/file-search/ensure-readiness-before-creating-runs",children:"successfully ingested"})," by the ",e.jsx(t.code,{children:"vector_stores"})," before creating run."]}),"\n",e.jsxs(t.li,{children:["Migrate to adding files to ",e.jsx(t.code,{children:"tool_resources.code_interpreter"})," instead of an Assistant or Message’s files, if you were using the ",e.jsx(t.code,{children:"code_interpreter"})," tool."]}),"\n",e.jsxs(t.li,{children:["Migrate to using Message ",e.jsx(t.code,{children:"attachments"})," instead of ",e.jsx(t.code,{children:"file_ids"}),"."]}),"\n"]}),"\n"]}),"\n",e.jsx(t.li,{children:"Upgrade to the latest version of our SDKs"}),"\n"]}),"\n",e.jsx(t.h2,{children:"Changing beta versions"}),"\n",e.jsx(t.h4,{children:"Without SDKs"}),"\n",e.jsx(t.p,{children:"Both beta versions can be accessed by passing the right API version header in your API requests:"}),"\n",e.jsxs(t.ol,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"v1"}),": ",e.jsx(t.code,{children:"OpenAI-Beta: assistants=v1"})]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"v2"}),": ",e.jsx(t.code,{children:"OpenAI-Beta: assistants=v2"})]}),"\n"]}),"\n",e.jsx(c,{defaultLanguage:"v2",code:{v1:'\ncurl "https://api.openai.com/v1/assistants" \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -H "OpenAI-Beta: assistants=v1" \\\n  -d \'{\n    "instructions": "You are a personal math tutor. When asked a question, write and run Python code to answer the question.",\n    "name": "Math Tutor",\n    "tools": [{"type": "code_interpreter"}],\n    "model": "gpt-4-turbo"\n  }\'\n'.trim(),v2:'\ncurl "https://api.openai.com/v1/assistants" \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -H "OpenAI-Beta: assistants=v2" \\\n  -d \'{\n    "instructions": "You are a personal math tutor. When asked a question, write and run Python code to answer the question.",\n    "name": "Math Tutor",\n    "tools": [{"type": "code_interpreter"}],\n    "model": "gpt-4-turbo"\n  }\'\n'.trim()}}),"\n",e.jsx(t.h3,{children:"With SDKs"}),"\n",e.jsxs(t.p,{children:["Versions of our SDKs that are released after the release of the ",e.jsx(t.strong,{children:e.jsx(t.code,{children:"v2"})})," beta will have the ",e.jsx(t.strong,{children:e.jsx(t.code,{children:"openai.beta"})})," namespace\npoint to the ",e.jsx(t.strong,{children:e.jsx(t.code,{children:"v2"})})," version of the API by default. You can still access the ",e.jsx(t.strong,{children:e.jsx(t.code,{children:"v1"})})," version of the API by using an older\nversion of the SDK (1.20.0 or earlier for python, 4.36.0 or earlier for node) or by overriding the version header."]}),"\n",e.jsx(t.p,{children:"To install an older version of the SDK, you can use the following commands:"}),"\n",e.jsx(c,{title:"Installing older versions of the SDK",defaultLanguage:"python",code:{python:"\npip install openai==1.20.0\n".trim(),"node.js":"\nnpm install openai@4.36.0\n".trim()}}),"\n",e.jsxs(t.p,{children:["You can also override this header in a newer SDK version, but we don't recommend this approach since the object types in these newer SDK versions will be different from the ",e.jsx(t.code,{children:"v1"})," objects."]}),"\n",e.jsx(c,{title:"Accessing the `v1` API version in new SDKs",defaultLanguage:"python",code:{python:'\nfrom openai import OpenAI\n\nclient = OpenAI(default_headers={"OpenAI-Beta": "assistants=v1"})\n'.trim(),"node.js":'\nimport OpenAI from "openai";\n\nconst openai = new OpenAI({ defaultHeaders: {"OpenAI-Beta": "assistants=v1"} });\n'.trim()}}),"\n",e.jsx(t.h2,{children:"Billing"}),"\n",e.jsxs(t.p,{children:["All ",e.jsx(t.a,{href:"/docs/api-reference/vector-stores/object",children:"vector stores"})," created before the release of the ",e.jsx(t.code,{children:"v2"})," API (April 17, 2024) will be free to use until the end of 2024. This implies that any vector stores that were created as a result of us mapping your ",e.jsx(t.code,{children:"v1"})," data to ",e.jsx(t.code,{children:"v2"}),", before the ",e.jsx(t.code,{children:"v2"})," launch will be free. After the end of 2024, they’ll be billed at whatever the fees for vector stores are at that point. See our ",e.jsx(t.a,{href:"https://openai.com/pricing",children:"pricing page"})," for the latest pricing information."]}),"\n",e.jsxs(t.p,{children:["Any vector store that is created before the release of the ",e.jsx(t.code,{children:"v2"})," API (April 17, 2024) but not used in a single Run between that release date and the end of 2024 will be deleted. This is to avoid us starting to bill you for something you created during the beta but never used."]}),"\n",e.jsxs(t.p,{children:["Vector stores created after the release of the ",e.jsx(t.code,{children:"v2"})," API will be billed at current rates as specified on the ",e.jsx(t.a,{href:"https://openai.com/pricing",children:"pricing page"}),"."]}),"\n",e.jsx(t.h2,{children:"Deleting files"}),"\n",e.jsxs(t.p,{children:["Deleting Assistant Files / Message Files via the ",e.jsx(t.code,{children:"v1"})," API also removes them from the ",e.jsx(t.code,{children:"v2"})," API. However, the inverse is not true - deletions in the ",e.jsx(t.code,{children:"v2"})," version of the API do not propogate to ",e.jsx(t.code,{children:"v1"}),". If you created a file on ",e.jsx(t.code,{children:"v1"}),' and would like to "fully" delete a file from your account on both ',e.jsx(t.code,{children:"v1"})," and ",e.jsx(t.code,{children:"v2"})," you should:"]}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["delete Assistant Files / Message Files you create using ",e.jsx(t.code,{children:"v1"})," APIs using the ",e.jsx(t.code,{children:"v1"})," endpoints, or"]}),"\n",e.jsxs(t.li,{children:["delete the underlying ",e.jsx(t.a,{href:"/docs/api-reference/files/delete",children:"file object"})," — this ensures it is fully removed from all objects in all versions of the API."]}),"\n"]}),"\n",e.jsx(t.h2,{children:"Playground"}),"\n",e.jsxs(t.p,{children:["The default playground experience has been migrated to use the ",e.jsx(t.code,{children:"v2"})," version of the API (you will still have a read-only view of the ",e.jsx(t.code,{children:"v1"})," version of objects, but will not be able to edit them). Any changes you make to tools and files via the Playground will only be accessible in the ",e.jsx(t.code,{children:"v2"})," version of the API."]}),"\n",e.jsxs(t.p,{children:["In order to make changes to files in the ",e.jsx(t.code,{children:"v1"})," version of the API, you will need to use the API directly."]})]})}function zs(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Ne,n)})):Ne(n)}function Ve(n){const t=Object.assign({p:"p",a:"a",div:"div",strong:"strong",ol:"ol",li:"li",h2:"h2",img:"img",table:"table",thead:"thead",tr:"tr",th:"th",tbody:"tbody",td:"td"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(I,{level:1,slug:"agents",children:e.jsxs(t.p,{children:["Assistants API Overview",e.jsx(H,{children:"Beta"})]})}),"\n",e.jsxs(t.p,{children:["The Assistants API allows you to build AI assistants within your own applications. An Assistant has instructions and can leverage models, tools, and files to respond to user queries.\nThe Assistants API currently supports three types of ",e.jsx(t.a,{href:"/docs/assistants/tools",children:"tools"}),": Code Interpreter, File Search, and Function calling."]}),"\n",e.jsxs(t.p,{children:["You can explore the capabilities of the Assistants API using the ",e.jsx(t.a,{href:"/playground?mode=assistant",children:"Assistants playground"})," or by building a step-by-step integration outlined in our ",e.jsx(t.a,{href:"/docs/assistants/quickstart",children:"Assistants API quickstart"}),"."]}),"\n",e.jsx(I,{level:2,slug:"how-assistants-work",children:e.jsx(t.p,{children:"How Assistants work"})}),"\n",e.jsx(t.p,{children:"The Assistants API is designed to help developers build powerful AI assistants capable of performing a variety of tasks."}),"\n",e.jsx(ne,{className:"mt-2 mb-2",icon:!1,color:xe.primary,body:e.jsxs(t.div,{className:"docs-samples-notice",children:["The Assistants API is in ",e.jsx(t.strong,{children:"beta"})," and we are actively working on adding more functionality. Share your feedback in our"," ",e.jsx(t.a,{href:"https://community.openai.com/",children:"Developer Forum"}),"!"]}),textSize:ge.large,dismissable:!1}),"\n",e.jsxs(t.ol,{children:["\n",e.jsxs(t.li,{children:["Assistants can call OpenAI’s ",e.jsx(t.strong,{children:e.jsx(t.a,{href:"/docs/models",children:"models"})})," with specific instructions to tune their personality and capabilities."]}),"\n",e.jsxs(t.li,{children:["Assistants can access ",e.jsx(t.strong,{children:"multiple tools in parallel"}),". These can be both OpenAI-hosted tools — like ",e.jsx(t.a,{href:"/docs/assistants/tools/code-interpreter",children:"code_interpreter"})," and ",e.jsx(t.a,{href:"/docs/assistants/tools/file-search",children:"file_search"})," — or tools you build / host (via ",e.jsx(t.a,{href:"/docs/assistants/tools/function-calling",children:"function calling"}),")."]}),"\n",e.jsxs(t.li,{children:["Assistants can access ",e.jsx(t.strong,{children:"persistent Threads"}),". Threads simplify AI application development by storing message history and truncating it when the conversation gets too long for the model’s context length. You create a Thread once, and simply append Messages to it as your users reply."]}),"\n",e.jsx(t.li,{children:"Assistants can access files in several formats — either as part of their creation or as part of Threads between Assistants and users. When using tools, Assistants can also create files (e.g., images, spreadsheets, etc) and cite files they reference in the Messages they create."}),"\n"]}),"\n",e.jsx(t.h2,{children:"Objects"}),"\n",e.jsx(t.p,{children:e.jsx(t.img,{src:"https://cdn.openai.com/API/docs/images/diagram-assistant.webp",alt:"Assistants object architecture diagram"})}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Object"}),e.jsx(t.th,{children:"What it represents"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"Assistant"}),e.jsxs(t.td,{children:["Purpose-built AI that uses OpenAI’s ",e.jsx(t.a,{href:"/docs/models",children:"models"})," and calls ",e.jsx(t.a,{href:"/docs/assistants/tools",children:"tools"})]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"Thread"}),e.jsx(t.td,{children:"A conversation session between an Assistant and a user. Threads store Messages and automatically handle truncation to fit content into a model’s context."})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"Message"}),e.jsx(t.td,{children:"A message created by an Assistant or a user. Messages can include text, images, and other files. Messages stored as a list on the Thread."})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"Run"}),e.jsx(t.td,{children:"An invocation of an Assistant on a Thread. The Assistant uses its configuration and the Thread’s Messages to perform tasks by calling models and tools. As part of a Run, the Assistant appends Messages to the Thread."})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"Run Step"}),e.jsx(t.td,{children:"A detailed list of steps the Assistant took as part of a Run. An Assistant can call tools or create Messages during its run. Examining Run Steps allows you to introspect how the Assistant is getting to its final results."})]})]})]})]})}function Ds(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Ve,n)})):Ve(n)}function ze(n){const t=Object.assign({p:"p",ol:"ol",li:"li",a:"a",h2:"h2",code:"code"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(I,{level:1,slug:"agents",children:e.jsxs(t.p,{children:["Assistants API Quickstart ",e.jsx(H,{children:"Beta"})]})}),"\n",e.jsx(t.p,{children:"A typical integration of the Assistants API has the following flow:"}),"\n",e.jsxs(t.ol,{children:["\n",e.jsxs(t.li,{children:["Create an ",e.jsx(t.a,{href:"/docs/api-reference/assistants/createAssistant",children:"Assistant"})," by defining its custom instructions and picking a model. If helpful, add files and enable tools like Code Interpreter, File Search, and Function calling."]}),"\n",e.jsxs(t.li,{children:["Create a ",e.jsx(t.a,{href:"/docs/api-reference/threads",children:"Thread"})," when a user starts a conversation."]}),"\n",e.jsxs(t.li,{children:["Add ",e.jsx(t.a,{href:"/docs/api-reference/messages",children:"Messages"})," to the Thread as the user asks questions."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"/docs/api-reference/runs",children:"Run"})," the Assistant on the Thread to generate a response by calling the model and the tools."]}),"\n"]}),"\n",e.jsxs(t.p,{children:["This starter guide walks through the key steps to create and run an Assistant that uses ",e.jsx(t.a,{href:"/docs/assistants/tools/code-interpreter",children:"Code Interpreter"}),". In this example, we're ",e.jsx(t.a,{href:"/docs/api-reference/assistants/createAssistant",children:"creating an Assistant"})," that is a personal math tutor, with the Code Interpreter tool enabled."]}),"\n",e.jsx(b,{children:e.jsxs(t.p,{children:["Calls to the Assistants API require that you pass a beta HTTP header. This is handled\nautomatically if you’re using OpenAI’s official Python or Node.js SDKs.\n",e.jsx("code",{children:"OpenAI-Beta: assistants=v2"})]})}),"\n",e.jsx(t.h2,{children:"Step 1: Create an Assistant"}),"\n",e.jsxs(t.p,{children:["An ",e.jsx(t.a,{href:"/docs/api-reference/assistants/object",children:"Assistant"})," represents an entity that can be configured to respond to a user's messages using several parameters like ",e.jsx(t.code,{children:"model"}),", ",e.jsx(t.code,{children:"instructions"}),", and ",e.jsx(t.code,{children:"tools"}),"."]}),"\n",e.jsx(c,{title:"Create an Assistant",defaultLanguage:"python",code:{python:'\nfrom openai import OpenAI\nclient = OpenAI()\n\nassistant = client.beta.assistants.create(\n  name="Math Tutor",\n  instructions="You are a personal math tutor. Write and run code to answer math questions.",\n  tools=[{"type": "code_interpreter"}],\n  model="gpt-4o",\n)\n'.trim(),"node.js":'\nimport OpenAI from "openai";\nconst openai = new OpenAI();\n\nasync function main() {\n  const assistant = await openai.beta.assistants.create({\n    name: "Math Tutor",\n    instructions: "You are a personal math tutor. Write and run code to answer math questions.",\n    tools: [{ type: "code_interpreter" }],\n    model: "gpt-4o"\n  });\n}\n\nmain();\n'.trim(),curl:'\ncurl "https://api.openai.com/v1/assistants" \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -H "OpenAI-Beta: assistants=v2" \\\n  -d \'{\n    "instructions": "You are a personal math tutor. Write and run code to answer math questions.",\n    "name": "Math Tutor",\n    "tools": [{"type": "code_interpreter"}],\n    "model": "gpt-4o"\n  }\'\n'.trim()}}),"\n",e.jsx(t.h2,{children:"Step 2: Create a Thread"}),"\n",e.jsxs(t.p,{children:["A ",e.jsx(t.a,{href:"/docs/api-reference/threads/object",children:"Thread"})," represents a conversation between a user and one or many Assistants. You can create a Thread when a user (or your AI application) starts a conversation with your Assistant."]}),"\n",e.jsx(c,{title:"Create a Thread",defaultLanguage:"python",code:{python:"\nthread = client.beta.threads.create()\n".trim(),"node.js":"\nconst thread = await openai.beta.threads.create();\n".trim(),curl:'\ncurl https://api.openai.com/v1/threads \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -H "OpenAI-Beta: assistants=v2" \\\n  -d \'\'\n'.trim()}}),"\n",e.jsx(t.h2,{children:"Step 3: Add a Message to the Thread"}),"\n",e.jsxs(t.p,{children:["The contents of the messages your users or applications create are added as ",e.jsx(t.a,{href:"/docs/api-reference/messages/object",children:"Message"})," objects to the Thread. Messages can contain both text and files. There is a limit of 100,000 Messages per Thread and we smartly truncate any context that does not fit into the model's context window."]}),"\n",e.jsx(c,{title:"Add a Message to the Thread",defaultLanguage:"python",code:{python:'\nmessage = client.beta.threads.messages.create(\n  thread_id=thread.id,\n  role="user",\n  content="I need to solve the equation `3x + 11 = 14`. Can you help me?"\n)\n'.trim(),"node.js":'\nconst message = await openai.beta.threads.messages.create(\n  thread.id,\n  {\n    role: "user",\n    content: "I need to solve the equation `3x + 11 = 14`. Can you help me?"\n  }\n);\n'.trim(),curl:'\ncurl https://api.openai.com/v1/threads/thread_abc123/messages \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -H "OpenAI-Beta: assistants=v2" \\\n  -d \'{\n      "role": "user",\n      "content": "I need to solve the equation `3x + 11 = 14`. Can you help me?"\n    }\'\n'.trim()}}),"\n",e.jsx(t.h2,{children:"Step 4: Create a Run"}),"\n",e.jsxs(t.p,{children:["Once all the user Messages have been added to the Thread, you can ",e.jsx(t.a,{href:"/docs/api-reference/runs/object",children:"Run"})," the Thread with any Assistant. Creating a Run uses the model and tools associated with the Assistant to generate a response. These responses are added to the Thread as ",e.jsx(t.code,{children:"assistant"})," Messages."]}),"\n",e.jsx(G,{options:[{value:"with-streaming",label:"With streaming",content:e.jsx(Os,{})},{value:"without-streaming",label:"Without streaming",content:e.jsx(Ss,{})}],initialValue:"with-streaming"}),"\n",e.jsx(t.h2,{children:"Next steps"}),"\n",e.jsxs(t.ol,{children:["\n",e.jsxs(t.li,{children:["Continue learning about Assistants Concepts in the ",e.jsx(t.a,{href:"/docs/assistants/deep-dive",children:"Deep Dive"})]}),"\n",e.jsxs(t.li,{children:["Learn more about ",e.jsx(t.a,{href:"/docs/assistants/tools",children:"Tools"})]}),"\n",e.jsxs(t.li,{children:["Explore the ",e.jsx(t.a,{href:"/playground?mode=assistant",children:"Assistants playground"})]}),"\n",e.jsxs(t.li,{children:["Check out our ",e.jsx(t.a,{href:"https://github.com/openai/openai-assistants-quickstart",children:"Assistants Quickstart app"})," on github"]}),"\n"]})]})}function Fs(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(ze,n)})):ze(n)}function De(n){const t=Object.assign({p:"p",a:"a",h2:"h2",h3:"h3",code:"code",ol:"ol",li:"li",pre:"pre",table:"table",thead:"thead",tr:"tr",th:"th",tbody:"tbody",td:"td"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(I,{level:1,children:e.jsxs(t.p,{children:["Code Interpreter ",e.jsx(H,{children:"Beta"})]})}),"\n",e.jsx(t.p,{children:"Code Interpreter allows Assistants to write and run Python code in a sandboxed execution environment. This tool can process files with diverse data and formatting, and generate files with data and images of graphs. Code Interpreter allows your Assistant to run code iteratively to solve challenging code and math problems. When your Assistant writes code that fails to run, it can iterate on this code by attempting to run different code until the code execution succeeds."}),"\n",e.jsxs(t.p,{children:["See a quickstart of how to get started with Code Interpreter ",e.jsx(t.a,{href:"/docs/assistants/overview/step-1-create-an-assistant?context=with-streaming",children:"here"}),"."]}),"\n",e.jsx(t.h2,{children:"How it works"}),"\n",e.jsx(t.p,{children:"Code Interpreter is charged at $0.03 per session. If your Assistant calls Code Interpreter simultaneously in two different threads (e.g., one thread per end-user), two Code Interpreter sessions are created. Each session is active by default for one hour, which means that you only pay for one session per if users interact with Code Interpreter in the same thread for up to one hour."}),"\n",e.jsx(t.h3,{children:"Enabling Code Interpreter"}),"\n",e.jsxs(t.p,{children:["Pass ",e.jsx(t.code,{children:"code_interpreter"})," in the ",e.jsx(t.code,{children:"tools"})," parameter of the Assistant object to enable Code Interpreter:"]}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\nassistant = client.beta.assistants.create(\n  instructions="You are a personal math tutor. When asked a math question, write and run code to answer the question.",\n  model="gpt-4o",\n  tools=[{"type": "code_interpreter"}]\n)\n'.trim(),"node.js":'\nconst assistant = await openai.beta.assistants.create({\n  instructions: "You are a personal math tutor. When asked a math question, write and run code to answer the question.",\n  model: "gpt-4o",\n  tools: [{"type": "code_interpreter"}]\n});\n'.trim(),curl:'\ncurl https://api.openai.com/v1/assistants \\\n  -u :$OPENAI_API_KEY \\\n  -H \'Content-Type: application/json\' \\\n  -H \'OpenAI-Beta: assistants=v2\' \\\n  -d \'{\n    "instructions": "You are a personal math tutor. When asked a math question, write and run code to answer the question.",\n    "tools": [\n      { "type": "code_interpreter" }\n    ],\n    "model": "gpt-4o"\n  }\'\n'.trim()}}),"\n",e.jsxs(t.p,{children:["The model then decides when to invoke Code Interpreter in a Run based on the nature of the user request. This behavior can be promoted by prompting in the Assistant's ",e.jsx(t.code,{children:"instructions"})," (e.g., “write code to solve this problem”)."]}),"\n",e.jsx(t.h3,{children:"Passing files to Code Interpreter"}),"\n",e.jsx(t.p,{children:"Files that are passed at the Assistant level are accessible by all Runs with this Assistant:"}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\n# Upload a file with an "assistants" purpose\nfile = client.files.create(\n  file=open("mydata.csv", "rb"),\n  purpose=\'assistants\'\n)\n\n# Create an assistant using the file ID\nassistant = client.beta.assistants.create(\n  instructions="You are a personal math tutor. When asked a math question, write and run code to answer the question.",\n  model="gpt-4o",\n  tools=[{"type": "code_interpreter"}],\n  tool_resources={\n    "code_interpreter": {\n      "file_ids": [file.id]\n    }\n  }\n)\n'.trim(),"node.js":'\n// Upload a file with an "assistants" purpose\nconst file = await openai.files.create({\n  file: fs.createReadStream("mydata.csv"),\n  purpose: "assistants",\n});\n\n// Create an assistant using the file ID\nconst assistant = await openai.beta.assistants.create({\n  instructions: "You are a personal math tutor. When asked a math question, write and run code to answer the question.",\n  model: "gpt-4o",\n  tools: [{"type": "code_interpreter"}],\n  tool_resources: {\n    "code_interpreter": {\n      "file_ids": [file.id]\n    }\n  }\n});\n'.trim(),curl:'\n# Upload a file with an "assistants" purpose\ncurl https://api.openai.com/v1/files \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -F purpose="assistants" \\\n  -F file="@/path/to/mydata.csv"\n\n# Create an assistant using the file ID\ncurl https://api.openai.com/v1/assistants \\\n  -u :$OPENAI_API_KEY \\\n  -H \'Content-Type: application/json\' \\\n  -H \'OpenAI-Beta: assistants=v2\' \\\n  -d \'{\n    "instructions": "You are a personal math tutor. When asked a math question, write and run code to answer the question.",\n    "tools": [{"type": "code_interpreter"}],\n    "model": "gpt-4o",\n    "tool_resources": {\n      "code_interpreter": {\n        "file_ids": ["file-BK7bzQj3FfZFXr7DbL6xJwfo"]\n      }\n    }\n  }\'\n'.trim()}}),"\n",e.jsxs(t.p,{children:["Files can also be passed at the Thread level. These files are only accessible in the specific Thread. Upload the File using the ",e.jsx(t.a,{href:"/docs/api-reference/files/create",children:"File upload"})," endpoint and then pass the File ID as part of the Message creation request:"]}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\nthread = client.beta.threads.create(\n  messages=[\n    {\n      "role": "user",\n      "content": "I need to solve the equation `3x + 11 = 14`. Can you help me?",\n      "attachments": [\n        {\n          "file_id": file.id,\n          "tools": [{"type": "code_interpreter"}]\n        }\n      ]\n    }\n  ]\n)\n'.trim(),"node.js":'\nconst thread = await openai.beta.threads.create({\n  messages: [\n    {\n      "role": "user",\n      "content": "I need to solve the equation `3x + 11 = 14`. Can you help me?",\n      "attachments": [\n        {\n          file_id: file.id,\n          tools: [{type: "code_interpreter"}]\n        }\n      ]\n    }\n  ]\n});\n'.trim(),curl:'\ncurl https://api.openai.com/v1/threads/thread_abc123/messages \\\n  -u :$OPENAI_API_KEY \\\n  -H \'Content-Type: application/json\' \\\n  -H \'OpenAI-Beta: assistants=v2\' \\\n  -d \'{\n    "role": "user",\n    "content": "I need to solve the equation `3x + 11 = 14`. Can you help me?",\n    "attachments": [\n      {\n        "file_id": "file-ACq8OjcLQm2eIG0BvRM4z5qX",\n        "tools": [{"type": "code_interpreter"}]\n      }\n    ]\n  }\'\n'.trim()}}),"\n",e.jsxs(t.p,{children:["Files have a maximum size of 512 MB. Code Interpreter supports a variety of file formats including ",e.jsx(t.code,{children:".csv"}),", ",e.jsx(t.code,{children:".pdf"}),", ",e.jsx(t.code,{children:".json"})," and many more. More details on the file extensions (and their corresponding MIME-types) supported can be found in the ",e.jsx(t.a,{href:"/docs/assistants/tools/code-interpreter/supported-files",children:"Supported files"})," section below."]}),"\n",e.jsx(t.h3,{children:"Reading images and files generated by Code Interpreter"}),"\n",e.jsx(t.p,{children:"Code Interpreter in the API also outputs files, such as generating image diagrams, CSVs, and PDFs. There are two types of files that are generated:"}),"\n",e.jsxs(t.ol,{children:["\n",e.jsx(t.li,{children:"Images"}),"\n",e.jsxs(t.li,{children:["Data files (e.g. a ",e.jsx(t.code,{children:"csv"})," file with data generated by the Assistant)"]}),"\n"]}),"\n",e.jsxs(t.p,{children:["When Code Interpreter generates an image, you can look up and download this file in the ",e.jsx(t.code,{children:"file_id"})," field of the Assistant Message response:"]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-json",children:'{\n	"id": "msg_abc123",\n	"object": "thread.message",\n	"created_at": 1698964262,\n	"thread_id": "thread_abc123",\n	"role": "assistant",\n	"content": [\n    {\n      "type": "image_file",\n      "image_file": {\n        "file_id": "file-abc123"\n      }\n    }\n  ]\n  # ...\n}\n'})}),"\n",e.jsx(t.p,{children:"The file content can then be downloaded by passing the file ID to the Files API:"}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nimage_data = client.files.content("file-abc123")\nimage_data_bytes = image_data.read()\n\nwith open("./my-image.png", "wb") as file:\n    file.write(image_data_bytes)\n'.trim(),"node.js":'\nimport fs from "fs";\nimport OpenAI from "openai";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const response = await openai.files.content("file-abc123");\n\n  // Extract the binary data from the Response object\n  const image_data = await response.arrayBuffer();\n\n  // Convert the binary data to a Buffer\n  const image_data_buffer = Buffer.from(image_data);\n\n  // Save the image to a specific location\n  fs.writeFileSync("./my-image.png", image_data_buffer);\n}\n\nmain();\n'.trim(),curl:'\ncurl https://api.openai.com/v1/files/file-abc123/content \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  --output image.png\n'.trim()}}),"\n",e.jsx(t.p,{children:"When Code Interpreter references a file path (e.g., ”Download this csv file”), file paths are listed as annotations. You can convert these annotations into links to download the file:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-json",children:'{\n  "id": "msg_abc123",\n  "object": "thread.message",\n  "created_at": 1699073585,\n  "thread_id": "thread_abc123",\n  "role": "assistant",\n  "content": [\n    {\n      "type": "text",\n      "text": {\n        "value": "The rows of the CSV file have been shuffled and saved to a new CSV file. You can download the shuffled CSV file from the following link:\\n\\n[Download Shuffled CSV File](sandbox:/mnt/data/shuffled_file.csv)",\n        "annotations": [\n          {\n            "type": "file_path",\n            "text": "sandbox:/mnt/data/shuffled_file.csv",\n            "start_index": 167,\n            "end_index": 202,\n            "file_path": {\n              "file_id": "file-abc123"\n            }\n          }\n          ...\n'})}),"\n",e.jsx(t.h3,{children:"Input and output logs of Code Interpreter"}),"\n",e.jsxs(t.p,{children:["By listing the steps of a Run that called Code Interpreter, you can inspect the code ",e.jsx(t.code,{children:"input"})," and ",e.jsx(t.code,{children:"outputs"})," logs of Code Interpreter:"]}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:"\nrun_steps = client.beta.threads.runs.steps.list(\n  thread_id=thread.id,\n  run_id=run.id\n)\n".trim(),"node.js":"\nconst runSteps = await openai.beta.threads.runs.steps.list(\n  thread.id,\n  run.id\n);\n".trim(),curl:'\ncurl https://api.openai.com/v1/threads/thread_abc123/runs/RUN_ID/steps \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -H "OpenAI-Beta: assistants=v2" \\\n'.trim()}}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-bash",children:'{\n  "object": "list",\n  "data": [\n    {\n      "id": "step_abc123",\n      "object": "thread.run.step",\n      "type": "tool_calls",\n      "run_id": "run_abc123",\n      "thread_id": "thread_abc123",\n      "status": "completed",\n      "step_details": {\n        "type": "tool_calls",\n        "tool_calls": [\n          {\n            "type": "code",\n            "code": {\n              "input": "# Calculating 2 + 2\\nresult = 2 + 2\\nresult",\n              "outputs": [\n                {\n                  "type": "logs",\n                  "logs": "4"\n                }\n						...\n }\n'})}),"\n",e.jsx(t.h2,{children:"Supported files"}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"File format"}),e.jsx(t.th,{children:"MIME type"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".c"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text/x-c"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".cs"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text/x-csharp"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".cpp"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text/x-c++"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".doc"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"application/msword"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".docx"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"application/vnd.openxmlformats-officedocument.wordprocessingml.document"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".html"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text/html"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".java"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text/x-java"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".json"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"application/json"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".md"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text/markdown"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".pdf"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"application/pdf"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".php"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text/x-php"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".pptx"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"application/vnd.openxmlformats-officedocument.presentationml.presentation"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".py"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text/x-python"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".py"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text/x-script.python"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".rb"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text/x-ruby"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".tex"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text/x-tex"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".txt"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text/plain"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".css"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text/css"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".js"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text/javascript"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".sh"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"application/x-sh"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".ts"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"application/typescript"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".csv"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"application/csv"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".jpeg"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"image/jpeg"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".jpg"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"image/jpeg"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".gif"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"image/gif"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".pkl"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"application/octet-stream"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".png"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"image/png"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".tar"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"application/x-tar"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".xlsx"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".xml"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:'application/xml or "text/xml"'})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".zip"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"application/zip"})})]})]})]})]})}function Rs(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(De,n)})):De(n)}function Fe(n){const t=Object.assign({p:"p",h2:"h2",h3:"h3",code:"code",a:"a",hr:"hr",ul:"ul",li:"li",strong:"strong",ol:"ol",h4:"h4",em:"em",table:"table",thead:"thead",tr:"tr",th:"th",tbody:"tbody",td:"td"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(I,{level:1,children:e.jsxs(t.p,{children:["File Search ",e.jsx(H,{children:"Beta"})]})}),"\n",e.jsx(t.p,{children:"File Search augments the Assistant with knowledge from outside its model, such as proprietary product information or documents provided by your users. OpenAI automatically parses and chunks your documents, creates and stores the embeddings, and use both vector and keyword search to retrieve relevant content to answer user queries."}),"\n",e.jsx(t.h2,{children:"Quickstart"}),"\n",e.jsx(t.p,{children:"In this example, we’ll create an assistant that can help answer questions about companies’ financial statements."}),"\n",e.jsx(t.h3,{children:"Step 1: Create a new Assistant with File Search Enabled"}),"\n",e.jsxs(t.p,{children:["Create a new assistant with ",e.jsx(t.code,{children:"file_search"})," enabled in the ",e.jsx(t.code,{children:"tools"})," parameter of the Assistant."]}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\nfrom openai import OpenAI\n \nclient = OpenAI()\n \nassistant = client.beta.assistants.create(\n  name="Financial Analyst Assistant",\n  instructions="You are an expert financial analyst. Use you knowledge base to answer questions about audited financial statements.",\n  model="gpt-4o",\n  tools=[{"type": "file_search"}],\n)'.trim(),"node.js":'\nimport OpenAI from "openai";\nconst openai = new OpenAI();\n \nasync function main() {\n  const assistant = await openai.beta.assistants.create({\n    name: "Financial Analyst Assistant",\n    instructions: "You are an expert financial analyst. Use you knowledge base to answer questions about audited financial statements.",\n    model: "gpt-4o",\n    tools: [{ type: "file_search" }],\n  });\n}\n \nmain();'.trim(),curl:'\ncurl https://api.openai.com/v1/assistants \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -H "OpenAI-Beta: assistants=v2" \\\n  -d \'{\n    "name": "Financial Analyst Assistant",\n    "instructions": "You are an expert financial analyst. Use you knowledge base to answer questions about audited financial statements.",\n    "tools": [{"type": "file_search"}],\n    "model": "gpt-4o"\n  }\'\n'.trim()}}),"\n",e.jsxs(t.p,{children:["Once the ",e.jsx(t.code,{children:"file_search"})," tool is enabled, the model decides when to retrieve content based on user messages."]}),"\n",e.jsx(t.h3,{children:"Step 2: Upload files and add them to a Vector Store"}),"\n",e.jsxs(t.p,{children:["To access your files, the ",e.jsx(t.code,{children:"file_search"})," tool uses the Vector Store object.\nUpload your files and create a Vector Store to contain them.\nOnce the Vector Store is created, you should poll its status until all files are out of the ",e.jsx(t.code,{children:"in_progress"})," state to\nensure that all content has finished processing. The SDK provides helpers to uploading and polling in one shot."]}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\n# Create a vector store caled "Financial Statements"\nvector_store = client.beta.vector_stores.create(name="Financial Statements")\n \n# Ready the files for upload to OpenAI\nfile_paths = ["edgar/goog-10k.pdf", "edgar/brka-10k.txt"]\nfile_streams = [open(path, "rb") for path in file_paths]\n \n# Use the upload and poll SDK helper to upload the files, add them to the vector store,\n# and poll the status of the file batch for completion.\nfile_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n  vector_store_id=vector_store.id, files=file_streams\n)\n \n# You can print the status and the file counts of the batch to see the result of this operation.\nprint(file_batch.status)\nprint(file_batch.file_counts)\n'.trim(),"node.js":'\nconst fileStreams = ["edgar/goog-10k.pdf", "edgar/brka-10k.txt"].map((path) =>\n  fs.createReadStream(path),\n);\n \n// Create a vector store including our two files.\nlet vectorStore = await openai.beta.vectorStores.create({\n  name: "Financial Statement",\n});\n \nawait openai.beta.vectorStores.fileBatches.uploadAndPoll(vectorStore.id, fileStreams)\n'.trim()}}),"\n",e.jsx(t.h3,{children:"Step 3: Update the assistant to use the new Vector Store"}),"\n",e.jsxs(t.p,{children:["To make the files accessible to your assistant, update the assistant’s ",e.jsx(t.code,{children:"tool_resources"})," with the new ",e.jsx(t.code,{children:"vector_store"})," id."]}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\nassistant = client.beta.assistants.update(\n  assistant_id=assistant.id,\n  tool_resources={"file_search": {"vector_store_ids": [vector_store.id]}},\n)\n        '.trim(),"node.js":"\nawait openai.beta.assistants.update(assistant.id, {\n  tool_resources: { file_search: { vector_store_ids: [vectorStore.id] } },\n});\n        ".trim()}}),"\n",e.jsx(t.h3,{children:"Step 4: Create a thread"}),"\n",e.jsxs(t.p,{children:["You can also attach files as Message attachments on your thread. Doing so will create another ",e.jsx(t.code,{children:"vector_store"})," associated with the thread, or, if there is already a vector store attached to this thread, attach the new files to the existing thread vector store. When you create a Run on this thread, the file search tool will query both the ",e.jsx(t.code,{children:"vector_store"})," from your assistant and the ",e.jsx(t.code,{children:"vector_store"})," on the thread."]}),"\n",e.jsx(t.p,{children:"In this example, the user attached a copy of Apple’s latest 10-K filing."}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\n# Upload the user provided file to OpenAI\nmessage_file = client.files.create(\n  file=open("edgar/aapl-10k.pdf", "rb"), purpose="assistants"\n)\n \n# Create a thread and attach the file to the message\nthread = client.beta.threads.create(\n  messages=[\n    {\n      "role": "user",\n      "content": "How many shares of AAPL were outstanding at the end of of October 2023?",\n      # Attach the new file to the message.\n      "attachments": [\n        { "file_id": message_file.id, "tools": [{"type": "file_search"}] }\n      ],\n    }\n  ]\n)\n \n# The thread now has a vector store with that file in its tool resources.\nprint(thread.tool_resources.file_search)\n    '.trim(),"node.js":'\n// A user wants to attach a file to a specific message, let\'s upload it.\nconst aapl10k = await openai.files.create({\n  file: fs.createReadStream("edgar/aapl-10k.pdf"),\n  purpose: "assistants",\n});\n\nconst thread = await openai.beta.threads.create({\n  messages: [\n    {\n      role: "user",\n      content:\n        "How many shares of AAPL were outstanding at the end of of October 2023?",\n      // Attach the new file to the message.\n      attachments: [{ file_id: aapl10k.id, tools: [{ type: "file_search" }] }],\n    },\n  ],\n});\n\n// The thread now has a vector store in its tool resources.\nconsole.log(thread.tool_resources?.file_search);\n    '.trim()}}),"\n",e.jsxs(t.p,{children:["Vector stores created using message attachments have a default expiration policy of 7 days after they were last active (defined as the last time the vector store was part of a run). This default exists to help you manage your vector storage costs. You can override these expiration policies at any time. Learn more ",e.jsx(t.a,{href:"/docs/assistants/tools/file-search/managing-costs-with-expiration-policies",children:"here"}),"."]}),"\n",e.jsx(t.h3,{children:"Step 5: Create a run and check the output"}),"\n",e.jsx(t.p,{children:"Now, create a Run and observe that the model uses the File Search tool to provide a response to the user’s question."}),"\n",e.jsx(G,{options:[{value:"streaming",label:"With streaming",content:e.jsx(c,{defaultLanguage:"python",code:{python:'\nfrom typing_extensions import override\nfrom openai import AssistantEventHandler, OpenAI\n \nclient = OpenAI()\n \nclass EventHandler(AssistantEventHandler):\n    @override\n    def on_text_created(self, text) -> None:\n        print(f"\\nassistant > ", end="", flush=True)\n\n    @override\n    def on_tool_call_created(self, tool_call):\n        print(f"\\nassistant > {tool_call.type}\\n", flush=True)\n\n    @override\n    def on_message_done(self, message) -> None:\n        # print a citation to the file searched\n        message_content = message.content[0].text\n        annotations = message_content.annotations\n        citations = []\n        for index, annotation in enumerate(annotations):\n            message_content.value = message_content.value.replace(\n                annotation.text, f"[{index}]"\n            )\n            if file_citation := getattr(annotation, "file_citation", None):\n                cited_file = client.files.retrieve(file_citation.file_id)\n                citations.append(f"[{index}] {cited_file.filename}")\n\n        print(message_content.value)\n        print("\\n".join(citations))\n\n\n# Then, we use the stream SDK helper\n# with the EventHandler class to create the Run\n# and stream the response.\n\nwith client.beta.threads.runs.stream(\n    thread_id=thread.id,\n    assistant_id=assistant.id,\n    instructions="Please address the user as Jane Doe. The user has a premium account.",\n    event_handler=EventHandler(),\n) as stream:\n    stream.until_done()\n'.trim(),"node.js":'\nconst stream = openai.beta.threads.runs\n  .stream(thread.id, {\n    assistant_id: assistant.id,\n  })\n  .on("textCreated", () => console.log("assistant >"))\n  .on("toolCallCreated", (event) => console.log("assistant " + event.type))\n  .on("messageDone", async (event) => {\n    if (event.content[0].type === "text") {\n      const { text } = event.content[0];\n      const { annotations } = text;\n      const citations: string[] = [];\n\n      let index = 0;\n      for (let annotation of annotations) {\n        text.value = text.value.replace(annotation.text, "[" + index + "]");\n        const { file_citation } = annotation;\n        if (file_citation) {\n          const citedFile = await openai.files.retrieve(file_citation.file_id);\n          citations.push("[" + index + "]" + citedFile.filename);\n        }\n        index++;\n      }\n\n      console.log(text.value);\n      console.log(citations.join("\\n"));\n    }\n'.trim()}})},{value:"without-streaming",label:"Without streaming",content:e.jsx(c,{defaultLanguage:"python",code:{python:'\n# Use the create and poll SDK helper to create a run and poll the status of\n# the run until it\'s in a terminal state.\n\nrun = client.beta.threads.runs.create_and_poll(\n    thread_id=thread.id, assistant_id=assistant.id\n)\n\nmessages = list(client.beta.threads.messages.list(thread_id=thread.id, run_id=run.id))\n\nmessage_content = messages[0].content[0].text\nannotations = message_content.annotations\ncitations = []\nfor index, annotation in enumerate(annotations):\n    message_content.value = message_content.value.replace(annotation.text, f"[{index}]")\n    if file_citation := getattr(annotation, "file_citation", None):\n        cited_file = client.files.retrieve(file_citation.file_id)\n        citations.append(f"[{index}] {cited_file.filename}")\n\nprint(message_content.value)\nprint("\\n".join(citations))\n    '.trim(),"node.js":'\nconst run = await openai.beta.threads.runs.createAndPoll(thread.id, {\n  assistant_id: assistant.id,\n});\n \nconst messages = await openai.beta.threads.messages.list(thread.id, {\n  run_id: run.id,\n});\n \nconst message = messages.data.pop()!;\nif (message.content[0].type === "text") {\n  const { text } = message.content[0];\n  const { annotations } = text;\n  const citations: string[] = [];\n\n  let index = 0;\n  for (let annotation of annotations) {\n    text.value = text.value.replace(annotation.text, "[" + index + "]");\n    const { file_citation } = annotation;\n    if (file_citation) {\n      const citedFile = await openai.files.retrieve(file_citation.file_id);\n      citations.push("[" + index + "]" + citedFile.filename);\n    }\n    index++;\n  }\n\n  console.log(text.value);\n  console.log(citations.join("\\n"));\n}\n'.trim()}})}],initialValue:"streaming"}),"\n",e.jsxs(t.p,{children:["Your new assistant will query both attached vector stores (one containing ",e.jsx(t.code,{children:"goog-10k.pdf"})," and ",e.jsx(t.code,{children:"brka-10k.txt"}),", and the other containing ",e.jsx(t.code,{children:"aapl-10k.pdf"}),") and return this result from ",e.jsx(t.code,{children:"aapl-10k.pdf"}),"."]}),"\n",e.jsxs(t.p,{children:["To retrieve the contents of the file search results that were used by the model, use the ",e.jsx(t.code,{children:"include"})," query parameter and provide a value of ",e.jsx(t.code,{children:"step_details.tool_calls[*].file_search.results[*].content"})," in the format ",e.jsx(t.code,{children:"?include[]=step_details.tool_calls[*].file_search.results[*].content"}),"."]}),"\n",e.jsx(t.hr,{}),"\n",e.jsx(t.h2,{children:"How it works"}),"\n",e.jsxs(t.p,{children:["The ",e.jsx(t.code,{children:"file_search"})," tool implements several retrieval best practices out of the box to help you extract the right data from your files and augment the model’s responses. The ",e.jsx(t.code,{children:"file_search"})," tool:"]}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Rewrites user queries to optimize them for search."}),"\n",e.jsx(t.li,{children:"Breaks down complex user queries into multiple searches it can run in parallel."}),"\n",e.jsx(t.li,{children:"Runs both keyword and semantic searches across both assistant and thread vector stores."}),"\n",e.jsx(t.li,{children:"Reranks search results to pick the most relevant ones before generating the final response."}),"\n"]}),"\n",e.jsxs(t.p,{children:["By default, the ",e.jsx(t.code,{children:"file_search"})," tool uses the following settings but these can be ",e.jsx(t.a,{href:"/docs/assistants/tools/file-search/customizing-file-search-settings",children:"configured"})," to suit your needs:"]}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Chunk size: 800 tokens"}),"\n",e.jsx(t.li,{children:"Chunk overlap: 400 tokens"}),"\n",e.jsxs(t.li,{children:["Embedding model: ",e.jsx(t.code,{children:"text-embedding-3-large"})," at 256 dimensions"]}),"\n",e.jsx(t.li,{children:"Maximum number of chunks added to context: 20 (could be fewer)"}),"\n",e.jsxs(t.li,{children:["Ranker: ",e.jsx(t.code,{children:"auto"})," (OpenAI will choose which ranker to use)"]}),"\n",e.jsx(t.li,{children:"Score threshold: 0 minimum ranking score"}),"\n"]}),"\n",e.jsx(t.p,{children:e.jsx(t.strong,{children:"Known Limitations"})}),"\n",e.jsx(t.p,{children:"We have a few known limitations we're working on adding support for in the coming months:"}),"\n",e.jsxs(t.ol,{children:["\n",e.jsx(t.li,{children:"Support for deterministic pre-search filtering using custom metadata."}),"\n",e.jsx(t.li,{children:"Support for parsing images within documents (including images of charts, graphs, tables etc.)"}),"\n",e.jsxs(t.li,{children:["Support for retrievals over structured file formats (like ",e.jsx(t.code,{children:"csv"})," or ",e.jsx(t.code,{children:"jsonl"}),")."]}),"\n",e.jsx(t.li,{children:"Better support for summarization — the tool today is optimized for search queries."}),"\n"]}),"\n",e.jsx(t.h2,{children:"Vector stores"}),"\n",e.jsxs(t.p,{children:["Vector Store objects give the File Search tool the ability to search your files. Adding a file to a ",e.jsx(t.code,{children:"vector_store"})," automatically parses, chunks, embeds and stores the file in a vector database that's capable of both keyword and semantic search. Each ",e.jsx(t.code,{children:"vector_store"})," can hold up to 10,000 files. Vector stores can be attached to both Assistants and Threads. Today, you can attach at most one vector store to an assistant and at most one vector store to a thread."]}),"\n",e.jsx(t.h4,{children:"Creating vector stores and adding files"}),"\n",e.jsx(t.p,{children:"You can create a vector store and add files to it in a single API call:"}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:"\nvector_store = client.beta.vector_stores.create(\n  name=\"Product Documentation\",\n  file_ids=['file_1', 'file_2', 'file_3', 'file_4', 'file_5']\n)\n".trim(),"node.js":"\nconst vectorStore = await openai.beta.vectorStores.create({\n  name: \"Product Documentation\",\n  file_ids: ['file_1', 'file_2', 'file_3', 'file_4', 'file_5']\n});\n".trim()}}),"\n",e.jsxs(t.p,{children:["Adding files to vector stores is an async operation. To ensure the operation is complete, we recommend that you use the 'create and poll' helpers in our official SDKs. If you're not using the SDKs, you can retrieve the ",e.jsx(t.code,{children:"vector_store"})," object and monitor it's ",e.jsx(t.a,{href:"/docs/api-reference/vector-stores/object#vector-stores/object-file_counts",children:e.jsx(t.code,{children:"file_counts"})})," property to see the result of the file ingestion operation."]}),"\n",e.jsxs(t.p,{children:["Files can also be added to a vector store after it's created by ",e.jsx(t.a,{href:"/docs/api-reference/vector-stores/createFile",children:"creating vector store files"}),"."]}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\nfile = client.beta.vector_stores.files.create_and_poll(\n  vector_store_id="vs_abc123",\n  file_id="file-abc123"\n)\n'.trim(),"node.js":'\nconst file = await openai.beta.vectorStores.files.createAndPoll(\n  "vs_abc123",\n  { file_id: "file-abc123" }\n);\n'.trim()}}),"\n",e.jsxs(t.p,{children:["Alternatively, you can add several files to a vector store by ",e.jsx(t.a,{href:"/docs/api-reference/vector-stores/createBatch",children:"creating batches"})," of up to 500 files."]}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:"\nbatch = client.beta.vector_stores.file_batches.create_and_poll(\n  vector_store_id=\"vs_abc123\",\n  file_ids=['file_1', 'file_2', 'file_3', 'file_4', 'file_5']\n)\n".trim(),"node.js":'\nconst batch = await openai.beta.vectorStores.fileBatches.createAndPoll(\n  "vs_abc123",\n  { file_ids: ["file_1", "file_2", "file_3", "file_4", "file_5"] },\n);\n'.trim()}}),"\n",e.jsx(t.p,{children:"Similarly, these files can be removed from a vector store by either:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["Deleting the ",e.jsx(t.a,{href:"/docs/api-reference/vector-stores/deleteFile",children:"vector store file object"})," or,"]}),"\n",e.jsxs(t.li,{children:["By deleting the underlying ",e.jsx(t.a,{href:"/docs/api-reference/files/delete",children:"file object"})," (which removes the file it from all ",e.jsx(t.code,{children:"vector_store"})," and ",e.jsx(t.code,{children:"code_interpreter"})," configurations across all assistants and threads in your organization)"]}),"\n"]}),"\n",e.jsx(t.p,{children:"The maximum file size is 512 MB. Each file should contain no more than 5,000,000 tokens per file (computed automatically when you attach a file)."}),"\n",e.jsxs(t.p,{children:["File Search supports a variety of file formats including ",e.jsx(t.code,{children:".pdf"}),", ",e.jsx(t.code,{children:".md"}),", and ",e.jsx(t.code,{children:".docx"}),". More details on the file extensions (and their corresponding MIME-types) supported can be found in the ",e.jsx(t.a,{href:"/docs/assistants/tools/file-search/supported-files",children:"Supported files"})," section below."]}),"\n",e.jsx(t.h4,{children:"Attaching vector stores"}),"\n",e.jsxs(t.p,{children:["You can attach vector stores to your Assistant or Thread using the ",e.jsx(t.code,{children:"tool_resources"})," parameter."]}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\nassistant = client.beta.assistants.create(\n  instructions="You are a helpful product support assistant and you answer questions based on the files provided to you.",\n  model="gpt-4o",\n  tools=[{"type": "file_search"}],\n  tool_resources={\n    "file_search": {\n      "vector_store_ids": ["vs_1"]\n    }\n  }\n)\n\nthread = client.beta.threads.create(\n  messages=[ { "role": "user", "content": "How do I cancel my subscription?"} ],\n  tool_resources={\n    "file_search": {\n      "vector_store_ids": ["vs_2"]\n    }\n  }\n)\n'.trim(),"node.js":'\nconst assistant = await openai.beta.assistants.create({\n  instructions: "You are a helpful product support assistant and you answer questions based on the files provided to you.",\n  model: "gpt-4o",\n  tools: [{"type": "file_search"}],\n  tool_resources: {\n    "file_search": {\n      "vector_store_ids": ["vs_1"]\n    }\n  }\n});\n\nconst thread = await openai.beta.threads.create({\n  messages: [ { role: "user", content: "How do I cancel my subscription?"} ],\n  tool_resources: {\n    "file_search": {\n      "vector_store_ids": ["vs_2"]\n    }\n  }\n});\n'.trim()}}),"\n",e.jsxs(t.p,{children:["You can also attach a vector store to Threads or Assistants after they're created by updating them with the right ",e.jsx(t.code,{children:"tool_resources"}),"."]}),"\n",e.jsx(t.h4,{children:"Ensuring vector store readiness before creating runs"}),"\n",e.jsxs(t.p,{children:["We highly recommend that you ensure all files in a ",e.jsx(t.code,{children:"vector_store"})," are fully processed before you create a run. This will ensure that all the data in your ",e.jsx(t.code,{children:"vector_store"})," is searchable. You can check for ",e.jsx(t.code,{children:"vector_store"})," readiness by using the polling helpers in our SDKs, or by manually polling the ",e.jsx(t.code,{children:"vector_store"})," object to ensure the ",e.jsx(t.a,{href:"/docs/api-reference/vector-stores/object#vector-stores/object-status",children:e.jsx(t.code,{children:"status"})})," is ",e.jsx(t.code,{children:"completed"}),"."]}),"\n",e.jsxs(t.p,{children:["As a fallback, we've built a ",e.jsx(t.strong,{children:"60 second maximum wait"})," in the Run object when the ",e.jsx(t.strong,{children:"thread’s"})," vector store contains files that are still being processed. This is to ensure that any files your users upload in a thread a fully searchable before the run proceeds. This fallback wait ",e.jsx(t.em,{children:"does not"})," apply to the assistant's vector store."]}),"\n",e.jsx(t.h4,{children:"Customizing File Search settings"}),"\n",e.jsxs(t.p,{children:["You can customize how the ",e.jsx(t.code,{children:"file_search"})," tool chunks your data and how many chunks it returns to the model context."]}),"\n",e.jsx(t.p,{children:e.jsx(t.strong,{children:"Chunking configuration"})}),"\n",e.jsxs(t.p,{children:["By default, ",e.jsx(t.code,{children:"max_chunk_size_tokens"})," is set to ",e.jsx(t.code,{children:"800"})," and ",e.jsx(t.code,{children:"chunk_overlap_tokens"})," is set to ",e.jsx(t.code,{children:"400"}),", meaning every file is indexed by being split up into 800-token chunks, with 400-token overlap between consecutive chunks."]}),"\n",e.jsxs(t.p,{children:["You can adjust this by setting ",e.jsx(t.a,{href:"/docs/api-reference/vector-stores-files/createFile#vector-stores-files-createfile-chunking_strategy",children:e.jsx(t.code,{children:"chunking_strategy"})})," when adding files to the vector store. There are certain limitations to ",e.jsx(t.code,{children:"chunking_strategy"}),":"]}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"max_chunk_size_tokens"})," must be between 100 and 4096 inclusive."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"chunk_overlap_tokens"})," must be non-negative and should not exceed ",e.jsx(t.code,{children:"max_chunk_size_tokens / 2"}),"."]}),"\n"]}),"\n",e.jsx(t.p,{children:e.jsx(t.strong,{children:"Number of chunks"})}),"\n",e.jsxs(t.p,{children:["By default, the ",e.jsx(t.code,{children:"file_search"})," tool outputs up to 20 chunks for ",e.jsx(t.code,{children:"gpt-4*"})," models and up to 5 chunks for ",e.jsx(t.code,{children:"gpt-3.5-turbo"}),". You can adjust this by setting ",e.jsx(t.a,{href:"/docs/api-reference/assistants/createAssistant#assistants-createassistant-tools",children:e.jsx(t.code,{children:"file_search.max_num_results"})})," in the tool when creating the assistant or the run."]}),"\n",e.jsxs(t.p,{children:["Note that the ",e.jsx(t.code,{children:"file_search"})," tool may output fewer than this number for a myriad of reasons:"]}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["The total number of chunks is fewer than ",e.jsx(t.code,{children:"max_num_results"}),"."]}),"\n",e.jsxs(t.li,{children:['The total token size of all the retrieved chunks exceeds the token "budget" assigned to the ',e.jsx(t.code,{children:"file_search"})," tool. The ",e.jsx(t.code,{children:"file_search"})," tool currently has a token bugdet of:","\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["4,000 tokens for ",e.jsx(t.code,{children:"gpt-3.5-turbo"})]}),"\n",e.jsxs(t.li,{children:["16,000 tokens for ",e.jsx(t.code,{children:"gpt-4*"})," models"]}),"\n"]}),"\n"]}),"\n"]}),"\n",e.jsx(t.h4,{children:"Improve file search result relevance with chunk ranking"}),"\n",e.jsx(t.p,{children:"By default, the file search tool will return all search results to the model that it thinks have any level of relevance when generating a response. However, if responses are generated using content that has low relevance, it can lead to lower quality responses. You can adjust this behavior by both inspecting the file search results that are returned when generating responses, and then tuning the behavior of the file search tool's ranker to change how relevant results must be before they are used to generate a response."}),"\n",e.jsx(t.p,{children:e.jsx(t.strong,{children:"Inspecting file search chunks"})}),"\n",e.jsxs(t.p,{children:["The first step in improving the quality of your file search results is inspecting the current behavior of your assistant. Most often, this will involve investigating responses from your assistant that are not not performing well. You can get ",e.jsx(t.a,{href:"/docs/api-reference/run-steps/getRunStep",children:"granular information about a past run step"})," using the REST API, specifically using the ",e.jsx(t.code,{children:"include"})," query parameter to get the file chunks that are being used to generate results."]}),"\n",e.jsx(c,{title:"Include file search results in response when creating a run",defaultLanguage:"python",code:{python:'\nfrom openai import OpenAI\nclient = OpenAI()\n\nrun_step = client.beta.threads.runs.steps.retrieve(\n    thread_id="thread_abc123",\n    run_id="run_abc123",\n    step_id="step_abc123",\n    include=["step_details.tool_calls[*].file_search.results[*].content"]\n)\n\nprint(run_step)\n'.trim(),"node.js":'\nimport OpenAI from "openai";\nconst openai = new OpenAI();\n\nconst runStep = await openai.beta.threads.runs.steps.retrieve(\n  "thread_abc123",\n  "run_abc123",\n  "step_abc123",\n  {\n    include: ["step_details.tool_calls[*].file_search.results[*].content"]\n  }\n);\n\nconsole.log(runStep);\n'.trim(),curl:'\ncurl -g https://api.openai.com/v1/threads/thread_abc123/runs/run_abc123/steps/step_abc123?include[]=step_details.tool_calls[*].file_search.results[*].content \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -H "Content-Type: application/json" \\\n  -H "OpenAI-Beta: assistants=v2"\n'.trim()}}),"\n",e.jsx(t.p,{children:"You can then log and inspect the search results used during the run step, and determine whether or not they are consistently relevant to the responses your assistant should generate."}),"\n",e.jsx(t.p,{children:e.jsx(t.strong,{children:"Configure ranking options"})}),"\n",e.jsxs(t.p,{children:["If you have determined that your file search results are not sufficiently relevant to generate high quality responses, you can adjust the settings of the result ranker used to choose which search results should be used to generate responses. You can adjust this setting ",e.jsx(t.a,{href:"/docs/api-reference/assistants/createAssistant#assistants-createassistant-tools",children:e.jsx(t.code,{children:"file_search.ranking_options"})})," in the tool when ",e.jsx(t.strong,{children:"creating the assistant"})," or ",e.jsx(t.strong,{children:"creating the run"}),"."]}),"\n",e.jsx(t.p,{children:"The settings you can configure are:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"ranker"})," - Which ranker to use in determining which chunks to use. The available values are ",e.jsx(t.code,{children:"auto"}),", which uses the latest available ranker, and ",e.jsx(t.code,{children:"default_2024_08_21"}),"."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"score_threshold"})," - a ranking between 0.0 and 1.0, with 1.0 being the highest ranking. A higher number will constrain the file chunks used to generate a result to only chunks with a higher possible relevance, at the cost of potentially leaving out relevant chunks."]}),"\n"]}),"\n",e.jsx(t.h4,{children:"Managing costs with expiration policies"}),"\n",e.jsxs(t.p,{children:["The ",e.jsx(t.code,{children:"file_search"})," tool uses the ",e.jsx(t.code,{children:"vector_stores"})," object as its resource and you will be billed based on the ",e.jsx(t.a,{href:"/docs/api-reference/vector-stores/object#vector-stores/object-bytes",children:"size"})," of the ",e.jsx(t.code,{children:"vector_store"})," objects created. The size of the vector store object is the sum of all the parsed chunks from your files and their corresponding embeddings."]}),"\n",e.jsx(t.p,{children:"You first GB is free and beyond that, usage is billed at $0.10/GB/day of vector storage. There are no other costs associated with vector store operations."}),"\n",e.jsxs(t.p,{children:["In order to help you manage the costs associated with these ",e.jsx(t.code,{children:"vector_store"})," objects, we have added support for expiration policies in the ",e.jsx(t.code,{children:"vector_store"})," object. You can set these policies when creating or updating the ",e.jsx(t.code,{children:"vector_store"})," object."]}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:"\nvector_store = client.beta.vector_stores.create_and_poll(\n  name=\"Product Documentation\",\n  file_ids=['file_1', 'file_2', 'file_3', 'file_4', 'file_5'],\n  expires_after={\n	  \"anchor\": \"last_active_at\",\n	  \"days\": 7\n  }\n)\n".trim(),"node.js":"\nlet vectorStore = await openai.beta.vectorStores.create({\n  name: \"rag-store\",\n  file_ids: ['file_1', 'file_2', 'file_3', 'file_4', 'file_5'],\n  expires_after: {\n    anchor: \"last_active_at\",\n    days: 7\n  }\n});".trim()}}),"\n",e.jsx(t.p,{children:e.jsx(t.strong,{children:"Thread vector stores have default expiration policies"})}),"\n",e.jsxs(t.p,{children:["Vector stores created using thread helpers (like ",e.jsx(t.a,{href:"/docs/api-reference/threads/createThread#threads-createthread-tool_resources",children:e.jsx(t.code,{children:"tool_resources.file_search.vector_stores"})})," in Threads or ",e.jsx(t.a,{href:"/docs/api-reference/messages/createMessage#messages-createmessage-attachments",children:"message.attachments"})," in Messages) have a default expiration policy of 7 days after they were last active (defined as the last time the vector store was part of a run)."]}),"\n",e.jsxs(t.p,{children:["When a vector store expires, runs on that thread will fail. To fix this, you can simply recreate a new ",e.jsx(t.code,{children:"vector_store"})," with the same files and reattach it to the thread."]}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\nall_files = list(client.beta.vector_stores.files.list("vs_expired"))\n\nvector_store = client.beta.vector_stores.create(name="rag-store")\nclient.beta.threads.update(\n    "thread_abc123",\n    tool_resources={"file_search": {"vector_store_ids": [vector_store.id]}},\n)\n\nfor file_batch in chunked(all_files, 100):\n    client.beta.vector_stores.file_batches.create_and_poll(\n        vector_store_id=vector_store.id, file_ids=[file.id for file in file_batch]\n    )\n'.trim(),"node.js":'\nconst fileIds = [];\nfor await (const file of openai.beta.vectorStores.files.list(\n  "vs_toWTk90YblRLCkbE2xSVoJlF",\n)) {\n  fileIds.push(file.id);\n}\n\nconst vectorStore = await openai.beta.vectorStores.create({\n  name: "rag-store",\n});\nawait openai.beta.threads.update("thread_abcd", {\n  tool_resources: { file_search: { vector_store_ids: [vectorStore.id] } },\n});\n\nfor (const fileBatch of _.chunk(fileIds, 100)) {\n  await openai.beta.vectorStores.fileBatches.create(vectorStore.id, {\n    file_ids: fileBatch,\n  });\n}\n'.trim()}}),"\n",e.jsx(t.h2,{children:"Supported files"}),"\n",e.jsx(t.p,{children:e.jsxs(t.em,{children:["For ",e.jsx(t.code,{children:"text/"})," MIME types, the encoding must be one of ",e.jsx(t.code,{children:"utf-8"}),", ",e.jsx(t.code,{children:"utf-16"}),", or ",e.jsx(t.code,{children:"ascii"}),"."]})}),"\n","\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"File format"}),e.jsx(t.th,{children:"MIME type"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".c"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text/x-c"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".cpp"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text/x-c++"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".css"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text/css"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".csv"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text/csv"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".docx"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"application/vnd.openxmlformats-officedocument.wordprocessingml.document"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".html"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text/html"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".java"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text/x-java"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".js"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text/javascript"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".json"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"application/json"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".md"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text/markdown"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".pdf"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"application/pdf"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".php"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text/x-php"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".pptx"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"application/vnd.openxmlformats-officedocument.presentationml.presentation"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".py"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text/x-python"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".py"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text/x-script.python"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".rb"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text/x-ruby"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".tex"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text/x-tex"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".ts"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"application/typescript"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".txt"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text/plain"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".xlsx"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:".xml"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text/xml"})})]})]})]})]})}function Gs(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Fe,n)})):Fe(n)}function Re(n){const t=Object.assign({p:"p"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.p,{children:"For the streaming case, we create an EventHandler class to handle events in the response stream and submit all tool outputs at once with the “submit tool outputs stream” helper in the Python and Node SDKs."}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\nfrom typing_extensions import override\nfrom openai import AssistantEventHandler\n \nclass EventHandler(AssistantEventHandler):\n    @override\n    def on_event(self, event):\n      # Retrieve events that are denoted with \'requires_action\'\n      # since these will have our tool_calls\n      if event.event == \'thread.run.requires_action\':\n        run_id = event.data.id  # Retrieve the run ID from the event data\n        self.handle_requires_action(event.data, run_id)\n \n    def handle_requires_action(self, data, run_id):\n      tool_outputs = []\n        \n      for tool in data.required_action.submit_tool_outputs.tool_calls:\n        if tool.function.name == "get_current_temperature":\n          tool_outputs.append({"tool_call_id": tool.id, "output": "57"})\n        elif tool.function.name == "get_rain_probability":\n          tool_outputs.append({"tool_call_id": tool.id, "output": "0.06"})\n        \n      # Submit all tool_outputs at the same time\n      self.submit_tool_outputs(tool_outputs, run_id)\n \n    def submit_tool_outputs(self, tool_outputs, run_id):\n      # Use the submit_tool_outputs_stream helper\n      with client.beta.threads.runs.submit_tool_outputs_stream(\n        thread_id=self.current_run.thread_id,\n        run_id=self.current_run.id,\n        tool_outputs=tool_outputs,\n        event_handler=EventHandler(),\n      ) as stream:\n        for text in stream.text_deltas:\n          print(text, end="", flush=True)\n        print()\n \n \nwith client.beta.threads.runs.stream(\n  thread_id=thread.id,\n  assistant_id=assistant.id,\n  event_handler=EventHandler()\n) as stream:\n  stream.until_done()\n        '.trim(),"node.js":'\nclass EventHandler extends EventEmitter {\n  constructor(client) {\n    super();\n    this.client = client;\n  }\n\n  async onEvent(event) {\n    try {\n      console.log(event);\n      // Retrieve events that are denoted with \'requires_action\'\n      // since these will have our tool_calls\n      if (event.event === "thread.run.requires_action") {\n        await this.handleRequiresAction(\n          event.data,\n          event.data.id,\n          event.data.thread_id,\n        );\n      }\n    } catch (error) {\n      console.error("Error handling event:", error);\n    }\n  }\n\n  async handleRequiresAction(data, runId, threadId) {\n    try {\n      const toolOutputs =\n        data.required_action.submit_tool_outputs.tool_calls.map((toolCall) => {\n          if (toolCall.function.name === "getCurrentTemperature") {\n            return {\n              tool_call_id: toolCall.id,\n              output: "57",\n            };\n          } else if (toolCall.function.name === "getRainProbability") {\n            return {\n              tool_call_id: toolCall.id,\n              output: "0.06",\n            };\n          }\n        });\n      // Submit all the tool outputs at the same time\n      await this.submitToolOutputs(toolOutputs, runId, threadId);\n    } catch (error) {\n      console.error("Error processing required action:", error);\n    }\n  }\n\n  async submitToolOutputs(toolOutputs, runId, threadId) {\n    try {\n      // Use the submitToolOutputsStream helper\n      const stream = this.client.beta.threads.runs.submitToolOutputsStream(\n        threadId,\n        runId,\n        { tool_outputs: toolOutputs },\n      );\n      for await (const event of stream) {\n        this.emit("event", event);\n      }\n    } catch (error) {\n      console.error("Error submitting tool outputs:", error);\n    }\n  }\n}\n\nconst eventHandler = new EventHandler(client);\neventHandler.on("event", eventHandler.onEvent.bind(eventHandler));\n\nconst stream = await client.beta.threads.runs.stream(\n  threadId,\n  { assistant_id: assistantId },\n  eventHandler,\n);\n\nfor await (const event of stream) {\n  eventHandler.emit("event", event);\n}\n        '.trim()}})]})}function Ws(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Re,n)})):Re(n)}function Ge(n){const t=Object.assign({p:"p",code:"code",a:"a"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsxs(t.p,{children:["Runs are asynchronous, which means you'll want to monitor their ",e.jsx(t.code,{children:"status"})," by polling the Run object until a\n",e.jsx(t.a,{href:"https://platform.openai.com/docs/assistants/how-it-works/runs-and-run-steps",children:"terminal status"})," is reached. For convenience, the 'create and poll' SDK helpers assist both in\ncreating the run and then polling for its completion. Once the Run completes, you can list the\nMessages added to the Thread by the Assistant. Finally, you would retrieve all the ",e.jsx(t.code,{children:"tool_outputs"})," from\n",e.jsx(t.code,{children:"required_action"})," and submit them at the same time to the 'submit tool outputs and poll' helper."]}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\nrun = client.beta.threads.runs.create_and_poll(\n  thread_id=thread.id,\n  assistant_id=assistant.id,\n)\n \nif run.status == \'completed\':\n  messages = client.beta.threads.messages.list(\n    thread_id=thread.id\n  )\n  print(messages)\nelse:\n  print(run.status)\n \n# Define the list to store tool outputs\ntool_outputs = []\n \n# Loop through each tool in the required action section\nfor tool in run.required_action.submit_tool_outputs.tool_calls:\n  if tool.function.name == "get_current_temperature":\n    tool_outputs.append({\n      "tool_call_id": tool.id,\n      "output": "57"\n    })\n  elif tool.function.name == "get_rain_probability":\n    tool_outputs.append({\n      "tool_call_id": tool.id,\n      "output": "0.06"\n    })\n \n# Submit all tool outputs at once after collecting them in a list\nif tool_outputs:\n  try:\n    run = client.beta.threads.runs.submit_tool_outputs_and_poll(\n      thread_id=thread.id,\n      run_id=run.id,\n      tool_outputs=tool_outputs\n    )\n    print("Tool outputs submitted successfully.")\n  except Exception as e:\n    print("Failed to submit tool outputs:", e)\nelse:\n  print("No tool outputs to submit.")\n \nif run.status == \'completed\':\n  messages = client.beta.threads.messages.list(\n    thread_id=thread.id\n  )\n  print(messages)\nelse:\n  print(run.status)\n        '.trim(),"node.js":'\nconst handleRequiresAction = async (run) => {\n  // Check if there are tools that require outputs\n  if (\n    run.required_action &&\n    run.required_action.submit_tool_outputs &&\n    run.required_action.submit_tool_outputs.tool_calls\n  ) {\n    // Loop through each tool in the required action section\n    const toolOutputs = run.required_action.submit_tool_outputs.tool_calls.map(\n      (tool) => {\n        if (tool.function.name === "getCurrentTemperature") {\n          return {\n            tool_call_id: tool.id,\n            output: "57",\n          };\n        } else if (tool.function.name === "getRainProbability") {\n          return {\n            tool_call_id: tool.id,\n            output: "0.06",\n          };\n        }\n      },\n    );\n\n    // Submit all tool outputs at once after collecting them in a list\n    if (toolOutputs.length > 0) {\n      run = await client.beta.threads.runs.submitToolOutputsAndPoll(\n        thread.id,\n        run.id,\n        { tool_outputs: toolOutputs },\n      );\n      console.log("Tool outputs submitted successfully.");\n    } else {\n      console.log("No tool outputs to submit.");\n    }\n\n    // Check status after submitting tool outputs\n    return handleRunStatus(run);\n  }\n};\n\nconst handleRunStatus = async (run) => {\n  // Check if the run is completed\n  if (run.status === "completed") {\n    let messages = await client.beta.threads.messages.list(thread.id);\n    console.log(messages.data);\n    return messages.data;\n  } else if (run.status === "requires_action") {\n    console.log(run.status);\n    return await handleRequiresAction(run);\n  } else {\n    console.error("Run did not complete:", run);\n  }\n};\n\n// Create and poll run\nlet run = await client.beta.threads.runs.createAndPoll(thread.id, {\n  assistant_id: assistant.id,\n});\n\nhandleRunStatus(run);\n'.trim()}})]})}function Ys(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Ge,n)})):Ge(n)}function We(n){const t=Object.assign({p:"p",h2:"h2",code:"code",a:"a",h3:"h3"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(I,{level:1,children:e.jsxs(t.p,{children:["Function calling ",e.jsx(H,{children:"Beta"})]})}),"\n",e.jsx(t.p,{children:"Similar to the Chat Completions API, the Assistants API supports function calling. Function calling allows you to describe functions to the Assistants API and have it intelligently return the functions that need to be called along with their arguments."}),"\n",e.jsx(t.h2,{children:"Quickstart"}),"\n",e.jsxs(t.p,{children:["In this example, we'll create a weather assistant and define two functions,\n",e.jsx(t.code,{children:"get_current_temperature"})," and ",e.jsx(t.code,{children:"get_rain_probability"}),", as tools that the Assistant can call.\nDepending on the user query, the model will invoke parallel function calling if using our\nlatest models released on or after Nov 6, 2023.\nIn our example that uses parallel function calling, we will ask the Assistant what the weather in\nSan Francisco is like today and the chances of rain. We also show how to output the Assistant's response with streaming."]}),"\n",e.jsx(b,{children:e.jsxs(t.p,{children:["With the launch of Structured Outputs, you can now use the parameter ",e.jsx(t.code,{children:"strict: true"})," when using function calling with the Assistants API.\nFor more information, refer to the ",e.jsx(t.a,{href:"/docs/guides/function-calling/function-calling-with-structured-outputs",children:"Function calling guide"}),".\nPlease note that Structured Outputs are not supported in the Assistants API when using vision."]})}),"\n",e.jsx(t.h3,{children:"Step 1: Define functions"}),"\n",e.jsxs(t.p,{children:["When creating your assistant, you will first define the functions under the ",e.jsx(t.code,{children:"tools"})," param of the assistant."]}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\nfrom openai import OpenAI\nclient = OpenAI()\n \nassistant = client.beta.assistants.create(\n  instructions="You are a weather bot. Use the provided functions to answer questions.",\n  model="gpt-4o",\n  tools=[\n    {\n      "type": "function",\n      "function": {\n        "name": "get_current_temperature",\n        "description": "Get the current temperature for a specific location",\n        "parameters": {\n          "type": "object",\n          "properties": {\n            "location": {\n              "type": "string",\n              "description": "The city and state, e.g., San Francisco, CA"\n            },\n            "unit": {\n              "type": "string",\n              "enum": ["Celsius", "Fahrenheit"],\n              "description": "The temperature unit to use. Infer this from the user\'s location."\n            }\n          },\n          "required": ["location", "unit"]\n        }\n      }\n    },\n    {\n      "type": "function",\n      "function": {\n        "name": "get_rain_probability",\n        "description": "Get the probability of rain for a specific location",\n        "parameters": {\n          "type": "object",\n          "properties": {\n            "location": {\n              "type": "string",\n              "description": "The city and state, e.g., San Francisco, CA"\n            }\n          },\n          "required": ["location"]\n        }\n      }\n    }\n  ]\n)\n'.trim(),"node.js":'\nconst assistant = await client.beta.assistants.create({\n  model: "gpt-4o",\n  instructions:\n    "You are a weather bot. Use the provided functions to answer questions.",\n  tools: [\n    {\n      type: "function",\n      function: {\n        name: "getCurrentTemperature",\n        description: "Get the current temperature for a specific location",\n        parameters: {\n          type: "object",\n          properties: {\n            location: {\n              type: "string",\n              description: "The city and state, e.g., San Francisco, CA",\n            },\n            unit: {\n              type: "string",\n              enum: ["Celsius", "Fahrenheit"],\n              description:\n                "The temperature unit to use. Infer this from the user\'s location.",\n            },\n          },\n          required: ["location", "unit"],\n        },\n      },\n    },\n    {\n      type: "function",\n      function: {\n        name: "getRainProbability",\n        description: "Get the probability of rain for a specific location",\n        parameters: {\n          type: "object",\n          properties: {\n            location: {\n              type: "string",\n              description: "The city and state, e.g., San Francisco, CA",\n            },\n          },\n          required: ["location"],\n        },\n      },\n    },\n  ],\n});'.trim()}}),"\n",e.jsx(t.h3,{children:"Step 2: Create a Thread and add Messages"}),"\n",e.jsx(t.p,{children:"Create a Thread when a user starts a conversation and add Messages to the Thread as the user asks questions."}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\nthread = client.beta.threads.create()\nmessage = client.beta.threads.messages.create(\n  thread_id=thread.id,\n  role="user",\n  content="What\'s the weather in San Francisco today and the likelihood it\'ll rain?",\n)\n'.trim(),"node.js":'\nconst thread = await client.beta.threads.create();\nconst message = client.beta.threads.messages.create(thread.id, {\n  role: "user",\n  content: "What\'s the weather in San Francisco today and the likelihood it\'ll rain?",\n});\n'.trim()}}),"\n",e.jsx(t.h3,{children:"Step 3: Initiate a Run"}),"\n",e.jsxs(t.p,{children:["When you initiate a Run on a Thread containing a user Message that triggers one or more functions,\nthe Run will enter a ",e.jsx(t.code,{children:"pending"})," status. After it processes, the run will enter a ",e.jsx(t.code,{children:"requires_action"})," state which you can\nverify by checking the Run’s ",e.jsx(t.code,{children:"status"}),". This indicates that you need to run tools and submit their outputs to the\nAssistant to continue Run execution. In our case, we will see two ",e.jsx(t.code,{children:"tool_calls"}),", which indicates that the\nuser query resulted in parallel function calling."]}),"\n",e.jsx(b,{children:e.jsx(t.p,{children:"Note that a runs expire ten minutes after creation. Be sure to submit your tool outputs before the 10 min mark."})}),"\n",e.jsxs(t.p,{children:["You will see two ",e.jsx(t.code,{children:"tool_calls"})," within ",e.jsx(t.code,{children:"required_action"}),", which indicates the user query triggered parallel function calling."]}),"\n",e.jsx(c,{defaultLanguage:"json",code:{json:'\n{\n  "id": "run_qJL1kI9xxWlfE0z1yfL0fGg9",\n  ...\n  "status": "requires_action",\n  "required_action": {\n    "submit_tool_outputs": {\n      "tool_calls": [\n        {\n          "id": "call_FthC9qRpsL5kBpwwyw6c7j4k",\n          "function": {\n            "arguments": "{"location": "San Francisco, CA"}",\n            "name": "get_rain_probability"\n          },\n          "type": "function"\n        },\n        {\n          "id": "call_RpEDoB8O0FTL9JoKTuCVFOyR",\n          "function": {\n            "arguments": "{"location": "San Francisco, CA", "unit": "Fahrenheit"}",\n            "name": "get_current_temperature"\n          },\n          "type": "function"\n        }\n      ]\n    },\n    ...\n    "type": "submit_tool_outputs"\n  }\n}\n'.trim()}}),"\n",e.jsx("figcaption",{children:"Run object truncated here for readability"}),"\n",e.jsx("br",{}),"\n",e.jsxs(t.p,{children:["How you initiate a Run and submit ",e.jsx(t.code,{children:"tool_calls"})," will differ depending on whether you are using streaming or not,\nalthough in both cases all ",e.jsx(t.code,{children:"tool_calls"})," need to be submitted at the same time.\nYou can then complete the Run by submitting the tool outputs from the functions you called.\nPass each ",e.jsx(t.code,{children:"tool_call_id"})," referenced in the ",e.jsx(t.code,{children:"required_action"})," object to match outputs to each function call."]}),"\n",e.jsx(G,{initialValue:"streaming",options:[{label:"With streaming",value:"streaming",content:e.jsx(Ws,{})},{label:"Without streaming",value:"without-streaming",content:e.jsx(Ys,{})}]}),"\n",e.jsx(t.h3,{children:"Using Structured Outputs"}),"\n",e.jsxs(t.p,{children:["When you enable ",e.jsx(t.a,{href:"/docs/guides/structured-outputs",children:"Structured Outputs"})," by supplying ",e.jsx(t.code,{children:"strict: true"}),", the OpenAI API will pre-process your supplied schema on your first request, and then use this artifact to constrain the model to your schema."]}),"\n",e.jsx(c,{defaultLanguage:"python",highlighted:!0,code:{python:'\nfrom openai import OpenAI\nclient = OpenAI()\n \nassistant = client.beta.assistants.create(\n  instructions="You are a weather bot. Use the provided functions to answer questions.",\n  model="gpt-4o-2024-08-06",\n  tools=[\n    {\n      "type": "function",\n      "function": {\n        "name": "get_current_temperature",\n        "description": "Get the current temperature for a specific location",\n        "parameters": {\n          "type": "object",\n          "properties": {\n            "location": {\n              "type": "string",\n              "description": "The city and state, e.g., San Francisco, CA"\n            },\n            "unit": {\n              "type": "string",\n              "enum": ["Celsius", "Fahrenheit"],\n              "description": "The temperature unit to use. Infer this from the user\'s location."\n            }\n          },\n          "required": ["location", "unit"],\n          // highlight-start\n          "additionalProperties": False\n          // highlight-end\n        },\n        // highlight-start\n        "strict": True\n        // highlight-end\n      }\n    },\n    {\n      "type": "function",\n      "function": {\n        "name": "get_rain_probability",\n        "description": "Get the probability of rain for a specific location",\n        "parameters": {\n          "type": "object",\n          "properties": {\n            "location": {\n              "type": "string",\n              "description": "The city and state, e.g., San Francisco, CA"\n            }\n          },\n          "required": ["location"],\n          // highlight-start\n          "additionalProperties": False\n          // highlight-end\n        },\n        // highlight-start\n        "strict": True\n        // highlight-end\n      }\n    }\n  ]\n)\n'.trim(),"node.js":'\nconst assistant = await client.beta.assistants.create({\n  model: "gpt-4o-2024-08-06",\n  instructions:\n    "You are a weather bot. Use the provided functions to answer questions.",\n  tools: [\n    {\n      type: "function",\n      function: {\n        name: "getCurrentTemperature",\n        description: "Get the current temperature for a specific location",\n        parameters: {\n          type: "object",\n          properties: {\n            location: {\n              type: "string",\n              description: "The city and state, e.g., San Francisco, CA",\n            },\n            unit: {\n              type: "string",\n              enum: ["Celsius", "Fahrenheit"],\n              description:\n                "The temperature unit to use. Infer this from the user\'s location.",\n            },\n          },\n          required: ["location", "unit"],\n          // highlight-start\n          additionalProperties: false\n          // highlight-end\n        },\n        // highlight-start\n        strict: true\n        // highlight-end\n      },\n    },\n    {\n      type: "function",\n      function: {\n        name: "getRainProbability",\n        description: "Get the probability of rain for a specific location",\n        parameters: {\n          type: "object",\n          properties: {\n            location: {\n              type: "string",\n              description: "The city and state, e.g., San Francisco, CA",\n            },\n          },\n          required: ["location"],\n          // highlight-start\n          additionalProperties: false\n          // highlight-end\n        },\n        // highlight-start\n        strict: true\n        // highlight-end\n      },\n    },\n  ],\n});'.trim()}})]})}function Bs(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(We,n)})):We(n)}function Ye(n){const t=Object.assign({p:"p",h2:"h2",ul:"ul",li:"li",a:"a"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(I,{level:1,slug:"assistant-tools",children:e.jsxs(t.p,{children:["Assistant Tools ",e.jsx(H,{children:"Beta"})]})}),"\n",e.jsx(t.p,{children:"Assistants created using the Assistants API can be equipped with tools that allow them to perform more complex tasks or interact with your application.\nWe provide built-in tools for assistants, but you can also define your own tools to extend their capabilities using Function Calling."}),"\n",e.jsx(t.p,{children:"The Assistants API currently supports the following tools:"}),"\n",e.jsx(A,{to:"/docs/assistants/tools/file-search",children:e.jsx(C,{icon:e.jsx(jn,{}),title:"File Search",className:"mt-6",children:e.jsx(t.p,{children:"Built-in RAG tool to process and search through files"})})}),"\n",e.jsx(A,{to:"/docs/assistants/tools/code-interpreter",children:e.jsx(C,{icon:e.jsx(on,{}),title:"Code Interpreter",className:"mt-6",children:e.jsx(t.p,{children:"Write and run python code, process files and diverse data"})})}),"\n",e.jsx(A,{to:"/docs/assistants/tools/function-calling",children:e.jsx(C,{icon:e.jsx(yn,{}),title:"Function Calling",className:"mt-6 mb-8",children:e.jsx(t.p,{children:"Use your own custom functions to interact with your application"})})}),"\n",e.jsx(t.h2,{children:"Next steps"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["\n",e.jsxs(t.p,{children:["See the API reference to ",e.jsx(t.a,{href:"/docs/api-reference/runs/submitToolOutputs",children:"submit tool outputs"})]}),"\n"]}),"\n",e.jsxs(t.li,{children:["\n",e.jsxs(t.p,{children:["Build a tool-using assistant with our ",e.jsx(t.a,{href:"https://github.com/openai/openai-assistants-quickstart",children:"Quickstart app"})]}),"\n"]}),"\n"]})]})}function Us(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Ye,n)})):Ye(n)}function Be(n){const t=Object.assign({p:"p",h2:"h2",code:"code",ul:"ul",li:"li",a:"a"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(I,{level:1,slug:"agents",children:e.jsxs(t.p,{children:["What's new in v2? ",e.jsx(H,{children:"Beta"})]})}),"\n",e.jsx(t.h2,{children:"April 2024"}),"\n",e.jsxs(t.p,{children:["We are announcing a variety of new features and improvements to the Assistants API and moving our Beta to a new API version, ",e.jsx(t.code,{children:"OpenAI-Beta: assistants=v2"}),". Here's what's new:"]}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["We're launching an ",e.jsxs(t.a,{href:"/docs/assistants/tools/file-search",children:["improved retrieval tool called ",e.jsx(t.code,{children:"file_search"})]}),", which can ingest up to 10,000 files per assistant - 500x more than before. It is faster, supports parallel queries through multi-threaded searches, and features enhanced reranking and query rewriting."]}),"\n",e.jsxs(t.li,{children:["Alongside ",e.jsx(t.code,{children:"file_search"}),", we're introducing ",e.jsxs(t.a,{href:"/docs/assistants/tools/file-search/vector-stores",children:[e.jsx(t.code,{children:"vector_store"})," objects"]})," in the API. Once a file is added to a vector store, it's automatically parsed, chunked, and embedded, made ready to be searched. Vector stores can be used across assistants and threads, simplifying file management and billing."]}),"\n",e.jsxs(t.li,{children:["You can now ",e.jsx(t.a,{href:"/docs/assistants/how-it-works/context-window-management",children:"control the maximum number of tokens"})," a run uses in the Assistants API, allowing you to manage token usage costs. You can also set limits on the number of previous / recent messages used in each run."]}),"\n",e.jsxs(t.li,{children:["We've added support for the ",e.jsxs(t.a,{href:"/docs/api-reference/runs/object#runs/object-tool_choice",children:[e.jsx(t.code,{children:"tool_choice"})," parameter"]})," which can be used to force the use of a specific tool (like ",e.jsx(t.code,{children:"file_search"}),", ",e.jsx(t.code,{children:"code_interpreter"}),", or a ",e.jsx(t.code,{children:"function"}),") in a particular run."]}),"\n",e.jsxs(t.li,{children:["You can now ",e.jsxs(t.a,{href:"/docs/api-reference/messages/createMessage#messages-createmessage-role",children:["create messages with the role ",e.jsx(t.code,{children:"assistant"})]})," to create custom conversation histories in Threads."]}),"\n",e.jsxs(t.li,{children:["Assistant and Run objects now support popular model configuration parameters like ",e.jsx(t.a,{href:"/docs/api-reference/assistants/createAssistant#assistants-createassistant-temperature",children:e.jsx(t.code,{children:"temperature"})}),", ",e.jsxs(t.a,{href:"/docs/api-reference/assistants/createAssistant#assistants-createassistant-response_format",children:[e.jsx(t.code,{children:"response_format"})," (JSON mode)"]}),", and ",e.jsx(t.a,{href:"/docs/api-reference/assistants/createAssistant#assistants-createassistant-top_p",children:e.jsx(t.code,{children:"top_p"})}),"."]}),"\n",e.jsxs(t.li,{children:["You can now use ",e.jsx(t.a,{href:"/docs/guides/fine-tuning",children:"fine-tuned models"})," in the Assistants API. At the moment, only fine-tuned versions of ",e.jsx(t.code,{children:"gpt-3.5-turbo-0125"})," are supported."]}),"\n",e.jsxs(t.li,{children:["Assistants API now supports ",e.jsx(t.a,{href:"/docs/assistants/overview/step-4-create-a-run?context=with-streaming",children:"streaming"}),"."]}),"\n",e.jsxs(t.li,{children:["We've added several streaming and polling helpers to our ",e.jsx(t.a,{href:"https://github.com/openai/openai-node/blob/master/helpers.md",children:"Node"})," and ",e.jsx(t.a,{href:"https://github.com/openai/openai-python/blob/main/helpers.md",children:"Python"})," SDKs."]}),"\n"]}),"\n",e.jsxs(t.p,{children:["See our ",e.jsx(t.a,{href:"/docs/assistants/migration",children:"migration guide"})," to learn more about how to migrate your tool usage to the latest version of the Assistants API."]})]})}function $s(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Be,n)})):Be(n)}function Ue(n){const t=Object.assign({h1:"h1",p:"p",table:"table",thead:"thead",tr:"tr",th:"th",tbody:"tbody",td:"td",code:"code",a:"a"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"Overview of OpenAI Crawlers"}),"\n",e.jsx(t.p,{children:"OpenAI uses web crawlers (“robots”) and user agents to perform actions for its products, either automatically or triggered by user request. OpenAI uses the following robots.txt tags to enable webmasters to manage how their sites and content work with AI. Each setting is independent of the others – for example, a webmaster can allow OAI-SearchBot to appear in search results while disallowing GPTbot to indicate that crawled content should not be used for training OpenAI’s generative AI foundation models. For search results, please note it can take ~24 hours from a site’s robots.txt update for our systems to adjust."}),"\n",e.jsx("div",{className:"docs-models-toc",children:e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"User agent"}),e.jsx(t.th,{children:"Description & details"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"OAI-SearchBot"}),e.jsxs(t.td,{children:["OAI-SearchBot is for search. OAI-SearchBot is used to link to and surface websites in search results in the SearchGPT prototype, and OpenAI search features. It is not used to crawl content to train OpenAI’s generative AI foundation models. To help ensure your site appears in search results, we recommend allowing OAI-Searchbot in your site’s robots.txt file and allowing requests from our published IP ranges below. ",e.jsx("br",{}),e.jsx("br",{}),"Full user-agent string: ",e.jsx(t.code,{children:"Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko); compatible; OAI-SearchBot/1.0; +https://openai.com/searchbot"})," ",e.jsx("br",{}),e.jsx("br",{}),"Published IP addresses: ",e.jsx(t.a,{href:"https://openai.com/searchbot.json",children:"https://openai.com/searchbot.json"})]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"ChatGPT-User"}),e.jsxs(t.td,{children:["ChatGPT-User is for user actions in ChatGPT and ",e.jsx(t.a,{href:"https://openai.com/index/introducing-gpts/",children:"Custom GPTs"}),". When users ask ChatGPT or a CustomGPT a question, it may visit a web page to help answer and include a link to the source in its response. ChatGPT users may also interact with external applications via ",e.jsx(t.a,{href:"/docs/actions/introduction",children:"GPT Actions"}),". ChatGPT-User governs which sites these user requests can be made to. It is not used for crawling the web in any automatic fashion, nor to crawl content for generative AI training. ",e.jsx("br",{}),e.jsx("br",{}),"Full user-agent string: ",e.jsx(t.code,{children:"Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko); compatible; ChatGPT-User/1.0; +https://openai.com/bot"})," ",e.jsx("br",{}),e.jsx("br",{}),"Published IP addresses: ",e.jsx(t.a,{href:"https://openai.com/chatgpt-user.json",children:"https://openai.com/chatgpt-user.json"})]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"GPTBot"}),e.jsxs(t.td,{children:["GPTBot is used to make our generative AI foundation models more useful and safe. It is used to crawl content that may be used in training our generative AI foundation models. Disallowing GPTBot indicates a site’s content should not be used in training generative AI foundation models. ",e.jsx("br",{}),e.jsx("br",{}),"Full user-agent string: ",e.jsx(t.code,{children:"Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko); compatible; GPTBot/1.1; +https://openai.com/gptbot"})," ",e.jsx("br",{}),e.jsx("br",{}),"Published IP addresses: ",e.jsx(t.a,{href:"https://openai.com/gptbot.json",children:"https://openai.com/gptbot.json"})]})]})]})]})})]})}function Zs(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Ue,n)})):Ue(n)}function Xs(n){const{children:t}=n;return bn.useState(i=>i.user)?e.jsx(e.Fragment,{children:t}):null}function $e(n){const t=Object.assign({h1:"h1",p:"p",h3:"h3",a:"a"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"Key concepts"}),"\n",e.jsx(b,{children:e.jsxs(t.p,{children:["At OpenAI, protecting user data is fundamental to our mission. We do not train our\nmodels on inputs and outputs through our API. Learn more on our"," ","\n",e.jsx("a",{href:"https://openai.com/api-data-privacy",children:"API data privacy page"}),"."]})}),"\n",e.jsx(t.h3,{children:"Text generation models"}),"\n",e.jsxs(t.p,{children:['OpenAI\'s text generation models (often referred to as generative pre-trained transformers or "GPT" models for short), like GPT-4 and GPT-3.5, have been trained to understand natural and formal language. Models like GPT-4 allows text outputs in response to their inputs. The inputs to these models are also referred to as "prompts". Designing a prompt is essentially how you "program" a model like GPT-4, usually by providing instructions or some examples of how to successfully complete a task. Models like GPT-4 can be used across a great variety of tasks including content or code generation, summarization, conversation, creative writing, and more. Read more in our introductory ',e.jsx(t.a,{href:"/docs/guides/text-generation",children:"text generation guide"})," and in our ",e.jsx(t.a,{href:"/docs/guides/prompt-engineering",children:"prompt engineering guide"}),"."]}),"\n",e.jsx(t.h3,{children:"Assistants"}),"\n",e.jsxs(t.p,{children:["Assistants refer to entities, which in the case of the OpenAI API are powered by large language models like GPT-4, that are capable of performing tasks for users. These assistants operate based on the instructions embedded within the context window of the model. They also usually have access to tools which allows the assistants to perform more complex tasks like running code or retrieving information from a file. Read more about assistants in our ",e.jsx(t.a,{href:"/docs/assistants",children:"Assistants API Overview"}),"."]}),"\n",e.jsx(t.h3,{children:"Embeddings"}),"\n",e.jsxs(t.p,{children:["An embedding is a vector representation of a piece of data (e.g. some text) that is meant to preserve aspects of its content and/or its meaning. Chunks of data that are similar in some way will tend to have embeddings that are closer together than unrelated data. OpenAI offers text embedding models that take as input a text string and produce as output an embedding vector. Embeddings are useful for search, clustering, recommendations, anomaly detection, classification, and more. Read more about embeddings in our ",e.jsx(t.a,{href:"/docs/guides/embeddings",children:"embeddings guide"}),"."]}),"\n",e.jsx(t.h3,{children:"Tokens"}),"\n",e.jsxs(t.p,{children:['Text generation and embeddings models process text in chunks called tokens. Tokens represent commonly occurring sequences of characters. For example, the string " tokenization" is decomposed as " token" and "ization", while a short and common word like " the" is represented as a single token. Note that in a sentence, the first token of each word typically starts with a space character. Check out our ',e.jsx(t.a,{href:"/tokenizer",children:"tokenizer tool"})," to test specific strings and see how they are translated into tokens. As a rough rule of thumb, 1 token is approximately 4 characters or 0.75 words for English text."]}),"\n",e.jsxs(t.p,{children:["One limitation to keep in mind is that for a text generation model the prompt and the generated output combined must be no more than the model's maximum context length. For embeddings models (which do not output tokens), the input must be shorter than the model's maximum context length. The maximum context lengths for each text generation and embeddings model can be found in the ",e.jsx(t.a,{href:"/docs/models/overview",children:"model index"}),"."]})]})}function Js(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx($e,n)})):$e(n)}function Ze(n){const t=Object.assign({h1:"h1",h2:"h2",p:"p",a:"a",code:"code",h3:"h3",table:"table",thead:"thead",tr:"tr",th:"th",tbody:"tbody",td:"td",h4:"h4"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"Deprecations"}),"\n",e.jsx(t.h2,{children:"Overview"}),"\n",e.jsxs(t.p,{children:["As we launch safer and more capable models, we regularly retire older models. Software relying on OpenAI models may need occasional updates to keep working. Impacted customers will always be notified by email and in our documentation along with ",e.jsx(t.a,{href:"https://openai.com/blog",children:"blog posts"})," for larger changes."]}),"\n",e.jsx(t.p,{children:"This page lists all API deprecations, along with recommended replacements."}),"\n",e.jsx(t.h2,{children:"Deprecation vs Legacy"}),"\n",e.jsx(t.p,{children:'We use the term "deprecation" to refer to the process of retiring a model or endpoint. When we announce that a model or endpoint is being deprecated, it immediately becomes deprecated. All deprecated models and endpoints will also have a shut down date. At the time of the shut down, the model or endpoint will no longer be accessible.'}),"\n",e.jsx(t.p,{children:'We use the term "legacy" to refer to models and endpoints that will no longer receive updates. We tag endpoints and models as legacy to signal to developers where we are moving as a platform and that they should likely migrate to newer models or endpoints. You can expect that a legacy model or endpoint will be deprecated at some point in the future.'}),"\n",e.jsx(t.h2,{children:"Incremental model updates"}),"\n",e.jsxs(t.p,{children:["As ",e.jsx(t.a,{href:"https://openai.com/blog/introducing-chatgpt-and-whisper-apis#:~:text=Chat%20guide.-,ChatGPT%20upgrades,-We%20are%20constantly",children:"announced"})," in March 2023, we regularly release new versions of ",e.jsx(t.code,{children:"gpt-4"})," and ",e.jsx(t.code,{children:"gpt-3.5-turbo"}),"."]}),"\n",e.jsxs(t.p,{children:["Each model version is dated with either ",e.jsx(t.code,{children:"-MMDD"})," or a ",e.jsx(t.code,{children:"YYYY-MM-DD"})," suffix; e.g., ",e.jsx(t.code,{children:"gpt-4-0613"})," or ",e.jsx(t.code,{children:"gpt-4o-2024-05-13"}),". The undated model name, e.g., ",e.jsx(t.code,{children:"gpt-4o"}),", will typically point to the latest version dated version. Users of undated model names will be notified by email typically 2 weeks before any change takes place."]}),"\n",e.jsx(t.h2,{children:"Migrating to replacements"}),"\n",e.jsx(t.p,{children:"Once a model is deprecated, be sure to migrate all usage to a suitable replacement before the shutdown date. Requests to models past the shutdown date will fail."}),"\n",e.jsxs(t.p,{children:["To help measure the performance of replacement models on your tasks, we have open-sourced ",e.jsx(t.a,{href:"https://github.com/openai/evals",children:"Evals"}),", a Python framework for evaluating language models."]}),"\n",e.jsxs(t.p,{children:["If new models perform worse on your tasks, let us know by submitting a ",e.jsx(t.a,{href:"https://github.com/openai/evals/pulls",children:"pull request"})," to our Evals repo with examples of the task."]}),"\n",e.jsx(t.h2,{children:"Deprecation history"}),"\n",e.jsx(t.p,{children:"All deprecations are listed below, with the most recent announcements at the top."}),"\n",e.jsx(t.h3,{children:"2024-08-29: Fine-tuning training on babbage-002 and davinci-002 models"}),"\n",e.jsxs(t.p,{children:["On August 29th, 2024, we notified developers fine-tuning ",e.jsx(t.code,{children:"babbage-002"})," and ",e.jsx(t.code,{children:"davinci-002"})," that new fine-tuning training runs on these models will no longer be supported starting October 28, 2024."]}),"\n",e.jsx(t.p,{children:"Fine-tuned models created from these base models are not affected by this deprecation, but you will no longer be able to create new fine-tuned versions with these models."}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Shutdown date"}),e.jsx(t.th,{children:"Model / system"}),e.jsx(t.th,{children:"Recommended replacement"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-10-28"}),e.jsxs(t.td,{children:["New fine-tuning training on ",e.jsx(t.code,{children:"babbage-002"})]}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4o-mini"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-10-28"}),e.jsxs(t.td,{children:["New fine-tuning training on ",e.jsx(t.code,{children:"davinci-002"})]}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4o-mini"})})]})]})]}),"\n",e.jsx(t.h3,{children:"2024-06-06: GPT-4-32K and Vision Preview models"}),"\n",e.jsxs(t.p,{children:["On June 6th, 2024, we notified developers using ",e.jsx(t.code,{children:"gpt-4-32k"})," and ",e.jsx(t.code,{children:"gpt-4-vision-preview"})," of their upcoming deprecations in one year and six months respectively. As of June 17, 2024, only existing users of these models will be able to continue using them."]}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Shutdown date"}),e.jsx(t.th,{children:"Deprecated model"}),e.jsx(t.th,{children:"Deprecated model price"}),e.jsx(t.th,{children:"Recommended replacement"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2025-06-06"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4-32k"})}),e.jsx(t.td,{children:"$60.00 / 1M input tokens + $120 / 1M output tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4o"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2025-06-06"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4-32k-0613"})}),e.jsx(t.td,{children:"$60.00 / 1M input tokens + $120 / 1M output tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4o"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2025-06-06"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4-32k-0314"})}),e.jsx(t.td,{children:"$60.00 / 1M input tokens + $120 / 1M output tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4o"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-12-06"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4-vision-preview"})}),e.jsx(t.td,{children:"$10.00 / 1M input tokens + $30 / 1M output tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4o"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-12-06"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4-1106-vision-preview"})}),e.jsx(t.td,{children:"$10.00 / 1M input tokens + $30 / 1M output tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4o"})})]})]})]}),"\n",e.jsx(t.h3,{children:"2023-11-06: Chat model updates"}),"\n",e.jsxs(t.p,{children:["On November 6th, 2023, we ",e.jsx(t.a,{href:"https://openai.com/blog/new-models-and-developer-products-announced-at-devday",children:"announced"})," the release of an updated GPT-3.5-Turbo model (which now comes by default with 16k context) along with deprecation of ",e.jsx(t.code,{children:"gpt-3.5-turbo-0613"})," and ",e.jsx(t.code,{children:" gpt-3.5-turbo-16k-0613"}),". As of June 17, 2024, only existing users of these models will be able to continue using them."]}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Shutdown date"}),e.jsx(t.th,{children:"Deprecated model"}),e.jsx(t.th,{children:"Deprecated model price"}),e.jsx(t.th,{children:"Recommended replacement"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-09-13"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-3.5-turbo-0613"})}),e.jsx(t.td,{children:"$1.50 / 1M input tokens + $2.00 / 1M output tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-3.5-turbo"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-09-13"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-3.5-turbo-16k-0613"})}),e.jsx(t.td,{children:"$3.00 / 1M input tokens + $4.00 / 1M output tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-3.5-turbo"})})]})]})]}),"\n",e.jsx(t.p,{children:"Fine-tuned models created from these base models are not affected by this deprecation, but you will no longer be able to create new fine-tuned versions with these models."}),"\n",e.jsx(t.h3,{children:"2023-08-22: Fine-tunes endpoint"}),"\n",e.jsxs(t.p,{children:["On August 22nd, 2023, we ",e.jsx(t.a,{href:"https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates",children:"announced"})," the new fine-tuning API (",e.jsx(t.code,{children:"/v1/fine_tuning/jobs"}),") and that the original ",e.jsx(t.code,{children:"/v1/fine-tunes"})," API along with legacy models (including those fine-tuned with the ",e.jsx(t.code,{children:"/v1/fine-tunes"})," API) will be shut down on January 04, 2024. This means that models fine-tuned using the ",e.jsx(t.code,{children:"/v1/fine-tunes"})," API will no longer be accessible and you would have to fine-tune new models with the updated endpoint and associated base models."]}),"\n",e.jsx(t.h4,{children:"Fine-tunes endpoint"}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Shutdown date"}),e.jsx(t.th,{children:"System"}),e.jsx(t.th,{children:"Recommended replacement"})]})}),e.jsx(t.tbody,{children:e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"/v1/fine-tunes"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"/v1/fine_tuning/jobs"})})]})})]}),"\n",e.jsx(t.h3,{children:"2023-07-06: GPT and embeddings"}),"\n",e.jsxs(t.p,{children:["On July 06, 2023, we ",e.jsx(t.a,{href:"https://openai.com/blog/gpt-4-api-general-availability",children:"announced"})," the upcoming retirements of older GPT-3 and GPT-3.5 models served via the completions endpoint. We also announced the upcoming retirement of our first-generation text embedding models. They will be shut down on January 04, 2024."]}),"\n",e.jsx(t.h4,{children:"InstructGPT models"}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Shutdown date"}),e.jsx(t.th,{children:"Deprecated model"}),e.jsx(t.th,{children:"Deprecated model price"}),e.jsx(t.th,{children:"Recommended replacement"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-ada-001"})}),e.jsx(t.td,{children:"$0.40 / 1M tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-3.5-turbo-instruct"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-babbage-001"})}),e.jsx(t.td,{children:"$0.50 / 1M tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-3.5-turbo-instruct"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-curie-001"})}),e.jsx(t.td,{children:"$2.00 / 1M tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-3.5-turbo-instruct"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-davinci-001"})}),e.jsx(t.td,{children:"$20.00 / 1M tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-3.5-turbo-instruct"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-davinci-002"})}),e.jsx(t.td,{children:"$20.00 / 1M tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-3.5-turbo-instruct"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-davinci-003"})}),e.jsx(t.td,{children:"$20.00 / 1M tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-3.5-turbo-instruct"})})]})]})]}),"\n",e.jsxs(t.p,{children:["Pricing for the replacement ",e.jsx(t.code,{children:"gpt-3.5-turbo-instruct"})," model can be found on the ",e.jsx(t.a,{href:"https://openai.com/pricing",children:"pricing page"}),"."]}),"\n",e.jsx(t.h4,{children:"Base GPT models"}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Shutdown date"}),e.jsx(t.th,{children:"Deprecated model"}),e.jsx(t.th,{children:"Deprecated model price"}),e.jsx(t.th,{children:"Recommended replacement"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"ada"})}),e.jsx(t.td,{children:"$0.40 / 1M tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"babbage-002"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"babbage"})}),e.jsx(t.td,{children:"$0.50 / 1M tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"babbage-002"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"curie"})}),e.jsx(t.td,{children:"$2.00 / 1M tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"davinci-002"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"davinci"})}),e.jsx(t.td,{children:"$20.00 / 1M tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"davinci-002"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"code-davinci-002"})}),e.jsx(t.td,{children:"---"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-3.5-turbo-instruct"})})]})]})]}),"\n",e.jsxs(t.p,{children:["Pricing for the replacement ",e.jsx(t.code,{children:"babbage-002"})," and ",e.jsx(t.code,{children:"davinci-002"})," models can be found on the ",e.jsx(t.a,{href:"https://openai.com/pricing",children:"pricing page"}),"."]}),"\n",e.jsx(t.h4,{children:"Edit models & endpoint"}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Shutdown date"}),e.jsx(t.th,{children:"Model / system"}),e.jsx(t.th,{children:"Recommended replacement"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-davinci-edit-001"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4o"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"code-davinci-edit-001"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4o"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"/v1/edits"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"/v1/chat/completions"})})]})]})]}),"\n",e.jsx(t.h4,{children:"Fine-tuning GPT models"}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Shutdown date"}),e.jsx(t.th,{children:"Deprecated model"}),e.jsx(t.th,{children:"Training price"}),e.jsx(t.th,{children:"Usage price"}),e.jsx(t.th,{children:"Recommended replacement"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"ada"})}),e.jsx(t.td,{children:"$0.40 / 1M tokens"}),e.jsx(t.td,{children:"$1.60 / 1M tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"babbage-002"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"babbage"})}),e.jsx(t.td,{children:"$0.60 / 1M tokens"}),e.jsx(t.td,{children:"$2.40 / 1M tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"babbage-002"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"curie"})}),e.jsx(t.td,{children:"$3.00 / 1M tokens"}),e.jsx(t.td,{children:"$12.00 / 1M tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"davinci-002"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"davinci"})}),e.jsx(t.td,{children:"$30.00 / 1M tokens"}),e.jsx(t.td,{children:"$120.00 / 1K tokens"}),e.jsxs(t.td,{children:[e.jsx(t.code,{children:"davinci-002"}),", ",e.jsx(t.code,{children:"gpt-3.5-turbo"}),", ",e.jsx(t.code,{children:"gpt-4o"})]})]})]})]}),"\n",e.jsx(t.h4,{children:"First-generation text embedding models"}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Shutdown date"}),e.jsx(t.th,{children:"Deprecated model"}),e.jsx(t.th,{children:"Deprecated model price"}),e.jsx(t.th,{children:"Recommended replacement"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-similarity-ada-001"})}),e.jsx(t.td,{children:"$4.00 / 1M tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-3-small"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-search-ada-doc-001"})}),e.jsx(t.td,{children:"$4.00 / 1M tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-3-small"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-search-ada-query-001"})}),e.jsx(t.td,{children:"$4.00 / 1M tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-3-small"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"code-search-ada-code-001"})}),e.jsx(t.td,{children:"$4.00 / 1M tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-3-small"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"code-search-ada-text-001"})}),e.jsx(t.td,{children:"$4.00 / 1M tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-3-small"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-similarity-babbage-001"})}),e.jsx(t.td,{children:"$5.00 / 1M tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-3-small"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-search-babbage-doc-001"})}),e.jsx(t.td,{children:"$5.00 / 1M tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-3-small"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-search-babbage-query-001"})}),e.jsx(t.td,{children:"$5.00 / 1M tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-3-small"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"code-search-babbage-code-001"})}),e.jsx(t.td,{children:"$5.00 / 1M tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-3-small"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"code-search-babbage-text-001"})}),e.jsx(t.td,{children:"$5.00 / 1M tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-3-small"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-similarity-curie-001"})}),e.jsx(t.td,{children:"$20.00 / 1M tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-3-small"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-search-curie-doc-001"})}),e.jsx(t.td,{children:"$20.00 / 1M tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-3-small"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-search-curie-query-001"})}),e.jsx(t.td,{children:"$20.00 / 1M tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-3-small"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-similarity-davinci-001"})}),e.jsx(t.td,{children:"$200.00 / 1M tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-3-small"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-search-davinci-doc-001"})}),e.jsx(t.td,{children:"$200.00 / 1M tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-3-small"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-01-04"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-search-davinci-query-001"})}),e.jsx(t.td,{children:"$200.00 / 1M tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-3-small"})})]})]})]}),"\n",e.jsx(t.h3,{children:"2023-06-13: Updated chat models"}),"\n",e.jsxs(t.p,{children:["On June 13, 2023, we announced new chat model versions in the ",e.jsx(t.a,{href:"https://openai.com/blog/function-calling-and-other-api-updates",children:"Function calling and other API updates"})," blog post. The three original versions will be retired in June 2024 at the earliest. As of January 10, 2024, only existing users of these models will be able to continue using them."]}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Shutdown date"}),e.jsx(t.th,{children:"Legacy model"}),e.jsx(t.th,{children:"Legacy model price"}),e.jsx(t.th,{children:"Recommended replacement"})]})}),e.jsx(t.tbody,{children:e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"at earliest 2024-06-13"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4-0314"})}),e.jsx(t.td,{children:"$30.00 / 1M input tokens + $60.00 / 1M output tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4o"})})]})})]}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Shutdown date"}),e.jsx(t.th,{children:"Deprecated model"}),e.jsx(t.th,{children:"Deprecated model price"}),e.jsx(t.th,{children:"Recommended replacement"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2024-09-13"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-3.5-turbo-0301"})}),e.jsx(t.td,{children:"$15.00 / 1M input tokens + $20.00 / 1M output tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-3.5-turbo"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2025-06-06"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4-32k-0314"})}),e.jsx(t.td,{children:"$60.00 / 1M input tokens + $120.00 / 1M output tokens"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4o"})})]})]})]}),"\n",e.jsx(t.h3,{children:"2023-03-20: Codex models"}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Shutdown date"}),e.jsx(t.th,{children:"Deprecated model"}),e.jsx(t.th,{children:"Recommended replacement"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2023-03-23"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"code-davinci-002"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4o"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2023-03-23"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"code-davinci-001"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4o"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2023-03-23"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"code-cushman-002"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4o"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2023-03-23"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"code-cushman-001"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4o"})})]})]})]}),"\n",e.jsx(t.h3,{children:"2022-06-03: Legacy endpoints"}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Shutdown date"}),e.jsx(t.th,{children:"System"}),e.jsx(t.th,{children:"Recommended replacement"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2022-12-03"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"/v1/engines"})}),e.jsx(t.td,{children:e.jsx(t.a,{href:"https://platform.openai.com/docs/api-reference/models/list",children:"/v1/models"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2022-12-03"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"/v1/search"})}),e.jsx(t.td,{children:e.jsx(t.a,{href:"https://help.openai.com/en/articles/6272952-search-transition-guide",children:"View transition guide"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2022-12-03"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"/v1/classifications"})}),e.jsx(t.td,{children:e.jsx(t.a,{href:"https://help.openai.com/en/articles/6272941-classifications-transition-guide",children:"View transition guide"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2022-12-03"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"/v1/answers"})}),e.jsx(t.td,{children:e.jsx(t.a,{href:"https://help.openai.com/en/articles/6233728-answers-transition-guide",children:"View transition guide"})})]})]})]})]})}function Ks(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Ze,n)})):Ze(n)}function Xe(n){const t=Object.assign({h1:"h1",h3:"h3",p:"p",code:"code",a:"a",em:"em"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"FAQ"}),"\n",e.jsx(t.h3,{children:"Which model should I use?"}),"\n",e.jsxs(t.p,{children:["We generally recommend that you default to using either ",e.jsx(t.code,{children:"gpt-4o"})," or ",e.jsx(t.code,{children:"gpt-4o-mini"}),"."]}),"\n",e.jsxs(t.p,{children:["If your use case requires high intelligence or reasoning about images as well as text, we recommend you use ",e.jsx(t.code,{children:"gpt-4o"})," and ",e.jsx(t.code,{children:"gpt-4-turbo"})," (although they have very similar intelligence, note that ",e.jsx(t.code,{children:"gpt-4o"})," is both faster and cheaper)."]}),"\n",e.jsxs(t.p,{children:["If your use case requires the fastest speed and lowest cost, we recommend ",e.jsx(t.code,{children:"gpt-4o-mini"})," since it is optimized for these aspects."]}),"\n",e.jsxs(t.p,{children:["We recommend using ",e.jsx(t.code,{children:"gpt-4o-mini"})," where you would have previously used ",e.jsx(t.code,{children:"gpt-3.5-turbo"})," because it is cheaper with higher intelligence, has a larger context window (up to 128,000 tokens compared to 4,096 tokens for ",e.jsx(t.code,{children:"gpt-3.5-turbo"}),") and is multimodal."]}),"\n",e.jsxs(t.p,{children:["You can experiment in the ",e.jsx(t.a,{href:"https://platform.openai.com/playground?mode=chat",children:"playground"})," to investigate which models provide the best price performance trade-off for your usage. A common design pattern is to use several distinct query types which are each dispatched to the model appropriate to handle them."]}),"\n",e.jsx(t.h3,{children:"How should I set the temperature parameter?"}),"\n",e.jsxs(t.p,{children:["You can think of temperature like ",e.jsx(t.em,{children:"randomness"}),", with 0 being ",e.jsx(t.em,{children:"least random"})," (or ",e.jsx(t.em,{children:"most deterministic"}),")\nand 2 being ",e.jsx(t.em,{children:"most random"})," (",e.jsx(t.em,{children:"least deterministic"}),"). When using low values for temperature (e.g. 0.2) the\nmodel responses will tend to be more consistent but may feel more robotic. Values higher than 1.0,\nespecially much higher values, can lead to erratic model outputs. If your goal is creative\noutputs, a combination of a slightly higher than normal temperature (e.g. 1.2) combined with a prompt\nspecifically asking the model to be creative may be your best bet, but we encourage experimentation."]}),"\n",e.jsx(t.h3,{children:"Is fine-tuning available for the latest models?"}),"\n",e.jsxs(t.p,{children:["See the ",e.jsx(t.a,{href:"/docs/guides/fine-tuning",children:"fine-tuning guide"})," for the latest information on which models are available for fine-tuning and how to get started."]}),"\n",e.jsx(t.h3,{children:"Do you store the data that is passed into the API?"}),"\n",e.jsxs(t.p,{children:["As of March 1st, 2023, we retain your API data for 30 days but no longer use your data sent via the API to improve our models. Learn more in our ",e.jsx(t.a,{href:"https://openai.com/policies/usage-policies",children:"data usage policy"}),". Some endpoints offer ",e.jsx(t.a,{href:"/docs/models/default-usage-policies-by-endpoint",children:"zero retention"}),"."]}),"\n",e.jsx(t.h3,{children:"How can I make my application more safe?"}),"\n",e.jsxs(t.p,{children:["If you want to add a moderation layer to the outputs of the Chat API, you can follow our ",e.jsx(t.a,{href:"/docs/guides/moderation",children:"moderation guide"})," to prevent content that violates OpenAI’s usage policies from being shown. We also encourage you to read our ",e.jsx(t.a,{href:"/docs/guides/safety-best-practices",children:"safety guide"})," for more information on how to build safer systems."]}),"\n",e.jsx(t.h3,{children:"Should I use ChatGPT or the API?"}),"\n",e.jsxs(t.p,{children:[e.jsx(t.a,{href:"https://chatgpt.com",children:"ChatGPT"})," offers a chat interface for our models and a range of built-in features such as integrated browsing, code execution, plugins, and more. By contrast, using OpenAI’s API provides more flexibility but requires that you write code or send the requests to our models programmatically."]})]})}function Qs(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Xe,n)})):Xe(n)}function N({name:n,children:t,style:s}){const i=wn(),r="".concat(i.pathPrefix,"/").concat(n),o=p.useCallback(d=>{d.preventDefault(),vn(window.location.origin+r),rn.success("Link copied!")},[r]),l={color:"var(--gray-800)",...s};return e.jsx("div",{className:"anchor-link-root",children:e.jsxs(A,{to:r,className:"anchor-link-link",onClick:o,style:l,children:[e.jsx(kn,{className:"anchor-link-icon",size:"15px"}),e.jsx("div",{className:"anchor-link","data-name":n,children:t})]})})}function Je(n){const t=Object.assign({h1:"h1",p:"p",a:"a",h3:"h3",ul:"ul",li:"li"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"Release notes"}),"\n",e.jsxs(t.p,{children:["Keep track of updates to OpenAI GPTs. You can also view all of the broader ",e.jsx(t.a,{href:"https://help.openai.com/en/articles/6825453-chatgpt-release-notes",children:"ChatGPT releases"})," which is used to share new features and capabilities. This page is maintained in a best effort fashion and may not reflect all changes\nbeing made."]}),"\n",e.jsx(t.h3,{children:"May 13th, 2024"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["\n",e.jsxs(N,{name:"actions-returning-files",children:["Actions can"," ",e.jsx($,{href:"/docs/actions/getting-started/returning-files",useSpan:!0,children:"return"})," ","of up to 10 files per request to be integrated into the conversation"]}),"\n"]}),"\n"]}),"\n",e.jsx(t.h3,{children:"April 8th, 2024"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["\n",e.jsxs(N,{name:"actions-sending-files",children:["Files created by Code Interpreter can now be"," ",e.jsx($,{href:"/docs/actions/getting-started/sending-files",useSpan:!0,children:"included"})," ","in POST requests"]}),"\n"]}),"\n"]}),"\n",e.jsx(t.h3,{children:"Mar 18th, 2024"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["\n",e.jsx(N,{name:"gpt-version-control",children:"GPT Builders can view and restore previous versions of their GPTs"}),"\n"]}),"\n"]}),"\n",e.jsx(t.h3,{children:"Mar 15th, 2024"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["\n",e.jsxs(N,{name:"actions-include-files",children:["POST requests can"," ",e.jsx($,{href:"/docs/actions/getting-started/including-files",useSpan:!0,children:"include up to ten files"})," ","(including DALL-E generated images) from the conversation"]}),"\n"]}),"\n"]}),"\n",e.jsx(t.h3,{children:"Feb 22nd, 2024"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["\n",e.jsx(N,{name:"gpt-ratings",children:"Users can now rate GPTs, which provides feedback for builders and signal for other users in the Store"}),"\n"]}),"\n",e.jsxs(t.li,{children:["\n",e.jsx(N,{name:"gpt-feedback",children:"Users can now leave private feedback for Builders if/when they opt in"}),"\n"]}),"\n",e.jsxs(t.li,{children:["\n",e.jsx(N,{name:"gpt-about",children:"Every GPT now has an About page with information about the GPT including Rating, Category, Conversation Count, Starter Prompts, and more"}),"\n"]}),"\n",e.jsxs(t.li,{children:["\n",e.jsx(N,{name:"builder-social",children:"Builders can now link their social profiles from Twitter, LinkedIn, and GitHub to their GPT"}),"\n"]}),"\n"]}),"\n",e.jsx(t.h3,{children:"Jan 10th, 2024"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["\n",e.jsxs(N,{name:"gpt-store",children:["The"," ",e.jsx($,{href:"https://openai.com/blog/introducing-gpts",useSpan:!0,children:"GPT Store"})," ","launched publicly, with categories and various leaderboards"]}),"\n"]}),"\n"]}),"\n",e.jsx(t.h3,{children:"Nov 6th, 2023"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["\n",e.jsxs(N,{name:"gpts-launched",children:[e.jsx($,{href:"https://openai.com/blog/introducing-gpts",useSpan:!0,children:"GPTs"})," ","allow users to customize ChatGPT for various use cases and share these with other users"]}),"\n"]}),"\n"]})]})}function ei(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Je,n)})):Je(n)}function Ke(n){const t=Object.assign({h1:"h1",p:"p",a:"a",ul:"ul",li:"li",code:"code",h2:"h2",pre:"pre",h3:"h3"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"Advanced Usage"}),"\n",e.jsx(t.p,{children:'OpenAI\'s text generation models (often called generative pre-trained transformers or large language models) have been trained to understand natural language, code, and images. The models provide text outputs in response to their inputs. The text inputs to these models are also referred to as "prompts". Designing a prompt is essentially how you “program” a large language model model, usually by providing instructions or some examples of how to successfully complete a task.'}),"\n",e.jsx(I,{level:2,slug:"reproducible-outputs",children:e.jsxs(t.p,{children:["Reproducible outputs ",e.jsx(H,{children:"Beta"})]})}),"\n",e.jsxs(t.p,{children:["Chat Completions are non-deterministic by default (which means model outputs may differ from request to request). That being said, we offer some control towards deterministic outputs by giving you access to the ",e.jsx(t.a,{href:"/docs/api-reference/chat/create#chat-create-seed",children:"seed"})," parameter and the ",e.jsx(t.a,{href:"/docs/api-reference/completions/object#completions/object-system_fingerprint",children:"system_fingerprint"})," response field."]}),"\n",e.jsx(t.p,{children:"To receive (mostly) deterministic outputs across API calls, you can:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["Set the ",e.jsx(t.a,{href:"/docs/api-reference/chat/create#chat-create-seed",children:"seed"})," parameter to any integer of your choice and use the same value across requests you'd like deterministic outputs for."]}),"\n",e.jsxs(t.li,{children:["Ensure all other parameters (like ",e.jsx(t.code,{children:"prompt"})," or ",e.jsx(t.code,{children:"temperature"}),") are the exact same across requests."]}),"\n"]}),"\n",e.jsxs(t.p,{children:["Sometimes, determinism may be impacted due to necessary changes OpenAI makes to model configurations on our end. To help you keep track of these changes, we expose the ",e.jsx(t.a,{href:"/docs/api-reference/chat/object#chat/object-system_fingerprint",children:"system_fingerprint"})," field. If this value is different, you may see different outputs due to changes we've made on our systems."]}),"\n",e.jsx("a",{href:"https://cookbook.openai.com/examples/reproducible_outputs_with_the_seed_parameter",target:"_blank",children:e.jsx(C,{icon:e.jsx(_n,{}),color:"purple",title:"Deterministic outputs",className:"mt-6",children:e.jsx(t.p,{children:"Explore the new seed parameter in the OpenAI cookbook"})})}),"\n",e.jsx(t.h2,{children:"Managing tokens"}),"\n",e.jsxs(t.p,{children:["Language models read and write text in chunks called tokens. In English, a token can be as short as one character or as long as one word (e.g., ",e.jsx(t.code,{children:"a"})," or ",e.jsx(t.code,{children:" apple"}),"), and in some languages tokens can be even shorter than one character or even longer than one word."]}),"\n",e.jsx(t.p,{children:"As a rough rule of thumb, 1 token is approximately 4 characters or 0.75 words for English text."}),"\n",e.jsx(b,{children:e.jsxs(t.p,{children:["Check out our ",e.jsx("a",{href:"https://platform.openai.com/tokenizer",target:"_blank",children:"Tokenizer tool"})," to test specific strings and see how they are translated into tokens."]})}),"\n",e.jsxs(t.p,{children:["For example, the string ",e.jsx(t.code,{children:'"ChatGPT is great!"'})," is encoded into six tokens: ",e.jsx(t.code,{children:'["Chat", "G", "PT", " is", " great", "!"]'}),"."]}),"\n",e.jsx(t.p,{children:"The total number of tokens in an API call affects:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"How much your API call costs, as you pay per token"}),"\n",e.jsx(t.li,{children:"How long your API call takes, as writing more tokens takes more time"}),"\n",e.jsxs(t.li,{children:["Whether your API call works at all, as total tokens must be below the model's maximum limit (4097 tokens for ",e.jsx(t.code,{children:"gpt-3.5-turbo"}),")"]}),"\n"]}),"\n",e.jsxs(t.p,{children:["Both input and output tokens count toward these quantities. For example, if your API call used 10 tokens in the message input and you received 20 tokens in the message output, you would be billed for 30 tokens. Note however that for some models the price per token is different for tokens in the input vs. the output (see the ",e.jsx(t.a,{href:"https://openai.com/pricing",children:"pricing"})," page for more information)."]}),"\n",e.jsxs(t.p,{children:["To see how many tokens are used by an API call, check the ",e.jsx(t.code,{children:"usage"})," field in the API response (e.g., ",e.jsx(t.code,{children:"response['usage']['total_tokens']"}),")."]}),"\n",e.jsxs(t.p,{children:["Chat models like ",e.jsx(t.code,{children:"gpt-3.5-turbo"})," and ",e.jsx(t.code,{children:"gpt-4-turbo-preview"})," use tokens in the same way as the models available in the completions API, but because of their message-based formatting, it's more difficult to count how many tokens will be used by a conversation."]}),"\n",e.jsxs(z,{title:"Counting tokens for chat API calls",children:[e.jsxs(t.p,{children:["Below is an example function for counting tokens for messages passed to ",e.jsx(t.code,{children:"gpt-3.5-turbo-0613"}),"."]}),e.jsx(t.p,{children:"The exact way that messages are converted into tokens may change from model to model. So when future model versions are released, the answers returned by this function may be only approximate."}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:'def num_tokens_from_messages(messages, model="gpt-3.5-turbo-0613"):\n  """Returns the number of tokens used by a list of messages."""\n  try:\n      encoding = tiktoken.encoding_for_model(model)\n  except KeyError:\n      encoding = tiktoken.get_encoding("cl100k_base")\n  if model == "gpt-3.5-turbo-0613":  # note: future models may deviate from this\n      num_tokens = 0\n      for message in messages:\n          num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n          for key, value in message.items():\n              num_tokens += len(encoding.encode(value))\n              if key == "name":  # if there\'s a name, the role is omitted\n                  num_tokens += -1  # role is always required and always 1 token\n      num_tokens += 2  # every reply is primed with <im_start>assistant\n      return num_tokens\n  else:\n      raise NotImplementedError(f"""num_tokens_from_messages() is not presently implemented for model {model}.""")\n'})}),e.jsx(t.p,{children:"Next, create a message and pass it to the function defined above to see the token count, this should match the value returned by the API usage parameter:"}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:'messages = [\n  {"role": "system", "content": "You are a helpful, pattern-following assistant that translates corporate jargon into plain English."},\n  {"role": "system", "name":"example_user", "content": "New synergies will help drive top-line growth."},\n  {"role": "system", "name": "example_assistant", "content": "Things working well together will increase revenue."},\n  {"role": "system", "name":"example_user", "content": "Let\'s circle back when we have more bandwidth to touch base on opportunities for increased leverage."},\n  {"role": "system", "name": "example_assistant", "content": "Let\'s talk later when we\'re less busy about how to do better."},\n  {"role": "user", "content": "This late pivot means we don\'t have time to boil the ocean for the client deliverable."},\n]\n\nmodel = "gpt-3.5-turbo-0613"\n\nprint(f"{num_tokens_from_messages(messages, model)} prompt tokens counted.")\n# Should show ~126 total_tokens\n'})}),e.jsx(t.p,{children:"To confirm the number generated by our function above is the same as what the API returns, create a new Chat Completion:"}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:"# example token count from the OpenAI API\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n  model=model,\n  messages=messages,\n  temperature=0,\n)\n\nprint(f'{response.usage.prompt_tokens} prompt tokens used.')\n"})})]}),"\n",e.jsxs(t.p,{children:["To see how many tokens are in a text string without making an API call, use OpenAI’s ",e.jsx(t.a,{href:"https://github.com/openai/tiktoken",children:"tiktoken"})," Python library. Example code can be found in the OpenAI Cookbook’s guide on ",e.jsx(t.a,{href:"https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken",children:"how to count tokens with tiktoken"}),"."]}),"\n",e.jsx(t.p,{children:"Each message passed to the API consumes the number of tokens in the content, role, and other fields, plus a few extra for behind-the-scenes formatting. This may change slightly in the future."}),"\n",e.jsxs(t.p,{children:["If a conversation has too many tokens to fit within a model’s maximum limit (e.g., more than 4097 tokens for ",e.jsx(t.code,{children:"gpt-3.5-turbo"})," or more than 128k tokens for ",e.jsx(t.code,{children:"gpt-4o"}),"), you will have to truncate, omit, or otherwise shrink your text until it fits. Beware that if a message is removed from the messages input, the model will lose all knowledge of it."]}),"\n",e.jsxs(t.p,{children:["Note that very long conversations are more likely to receive incomplete replies. For example, a ",e.jsx(t.code,{children:"gpt-3.5-turbo"})," conversation that is 4090 tokens long will have its reply cut off after just 6 tokens."]}),"\n",e.jsx(t.h2,{children:"Parameter details"}),"\n",e.jsx(t.h3,{children:"Frequency and presence penalties"}),"\n",e.jsxs(t.p,{children:["The frequency and presence penalties found in the ",e.jsx(t.a,{href:"/docs/api-reference/chat/create",children:"Chat Completions API"})," and ",e.jsx(t.a,{href:"/docs/api-reference/completions",children:"Legacy Completions API"})," can be used to reduce the likelihood of sampling repetitive sequences of tokens."]}),"\n",e.jsxs(z,{title:"Penalties behind the scenes",children:[e.jsx(t.p,{children:"They work by directly modifying the logits (un-normalized log-probabilities) with an additive contribution."}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:"mu[j] -> mu[j] - c[j] * alpha_frequency - float(c[j] > 0) * alpha_presence\n"})}),e.jsx(t.p,{children:"Where:"}),e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"mu[j]"})," is the logits of the j-th token"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"c[j]"})," is how often that token was sampled prior to the current position"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"float(c[j] > 0)"})," is 1 if ",e.jsx(t.code,{children:"c[j] > 0"})," and 0 otherwise"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"alpha_frequency"})," is the frequency penalty coefficient"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"alpha_presence"})," is the presence penalty coefficient"]}),"\n"]}),e.jsx(t.p,{children:"As we can see, the presence penalty is a one-off additive contribution that applies to all tokens that have been sampled at least once and the frequency penalty is a contribution that is proportional to how often a particular token has already been sampled."})]}),"\n",e.jsx(t.p,{children:"Reasonable values for the penalty coefficients are around 0.1 to 1 if the aim is to just reduce repetitive samples somewhat. If the aim is to strongly suppress repetition, then one can increase the coefficients up to 2, but this can noticeably degrade the quality of samples. Negative values can be used to increase the likelihood of repetition."}),"\n",e.jsx(t.h3,{children:"Token log probabilities"}),"\n",e.jsxs(t.p,{children:["The ",e.jsx(t.a,{href:"/docs/api-reference/chat/create#chat-create-logprobs",children:"logprobs"})," parameter found in the ",e.jsx(t.a,{href:"/docs/api-reference/chat/create",children:"Chat Completions API"})," and ",e.jsx(t.a,{href:"/docs/api-reference/completions",children:"Legacy Completions API"}),", when requested, provides the log probabilities of each output token, and a limited number of the most likely tokens at each token position alongside their log probabilities. This can be useful in some cases to assess the confidence of the model in its output, or to examine alternative responses the model might have given."]}),"\n",e.jsx(t.h3,{children:"Other parameters"}),"\n",e.jsxs(t.p,{children:["See the full ",e.jsx(t.a,{href:"https://platform.openai.com/docs/api-reference/chat",children:"API reference documentation"})," to learn more."]})]})}function ti(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Ke,n)})):Ke(n)}function Qe(n){const t=Object.assign({h1:"h1",p:"p",a:"a",h2:"h2",ol:"ol",li:"li",strong:"strong",h3:"h3",code:"code",pre:"pre",table:"table",thead:"thead",tr:"tr",th:"th",tbody:"tbody",td:"td",ul:"ul"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"Batch API"}),"\n",e.jsxs(t.p,{children:["Learn how to use OpenAI's Batch API to send asynchronous groups of requests with 50% lower costs, a separate pool of significantly higher rate limits, and a clear 24-hour turnaround time. The service is ideal for processing jobs that don't require immediate responses. You can also ",e.jsx(t.a,{href:"/docs/api-reference/batch",children:"explore the API reference directly here"}),"."]}),"\n",e.jsx(t.h2,{children:"Overview"}),"\n",e.jsxs(t.p,{children:["While some uses of the OpenAI Platform require you to send synchronous requests, there are many cases where requests do not need an immediate response or ",e.jsx(t.a,{href:"/docs/guides/rate-limits",children:"rate limits"})," prevent you from executing a large number of queries quickly. Batch processing jobs are often helpful in use cases like:"]}),"\n",e.jsxs(t.ol,{children:["\n",e.jsx(t.li,{children:"running evaluations"}),"\n",e.jsx(t.li,{children:"classifying large datasets"}),"\n",e.jsx(t.li,{children:"embedding content repositories"}),"\n"]}),"\n",e.jsx(t.p,{children:"The Batch API offers a straightforward set of endpoints that allow you to collect a set of requests into a single file, kick off a batch processing job to execute these requests, query for the status of that batch while the underlying requests execute, and eventually retrieve the collected results when the batch is complete."}),"\n",e.jsx(t.p,{children:"Compared to using standard endpoints directly, Batch API has:"}),"\n",e.jsxs(t.ol,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Better cost efficiency:"})," 50% cost discount compared to synchronous APIs"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Higher rate limits:"})," ",e.jsx(t.a,{href:"/settings/organization/limits",children:"Substantially more headroom"})," compared to the synchronous APIs"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Fast completion times:"})," Each batch completes within 24 hours (and often more quickly)"]}),"\n"]}),"\n",e.jsx(t.h2,{children:"Getting Started"}),"\n",e.jsx(t.h3,{children:"1. Preparing Your Batch File"}),"\n",e.jsxs(t.p,{children:["Batches start with a ",e.jsx(t.code,{children:".jsonl"})," file where each line contains the details of an individual request to the API. For now, the available endpoints are ",e.jsx(t.code,{children:"/v1/chat/completions"})," (",e.jsx(t.a,{href:"/docs/api-reference/chat",children:"Chat Completions API"}),") and ",e.jsx(t.code,{children:"/v1/embeddings"})," (",e.jsx(t.a,{href:"/docs/api-reference/embeddings",children:"Embeddings API"}),"). For a given input file, the parameters in each line's ",e.jsx(t.code,{children:"body"})," field are the same as the parameters for the underlying endpoint. Each request must include a unique ",e.jsx(t.code,{children:"custom_id"})," value, which you can use to reference results after completion. Here's an example of an input file with 2 requests. Note that each input file can only include requests to a single model."]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-jsonl",children:'{"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-3.5-turbo-0125", "messages": [{"role": "system", "content": "You are a helpful assistant."},{"role": "user", "content": "Hello world!"}],"max_tokens": 1000}}\n{"custom_id": "request-2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-3.5-turbo-0125", "messages": [{"role": "system", "content": "You are an unhelpful assistant."},{"role": "user", "content": "Hello world!"}],"max_tokens": 1000}}\n'})}),"\n",e.jsx(t.h3,{children:"2. Uploading Your Batch Input File"}),"\n",e.jsxs(t.p,{children:["Similar to our ",e.jsx(t.a,{href:"/docs/guides/fine-tuning/",children:"Fine-tuning API"}),", you must first upload your input file so that you can reference it correctly when kicking off batches. Upload your ",e.jsx(t.code,{children:".jsonl"})," file using the ",e.jsx(t.a,{href:"/docs/api-reference/files",children:"Files API"}),"."]}),"\n",e.jsx(c,{title:"Upload files for Batch API",defaultLanguage:"python",code:{python:'\nfrom openai import OpenAI\nclient = OpenAI()\n\nbatch_input_file = client.files.create(\n  file=open("batchinput.jsonl", "rb"),\n  purpose="batch"\n)\n'.trim(),curl:'\ncurl https://api.openai.com/v1/files \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -F purpose="batch" \\\n  -F file="@batchinput.jsonl"\n'.trim(),node:'\nimport fs from "fs";\nimport OpenAI from "openai";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const file = await openai.files.create({\n    file: fs.createReadStream("batchinput.jsonl"),\n    purpose: "batch",\n  });\n\n  console.log(file);\n}\n\nmain();\n'.trim()}}),"\n",e.jsx(t.h3,{children:"3. Creating the Batch"}),"\n",e.jsxs(t.p,{children:["Once you've successfully uploaded your input file, you can use the input File object's ID to create a batch. In this case, let's assume the file ID is ",e.jsx(t.code,{children:"file-abc123"}),". For now, the completion window can only be set to ",e.jsx(t.code,{children:"24h"}),". You can also provide custom metadata via an optional ",e.jsx(t.code,{children:"metadata"})," parameter."]}),"\n",e.jsx(c,{title:"Create the Batch",defaultLanguage:"python",code:{python:'\nbatch_input_file_id = batch_input_file.id\n\nclient.batches.create(\n    input_file_id=batch_input_file_id,\n    endpoint="/v1/chat/completions",\n    completion_window="24h",\n    metadata={\n      "description": "nightly eval job"\n    }\n)\n'.trim(),curl:'\ncurl https://api.openai.com/v1/batches \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "input_file_id": "file-abc123",\n    "endpoint": "/v1/chat/completions",\n    "completion_window": "24h"\n  }\'\n'.trim(),node:'\nimport OpenAI from "openai";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const batch = await openai.batches.create({\n    input_file_id: "file-abc123",\n    endpoint: "/v1/chat/completions",\n    completion_window: "24h"\n  });\n\n  console.log(batch);\n}\n\nmain();\n'.trim()}}),"\n",e.jsxs(t.p,{children:["This request will return a ",e.jsx(t.a,{href:"/docs/api-reference/batch/object",children:"Batch object"})," with metadata about your batch:"]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:'{\n  "id": "batch_abc123",\n  "object": "batch",\n  "endpoint": "/v1/chat/completions",\n  "errors": null,\n  "input_file_id": "file-abc123",\n  "completion_window": "24h",\n  "status": "validating",\n  "output_file_id": null,\n  "error_file_id": null,\n  "created_at": 1714508499,\n  "in_progress_at": null,\n  "expires_at": 1714536634,\n  "completed_at": null,\n  "failed_at": null,\n  "expired_at": null,\n  "request_counts": {\n    "total": 0,\n    "completed": 0,\n    "failed": 0\n  },\n  "metadata": null\n}\n'})}),"\n",e.jsx(t.h3,{children:"4. Checking the Status of a Batch"}),"\n",e.jsx(t.p,{children:"You can check the status of a batch at any time, which will also return a Batch object."}),"\n",e.jsx(c,{title:"Check the status of a batch",defaultLanguage:"python",code:{python:'\nfrom openai import OpenAI\nclient = OpenAI()\n\nclient.batches.retrieve("batch_abc123")\n'.trim(),curl:'\ncurl https://api.openai.com/v1/batches/batch_abc123 \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -H "Content-Type: application/json" \\\n'.trim(),node:'\nimport OpenAI from "openai";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const batch = await openai.batches.retrieve("batch_abc123");\n\n  console.log(batch);\n}\n\nmain();\n'.trim()}}),"\n",e.jsx(t.p,{children:"The status of a given Batch object can be any of the following:"}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Status"}),e.jsx(t.th,{children:"Description"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"validating"})}),e.jsx(t.td,{children:"the input file is being validated before the batch can begin"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"failed"})}),e.jsx(t.td,{children:"the input file has failed the validation process"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"in_progress"})}),e.jsx(t.td,{children:"the input file was successfully validated and the batch is currently being run"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"finalizing"})}),e.jsx(t.td,{children:"the batch has completed and the results are being prepared"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"completed"})}),e.jsx(t.td,{children:"the batch has been completed and the results are ready"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"expired"})}),e.jsx(t.td,{children:"the batch was not able to be completed within the 24-hour time window"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"cancelling"})}),e.jsx(t.td,{children:"the batch is being cancelled (may take up to 10 minutes)"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"cancelled"})}),e.jsx(t.td,{children:"the batch was cancelled"})]})]})]}),"\n",e.jsx(t.h3,{children:"5. Retrieving the Results"}),"\n",e.jsxs(t.p,{children:["Once the batch is complete, you can download the output by making a request against the ",e.jsx(t.a,{href:"/docs/api-reference/files",children:"Files API"})," via the ",e.jsx(t.code,{children:"output_file_id"})," field from the Batch object and writing it to a file on your machine, in this case ",e.jsx(t.code,{children:"batch_output.jsonl"})]}),"\n",e.jsx(c,{title:"Retrieving the batch results",defaultLanguage:"python",code:{python:'\nfrom openai import OpenAI\nclient = OpenAI()\n\nfile_response = client.files.content("file-xyz123")\nprint(file_response.text)\n'.trim(),curl:'\ncurl https://api.openai.com/v1/files/file-xyz123/content \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" > batch_output.jsonl\n'.trim(),node:'\nimport OpenAI from "openai";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const fileResponse = await openai.files.content("file-xyz123");\n  const fileContents = await fileResponse.text();\n\n  console.log(fileContents);\n}\n\nmain();\n'.trim()}}),"\n",e.jsxs(t.p,{children:["The output ",e.jsx(t.code,{children:".jsonl"})," file will have one response line for every successful request line in the input file. Any failed requests in the batch will have their error information written to an error file that can be found via the batch's ",e.jsx(t.code,{children:"error_file_id"}),"."]}),"\n",e.jsx(b,{children:e.jsxs(t.p,{children:["Note that the output line order ",e.jsx(t.strong,{children:"may not match"})," the input line order. Instead of\nrelying on order to process your results, use the custom_id field which will be\npresent in each line of your output file and allow you to map requests in your input\nto results in your output."]})}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-jsonl",children:'{"id": "batch_req_123", "custom_id": "request-2", "response": {"status_code": 200, "request_id": "req_123", "body": {"id": "chatcmpl-123", "object": "chat.completion", "created": 1711652795, "model": "gpt-3.5-turbo-0125", "choices": [{"index": 0, "message": {"role": "assistant", "content": "Hello."}, "logprobs": null, "finish_reason": "stop"}], "usage": {"prompt_tokens": 22, "completion_tokens": 2, "total_tokens": 24}, "system_fingerprint": "fp_123"}}, "error": null}\n{"id": "batch_req_456", "custom_id": "request-1", "response": {"status_code": 200, "request_id": "req_789", "body": {"id": "chatcmpl-abc", "object": "chat.completion", "created": 1711652789, "model": "gpt-3.5-turbo-0125", "choices": [{"index": 0, "message": {"role": "assistant", "content": "Hello! How can I assist you today?"}, "logprobs": null, "finish_reason": "stop"}], "usage": {"prompt_tokens": 20, "completion_tokens": 9, "total_tokens": 29}, "system_fingerprint": "fp_3ba"}}, "error": null}\n'})}),"\n",e.jsx(t.h3,{children:"6. Cancelling a Batch"}),"\n",e.jsxs(t.p,{children:["If necessary, you can cancel an ongoing batch. The batch's status will change to ",e.jsx(t.code,{children:"cancelling"})," until in-flight requests are complete (up to 10 minutes), after which the status will change to ",e.jsx(t.code,{children:"cancelled"}),"."]}),"\n",e.jsx(c,{title:"Cancelling a batch",defaultLanguage:"python",code:{python:'\nfrom openai import OpenAI\nclient = OpenAI()\n\nclient.batches.cancel("batch_abc123")\n'.trim(),curl:'\ncurl https://api.openai.com/v1/batches/batch_abc123/cancel \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -H "Content-Type: application/json" \\\n  -X POST\n'.trim(),node:'\nimport OpenAI from "openai";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const batch = await openai.batches.cancel("batch_abc123");\n\n  console.log(batch);\n}\n\nmain();\n'.trim()}}),"\n",e.jsx(t.h3,{children:"7. Getting a List of All Batches"}),"\n",e.jsxs(t.p,{children:["At any time, you can see all your batches. For users with many batches, you can use the ",e.jsx(t.code,{children:"limit"})," and ",e.jsx(t.code,{children:"after"})," parameters to paginate your results."]}),"\n",e.jsx(c,{title:"Getting a list of all batches",defaultLanguage:"python",code:{python:"\nfrom openai import OpenAI\nclient = OpenAI()\n\nclient.batches.list(limit=10)\n".trim(),curl:'\ncurl https://api.openai.com/v1/batches?limit=10 \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -H "Content-Type: application/json"\n'.trim(),node:'\nimport OpenAI from "openai";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const list = await openai.batches.list();\n\n  for await (const batch of list) {\n    console.log(batch);\n  }\n}\n\nmain();\n'.trim()}}),"\n",e.jsx(t.h2,{children:"Model Availability"}),"\n",e.jsx(t.p,{children:"The Batch API can currently be used to execute queries against the following models. The Batch API supports text and vision inputs in the same format as the endpoints for these models:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"gpt-4o"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"gpt-4o-2024-08-06"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"gpt-4o-mini"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"gpt-4-turbo"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"gpt-4"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"gpt-4-32k"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"gpt-3.5-turbo"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"gpt-3.5-turbo-16k"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"gpt-4-turbo-preview"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"gpt-4-vision-preview"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"gpt-4-turbo-2024-04-09"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"gpt-4-0314"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"gpt-4-32k-0314"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"gpt-4-32k-0613"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"gpt-3.5-turbo-0301"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"gpt-3.5-turbo-16k-0613"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"gpt-3.5-turbo-1106"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"gpt-3.5-turbo-0613"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"text-embedding-3-large"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"text-embedding-3-small"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"text-embedding-ada-002"})}),"\n"]}),"\n",e.jsxs(t.p,{children:["The Batch API also supports ",e.jsx(t.a,{href:"/docs/guides/fine-tuning/what-models-can-be-fine-tuned",children:"fine-tuned models"}),"."]}),"\n",e.jsx(t.h2,{children:"Rate Limits"}),"\n",e.jsx(t.p,{children:"Batch API rate limits are separate from existing per-model rate limits. The Batch API has two new types of rate limits:"}),"\n",e.jsxs(t.ol,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Per-batch limits:"})," A single batch may include up to 50,000 requests, and a batch input file can be up to 100 MB in size. Note that ",e.jsx(t.code,{children:"/v1/embeddings"})," batches are also restricted to a maximum of 50,000 embedding inputs across all requests in the batch."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Enqueued prompt tokens per model:"})," Each model has a maximum number of enqueued prompt tokens allowed for batch processing. You can find these limits on the ",e.jsx(t.a,{href:"/settings/organization/limits",children:"Platform Settings page"}),"."]}),"\n"]}),"\n",e.jsxs(t.p,{children:["There are no limits for output tokens or number of submitted requests for the Batch API today. Because Batch API rate limits are a new, separate pool, ",e.jsx(t.strong,{children:"using the Batch API will not consume tokens from your standard per-model rate limits"}),", thereby offering you a convenient way to increase the number of requests and processed tokens you can use when querying our API."]}),"\n",e.jsx(t.h2,{children:"Batch Expiration"}),"\n",e.jsxs(t.p,{children:["Batches that do not complete in time eventually move to an ",e.jsx(t.code,{children:"expired"})," state; unfinished requests within that batch are cancelled, and any responses to completed requests are made available via the batch's output file. You will be charged for tokens consumed from any completed requests."]}),"\n",e.jsx(t.h2,{children:"Other Resources"}),"\n",e.jsxs(t.p,{children:["For more concrete examples, visit ",e.jsx(t.strong,{children:e.jsx(t.a,{href:"https://cookbook.openai.com/examples/batch_processing",children:"the OpenAI Cookbook"})}),", which contains sample code for use cases like classification, sentiment analysis, and summary generation."]})]})}function ni(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Qe,n)})):Qe(n)}function et(n){const t=Object.assign({h1:"h1",p:"p",a:"a",hr:"hr",code:"code",h2:"h2",h3:"h3",ul:"ul",li:"li",pre:"pre"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"Chat Completions"}),"\n",e.jsxs(t.p,{children:["Learn how to use OpenAI's Core API endpoint to get responses from ",e.jsx(t.a,{href:"/docs/models",children:"language models"}),"."]}),"\n",e.jsx(t.hr,{}),"\n",e.jsx(A,{to:"/playground?mode=chat&model=gpt-4o",target:"_blank",children:e.jsx(C,{icon:e.jsx(Cn,{}),title:"Try GPT-4o",className:"mt-6",children:e.jsx(t.p,{children:"Try out GPT-4o in the playground"})})}),"\n",e.jsx(A,{to:"/docs/guides/vision",children:e.jsx(C,{icon:e.jsx(an,{}),title:"Explore GPT-4o with image inputs",className:"mt-6",children:e.jsx(t.p,{children:"Check out the vision guide for image understanding"})})}),"\n",e.jsx(t.hr,{}),"\n",e.jsx(t.p,{children:"To use one of these models via the OpenAI API, you’ll send a request to the Chat Completions API containing the inputs and your API key, and receive a response containing the model’s output."}),"\n",e.jsxs(t.p,{children:["You can experiment with various models in the ",e.jsx(t.a,{href:"https://platform.openai.com/playground?mode=chat",children:"chat playground"}),". If you’re not sure which model to use then try ",e.jsx(t.code,{children:"gpt-4o"})," if you need high intelligence or ",e.jsx(t.code,{children:"gpt-4o-mini"})," if you need the fastest speed and lowest cost."]}),"\n",e.jsx(t.h2,{children:"Overview"}),"\n",e.jsxs(t.p,{children:["The Chat Completions API supports text and ",e.jsx(t.a,{href:"/docs/guides/vision",children:"image inputs"}),", and can output text content (including code and ",e.jsx(t.a,{href:"/docs/guides/json-mode",children:"JSON"}),")."]}),"\n",e.jsxs(t.p,{children:["It accepts inputs via the ",e.jsx(t.code,{children:"messages"})," parameter, which is an array of message objects."]}),"\n",e.jsx(t.h3,{children:"Message roles"}),"\n",e.jsxs(t.p,{children:["Each message object has a role (either ",e.jsx(t.code,{children:"system"}),", ",e.jsx(t.code,{children:"user"}),", or ",e.jsx(t.code,{children:"assistant"}),") and content."]}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"The system message is optional and can be used to set the behavior of the assistant"}),"\n",e.jsx(t.li,{children:"The user messages provide requests or comments for the assistant to respond to"}),"\n",e.jsxs(t.li,{children:["Assistant messages store previous assistant responses, but can also be written by you to give examples of desired behavior (",e.jsx(t.a,{href:"/docs/guides/prompt-engineering/tactic-provide-examples",children:"few-shot examples"}),")"]}),"\n"]}),"\n",e.jsx(b,{children:e.jsx(t.p,{children:"By default, there is no system message.\nUse system messages to give instructions to the model outside of the user context.\nYou can set multiple system messages per conversation, the model will read and interpret messages\nin the order it receives them."})}),"\n",e.jsx(t.h2,{children:"Getting started"}),"\n",e.jsx(t.p,{children:"Chat models take a list of messages as input and return a model-generated message as output. Although the chat format is designed to make multi-turn conversations easy, it’s just as useful for single-turn tasks without any conversation."}),"\n",e.jsx(t.p,{children:"An example Chat Completions API call looks like the following:"}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n  model="gpt-4o-mini",\n  messages=[\n    {"role": "system", "content": "You are a helpful assistant."},\n    {"role": "user", "content": "Who won the world series in 2020?"},\n    {"role": "assistant", "content": "The Los Angeles Dodgers won the World Series in 2020."},\n    {"role": "user", "content": "Where was it played?"}\n  ]\n)\n'.trim(),"node.js":'\nimport OpenAI from "openai";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const completion = await openai.chat.completions.create({\n    messages: [{"role": "system", "content": "You are a helpful assistant."},\n        {"role": "user", "content": "Who won the world series in 2020?"},\n        {"role": "assistant", "content": "The Los Angeles Dodgers won the World Series in 2020."},\n        {"role": "user", "content": "Where was it played?"}],\n    model: "gpt-4o-mini",\n  });\n\n  console.log(completion.choices[0]);\n}\nmain();\n'.trim(),curl:'\ncurl https://api.openai.com/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -d \'{\n    "model": "gpt-4o-mini",\n    "messages": [\n      {\n        "role": "system",\n        "content": "You are a helpful assistant."\n      },\n      {\n        "role": "user",\n        "content": "Who won the world series in 2020?"\n      },\n      {\n        "role": "assistant",\n        "content": "The Los Angeles Dodgers won the World Series in 2020."\n      },\n      {\n        "role": "user",\n        "content": "Where was it played?"\n      }\n    ]\n  }\'\n'.trim()}}),"\n",e.jsxs(t.p,{children:["To learn more, you can view the full ",e.jsx(t.a,{href:"https://platform.openai.com/docs/api-reference/chat",children:"API reference documentation"})," for the Chat API."]}),"\n",e.jsxs(t.p,{children:['Including conversation history is important when user instructions refer to prior messages. In the example above, the user\'s final question of "Where was it played?" only makes sense in the context of the prior messages about the World Series of 2020. Because the models have no memory of past requests, all relevant information must be supplied as part of the conversation history in each request. If a conversation cannot fit within the model’s token limit, it will need to be ',e.jsx(t.a,{href:"/docs/guides/prompt-engineering/tactic-for-dialogue-applications-that-require-very-long-conversations-summarize-or-filter-previous-dialogue",children:"shortened"})," in some way."]}),"\n",e.jsx(b,{children:e.jsxs(t.p,{children:["To mimic the effect seen in ChatGPT where the text is returned iteratively, set the"," ","\n",e.jsx("a",{href:"/docs/api-reference/chat/create#chat/create-stream",children:"stream"})," parameter to\ntrue."]})}),"\n",e.jsx(I,{level:2,slug:"response-format",children:e.jsx(t.p,{children:"Chat Completions response format"})}),"\n",e.jsx(t.p,{children:"An example Chat Completions API response looks as follows:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{children:'{\n  "choices": [\n    {\n      "finish_reason": "stop",\n      "index": 0,\n      "message": {\n        "content": "The 2020 World Series was played in Texas at Globe Life Field in Arlington.",\n        "role": "assistant"\n      },\n      "logprobs": null\n    }\n  ],\n  "created": 1677664795,\n  "id": "chatcmpl-7QyqpwdfhqwajicIEznoc6Q47XAyW",\n  "model": "gpt-4o-mini",\n  "object": "chat.completion",\n  "usage": {\n    "completion_tokens": 17,\n    "prompt_tokens": 57,\n    "total_tokens": 74\n  }\n}\n'})}),"\n",e.jsx(t.p,{children:"The assistant's reply can be extracted with:"}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:"\nmessage = completion.choices[0].message.content\n".trim(),"node.js":"\nmessage = completion.choices[0].message.content\n".trim()}}),"\n",e.jsxs(t.p,{children:["Every response will include a ",e.jsx(t.code,{children:"finish_reason"}),". The possible values for ",e.jsx(t.code,{children:"finish_reason"})," are:"]}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"stop"}),": API returned complete message, or a message terminated by one of the stop sequences provided via the ",e.jsx(t.a,{href:"/docs/api-reference/chat/create#chat/create-stop",children:"stop"})," parameter"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"length"}),": Incomplete model output due to ",e.jsx(t.a,{href:"/docs/api-reference/chat/create#chat/create-max_tokens",children:e.jsx(t.code,{children:"max_tokens"})})," parameter or token limit"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"function_call"}),": The model decided to call a function"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"content_filter"}),": Omitted content due to a flag from our content filters"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"null"}),": API response still in progress or incomplete"]}),"\n"]}),"\n",e.jsx(t.p,{children:"Depending on input parameters, the model response may include different information."}),"\n",e.jsx(t.h2,{children:"Next steps"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["Explore additional capabilities of the Chat Completions API such as ",e.jsx(t.a,{href:"/docs/guides/vision",children:"Vision"}),", ",e.jsx(t.a,{href:"/docs/guides/function-calling",children:"Function Calling"})," or ",e.jsx(t.a,{href:"/docs/guides/structured-outputs",children:"Structured Outputs"})]}),"\n",e.jsxs(t.li,{children:["Check out our ",e.jsx(t.a,{href:"/docs/advanced-usage",children:"Advanced Usage"})," page to learn about the different parameters and how to manage tokens"]}),"\n"]})]})}function si(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(et,n)})):et(n)}function tt(n){const t=Object.assign({p:"p",code:"code",a:"a",h4:"h4",blockquote:"blockquote",strong:"strong",ol:"ol",li:"li",h3:"h3",pre:"pre",h2:"h2"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(I,{level:1,children:e.jsxs(t.p,{children:["Completions API ",e.jsx(H,{color:"orange",children:"Legacy"})]})}),"\n",e.jsxs(t.p,{children:["The completions API endpoint received its final update in July 2023 and has a different interface than the new chat completions endpoint. Instead of the input being a list of messages, the input is a freeform text string called a ",e.jsx(t.code,{children:"prompt"}),"."]}),"\n",e.jsx(t.p,{children:"An example legacy Completions API call looks like the following:"}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.completions.create(\n  model="gpt-3.5-turbo-instruct",\n  prompt="Write a tagline for an ice cream shop."\n)\n'.trim(),"node.js":"\nconst completion = await openai.completions.create({\n    model: 'gpt-3.5-turbo-instruct',\n    prompt: 'Write a tagline for an ice cream shop.'\n});\n".trim()}}),"\n",e.jsxs(t.p,{children:["See the full ",e.jsx(t.a,{href:"https://platform.openai.com/docs/api-reference/completions",children:"API reference documentation"})," to learn more."]}),"\n",e.jsx(t.h4,{children:"Inserting text"}),"\n",e.jsxs(t.p,{children:["The completions endpoint also supports inserting text by providing a ",e.jsx(t.a,{href:"/docs/api-reference/completions/create#completions-create-suffix",children:"suffix"})," in addition to the standard prompt which is treated as a prefix. This need naturally arises when writing long-form text, transitioning between paragraphs, following an outline, or guiding the model towards an ending. This also works on code, and can be used to insert in the middle of a function or file."]}),"\n",e.jsxs(z,{title:"Inserting text",children:[e.jsx(t.p,{children:"To illustrate how suffix context effects generated text, consider the prompt, “Today I decided to make a big change.” There’s many ways one could imagine completing the sentence. But if we now supply the ending of the story: “I’ve gotten many compliments on my new hair!”, the intended completion becomes clear."}),e.jsxs(t.blockquote,{children:["\n",e.jsx(t.p,{children:"I went to college at Boston University. After getting my degree, I decided to make a change**. A big change!**"}),"\n"]}),e.jsxs(t.blockquote,{children:["\n",e.jsx(t.p,{children:e.jsx(t.strong,{children:"I packed my bags and moved to the west coast of the United States."})}),"\n"]}),e.jsxs(t.blockquote,{children:["\n",e.jsx(t.p,{children:"Now, I can't get enough of the Pacific Ocean!"}),"\n"]}),e.jsx(t.p,{children:"By providing the model with additional context, it can be much more steerable. However, this is a more constrained and challenging task for the model. To get the best results, we recommend the following:"}),e.jsxs(t.p,{children:[e.jsxs(t.strong,{children:["Use ",e.jsx(t.code,{children:"max_tokens"})," > 256."]})," The model is better at inserting longer completions. With too small ",e.jsx(t.code,{children:"max_tokens"}),", the model may be cut off before it's able to connect to the suffix. Note that you will only be charged for the number of tokens produced even when using larger ",e.jsx(t.code,{children:"max_tokens"}),"."]}),e.jsxs(t.p,{children:[e.jsxs(t.strong,{children:["Prefer ",e.jsx(t.code,{children:"finish_reason"}),' == "stop".']})," When the model reaches a natural stopping point or a user provided stop sequence, it will set ",e.jsx(t.code,{children:"finish_reason"}),' as "stop". This indicates that the model has managed to connect to the suffix well and is a good signal for the quality of a completion. This is especially relevant for choosing between a few completions when using n > 1 or resampling (see the next point).']}),e.jsxs(t.p,{children:[e.jsx(t.strong,{children:"Resample 3-5 times."}),' While almost all completions connect to the prefix, the model may struggle to connect the suffix in harder cases. We find that resampling 3 or 5 times (or using best_of with k=3,5) and picking the samples with "stop" as their ',e.jsx(t.code,{children:"finish_reason"})," can be an effective way in such cases. While resampling, you would typically want a higher temperatures to increase diversity."]}),e.jsxs(t.p,{children:["Note: if all the returned samples have ",e.jsx(t.code,{children:"finish_reason"}),' == "length", it\'s likely that max_tokens is too small and model runs out of tokens before it manages to connect the prompt and the suffix naturally. Consider increasing ',e.jsx(t.code,{children:"max_tokens"})," before resampling."]}),e.jsxs(t.p,{children:[e.jsx(t.strong,{children:"Try giving more clues."})," In some cases to better help the model’s generation, you can provide clues by giving a few examples of patterns that the model can follow to decide a natural place to stop."]}),e.jsxs(t.blockquote,{children:["\n",e.jsx(t.p,{children:"How to make a delicious hot chocolate:"}),"\n",e.jsxs(t.p,{children:["1.** Boil water**\n",e.jsx(t.strong,{children:"2. Put hot chocolate in a cup"}),"\n",e.jsx(t.strong,{children:"3. Add boiling water to the cup"}),"\n4. Enjoy the hot chocolate"]}),"\n"]}),e.jsxs(t.blockquote,{children:["\n",e.jsxs(t.ol,{children:["\n",e.jsx(t.li,{children:"Dogs are loyal animals."}),"\n",e.jsx(t.li,{children:"Lions are ferocious animals."}),"\n",e.jsx(t.li,{children:"Dolphins** are playful animals.**"}),"\n",e.jsx(t.li,{children:"Horses are majestic animals."}),"\n"]}),"\n"]})]}),"\n",e.jsx(t.h3,{children:"Completions response format"}),"\n",e.jsx(t.p,{children:"An example completions API response looks as follows:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{children:'{\n  "choices": [\n    {\n      "finish_reason": "length",\n      "index": 0,\n      "logprobs": null,\n      "text": "\\n\\n\\"Let Your Sweet Tooth Run Wild at Our Creamy Ice Cream Shack"\n    }\n  ],\n  "created": 1683130927,\n  "id": "cmpl-7C9Wxi9Du4j1lQjdjhxBlO22M61LD",\n  "model": "gpt-3.5-turbo-instruct",\n  "object": "text_completion",\n  "usage": {\n    "completion_tokens": 16,\n    "prompt_tokens": 10,\n    "total_tokens": 26\n  }\n}\n'})}),"\n",e.jsxs(t.p,{children:["In Python, the output can be extracted with ",e.jsx(t.code,{children:"response['choices'][0]['text']"}),"."]}),"\n",e.jsx(t.p,{children:"The response format is similar to the response format of the Chat Completions API."}),"\n",e.jsx(t.h3,{children:"Inserting text"}),"\n",e.jsxs(t.p,{children:["The completions endpoint also supports inserting text by providing a ",e.jsx(t.a,{href:"/docs/api-reference/completions/create#completions-create-suffix",children:"suffix"})," in addition to the standard prompt which is treated as a prefix. This need naturally arises when writing long-form text, transitioning between paragraphs, following an outline, or guiding the model towards an ending. This also works on code, and can be used to insert in the middle of a function or file."]}),"\n",e.jsxs(z,{title:"Inserting text",children:[e.jsx(t.p,{children:"To illustrate how suffix context effects generated text, consider the prompt, “Today I decided to make a big change.” There’s many ways one could imagine completing the sentence. But if we now supply the ending of the story: “I’ve gotten many compliments on my new hair!”, the intended completion becomes clear."}),e.jsxs(t.blockquote,{children:["\n",e.jsx(t.p,{children:"I went to college at Boston University. After getting my degree, I decided to make a change**. A big change!**"}),"\n"]}),e.jsxs(t.blockquote,{children:["\n",e.jsx(t.p,{children:e.jsx(t.strong,{children:"I packed my bags and moved to the west coast of the United States."})}),"\n"]}),e.jsxs(t.blockquote,{children:["\n",e.jsx(t.p,{children:"Now, I can’t get enough of the Pacific Ocean!"}),"\n"]}),e.jsx(t.p,{children:"By providing the model with additional context, it can be much more steerable. However, this is a more constrained and challenging task for the model. To get the best results, we recommend the following:"}),e.jsxs(t.p,{children:[e.jsxs(t.strong,{children:["Use ",e.jsx(t.code,{children:"max_tokens"})," > 256."]})," The model is better at inserting longer completions. With too small ",e.jsx(t.code,{children:"max_tokens"}),", the model may be cut off before it's able to connect to the suffix. Note that you will only be charged for the number of tokens produced even when using larger ",e.jsx(t.code,{children:"max_tokens"}),"."]}),e.jsxs(t.p,{children:[e.jsxs(t.strong,{children:["Prefer ",e.jsx(t.code,{children:"finish_reason"}),' == "stop".']})," When the model reaches a natural stopping point or a user provided stop sequence, it will set ",e.jsx(t.code,{children:"finish_reason"}),' as "stop". This indicates that the model has managed to connect to the suffix well and is a good signal for the quality of a completion. This is especially relevant for choosing between a few completions when using n > 1 or resampling (see the next point).']}),e.jsxs(t.p,{children:[e.jsx(t.strong,{children:"Resample 3-5 times."}),' While almost all completions connect to the prefix, the model may struggle to connect the suffix in harder cases. We find that resampling 3 or 5 times (or using best_of with k=3,5) and picking the samples with "stop" as their ',e.jsx(t.code,{children:"finish_reason"})," can be an effective way in such cases. While resampling, you would typically want a higher temperatures to increase diversity."]}),e.jsxs(t.p,{children:["Note: if all the returned samples have ",e.jsx(t.code,{children:"finish_reason"}),' == "length", it\'s likely that max_tokens is too small and model runs out of tokens before it manages to connect the prompt and the suffix naturally. Consider increasing ',e.jsx(t.code,{children:"max_tokens"})," before resampling."]}),e.jsxs(t.p,{children:[e.jsx(t.strong,{children:"Try giving more clues."})," In some cases to better help the model’s generation, you can provide clues by giving a few examples of patterns that the model can follow to decide a natural place to stop."]}),e.jsxs(t.blockquote,{children:["\n",e.jsx(t.p,{children:"How to make a delicious hot chocolate:"}),"\n",e.jsxs(t.p,{children:["1.** Boil water**\n",e.jsx(t.strong,{children:"2. Put hot chocolate in a cup"}),"\n",e.jsx(t.strong,{children:"3. Add boiling water to the cup"}),"\n4. Enjoy the hot chocolate"]}),"\n"]}),e.jsxs(t.blockquote,{children:["\n",e.jsxs(t.ol,{children:["\n",e.jsx(t.li,{children:"Dogs are loyal animals."}),"\n",e.jsx(t.li,{children:"Lions are ferocious animals."}),"\n",e.jsx(t.li,{children:"Dolphins** are playful animals.**"}),"\n",e.jsx(t.li,{children:"Horses are majestic animals."}),"\n"]}),"\n"]})]}),"\n",e.jsx(t.h2,{children:"Chat Completions vs. Completions"}),"\n",e.jsx(t.p,{children:"The Chat Completions format can be made similar to the completions format by constructing a request using a single user message. For example, one can translate from English to French with the following completions prompt:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{children:'Translate the following English text to French: "{text}"\n'})}),"\n",e.jsx(t.p,{children:"And an equivalent chat prompt would be:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{children:'[{"role": "user", "content": \'Translate the following English text to French: "{text}"\'}]\n'})}),"\n",e.jsxs(t.p,{children:["Likewise, the completions API can be used to simulate a chat between a user and an assistant by formatting the input ",e.jsx(t.a,{href:"https://platform.openai.com/playground/p/default-chat?model=gpt-3.5-turbo-instruct",children:"accordingly"}),"."]}),"\n",e.jsxs(t.p,{children:["The difference between these APIs is the underlying models that are available in each. The chat completions API is the interface to our most capable model (",e.jsx(t.code,{children:"gpt-4o"}),"), and our most cost effective model (",e.jsx(t.code,{children:"gpt-4o-mini"}),")."]})]})}function ii(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(tt,n)})):tt(n)}function V({src:n,png:t,webp:s,alt:i="",...r}){n=n||t||s;const o=e.jsx("img",{src:n,alt:i,...r});return!s&&!t?o:e.jsxs("picture",{children:[s&&e.jsx("source",{type:"image/webp",srcSet:s}),t&&e.jsx("source",{type:"image/png",srcSet:t}),o]})}let oi=0;function j({className:n,label:t,children:s,autoScroll:i=!1,defaultExpanded:r=!1}){const[o]=p.useState(()=>"expander-".concat(oi++)),[l,d]=p.useState(r),u=p.useRef(null),m=p.useCallback(()=>{d(!l),!l&&u.current&&i&&setTimeout(()=>{u.current.scrollIntoView({behavior:"smooth",block:"nearest"})},100)},[l,i]);return e.jsxs("div",{className:B("expn",l&&"expanded",n),ref:u,children:[e.jsxs("div",{className:"expn-title",role:"button",onClick:m,"aria-expanded":l?"true":"false","aria-controls":o,children:[e.jsx("div",{className:"expn-icon",children:l?e.jsx(nn,{}):e.jsx(An,{})}),e.jsx("div",{className:"expn-label",children:t})]}),e.jsx("div",{className:B("expn-content",!l&&"hidden"),id:o,children:typeof s=="function"?s({expanded:l}):s})]})}function E({href:n,children:t,newTab:s,className:i}){return e.jsxs("a",{href:n,target:s?"_blank":void 0,rel:"noreferrer",className:B("tag-link",i),children:[t,s&&e.jsx(In,{className:"tag-link-icon"})]})}function nt(n){const t=Object.assign({p:"p",div:"div",b:"b",code:"code",h2:"h2",ul:"ul",li:"li",strong:"strong",a:"a",table:"table",thead:"thead",tr:"tr",th:"th",tbody:"tbody",td:"td",h3:"h3",pre:"pre",h4:"h4"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx("h1",{children:"Embeddings"}),"\n",e.jsx(t.p,{children:"Learn how to turn text into numbers, unlocking use cases like search."}),"\n",e.jsx(ne,{className:"mt-2 mb-2",icon:!1,color:xe.primary,body:e.jsxs(t.div,{className:"docs-samples-notice",children:[e.jsx(t.p,{children:e.jsx(t.b,{children:"New embedding models"})}),e.jsx(t.code,{children:"text-embedding-3-small"})," and ",e.jsx(t.code,{children:"text-embedding-3-large"}),", our newest and most performant embedding models are now available, with lower costs, higher multilingual performance, and new parameters to control the overall size."]}),textSize:ge.large,dismissable:!1}),"\n",e.jsx(t.h2,{children:"What are embeddings?"}),"\n",e.jsx(t.p,{children:"OpenAI’s text embeddings measure the relatedness of text strings. Embeddings are commonly used for:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Search"})," (where results are ranked by relevance to a query string)"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Clustering"})," (where text strings are grouped by similarity)"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Recommendations"})," (where items with related text strings are recommended)"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Anomaly detection"})," (where outliers with little relatedness are identified)"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Diversity measurement"})," (where similarity distributions are analyzed)"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Classification"})," (where text strings are classified by their most similar label)"]}),"\n"]}),"\n",e.jsxs(t.p,{children:["An embedding is a vector (list) of floating point numbers. The ",e.jsx(t.a,{href:"/docs/guides/embeddings/which-distance-function-should-i-use",children:"distance"})," between two vectors measures their relatedness. Small distances suggest high relatedness and large distances suggest low relatedness."]}),"\n",e.jsxs(t.p,{children:["Visit our ",e.jsx(t.a,{href:"https://openai.com/api/pricing/",children:"pricing page"})," to learn about Embeddings pricing. Requests are billed based on the number of ",e.jsx(t.a,{href:"/tokenizer",children:"tokens"})," in the ",e.jsx(t.a,{href:"/docs/api-reference/embeddings/create#embeddings/create-input",children:"input"}),"."]}),"\n",e.jsx(t.h2,{children:"How to get embeddings"}),"\n",e.jsxs(t.p,{children:["To get an embedding, send your text string to the ",e.jsx(t.a,{href:"/docs/api-reference/embeddings",children:"embeddings API endpoint"})," along with the embedding model name (e.g. ",e.jsx(t.code,{children:"text-embedding-3-small"}),"). The response will contain an embedding (list of floating point numbers), which you can extract, save in a vector database, and use for many different use cases:"]}),"\n",e.jsx(c,{title:"Example: Getting embeddings",defaultLanguage:"curl",code:{python:'\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.embeddings.create(\n    input="Your text string goes here",\n    model="text-embedding-3-small"\n)\n\nprint(response.data[0].embedding)\n'.trim(),curl:'\ncurl https://api.openai.com/v1/embeddings \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -d \'{\n    "input": "Your text string goes here",\n    "model": "text-embedding-3-small"\n  }\'\n'.trim(),node:'\nimport OpenAI from "openai";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const embedding = await openai.embeddings.create({\n    model: "text-embedding-3-small",\n    input: "Your text string goes here",\n    encoding_format: "float",\n  });\n\n  console.log(embedding);\n}\n\nmain();\n'.trim()}}),"\n",e.jsx(t.p,{children:"The response will contain the embedding vector along with some additional metadata."}),"\n",e.jsx(c,{title:"Example embedding response",defaultLanguage:"json",code:{json:'\n{\n  "object": "list",\n  "data": [\n    {\n      "object": "embedding",\n      "index": 0,\n      "embedding": [\n        -0.006929283495992422,\n        -0.005336422007530928,\n        ... (omitted for spacing)\n        -4.547132266452536e-05,\n        -0.024047505110502243\n      ],\n    }\n  ],\n  "model": "text-embedding-3-small",\n  "usage": {\n    "prompt_tokens": 5,\n    "total_tokens": 5\n  }\n}\n'.trim()}}),"\n",e.jsxs(t.p,{children:["By default, the length of the embedding vector will be 1536 for ",e.jsx(t.code,{children:"text-embedding-3-small"})," or 3072 for ",e.jsx(t.code,{children:"text-embedding-3-large"}),". You can reduce the dimensions of the embedding by passing in the ",e.jsx(t.a,{href:"/docs/api-reference/embeddings/create#embeddings-create-dimensions",children:"dimensions parameter"})," without the embedding losing its concept-representing properties. We go into more detail on embedding dimensions in the ",e.jsx(t.a,{href:"/docs/guides/embeddings/use-cases",children:"embedding use case section"}),"."]}),"\n",e.jsx(t.h2,{children:"Embedding models"}),"\n",e.jsxs(t.p,{children:["OpenAI offers two powerful third-generation embedding model (denoted by ",e.jsx(t.code,{children:"-3"})," in the model ID). You can read the embedding v3 ",e.jsx(t.a,{href:"https://openai.com/blog/new-embedding-models-and-api-updates",children:"announcement blog post"})," for more details."]}),"\n",e.jsx(t.p,{children:"Usage is priced per input token, below is an example of pricing pages of text per US dollar (assuming ~800 tokens per page):"}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Model"}),e.jsx(t.th,{children:"~ Pages per dollar"}),e.jsxs(t.th,{children:["Performance on ",e.jsx(t.a,{href:"https://github.com/embeddings-benchmark/mteb",children:"MTEB"})," eval"]}),e.jsx(t.th,{children:"Max input"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"text-embedding-3-small"}),e.jsx(t.td,{children:"62,500"}),e.jsx(t.td,{children:"62.3%"}),e.jsx(t.td,{children:"8191"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"text-embedding-3-large"}),e.jsx(t.td,{children:"9,615"}),e.jsx(t.td,{children:"64.6%"}),e.jsx(t.td,{children:"8191"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"text-embedding-ada-002"}),e.jsx(t.td,{children:"12,500"}),e.jsx(t.td,{children:"61.0%"}),e.jsx(t.td,{children:"8191"})]})]})]}),"\n",e.jsx(t.h2,{children:"Use cases"}),"\n",e.jsxs(t.p,{children:["Here we show some representative use cases. We will use the ",e.jsx(t.a,{href:"https://www.kaggle.com/snap/amazon-fine-food-reviews",children:"Amazon fine-food reviews dataset"})," for the following examples."]}),"\n",e.jsx(t.h3,{children:"Obtaining the embeddings"}),"\n",e.jsx(t.p,{children:"The dataset contains a total of 568,454 food reviews Amazon users left up to October 2012. We will use a subset of 1,000 most recent reviews for illustration purposes. The reviews are in English and tend to be positive or negative. Each review has a ProductId, UserId, Score, review title (Summary) and review body (Text). For example:"}),"\n",e.jsx("div",{className:"docs-embeddings-sample-data-table",children:e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Product Id"}),e.jsx(t.th,{children:"User Id"}),e.jsx(t.th,{children:"Score"}),e.jsx(t.th,{children:"Summary"}),e.jsx(t.th,{children:"Text"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"B001E4KFG0"}),e.jsx(t.td,{children:"A3SGXH7AUHU8GW"}),e.jsx(t.td,{children:"5"}),e.jsx(t.td,{children:"Good Quality Dog Food"}),e.jsx(t.td,{children:"I have bought several of the Vitality canned..."})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"B00813GRG4"}),e.jsx(t.td,{children:"A1D87F6ZCVE5NK"}),e.jsx(t.td,{children:"1"}),e.jsx(t.td,{children:"Not as Advertised"}),e.jsx(t.td,{children:"Product arrived labeled as Jumbo Salted Peanut..."})]})]})]})}),"\n",e.jsx(t.p,{children:"We will combine the review summary and review text into a single combined text. The model will encode this combined text and output a single vector embedding."}),"\n",e.jsx(E,{href:"https://cookbook.openai.com/examples/get_embeddings_from_dataset",newTab:!0,children:e.jsx(t.p,{children:"Get_embeddings_from_dataset.ipynb"})}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:"from openai import OpenAI\nclient = OpenAI()\n\ndef get_embedding(text, model=\"text-embedding-3-small\"):\n   text = text.replace(\"\\n\", \" \")\n   return client.embeddings.create(input = [text], model=model).data[0].embedding\n\ndf['ada_embedding'] = df.combined.apply(lambda x: get_embedding(x, model='text-embedding-3-small'))\ndf.to_csv('output/embedded_1k_reviews.csv', index=False)\n"})}),"\n",e.jsx(t.p,{children:"To load the data from a saved file, you can run the following:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:"import pandas as pd\n\ndf = pd.read_csv('output/embedded_1k_reviews.csv')\ndf['ada_embedding'] = df.ada_embedding.apply(eval).apply(np.array)\n"})}),"\n",e.jsxs(j,{label:"Reducing embedding dimensions",autoScroll:!0,showCollapse:!0,children:[e.jsx(t.p,{children:"Using larger embeddings, for example storing them in a vector store for retrieval, generally costs more and consumes more compute, memory and storage than using smaller embeddings."}),e.jsxs(t.p,{children:["Both of our new embedding models were trained ",e.jsx(t.a,{href:"https://arxiv.org/abs/2205.13147",children:"with a technique"})," that allows developers to trade-off performance and cost of using embeddings. Specifically, developers can shorten embeddings (i.e. remove some numbers from the end of the sequence) without the embedding losing its concept-representing properties by passing in the ",e.jsxs(t.a,{href:"/docs/api-reference/embeddings/create#embeddings-create-dimensions",children:[e.jsx(t.code,{children:"dimensions"})," API parameter"]}),". For example, on the MTEB benchmark, a ",e.jsx(t.code,{children:"text-embedding-3-large"})," embedding can be shortened to a size of 256 while still outperforming an unshortened ",e.jsx(t.code,{children:"text-embedding-ada-002"})," embedding with a size of 1536. You can read more about how changing the dimensions impacts performance in our ",e.jsx(t.a,{href:"https://openai.com/blog/new-embedding-models-and-api-updates#:~:text=Native%20support%20for%20shortening%20embeddings",children:"embeddings v3 launch blog post"}),"."]}),e.jsxs(t.p,{children:["In general, using the ",e.jsx(t.code,{children:"dimensions"})," parameter when creating the embedding is the suggested approach. In certain cases, you may need to change the embedding dimension after you generate it. When you change the dimension manually, you need to be sure to normalize the dimensions of the embedding as is shown below."]}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:'from openai import OpenAI\nimport numpy as np\n\nclient = OpenAI()\n\ndef normalize_l2(x):\n    x = np.array(x)\n    if x.ndim == 1:\n        norm = np.linalg.norm(x)\n        if norm == 0:\n            return x\n        return x / norm\n    else:\n        norm = np.linalg.norm(x, 2, axis=1, keepdims=True)\n        return np.where(norm == 0, x, x / norm)\n\n\nresponse = client.embeddings.create(\n    model="text-embedding-3-small", input="Testing 123", encoding_format="float"\n)\n\ncut_dim = response.data[0].embedding[:256]\nnorm_dim = normalize_l2(cut_dim)\n\nprint(norm_dim)\n'})}),e.jsxs(t.p,{children:["Dynamically changing the dimensions enables very flexible usage. For example, when using a vector data store that only supports embeddings up to 1024 dimensions long, developers can now still use our best embedding model ",e.jsx(t.code,{children:"text-embedding-3-large"})," and specify a value of 1024 for the ",e.jsx(t.code,{children:"dimensions"})," API parameter, which will shorten the embedding down from 3072 dimensions, trading off some accuracy in exchange for the smaller vector size."]})]}),"\n",e.jsxs(j,{label:"Question answering using embeddings-based search",autoScroll:!0,showCollapse:!0,children:[e.jsx(E,{href:"https://cookbook.openai.com/examples/question_answering_using_embeddings",newTab:!0,children:"Question_answering_using_embeddings.ipynb"}),e.jsx(t.p,{children:"There are many common cases where the model is not trained on data which contains key facts and information you want to make accessible when generating responses to a user query. One way of solving this, as shown below, is to put additional information into the context window of the model. This is effective in many use cases but leads to higher token costs. In this notebook, we explore the tradeoff between this approach and embeddings bases search."}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:"query = f\"\"\"Use the below article on the 2022 Winter Olympics to answer the subsequent question. If the answer cannot be found, write \"I don't know.\"\n\nArticle:\n\\\"\\\"\\\"\n{wikipedia_article_on_curling}\n\\\"\\\"\\\"\n\nQuestion: Which athletes won the gold medal in curling at the 2022 Winter Olympics?\"\"\"\n\nresponse = client.chat.completions.create(\n    messages=[\n        {'role': 'system', 'content': 'You answer questions about the 2022 Winter Olympics.'},\n        {'role': 'user', 'content': query},\n    ],\n    model=GPT_MODEL,\n    temperature=0,\n)\n\nprint(response.choices[0].message.content)\n"})})]}),"\n",e.jsxs(j,{label:"Text search using embeddings",autoScroll:!0,showCollapse:!0,children:[e.jsx(E,{href:"https://cookbook.openai.com/examples/semantic_text_search_using_embeddings",newTab:!0,children:"Semantic_text_search_using_embeddings.ipynb"}),e.jsx(t.p,{children:"To retrieve the most relevant documents we use the cosine similarity between the embedding vectors of the query and each document, and return the highest scored documents."}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:"from openai.embeddings_utils import get_embedding, cosine_similarity\n\ndef search_reviews(df, product_description, n=3, pprint=True):\n   embedding = get_embedding(product_description, model='text-embedding-3-small')\n   df['similarities'] = df.ada_embedding.apply(lambda x: cosine_similarity(x, embedding))\n   res = df.sort_values('similarities', ascending=False).head(n)\n   return res\n\nres = search_reviews(df, 'delicious beans', n=3)\n"})})]}),"\n",e.jsxs(j,{label:"Code search using embeddings",autoScroll:!0,showCollapse:!0,children:[e.jsx(E,{href:"https://cookbook.openai.com/examples/code_search_using_embeddings",newTab:!0,children:"Code_search.ipynb"}),e.jsxs(t.p,{children:["Code search works similarly to embedding-based text search. We provide a method to extract Python functions from all the Python files in a given repository. Each function is then indexed by the ",e.jsx(t.code,{children:"text-embedding-3-small"})," model."]}),e.jsx(t.p,{children:"To perform a code search, we embed the query in natural language using the same model. Then we calculate cosine similarity between the resulting query embedding and each of the function embeddings. The highest cosine similarity results are most relevant."}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:"from openai.embeddings_utils import get_embedding, cosine_similarity\n\ndf['code_embedding'] = df['code'].apply(lambda x: get_embedding(x, model='text-embedding-3-small'))\n\ndef search_functions(df, code_query, n=3, pprint=True, n_lines=7):\n   embedding = get_embedding(code_query, model='text-embedding-3-small')\n   df['similarities'] = df.code_embedding.apply(lambda x: cosine_similarity(x, embedding))\n\n   res = df.sort_values('similarities', ascending=False).head(n)\n   return res\nres = search_functions(df, 'Completions API tests', n=3)\n"})})]}),"\n",e.jsxs(j,{label:"Recommendations using embeddings",autoScroll:!0,showCollapse:!0,children:[e.jsx(E,{href:"https://cookbook.openai.com/examples/recommendation_using_embeddings",newTab:!0,children:"Recommendation_using_embeddings.ipynb"}),e.jsx(t.p,{children:"Because shorter distances between embedding vectors represent greater similarity, embeddings can be useful for recommendation."}),e.jsxs(t.p,{children:["Below, we illustrate a basic recommender. It takes in a list of strings and one 'source' string, computes their embeddings, and then returns a ranking of the strings, ranked from most similar to least similar. As a concrete example, the linked notebook below applies a version of this function to the ",e.jsx(t.a,{href:"http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html",children:"AG news dataset"})," (sampled down to 2,000 news article descriptions) to return the top 5 most similar articles to any given source article."]}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:'def recommendations_from_strings(\n   strings: List[str],\n   index_of_source_string: int,\n   model="text-embedding-3-small",\n) -> List[int]:\n   """Return nearest neighbors of a given string."""\n\n   # get embeddings for all strings\n   embeddings = [embedding_from_string(string, model=model) for string in strings]\n\n   # get the embedding of the source string\n   query_embedding = embeddings[index_of_source_string]\n\n   # get distances between the source embedding and other embeddings (function from embeddings_utils.py)\n   distances = distances_from_embeddings(query_embedding, embeddings, distance_metric="cosine")\n\n   # get indices of nearest neighbors (function from embeddings_utils.py)\n   indices_of_nearest_neighbors = indices_of_nearest_neighbors_from_distances(distances)\n   return indices_of_nearest_neighbors\n'})})]}),"\n",e.jsxs(j,{label:"Data visualization in 2D",autoScroll:!0,showCollapse:!0,children:[e.jsx(E,{href:"https://cookbook.openai.com/examples/visualizing_embeddings_in_2d",newTab:!0,children:"Visualizing_embeddings_in_2D.ipynb"}),e.jsx(t.p,{children:"The size of the embeddings varies with the complexity of the underlying model. In order to visualize this high dimensional data we use the t-SNE algorithm to transform the data into two dimensions."}),e.jsx(t.p,{children:"We color the individual reviews based on the star rating which the reviewer has given:"}),e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"1-star: red"}),"\n",e.jsx(t.li,{children:"2-star: dark orange"}),"\n",e.jsx(t.li,{children:"3-star: gold"}),"\n",e.jsx(t.li,{children:"4-star: turquoise"}),"\n",e.jsx(t.li,{children:"5-star: dark green"}),"\n"]}),e.jsx(V,{png:"https://cdn.openai.com/API/docs/images/embeddings-tsne.png",webp:"https://cdn.openai.com/API/docs/images/embeddings-tsne.webp",alt:"Amazon ratings visualized in language using t-SNE",width:"414",height:"290"}),e.jsx(t.p,{children:"The visualization seems to have produced roughly 3 clusters, one of which has mostly negative reviews."}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:'import pandas as pd\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport matplotlib\n\ndf = pd.read_csv(\'output/embedded_1k_reviews.csv\')\nmatrix = df.ada_embedding.apply(eval).to_list()\n\n# Create a t-SNE model and transform the data\ntsne = TSNE(n_components=2, perplexity=15, random_state=42, init=\'random\', learning_rate=200)\nvis_dims = tsne.fit_transform(matrix)\n\ncolors = ["red", "darkorange", "gold", "turquiose", "darkgreen"]\nx = [x for x,y in vis_dims]\ny = [y for x,y in vis_dims]\ncolor_indices = df.Score.values - 1\n\ncolormap = matplotlib.colors.ListedColormap(colors)\nplt.scatter(x, y, c=color_indices, cmap=colormap, alpha=0.3)\nplt.title("Amazon ratings visualized in language using t-SNE")\n'})})]}),"\n",e.jsxs(j,{label:"Embedding as a text feature encoder for ML algorithms",autoScroll:!0,showCollapse:!0,children:[e.jsx(E,{href:"https://cookbook.openai.com/examples/regression_using_embeddings",newTab:!0,children:"Regression_using_embeddings.ipynb"}),e.jsx(t.p,{children:"An embedding can be used as a general free-text feature encoder within a machine learning model. Incorporating embeddings will improve the performance of any machine learning model, if some of the relevant inputs are free text. An embedding can also be used as a categorical feature encoder within a ML model. This adds most value if the names of categorical variables are meaningful and numerous, such as job titles. Similarity embeddings generally perform better than search embeddings for this task."}),e.jsx(t.p,{children:"We observed that generally the embedding representation is very rich and information dense. For example, reducing the dimensionality of the inputs using SVD or PCA, even by 10%, generally results in worse downstream performance on specific tasks."}),e.jsx(t.p,{children:"This code splits the data into a training set and a testing set, which will be used by the following two use cases, namely regression and classification."}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    list(df.ada_embedding.values),\n    df.Score,\n    test_size = 0.2,\n    random_state=42\n)\n"})}),e.jsx(t.h4,{children:"Regression using the embedding features"}),e.jsx(t.p,{children:"Embeddings present an elegant way of predicting a numerical value. In this example we predict the reviewer’s star rating, based on the text of their review. Because the semantic information contained within embeddings is high, the prediction is decent even with very few reviews."}),e.jsx(t.p,{children:"We assume the score is a continuous variable between 1 and 5, and allow the algorithm to predict any floating point value. The ML algorithm minimizes the distance of the predicted value to the true score, and achieves a mean absolute error of 0.39, which means that on average the prediction is off by less than half a star."}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:"from sklearn.ensemble import RandomForestRegressor\n\nrfr = RandomForestRegressor(n_estimators=100)\nrfr.fit(X_train, y_train)\npreds = rfr.predict(X_test)\n"})})]}),"\n",e.jsxs(j,{label:"Classification using the embedding features",autoScroll:!0,showCollapse:!0,children:[e.jsx(E,{href:"https://cookbook.openai.com/examples/classification_using_embeddings",newTab:!0,children:"Classification_using_embeddings.ipynb"}),e.jsx(t.p,{children:"This time, instead of having the algorithm predict a value anywhere between 1 and 5, we will attempt to classify the exact number of stars for a review into 5 buckets, ranging from 1 to 5 stars."}),e.jsx(t.p,{children:"After the training, the model learns to predict 1 and 5-star reviews much better than the more nuanced reviews (2-4 stars), likely due to more extreme sentiment expression."}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\n\nclf = RandomForestClassifier(n_estimators=100)\nclf.fit(X_train, y_train)\npreds = clf.predict(X_test)\n"})})]}),"\n",e.jsxs(j,{label:"Zero-shot classification",autoScroll:!0,showCollapse:!0,children:[e.jsx(E,{href:"https://cookbook.openai.com/examples/zero-shot_classification_with_embeddings",newTab:!0,children:"Zero-shot_classification_with_embeddings.ipynb"}),e.jsx(t.p,{children:"We can use embeddings for zero shot classification without any labeled training data. For each class, we embed the class name or a short description of the class. To classify some new text in a zero-shot manner, we compare its embedding to all class embeddings and predict the class with the highest similarity."}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:"from openai.embeddings_utils import cosine_similarity, get_embedding\n\ndf= df[df.Score!=3]\ndf['sentiment'] = df.Score.replace({1:'negative', 2:'negative', 4:'positive', 5:'positive'})\n\nlabels = ['negative', 'positive']\nlabel_embeddings = [get_embedding(label, model=model) for label in labels]\n\ndef label_score(review_embedding, label_embeddings):\n   return cosine_similarity(review_embedding, label_embeddings[1]) - cosine_similarity(review_embedding, label_embeddings[0])\n\nprediction = 'positive' if label_score('Sample Review', label_embeddings) > 0 else 'negative'\n"})})]}),"\n",e.jsxs(j,{label:"Obtaining user and product embeddings for cold-start recommendation",autoScroll:!0,showCollapse:!0,children:[e.jsx(E,{href:"https://cookbook.openai.com/examples/user_and_product_embeddings",newTab:!0,children:"User_and_product_embeddings.ipynb"}),e.jsx(t.p,{children:"We can obtain a user embedding by averaging over all of their reviews. Similarly, we can obtain a product embedding by averaging over all the reviews about that product. In order to showcase the usefulness of this approach we use a subset of 50k reviews to cover more reviews per user and per product."}),e.jsx(t.p,{children:"We evaluate the usefulness of these embeddings on a separate test set, where we plot similarity of the user and product embedding as a function of the rating. Interestingly, based on this approach, even before the user receives the product we can predict better than random whether they would like the product."}),e.jsx(V,{png:"https://cdn.openai.com/API/docs/images/embeddings-boxplot.png",webp:"https://cdn.openai.com/API/docs/images/embeddings-boxplot.webp",alt:"Boxplot grouped by Score",width:"420",height:"312"}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:"user_embeddings = df.groupby('UserId').ada_embedding.apply(np.mean)\nprod_embeddings = df.groupby('ProductId').ada_embedding.apply(np.mean)\n"})})]}),"\n",e.jsxs(j,{label:"Clustering",autoScroll:!0,showCollapse:!0,children:[e.jsx(E,{href:"https://cookbook.openai.com/examples/clustering",newTab:!0,children:"Clustering.ipynb"}),e.jsx(t.p,{children:"Clustering is one way of making sense of a large volume of textual data. Embeddings are useful for this task, as they provide semantically meaningful vector representations of each text. Thus, in an unsupervised way, clustering will uncover hidden groupings in our dataset."}),e.jsx(t.p,{children:"In this example, we discover four distinct clusters: one focusing on dog food, one on negative reviews, and two on positive reviews."}),e.jsx(V,{png:"https://cdn.openai.com/API/docs/images/embeddings-cluster.png",webp:"https://cdn.openai.com/API/docs/images/embeddings-cluster.webp",alt:"Clusters identified visualized in language 2d using t-SNE",width:"418",height:"290"}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:"import numpy as np\nfrom sklearn.cluster import KMeans\n\nmatrix = np.vstack(df.ada_embedding.values)\nn_clusters = 4\n\nkmeans = KMeans(n_clusters = n_clusters, init='k-means++', random_state=42)\nkmeans.fit(matrix)\ndf['Cluster'] = kmeans.labels_\n"})})]}),"\n",e.jsx(t.h2,{children:"FAQ"}),"\n",e.jsx(t.h3,{children:"How can I tell how many tokens a string has before I embed it?"}),"\n",e.jsxs(t.p,{children:["In Python, you can split a string into tokens with OpenAI's tokenizer ",e.jsx(t.a,{href:"https://github.com/openai/tiktoken",children:e.jsx(t.code,{children:"tiktoken"})}),"."]}),"\n",e.jsx(t.p,{children:"Example code:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:'import tiktoken\n\ndef num_tokens_from_string(string: str, encoding_name: str) -> int:\n    """Returns the number of tokens in a text string."""\n    encoding = tiktoken.get_encoding(encoding_name)\n    num_tokens = len(encoding.encode(string))\n    return num_tokens\n\nnum_tokens_from_string("tiktoken is great!", "cl100k_base")\n'})}),"\n",e.jsxs(t.p,{children:["For third-generation embedding models like ",e.jsx(t.code,{children:"text-embedding-3-small"}),", use the ",e.jsx(t.code,{children:"cl100k_base"})," encoding."]}),"\n",e.jsxs(t.p,{children:["More details and example code are in the OpenAI Cookbook guide ",e.jsx(t.a,{href:"https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken",children:"how to count tokens with tiktoken"}),"."]}),"\n",e.jsx(t.h3,{children:"How can I retrieve K nearest embedding vectors quickly?"}),"\n",e.jsxs(t.p,{children:["For searching over many vectors quickly, we recommend using a vector database. You can find examples of working with vector databases and the OpenAI API ",e.jsx(t.a,{href:"https://cookbook.openai.com/examples/vector_databases/readme",children:"in our Cookbook"})," on GitHub."]}),"\n",e.jsx(t.h3,{children:"Which distance function should I use?"}),"\n",e.jsxs(t.p,{children:["We recommend ",e.jsx(t.a,{href:"https://en.wikipedia.org/wiki/Cosine_similarity",children:"cosine similarity"}),". The choice of distance function typically doesn’t matter much."]}),"\n",e.jsx(t.p,{children:"OpenAI embeddings are normalized to length 1, which means that:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Cosine similarity can be computed slightly faster using just a dot product"}),"\n",e.jsx(t.li,{children:"Cosine similarity and Euclidean distance will result in the identical rankings"}),"\n"]}),"\n",e.jsx(t.h3,{children:"Can I share my embeddings online?"}),"\n",e.jsxs(t.p,{children:["Yes, customers own their input and output from our models, including in the case of embeddings. You are responsible for ensuring that the content you input to our API does not violate any applicable law or our ",e.jsx(t.a,{href:"https://openai.com/policies/terms-of-use",children:"Terms of Use"}),"."]}),"\n",e.jsx(t.h3,{children:"Do V3 embedding models know about recent events?"}),"\n",e.jsxs(t.p,{children:["No, the ",e.jsx(t.code,{children:"text-embedding-3-large"})," and ",e.jsx(t.code,{children:"text-embedding-3-small"})," models lack knowledge of events that occurred after September 2021. This is generally not as much of a limitation as it would be for text generation models but in certain edge cases it can reduce performance."]})]})}function ri(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(nt,n)})):nt(n)}function st(n){const t=Object.assign({p:"p",a:"a",h2:"h2",table:"table",thead:"thead",tr:"tr",th:"th",tbody:"tbody",td:"td",strong:"strong",ul:"ul",li:"li",code:"code",h3:"h3",pre:"pre"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx("h1",{children:"Error codes"}),"\n",e.jsxs(t.p,{children:["This guide includes an overview on error codes you might see from both the ",e.jsx(t.a,{href:"/docs/introduction",children:"API"})," and our ",e.jsx(t.a,{href:"/docs/libraries/python-library",children:"official Python library"}),". Each error code mentioned in the overview has a dedicated section with further guidance."]}),"\n",e.jsx(t.h2,{children:"API errors"}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Code"}),e.jsx(t.th,{children:"Overview"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"401 - Invalid Authentication"}),e.jsxs(t.td,{children:[e.jsx(t.strong,{children:"Cause:"})," Invalid Authentication ",e.jsx("br",{})," ",e.jsx(t.strong,{children:"Solution:"})," Ensure the correct ",e.jsx(t.a,{href:"/account/api-keys",children:"API key"})," and requesting organization are being used."]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"401 - Incorrect API key provided"}),e.jsxs(t.td,{children:[e.jsx(t.strong,{children:"Cause:"})," The requesting API key is not correct. ",e.jsx("br",{})," ",e.jsx(t.strong,{children:"Solution:"})," Ensure the API key used is correct, clear your browser cache, or ",e.jsx(t.a,{href:"/account/api-keys",children:"generate a new one"}),"."]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"401 - You must be a member of an organization to use the API"}),e.jsxs(t.td,{children:[e.jsx(t.strong,{children:"Cause:"})," Your account is not part of an organization. ",e.jsx("br",{})," ",e.jsx(t.strong,{children:"Solution:"})," Contact us to get added to a new organization or ask your organization manager to ",e.jsx(t.a,{href:"/account/team",children:"invite you to an organization"}),"."]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"403 - Country, region, or territory not supported"}),e.jsxs(t.td,{children:[e.jsx(t.strong,{children:"Cause:"})," You are accessing the API from an unsupported country, region, or territory. ",e.jsx("br",{})," ",e.jsx(t.strong,{children:"Solution:"})," Please see ",e.jsx(t.a,{href:"/docs/supported-countries",children:"this page"})," for more information."]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"429 - Rate limit reached for requests"}),e.jsxs(t.td,{children:[e.jsx(t.strong,{children:"Cause:"})," You are sending requests too quickly. ",e.jsx("br",{})," ",e.jsx(t.strong,{children:"Solution:"})," Pace your requests. Read the ",e.jsx(t.a,{href:"/docs/guides/rate-limits",children:"Rate limit guide"}),"."]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"429 - You exceeded your current quota, please check your plan and billing details"}),e.jsxs(t.td,{children:[e.jsx(t.strong,{children:"Cause:"})," You have run out of credits or hit your maximum monthly spend. ",e.jsx("br",{})," ",e.jsx(t.strong,{children:"Solution:"})," ",e.jsx(t.a,{href:"/account/billing",children:"Buy more credits"})," or learn how to ",e.jsx(t.a,{href:"/account/limits",children:"increase your limits"}),"."]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"500 - The server had an error while processing your request"}),e.jsxs(t.td,{children:[e.jsx(t.strong,{children:"Cause:"})," Issue on our servers. ",e.jsx("br",{})," ",e.jsx(t.strong,{children:"Solution:"})," Retry your request after a brief wait and contact us if the issue persists. Check the ",e.jsx(t.a,{href:"https://status.openai.com/",children:"status page"}),"."]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"503 - The engine is currently overloaded, please try again later"}),e.jsxs(t.td,{children:[e.jsx(t.strong,{children:"Cause:"})," Our servers are experiencing high traffic. ",e.jsx("br",{})," ",e.jsx(t.strong,{children:"Solution:"})," Please retry your requests after a brief wait."]})]})]})]}),"\n",e.jsxs(j,{label:"401 - Invalid Authentication",autoScroll:!0,showCollapse:!0,children:[e.jsx(t.p,{children:"This error message indicates that your authentication credentials are invalid. This could happen for several reasons, such as:"}),e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"You are using a revoked API key."}),"\n",e.jsx(t.li,{children:"You are using a different API key than the one assigned to the requesting organization or project."}),"\n",e.jsx(t.li,{children:"You are using an API key that does not have the required permissions for the endpoint you are calling."}),"\n"]}),e.jsx(t.p,{children:"To resolve this error, please follow these steps:"}),e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["Check that you are using the correct API key and organization ID in your request header. You can find your API key and organization ID in ",e.jsx(t.a,{href:"/account/api-keys",children:"your account settings"})," or your can find specific project related keys under ",e.jsx(t.a,{href:"/settings/organization/general",children:"General settings"})," by selecting the desired project."]}),"\n",e.jsxs(t.li,{children:["If you are unsure whether your API key is valid, you can ",e.jsx(t.a,{href:"/account/api-keys",children:"generate a new one"}),". Make sure to replace your old API key with the new one in your requests and follow our ",e.jsx(t.a,{href:"https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety",children:"best practices guide"}),"."]}),"\n"]})]}),"\n",e.jsxs(j,{label:"401 - Incorrect API key provided",autoScroll:!0,showCollapse:!0,children:[e.jsx(t.p,{children:"This error message indicates that the API key you are using in your request is not correct. This could happen for several reasons, such as:"}),e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"There is a typo or an extra space in your API key."}),"\n",e.jsx(t.li,{children:"You are using an API key that belongs to a different organization or project."}),"\n",e.jsx(t.li,{children:"You are using an API key that has been deleted or deactivated."}),"\n",e.jsx(t.li,{children:"An old, revoked API key might be cached locally."}),"\n"]}),e.jsx(t.p,{children:"To resolve this error, please follow these steps:"}),e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Try clearing your browser's cache and cookies, then try again."}),"\n",e.jsx(t.li,{children:"Check that you are using the correct API key in your request header."}),"\n",e.jsxs(t.li,{children:["If you are unsure whether your API key is correct, you can ",e.jsx(t.a,{href:"/account/api-keys",children:"generate a new one"}),". Make sure to replace your old API key in your codebase and follow our ",e.jsx(t.a,{href:"https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety",children:"best practices guide"}),"."]}),"\n"]})]}),"\n",e.jsxs(j,{label:"401 - You must be a member of an organization to use the API",autoScroll:!0,showCollapse:!0,children:[e.jsx(t.p,{children:"This error message indicates that your account is not part of an organization. This could happen for several reasons, such as:"}),e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"You have left or been removed from your previous organization."}),"\n",e.jsx(t.li,{children:"You have left or been removed from your previous project."}),"\n",e.jsx(t.li,{children:"Your organization has been deleted."}),"\n"]}),e.jsx(t.p,{children:"To resolve this error, please follow these steps:"}),e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"If you have left or been removed from your previous organization, you can either request a new organization or get invited to an existing one."}),"\n",e.jsx(t.li,{children:"To request a new organization, reach out to us via help.openai.com"}),"\n",e.jsxs(t.li,{children:["Existing organization owners can invite you to join their organization via the ",e.jsx(t.a,{href:"/account/team",children:"Team page"})," or can create a new project from the ",e.jsx(t.a,{href:"settings/organization/general",children:"Settings page"})]}),"\n",e.jsx(t.li,{children:"If you have left or been removed from a previous project, you can ask your organization or project owner to add you to it, or create a new one."}),"\n"]})]}),"\n",e.jsxs(j,{label:"429 - Rate limit reached for requests",autoScroll:!0,showCollapse:!0,children:[e.jsx(t.p,{children:"This error message indicates that you have hit your assigned rate limit for the API. This means that you have submitted too many tokens or requests in a short period of time and have exceeded the number of requests allowed. This could happen for several reasons, such as:"}),e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"You are using a loop or a script that makes frequent or concurrent requests."}),"\n",e.jsx(t.li,{children:"You are sharing your API key with other users or applications."}),"\n",e.jsx(t.li,{children:"You are using a free plan that has a low rate limit."}),"\n",e.jsx(t.li,{children:"You have reached the defined limit on your project"}),"\n"]}),e.jsx(t.p,{children:"To resolve this error, please follow these steps:"}),e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Pace your requests and avoid making unnecessary or redundant calls."}),"\n",e.jsxs(t.li,{children:["If you are using a loop or a script, make sure to implement a backoff mechanism or a retry logic that respects the rate limit and the response headers. You can read more about our rate limiting policy and best practices in our ",e.jsx(t.a,{href:"/docs/guides/rate-limits",children:"rate limit guide"}),"."]}),"\n",e.jsx(t.li,{children:"If you are sharing your organization with other users, note that limits are applied per organization and not per user. It is worth checking on the usage of the rest of your team as this will contribute to the limit."}),"\n",e.jsxs(t.li,{children:["If you are using a free or low-tier plan, consider upgrading to a pay-as-you-go plan that offers a higher rate limit. You can compare the restrictions of each plan in our ",e.jsx(t.a,{href:"/docs/guides/rate-limits",children:"rate limit guide"}),"."]}),"\n",e.jsx(t.li,{children:"Reach out to your organization owner to increase the rate limits on your project"}),"\n"]})]}),"\n",e.jsxs(j,{label:"429 - You exceeded your current quota, please check your plan and billing details",autoScroll:!0,showCollapse:!0,children:[e.jsxs(t.p,{children:["This error message indicates that you hit your monthly ",e.jsx(t.a,{href:"/account/limits",children:"usage limit"})," for the API, or for prepaid credits customers that you've consumed all your credits. You can view your maximum usage limit on the ",e.jsx(t.a,{href:"/account/limits",children:"limits page"}),". This could happen for several reasons, such as:"]}),e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"You are using a high-volume or complex service that consumes a lot of credits or tokens."}),"\n",e.jsx(t.li,{children:"Your monthly budget is set too low for your organization’s usage."}),"\n",e.jsx(t.li,{children:"Your monthly budget is set too low for your project's usage."}),"\n"]}),e.jsx(t.p,{children:"To resolve this error, please follow these steps:"}),e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["Check your ",e.jsx(t.a,{href:"/account/usage",children:"current usage"})," of your account, and compare that to your account's ",e.jsx(t.a,{href:"/account/limits",children:"limits"}),"."]}),"\n",e.jsxs(t.li,{children:["If you are on a free plan, consider ",e.jsx(t.a,{href:"/account/billing",children:"upgrading to a paid plan"})," to get higher limits."]}),"\n",e.jsx(t.li,{children:"Reach out to your organization owner to increase the budgets for your project."}),"\n"]})]}),"\n",e.jsxs(j,{label:"503 - The engine is currently overloaded, please try again later",autoScroll:!0,showCollapse:!0,children:[e.jsx(t.p,{children:"This error message indicates that our servers are experiencing high traffic and are unable to process your request at the moment. This could happen for several reasons, such as:"}),e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"There is a sudden spike or surge in demand for our services."}),"\n",e.jsx(t.li,{children:"There is scheduled or unscheduled maintenance or update on our servers."}),"\n",e.jsx(t.li,{children:"There is an unexpected or unavoidable outage or incident on our servers."}),"\n"]}),e.jsx(t.p,{children:"To resolve this error, please follow these steps:"}),e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["Retry your request after a brief wait. We recommend using an exponential backoff strategy or a retry logic that respects the response headers and the rate limit. You can read more about our rate limit ",e.jsx(t.a,{href:"https://help.openai.com/en/articles/6891753-rate-limit-advice",children:"best practices"}),"."]}),"\n",e.jsxs(t.li,{children:["Check our ",e.jsx(t.a,{href:"https://status.openai.com/",children:"status page"})," for any updates or announcements regarding our services and servers."]}),"\n",e.jsx(t.li,{children:"If you are still getting this error after a reasonable amount of time, please contact us for further assistance. We apologize for any inconvenience and appreciate your patience and understanding."}),"\n"]})]}),"\n",e.jsx(t.h2,{children:"Python library error types"}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Type"}),e.jsx(t.th,{children:"Overview"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"APIConnectionError"}),e.jsxs(t.td,{children:[e.jsx(t.strong,{children:"Cause:"})," Issue connecting to our services. ",e.jsx("br",{})," ",e.jsx(t.strong,{children:"Solution:"})," Check your network settings, proxy configuration, SSL certificates, or firewall rules."]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"APITimeoutError"}),e.jsxs(t.td,{children:[e.jsx(t.strong,{children:"Cause:"})," Request timed out. ",e.jsx("br",{})," ",e.jsx(t.strong,{children:"Solution:"})," Retry your request after a brief wait and contact us if the issue persists."]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"AuthenticationError"}),e.jsxs(t.td,{children:[e.jsx(t.strong,{children:"Cause:"})," Your API key or token was invalid, expired, or revoked. ",e.jsx("br",{})," ",e.jsx(t.strong,{children:"Solution:"})," Check your API key or token and make sure it is correct and active. You may need to generate a new one from your account dashboard."]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"BadRequestError"}),e.jsxs(t.td,{children:[e.jsx(t.strong,{children:"Cause:"})," Your request was malformed or missing some required parameters, such as a token or an input. ",e.jsx("br",{})," ",e.jsx(t.strong,{children:"Solution:"})," The error message should advise you on the specific error made. Check the ",e.jsx(t.a,{href:"/docs/api-reference/",children:"documentation"})," for the specific API method you are calling and make sure you are sending valid and complete parameters. You may also need to check the encoding, format, or size of your request data."]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"ConflictError"}),e.jsxs(t.td,{children:[e.jsx(t.strong,{children:"Cause:"})," The resource was updated by another request. ",e.jsx("br",{})," ",e.jsx(t.strong,{children:"Solution:"})," Try to update the resource again and ensure no other requests are trying to update it."]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"InternalServerError"}),e.jsxs(t.td,{children:[e.jsx(t.strong,{children:"Cause:"})," Issue on our side. ",e.jsx("br",{})," ",e.jsx(t.strong,{children:"Solution:"})," Retry your request after a brief wait and contact us if the issue persists."]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"NotFoundError"}),e.jsxs(t.td,{children:[e.jsx(t.strong,{children:"Cause:"})," Requested resource does not exist. ",e.jsx("br",{})," ",e.jsx(t.strong,{children:"Solution:"})," Ensure you are the correct resource identifier."]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"PermissionDeniedError"}),e.jsxs(t.td,{children:[e.jsx(t.strong,{children:"Cause:"})," You don't have access to the requested resource. ",e.jsx("br",{})," ",e.jsx(t.strong,{children:"Solution:"})," Ensure you are using the correct API key, organization ID, and resource ID."]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"RateLimitError"}),e.jsxs(t.td,{children:[e.jsx(t.strong,{children:"Cause:"})," You have hit your assigned rate limit. ",e.jsx("br",{})," ",e.jsx(t.strong,{children:"Solution:"})," Pace your requests. Read more in our ",e.jsx(t.a,{href:"/docs/guides/rate-limits",children:"Rate limit guide"}),"."]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"UnprocessableEntityError"}),e.jsxs(t.td,{children:[e.jsx(t.strong,{children:"Cause:"})," Unable to process the request despite the format being correct. ",e.jsx("br",{})," ",e.jsx(t.strong,{children:"Solution:"})," Please try the request again."]})]})]})]}),"\n",e.jsxs(j,{label:"APIConnectionError",autoScroll:!0,showCollapse:!0,children:[e.jsxs(t.p,{children:["An ",e.jsx(t.code,{children:"APIConnectionError"})," indicates that your request could not reach our servers or establish a secure connection. This could be due to a network issue, a proxy configuration, an SSL certificate, or a firewall rule."]}),e.jsxs(t.p,{children:["If you encounter an ",e.jsx(t.code,{children:"APIConnectionError"}),", please try the following steps:"]}),e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Check your network settings and make sure you have a stable and fast internet connection. You may need to switch to a different network, use a wired connection, or reduce the number of devices or applications using your bandwidth."}),"\n",e.jsx(t.li,{children:"Check your proxy configuration and make sure it is compatible with our services. You may need to update your proxy settings, use a different proxy, or bypass the proxy altogether."}),"\n",e.jsx(t.li,{children:"Check your SSL certificates and make sure they are valid and up-to-date. You may need to install or renew your certificates, use a different certificate authority, or disable SSL verification."}),"\n",e.jsx(t.li,{children:"Check your firewall rules and make sure they are not blocking or filtering our services. You may need to modify your firewall settings."}),"\n",e.jsx(t.li,{children:"If appropriate, check that your container has the correct permissions to send and receive traffic."}),"\n",e.jsx(t.li,{children:"If the issue persists, check out our persistent errors next steps section."}),"\n"]})]}),"\n",e.jsxs(j,{label:"APITimeoutError",autoScroll:!0,showCollapse:!0,children:[e.jsxs(t.p,{children:["A ",e.jsx(t.code,{children:"APITimeoutError"})," error indicates that your request took too long to complete and our server closed the connection. This could be due to a network issue, a heavy load on our services, or a complex request that requires more processing time."]}),e.jsxs(t.p,{children:["If you encounter a ",e.jsx(t.code,{children:"APITimeoutError"})," error, please try the following steps:"]}),e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Wait a few seconds and retry your request. Sometimes, the network congestion or the load on our services may be reduced and your request may succeed on the second attempt."}),"\n",e.jsx(t.li,{children:"Check your network settings and make sure you have a stable and fast internet connection. You may need to switch to a different network, use a wired connection, or reduce the number of devices or applications using your bandwidth."}),"\n",e.jsx(t.li,{children:"If the issue persists, check out our persistent errors next steps section."}),"\n"]})]}),"\n",e.jsxs(j,{label:"AuthenticationError",autoScroll:!0,showCollapse:!0,children:[e.jsxs(t.p,{children:["An ",e.jsx(t.code,{children:"AuthenticationError"})," indicates that your API key or token was invalid, expired, or revoked. This could be due to a typo, a formatting error, or a security breach."]}),e.jsxs(t.p,{children:["If you encounter an ",e.jsx(t.code,{children:"AuthenticationError"}),", please try the following steps:"]}),e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Check your API key or token and make sure it is correct and active. You may need to generate a new key from the API Key dashboard, ensure there are no extra spaces or characters, or use a different key or token if you have multiple ones."}),"\n",e.jsx(t.li,{children:"Ensure that you have followed the correct formatting."}),"\n"]})]}),"\n",e.jsxs(j,{label:"BadRequestError",autoScroll:!0,showCollapse:!0,children:[e.jsxs(t.p,{children:["An ",e.jsx(t.code,{children:"BadRequestError"})," (formerly ",e.jsx(t.code,{children:"InvalidRequestError"}),") indicates that your request was malformed or missing some required parameters, such as a token or an input. This could be due to a typo, a formatting error, or a logic error in your code."]}),e.jsxs(t.p,{children:["If you encounter an ",e.jsx(t.code,{children:"BadRequestError"}),", please try the following steps:"]}),e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Read the error message carefully and identify the specific error made. The error message should advise you on what parameter was invalid or missing, and what value or format was expected."}),"\n",e.jsxs(t.li,{children:["Check the ",e.jsx(t.a,{href:"/docs/api-reference/",children:"API Reference"})," for the specific API method you were calling and make sure you are sending valid and complete parameters. You may need to review the parameter names, types, values, and formats, and ensure they match the documentation."]}),"\n",e.jsx(t.li,{children:"Check the encoding, format, or size of your request data and make sure they are compatible with our services. You may need to encode your data in UTF-8, format your data in JSON, or compress your data if it is too large."}),"\n",e.jsx(t.li,{children:"Test your request using a tool like Postman or curl and make sure it works as expected. You may need to debug your code and fix any errors or inconsistencies in your request logic."}),"\n",e.jsx(t.li,{children:"If the issue persists, check out our persistent errors next steps section."}),"\n"]})]}),"\n",e.jsxs(j,{label:"InternalServerError",autoScroll:!0,showCollapse:!0,children:[e.jsxs(t.p,{children:["An ",e.jsx(t.code,{children:"InternalServerError"})," indicates that something went wrong on our side when processing your request. This could be due to a temporary error, a bug, or a system outage."]}),e.jsxs(t.p,{children:["We apologize for any inconvenience and we are working hard to resolve any issues as soon as possible. You can ",e.jsx(t.a,{href:"https://status.openai.com/",children:"check our system status page"})," for more information."]}),e.jsxs(t.p,{children:["If you encounter an ",e.jsx(t.code,{children:"InternalServerError"}),", please try the following steps:"]}),e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Wait a few seconds and retry your request. Sometimes, the issue may be resolved quickly and your request may succeed on the second attempt."}),"\n",e.jsx(t.li,{children:"Check our status page for any ongoing incidents or maintenance that may affect our services. If there is an active incident, please follow the updates and wait until it is resolved before retrying your request."}),"\n",e.jsx(t.li,{children:"If the issue persists, check out our Persistent errors next steps section."}),"\n"]}),e.jsxs(t.p,{children:["Our support team will investigate the issue and get back to you as soon as possible. Note that our support queue times may be long due to high demand. You can also ",e.jsx(t.a,{href:"https://community.openai.com",children:"post in our Community Forum"})," but be sure to omit any sensitive information."]})]}),"\n",e.jsxs(j,{label:"RateLimitError",autoScroll:!0,showCollapse:!0,children:[e.jsxs(t.p,{children:["A ",e.jsx(t.code,{children:"RateLimitError"})," indicates that you have hit your assigned rate limit. This means that you have sent too many tokens or requests in a given period of time, and our services have temporarily blocked you from sending more."]}),e.jsx(t.p,{children:"We impose rate limits to ensure fair and efficient use of our resources and to prevent abuse or overload of our services."}),e.jsxs(t.p,{children:["If you encounter a ",e.jsx(t.code,{children:"RateLimitError"}),", please try the following steps:"]}),e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["Send fewer tokens or requests or slow down. You may need to reduce the frequency or volume of your requests, batch your tokens, or implement exponential backoff. You can read our ",e.jsx(t.a,{href:"/docs/guides/rate-limits",children:"Rate limit guide"})," for more details."]}),"\n",e.jsx(t.li,{children:"Wait until your rate limit resets (one minute) and retry your request. The error message should give you a sense of your usage rate and permitted usage."}),"\n",e.jsx(t.li,{children:"You can also check your API usage statistics from your account dashboard."}),"\n"]})]}),"\n",e.jsx(t.h3,{children:"Persistent errors"}),"\n",e.jsxs(t.p,{children:["If the issue persists, ",e.jsx(t.a,{href:"https://help.openai.com/en/",children:"contact our support team via chat"})," and provide them with the following information:"]}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"The model you were using"}),"\n",e.jsx(t.li,{children:"The error message and code you received"}),"\n",e.jsx(t.li,{children:"The request data and headers you sent"}),"\n",e.jsx(t.li,{children:"The timestamp and timezone of your request"}),"\n",e.jsx(t.li,{children:"Any other relevant details that may help us diagnose the issue"}),"\n"]}),"\n",e.jsxs(t.p,{children:["Our support team will investigate the issue and get back to you as soon as possible. Note that our support queue times may be long due to high demand. You can also ",e.jsx(t.a,{href:"https://community.openai.com",children:"post in our Community Forum"})," but be sure to omit any sensitive information."]}),"\n",e.jsx(t.h3,{children:"Handling errors"}),"\n",e.jsx(t.p,{children:"We advise you to programmatically handle errors returned by the API. To do so, you may want to use a code snippet like below:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:'import openai\nfrom openai import OpenAI\nclient = OpenAI()\n\ntry:\n  #Make your OpenAI API request here\n  response = client.chat.completions.create(\n    prompt="Hello world",\n    model="gpt-4o-mini"\n  )\nexcept openai.APIError as e:\n  #Handle API error here, e.g. retry or log\n  print(f"OpenAI API returned an API Error: {e}")\n  pass\nexcept openai.APIConnectionError as e:\n  #Handle connection error here\n  print(f"Failed to connect to OpenAI API: {e}")\n  pass\nexcept openai.RateLimitError as e:\n  #Handle rate limit error (we recommend using exponential backoff)\n  print(f"OpenAI API request exceeded rate limit: {e}")\n  pass\n'})})]})}function ai(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(st,n)})):st(n)}var li=function(n,t,s,i){function r(o){return o instanceof s?o:new s(function(l){l(o)})}return new(s||(s=Promise))(function(o,l){function d(h){try{m(i.next(h))}catch(x){l(x)}}function u(h){try{m(i.throw(h))}catch(x){l(x)}}function m(h){h.done?o(h.value):r(h.value).then(d,u)}m((i=i.apply(n,t||[])).next())})};function ci(n,t){return li(this,void 0,void 0,function*(){const s=new AudioContext({sampleRate:t});return s.decodeAudioData(n).finally(()=>s.close())})}function di(n){const t=n[0];if(t.some(s=>s>1||s<-1)){const s=t.length;let i=0;for(let r=0;r<s;r++){const o=Math.abs(t[r]);o>i&&(i=o)}for(const r of n)for(let o=0;o<s;o++)r[o]/=i}return n}function hi(n,t){return typeof n[0]=="number"&&(n=[n]),di(n),{duration:t,length:n[0].length,sampleRate:n[0].length/t,numberOfChannels:n.length,getChannelData:s=>n==null?void 0:n[s],copyFromChannel:AudioBuffer.prototype.copyFromChannel,copyToChannel:AudioBuffer.prototype.copyToChannel}}const le={decode:ci,createBuffer:hi};var it=function(n,t,s,i){function r(o){return o instanceof s?o:new s(function(l){l(o)})}return new(s||(s=Promise))(function(o,l){function d(h){try{m(i.next(h))}catch(x){l(x)}}function u(h){try{m(i.throw(h))}catch(x){l(x)}}function m(h){h.done?o(h.value):r(h.value).then(d,u)}m((i=i.apply(n,t||[])).next())})};function ui(n,t,s){var i,r;return it(this,void 0,void 0,function*(){const o=yield fetch(n,s);{const l=(i=o.clone().body)===null||i===void 0?void 0:i.getReader(),d=Number((r=o.headers)===null||r===void 0?void 0:r.get("Content-Length"));let u=0;const m=(h,x)=>it(this,void 0,void 0,function*(){if(h)return;u+=(x==null?void 0:x.length)||0;const w=Math.round(u/d*100);return t(w),l==null?void 0:l.read().then(({done:v,value:y})=>m(v,y))});l==null||l.read().then(({done:h,value:x})=>m(h,x))}return o.blob()})}const pi={fetchBlob:ui};class se{constructor(){this.listeners={},this.on=this.addEventListener,this.un=this.removeEventListener}addEventListener(t,s,i){if(this.listeners[t]||(this.listeners[t]=new Set),this.listeners[t].add(s),i!=null&&i.once){const r=()=>{this.removeEventListener(t,r),this.removeEventListener(t,s)};return this.addEventListener(t,r),r}return()=>this.removeEventListener(t,s)}removeEventListener(t,s){var i;(i=this.listeners[t])===null||i===void 0||i.delete(s)}once(t,s){return this.on(t,s,{once:!0})}unAll(){this.listeners={}}emit(t,...s){this.listeners[t]&&this.listeners[t].forEach(i=>i(...s))}}class mi extends se{constructor(t){super(),this.isExternalMedia=!1,t.media?(this.media=t.media,this.isExternalMedia=!0):this.media=document.createElement("audio"),t.mediaControls&&(this.media.controls=!0),t.autoplay&&(this.media.autoplay=!0),t.playbackRate!=null&&this.onceMediaEvent("canplay",()=>{t.playbackRate!=null&&(this.media.playbackRate=t.playbackRate)})}onMediaEvent(t,s,i){return this.media.addEventListener(t,s,i),()=>this.media.removeEventListener(t,s)}onceMediaEvent(t,s){return this.onMediaEvent(t,s,{once:!0})}getSrc(){return this.media.currentSrc||this.media.src||""}revokeSrc(){const t=this.getSrc();t.startsWith("blob:")&&URL.revokeObjectURL(t)}setSrc(t,s){if(this.getSrc()===t)return;this.revokeSrc();const r=s instanceof Blob?URL.createObjectURL(s):t;this.media.src=r,this.media.load()}destroy(){this.media.pause(),!this.isExternalMedia&&(this.media.remove(),this.revokeSrc(),this.media.src="",this.media.load())}setMediaElement(t){this.media=t}play(){return this.media.play()}pause(){this.media.pause()}isPlaying(){return!this.media.paused&&!this.media.ended}setTime(t){this.media.currentTime=t}getDuration(){return this.media.duration}getCurrentTime(){return this.media.currentTime}getVolume(){return this.media.volume}setVolume(t){this.media.volume=t}getMuted(){return this.media.muted}setMuted(t){this.media.muted=t}getPlaybackRate(){return this.media.playbackRate}setPlaybackRate(t,s){s!=null&&(this.media.preservesPitch=s),this.media.playbackRate=t}getMediaElement(){return this.media}setSinkId(t){return this.media.setSinkId(t)}}function xi(n,t,s,i,r=5){let o=()=>{};if(!n)return o;const l=d=>{if(d.button===2)return;d.preventDefault(),d.stopPropagation(),n.style.touchAction="none";let u=d.clientX,m=d.clientY,h=!1;const x=y=>{y.preventDefault(),y.stopPropagation();const _=y.clientX,T=y.clientY;if(h||Math.abs(_-u)>=r||Math.abs(T-m)>=r){const{left:L,top:S}=n.getBoundingClientRect();h||(h=!0,s==null||s(u-L,m-S)),t(_-u,T-m,_-L,T-S),u=_,m=T}},w=y=>{h&&(y.preventDefault(),y.stopPropagation())},v=()=>{n.style.touchAction="",h&&(i==null||i()),o()};document.addEventListener("pointermove",x),document.addEventListener("pointerup",v),document.addEventListener("pointerleave",v),document.addEventListener("click",w,!0),o=()=>{document.removeEventListener("pointermove",x),document.removeEventListener("pointerup",v),document.removeEventListener("pointerleave",v),setTimeout(()=>{document.removeEventListener("click",w,!0)},10)}};return n.addEventListener("pointerdown",l),()=>{o(),n.removeEventListener("pointerdown",l)}}class ie extends se{constructor(t,s){super(),this.timeouts=[],this.isScrolling=!1,this.audioData=null,this.resizeObserver=null,this.isDragging=!1,this.options=t;const i=this.parentFromOptionsContainer(t.container);this.parent=i;const[r,o]=this.initHtml();i.appendChild(r),this.container=r,this.scrollContainer=o.querySelector(".scroll"),this.wrapper=o.querySelector(".wrapper"),this.canvasWrapper=o.querySelector(".canvases"),this.progressWrapper=o.querySelector(".progress"),this.cursor=o.querySelector(".cursor"),s&&o.appendChild(s),this.initEvents()}parentFromOptionsContainer(t){let s;if(typeof t=="string"?s=document.querySelector(t):t instanceof HTMLElement&&(s=t),!s)throw new Error("Container not found");return s}initEvents(){const t=i=>{const r=this.wrapper.getBoundingClientRect(),o=i.clientX-r.left,l=i.clientX-r.left,d=o/r.width,u=l/r.height;return[d,u]};this.wrapper.addEventListener("click",i=>{const[r,o]=t(i);this.emit("click",r,o)}),this.wrapper.addEventListener("dblclick",i=>{const[r,o]=t(i);this.emit("dblclick",r,o)}),this.options.dragToSeek&&this.initDrag(),this.scrollContainer.addEventListener("scroll",()=>{const{scrollLeft:i,scrollWidth:r,clientWidth:o}=this.scrollContainer,l=i/r,d=(i+o)/r;this.emit("scroll",l,d)});const s=this.createDelay(100);this.resizeObserver=new ResizeObserver(()=>{s(()=>this.reRender())}),this.resizeObserver.observe(this.scrollContainer)}initDrag(){xi(this.wrapper,(t,s,i)=>{this.emit("drag",Math.max(0,Math.min(1,i/this.wrapper.getBoundingClientRect().width)))},()=>this.isDragging=!0,()=>this.isDragging=!1)}getHeight(){return this.options.height==null?128:isNaN(Number(this.options.height))?this.options.height==="auto"&&this.parent.clientHeight||128:Number(this.options.height)}initHtml(){const t=document.createElement("div"),s=t.attachShadow({mode:"open"});return s.innerHTML="\n      <style>\n        :host {\n          user-select: none;\n          min-width: 1px;\n        }\n        :host audio {\n          display: block;\n          width: 100%;\n        }\n        :host .scroll {\n          overflow-x: auto;\n          overflow-y: hidden;\n          width: 100%;\n          position: relative;\n        }\n        :host .noScrollbar {\n          scrollbar-color: transparent;\n          scrollbar-width: none;\n        }\n        :host .noScrollbar::-webkit-scrollbar {\n          display: none;\n          -webkit-appearance: none;\n        }\n        :host .wrapper {\n          position: relative;\n          overflow: visible;\n          z-index: 2;\n        }\n        :host .canvases {\n          min-height: ".concat(this.getHeight(),'px;\n        }\n        :host .canvases > div {\n          position: relative;\n        }\n        :host canvas {\n          display: block;\n          position: absolute;\n          top: 0;\n          image-rendering: pixelated;\n        }\n        :host .progress {\n          pointer-events: none;\n          position: absolute;\n          z-index: 2;\n          top: 0;\n          left: 0;\n          width: 0;\n          height: 100%;\n          overflow: hidden;\n        }\n        :host .progress > div {\n          position: relative;\n        }\n        :host .cursor {\n          pointer-events: none;\n          position: absolute;\n          z-index: 5;\n          top: 0;\n          left: 0;\n          height: 100%;\n          border-radius: 2px;\n        }\n      </style>\n\n      <div class="scroll" part="scroll">\n        <div class="wrapper" part="wrapper">\n          <div class="canvases"></div>\n          <div class="progress" part="progress"></div>\n          <div class="cursor" part="cursor"></div>\n        </div>\n      </div>\n    '),[t,s]}setOptions(t){if(this.options.container!==t.container){const s=this.parentFromOptionsContainer(t.container);s.appendChild(this.container),this.parent=s}t.dragToSeek&&!this.options.dragToSeek&&this.initDrag(),this.options=t,this.reRender()}getWrapper(){return this.wrapper}getScroll(){return this.scrollContainer.scrollLeft}destroy(){var t;this.container.remove(),(t=this.resizeObserver)===null||t===void 0||t.disconnect()}createDelay(t=10){const s={};return this.timeouts.push(s),i=>{s.timeout&&clearTimeout(s.timeout),s.timeout=setTimeout(i,t)}}convertColorValues(t){if(!Array.isArray(t))return t||"";if(t.length<2)return t[0]||"";const s=document.createElement("canvas"),r=s.getContext("2d").createLinearGradient(0,0,0,s.height),o=1/(t.length-1);return t.forEach((l,d)=>{const u=d*o;r.addColorStop(u,l)}),r}renderBarWaveform(t,s,i,r){const o=t[0],l=t[1]||t[0],d=o.length,{width:u,height:m}=i.canvas,h=m/2,x=window.devicePixelRatio||1,w=s.barWidth?s.barWidth*x:1,v=s.barGap?s.barGap*x:s.barWidth?w/2:0,y=s.barRadius||0,_=u/(w+v)/d,T=y&&"roundRect"in i?"roundRect":"rect";i.beginPath();let L=0,S=0,M=0;for(let k=0;k<=d;k++){const P=Math.round(k*_);if(P>L){const W=Math.round(S*h*r),K=Math.round(M*h*r),U=W+K||1;let oe=h-W;s.barAlign==="top"?oe=0:s.barAlign==="bottom"&&(oe=m-U),i[T](L*(w+v),oe,w,U,y),L=P,S=0,M=0}const O=Math.abs(o[k]||0),q=Math.abs(l[k]||0);O>S&&(S=O),q>M&&(M=q)}i.fill(),i.closePath()}renderLineWaveform(t,s,i,r){const o=l=>{const d=t[l]||t[0],u=d.length,{height:m}=i.canvas,h=m/2,x=i.canvas.width/u;i.moveTo(0,h);let w=0,v=0;for(let y=0;y<=u;y++){const _=Math.round(y*x);if(_>w){const L=Math.round(v*h*r)||1,S=h+L*(l===0?-1:1);i.lineTo(w,S),w=_,v=0}const T=Math.abs(d[y]||0);T>v&&(v=T)}i.lineTo(w,h)};i.beginPath(),o(0),o(1),i.fill(),i.closePath()}renderWaveform(t,s,i){if(i.fillStyle=this.convertColorValues(s.waveColor),s.renderFunction){s.renderFunction(t,i);return}let r=s.barHeight||1;if(s.normalize){const o=Array.from(t[0]).reduce((l,d)=>Math.max(l,Math.abs(d)),0);r=o?1/o:1}if(s.barWidth||s.barGap||s.barAlign){this.renderBarWaveform(t,s,i,r);return}this.renderLineWaveform(t,s,i,r)}renderSingleCanvas(t,s,i,r,o,l,d,u){const m=window.devicePixelRatio||1,h=document.createElement("canvas"),x=t[0].length;h.width=Math.round(i*(l-o)/x),h.height=r*m,h.style.width="".concat(Math.floor(h.width/m),"px"),h.style.height="".concat(r,"px"),h.style.left="".concat(Math.floor(o*i/m/x),"px"),d.appendChild(h);const w=h.getContext("2d");if(this.renderWaveform(t.map(v=>v.slice(o,l)),s,w),h.width>0&&h.height>0){const v=h.cloneNode(),y=v.getContext("2d");y.drawImage(h,0,0),y.globalCompositeOperation="source-in",y.fillStyle=this.convertColorValues(s.progressColor),y.fillRect(0,0,h.width,h.height),u.appendChild(v)}}renderChannel(t,s,i){const r=document.createElement("div"),o=this.getHeight();r.style.height="".concat(o,"px"),this.canvasWrapper.style.minHeight="".concat(o,"px"),this.canvasWrapper.appendChild(r);const l=r.cloneNode();this.progressWrapper.appendChild(l);const{scrollLeft:d,scrollWidth:u,clientWidth:m}=this.scrollContainer,h=t[0].length,x=h/u;let w=Math.min(ie.MAX_CANVAS_WIDTH,m);if(s.barWidth||s.barGap){const P=s.barWidth||.5,O=s.barGap||P/2,q=P+O;w%q!==0&&(w=Math.floor(w/q)*q)}const v=Math.floor(Math.abs(d)*x),y=Math.floor(v+w*x),_=y-v,T=(P,O)=>{this.renderSingleCanvas(t,s,i,o,Math.max(0,P),Math.min(O,h),r,l)},L=this.createDelay(),S=this.createDelay(),M=(P,O)=>{T(P,O),P>0&&L(()=>{M(P-_,O-_)})},k=(P,O)=>{T(P,O),O<h&&S(()=>{k(P+_,O+_)})};M(v,y),y<h&&k(y,y+_)}render(t){this.timeouts.forEach(d=>d.timeout&&clearTimeout(d.timeout)),this.timeouts=[],this.canvasWrapper.innerHTML="",this.progressWrapper.innerHTML="",this.wrapper.style.width="",this.options.width!=null&&(this.scrollContainer.style.width=typeof this.options.width=="number"?"".concat(this.options.width,"px"):this.options.width);const s=window.devicePixelRatio||1,i=this.scrollContainer.clientWidth,r=Math.ceil(t.duration*(this.options.minPxPerSec||0));this.isScrolling=r>i;const o=this.options.fillParent&&!this.isScrolling,l=(o?i:r)*s;if(this.wrapper.style.width=o?"100%":"".concat(r,"px"),this.scrollContainer.style.overflowX=this.isScrolling?"auto":"hidden",this.scrollContainer.classList.toggle("noScrollbar",!!this.options.hideScrollbar),this.cursor.style.backgroundColor="".concat(this.options.cursorColor||this.options.progressColor),this.cursor.style.width="".concat(this.options.cursorWidth,"px"),this.options.splitChannels)for(let d=0;d<t.numberOfChannels;d++){const u=Object.assign(Object.assign({},this.options),this.options.splitChannels[d]);this.renderChannel([t.getChannelData(d)],u,l)}else{const d=[t.getChannelData(0)];t.numberOfChannels>1&&d.push(t.getChannelData(1)),this.renderChannel(d,this.options,l)}this.audioData=t,this.emit("render")}reRender(){if(!this.audioData)return;const t=this.progressWrapper.clientWidth;this.render(this.audioData);const s=this.progressWrapper.clientWidth;this.scrollContainer.scrollLeft+=s-t}zoom(t){this.options.minPxPerSec=t,this.reRender()}scrollIntoView(t,s=!1){const{clientWidth:i,scrollLeft:r,scrollWidth:o}=this.scrollContainer,l=o*t,d=i/2,u=s&&this.options.autoCenter&&!this.isDragging?d:i;if(l>r+u||l<r)if(this.options.autoCenter&&!this.isDragging){const m=d/20;l-(r+d)>=m&&l<r+i?this.scrollContainer.scrollLeft+=m:this.scrollContainer.scrollLeft=l-d}else this.isDragging?this.scrollContainer.scrollLeft=l<r?l-10:l-i+10:this.scrollContainer.scrollLeft=l;{const{scrollLeft:m}=this.scrollContainer,h=m/o,x=(m+i)/o;this.emit("scroll",h,x)}}renderProgress(t,s){if(isNaN(t))return;const i=t*100;this.canvasWrapper.style.clipPath="polygon(".concat(i,"% 0, 100% 0, 100% 100%, ").concat(i,"% 100%)"),this.progressWrapper.style.width="".concat(i,"%"),this.cursor.style.left="".concat(i,"%"),this.cursor.style.marginLeft=Math.round(i)===100?"-".concat(this.options.cursorWidth,"px"):"",this.isScrolling&&this.options.autoScroll&&this.scrollIntoView(t,s)}}ie.MAX_CANVAS_WIDTH=4e3;class gi extends se{constructor(){super(...arguments),this.unsubscribe=()=>{}}start(){this.unsubscribe=this.on("tick",()=>{requestAnimationFrame(()=>{this.emit("tick")})}),this.emit("tick")}stop(){this.unsubscribe()}destroy(){this.unsubscribe()}}var ce=function(n,t,s,i){function r(o){return o instanceof s?o:new s(function(l){l(o)})}return new(s||(s=Promise))(function(o,l){function d(h){try{m(i.next(h))}catch(x){l(x)}}function u(h){try{m(i.throw(h))}catch(x){l(x)}}function m(h){h.done?o(h.value):r(h.value).then(d,u)}m((i=i.apply(n,t||[])).next())})};class fi extends se{constructor(t=new AudioContext){super(),this.bufferNode=null,this.autoplay=!1,this.playStartTime=0,this.playedDuration=0,this._muted=!1,this.buffer=null,this.currentSrc="",this.paused=!0,this.crossOrigin=null,this.audioContext=t,this.gainNode=this.audioContext.createGain(),this.gainNode.connect(this.audioContext.destination)}load(){return ce(this,void 0,void 0,function*(){})}get src(){return this.currentSrc}set src(t){this.currentSrc=t,fetch(t).then(s=>s.arrayBuffer()).then(s=>this.audioContext.decodeAudioData(s)).then(s=>{this.buffer=s,this.emit("loadedmetadata"),this.emit("canplay"),this.autoplay&&this.play()})}_play(){var t;this.paused&&(this.paused=!1,(t=this.bufferNode)===null||t===void 0||t.disconnect(),this.bufferNode=this.audioContext.createBufferSource(),this.bufferNode.buffer=this.buffer,this.bufferNode.connect(this.gainNode),this.playedDuration>=this.duration&&(this.playedDuration=0),this.bufferNode.start(this.audioContext.currentTime,this.playedDuration),this.playStartTime=this.audioContext.currentTime,this.bufferNode.onended=()=>{this.currentTime>=this.duration&&(this.pause(),this.emit("ended"))})}_pause(){var t;this.paused||(this.paused=!0,(t=this.bufferNode)===null||t===void 0||t.stop(),this.playedDuration+=this.audioContext.currentTime-this.playStartTime)}play(){return ce(this,void 0,void 0,function*(){this._play(),this.emit("play")})}pause(){this._pause(),this.emit("pause")}setSinkId(t){return ce(this,void 0,void 0,function*(){return this.audioContext.setSinkId(t)})}get playbackRate(){var t,s;return(s=(t=this.bufferNode)===null||t===void 0?void 0:t.playbackRate.value)!==null&&s!==void 0?s:1}set playbackRate(t){this.bufferNode&&(this.bufferNode.playbackRate.value=t)}get currentTime(){return this.paused?this.playedDuration:this.playedDuration+this.audioContext.currentTime-this.playStartTime}set currentTime(t){this.emit("seeking"),this.paused?this.playedDuration=t:(this._pause(),this.playedDuration=t,this._play()),this.emit("timeupdate")}get duration(){var t;return((t=this.buffer)===null||t===void 0?void 0:t.duration)||0}get volume(){return this.gainNode.gain.value}set volume(t){this.gainNode.gain.value=t,this.emit("volumechange")}get muted(){return this._muted}set muted(t){this._muted!==t&&(this._muted=t,this._muted?this.gainNode.disconnect():this.gainNode.connect(this.audioContext.destination))}getGainNode(){return this.gainNode}}var Y=function(n,t,s,i){function r(o){return o instanceof s?o:new s(function(l){l(o)})}return new(s||(s=Promise))(function(o,l){function d(h){try{m(i.next(h))}catch(x){l(x)}}function u(h){try{m(i.throw(h))}catch(x){l(x)}}function m(h){h.done?o(h.value):r(h.value).then(d,u)}m((i=i.apply(n,t||[])).next())})};const ji={waveColor:"#999",progressColor:"#555",cursorWidth:1,minPxPerSec:0,fillParent:!0,interact:!0,dragToSeek:!1,autoScroll:!0,autoCenter:!0,sampleRate:8e3};class je extends mi{static create(t){return new je(t)}constructor(t){const s=t.media||(t.backend==="WebAudio"?new fi:void 0);super({media:s,mediaControls:t.mediaControls,autoplay:t.autoplay,playbackRate:t.audioRate}),this.plugins=[],this.decodedData=null,this.subscriptions=[],this.mediaSubscriptions=[],this.options=Object.assign({},ji,t),this.timer=new gi;const i=s?void 0:this.getMediaElement();this.renderer=new ie(this.options,i),this.initPlayerEvents(),this.initRendererEvents(),this.initTimerEvents(),this.initPlugins();const r=this.options.url||this.getSrc();r?this.load(r,this.options.peaks,this.options.duration):this.options.peaks&&this.options.duration&&this.loadPredecoded()}initTimerEvents(){this.subscriptions.push(this.timer.on("tick",()=>{const t=this.getCurrentTime();this.renderer.renderProgress(t/this.getDuration(),!0),this.emit("timeupdate",t),this.emit("audioprocess",t)}))}initPlayerEvents(){this.mediaSubscriptions.push(this.onMediaEvent("timeupdate",()=>{const t=this.getCurrentTime();this.renderer.renderProgress(t/this.getDuration(),this.isPlaying()),this.emit("timeupdate",t)}),this.onMediaEvent("play",()=>{this.emit("play"),this.timer.start()}),this.onMediaEvent("pause",()=>{this.emit("pause"),this.timer.stop()}),this.onMediaEvent("emptied",()=>{this.timer.stop()}),this.onMediaEvent("ended",()=>{this.emit("finish")}),this.onMediaEvent("seeking",()=>{this.emit("seeking",this.getCurrentTime())}))}initRendererEvents(){this.subscriptions.push(this.renderer.on("click",(t,s)=>{this.options.interact&&(this.seekTo(t),this.emit("interaction",t*this.getDuration()),this.emit("click",t,s))}),this.renderer.on("dblclick",(t,s)=>{this.emit("dblclick",t,s)}),this.renderer.on("scroll",(t,s)=>{const i=this.getDuration();this.emit("scroll",t*i,s*i)}),this.renderer.on("render",()=>{this.emit("redraw")}));{let t;this.subscriptions.push(this.renderer.on("drag",s=>{this.options.interact&&(this.renderer.renderProgress(s),clearTimeout(t),t=setTimeout(()=>{this.seekTo(s)},this.isPlaying()?0:200),this.emit("interaction",s*this.getDuration()),this.emit("drag",s))}))}}initPlugins(){var t;!((t=this.options.plugins)===null||t===void 0)&&t.length&&this.options.plugins.forEach(s=>{this.registerPlugin(s)})}unsubscribePlayerEvents(){this.mediaSubscriptions.forEach(t=>t()),this.mediaSubscriptions=[]}setOptions(t){this.options=Object.assign({},this.options,t),this.renderer.setOptions(this.options),t.audioRate&&this.setPlaybackRate(t.audioRate),t.mediaControls!=null&&(this.getMediaElement().controls=t.mediaControls)}registerPlugin(t){return t.init(this),this.plugins.push(t),this.subscriptions.push(t.once("destroy",()=>{this.plugins=this.plugins.filter(s=>s!==t)})),t}getWrapper(){return this.renderer.getWrapper()}getScroll(){return this.renderer.getScroll()}getActivePlugins(){return this.plugins}loadPredecoded(){return Y(this,void 0,void 0,function*(){this.options.peaks&&this.options.duration&&(this.decodedData=le.createBuffer(this.options.peaks,this.options.duration),yield Promise.resolve(),this.renderDecoded())})}renderDecoded(){return Y(this,void 0,void 0,function*(){this.decodedData&&(this.emit("decode",this.getDuration()),this.renderer.render(this.decodedData))})}loadAudio(t,s,i,r){return Y(this,void 0,void 0,function*(){if(this.emit("load",t),!this.options.media&&this.isPlaying()&&this.pause(),this.decodedData=null,!s&&!i){const o=l=>this.emit("loading",l);s=yield pi.fetchBlob(t,o,this.options.fetchParams)}if(this.setSrc(t,s),r=(yield Promise.resolve(r||this.getDuration()))||(yield new Promise(o=>{this.onceMediaEvent("loadedmetadata",()=>o(this.getDuration()))}))||(yield Promise.resolve(0)),i)this.decodedData=le.createBuffer(i,r);else if(s){const o=yield s.arrayBuffer();this.decodedData=yield le.decode(o,this.options.sampleRate)}this.renderDecoded(),this.emit("ready",this.getDuration())})}load(t,s,i){return Y(this,void 0,void 0,function*(){yield this.loadAudio(t,void 0,s,i)})}loadBlob(t,s,i){return Y(this,void 0,void 0,function*(){yield this.loadAudio("blob",t,s,i)})}zoom(t){if(!this.decodedData)throw new Error("No audio loaded");this.renderer.zoom(t),this.emit("zoom",t)}getDecodedData(){return this.decodedData}exportPeaks({channels:t=2,maxLength:s=8e3,precision:i=1e4}={}){if(!this.decodedData)throw new Error("The audio has not been decoded yet");const r=Math.min(t,this.decodedData.numberOfChannels),o=[];for(let l=0;l<r;l++){const d=this.decodedData.getChannelData(l),u=[],m=Math.round(d.length/s);for(let h=0;h<s;h++){const x=d.slice(h*m,(h+1)*m),w=Math.max(...x);u.push(Math.round(w*i)/i)}o.push(u)}return o}getDuration(){let t=super.getDuration()||0;return(t===0||t===1/0)&&this.decodedData&&(t=this.decodedData.duration),t}toggleInteraction(t){this.options.interact=t}seekTo(t){const s=this.getDuration()*t;this.setTime(s)}playPause(){return Y(this,void 0,void 0,function*(){return this.isPlaying()?this.pause():this.play()})}stop(){this.pause(),this.setTime(0)}skip(t){this.setTime(this.getCurrentTime()+t)}empty(){this.load("",[[0]],.001)}setMediaElement(t){this.unsubscribePlayerEvents(),super.setMediaElement(t),this.initPlayerEvents()}destroy(){this.emit("destroy"),this.plugins.forEach(t=>t.destroy()),this.subscriptions.forEach(t=>t()),this.unsubscribePlayerEvents(),this.timer.destroy(),this.renderer.destroy(),super.destroy()}}const yi=[[0,0,0,0,0,0,0,0,0,0,-2,0,-1,1,0,1,-12,5,-17,9,-16,8,-14,10,-15,12,-14,15,-16,17,-19,20,-19,15,-24,24,-19,19,-21,19,-18,19,-16,18,-12,12,-5,4,-28,43,-30,39,-28,38,-33,38,-35,36,-40,33,-40,33,-39,31,-41,37,-31,33,-24,31,-19,27,-19,27,-18,25,-18,23,-18,22,-24,21,-28,18,-29,18,-30,25,-39,35,-36,36,-38,43,-39,49,-31,50,-40,48,-36,48,-34,51,-39,48,-52,41,-54,41,-53,40,-43,38,-41,33,-30,24,-18,19,-12,18,-9,17,-8,18,-12,21,-10,18,-13,19,-14,18,-15,18,-14,18,-13,17,-11,16,-10,15,-9,13,-16,12,-9,14,-7,12,-9,11,-13,15,-12,15,-13,14,-11,12,-13,16,-15,17,-15,17,-15,17,-15,16,-15,17,-14,16,-14,16,-14,16,-13,15,-12,14,-8,12,-7,5,-1,4,0,0,0,0,0,0,0,0,0,0,-3,5,-7,7,-8,13,-14,13,-13,14,-13,17,-15,18,-17,21,-18,21,-19,21,-21,21,-21,21,-20,20,-20,19,-18,18,-14,12,-7,7,-8,8,-11,11,-9,11,-9,10,-11,11,-11,9,-10,11,-12,13,-10,9,-8,10,-8,7,0,0,0,0,0,0,0,0,-7,4,-13,15,-8,12,-1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1,1,-2,2,-3,4,-5,7,-6,12,-6,13,-5,12,-6,12,-8,13,-9,13,-9,13,-5,10,-3,5,-6,7,-13,12,-9,10,-15,14,-13,12,-10,13,-16,16,-19,18,-13,12,-14,11,-4,3,-15,34,-14,21,-17,25,-20,22,-18,24,-11,12,-8,10,-4,6,0,0,0,0,0,0,0,0,0,0,-1,1,-7,7,-19,12,-10,9,-8,8,-7,7,-3,3,-2,1,-13,14,-14,13,-13,13,-13,12,-13,10,-13,10,-12,9,-11,8,-11,6,-10,6,-8,6,-8,6,-7,7,-7,6,-7,5,-7,5,-6,4,-5,4,-4,4,-3,5,-6,5,-8,12,-11,14,-14,21,-12,18,-14,18,-18,21,-19,14,-18,16,-25,22,-23,21,-11,19,-4,5,-3,4,-2,2,-2,6,-4,4,-1,1,-5,6,-9,8,-6,6,-4,4,-6,7,-8,8,-9,9,-7,7,-7,7,-7,8,-10,10,-10,8,-6,8,-4,5,0,0,0,0,0,0,0,0,0,0,0,0,-7,9,-14,12,-8,9,-3,4,-2,1,-1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-2,2,-10,4,-20,27,-21,32,-20,33,-19,32,-14,23,-12,12,-15,18,-20,20,-26,21,-24,21,-19,20,-19,16,-30,24,-29,23,-28,30,-25,26,-18,24,-23,20,-13,14,-39,33,-37,34,-31,32,-26,32,-19,31,-24,30,-25,30,-29,31,-28,28,-12,16,-1,1,0,0,0,0,-7,5,-9,22,-25,31,-23,35,-13,31,-12,30,-8,28,-9,15,-8,9,-3,5,-2,2,-2,1,-2,3,-2,2,-1,2,-1,1,-2,2,-3,2,-4,6,-49,48,-43,42,-38,50,-34,47,-44,45,-46,44,-45,36,-30,51,-28,48,-40,44,-40,43,-39,35,-32,27,-24,14,-11,9,-8,5,-7,4,0,1,0,0,0,0,0,0,0,0,0,0,0,0,-7,6,-16,17,-6,7,-3,3,-3,3,-3,3,-3,3,-4,4,-4,4,-2,1,-6,5,-14,13,-11,11,-12,12,-13,10,-11,10,-9,10,-10,10,-11,8,-11,12,-4,3,-1,1,0,0,0,0,0,0,0,0,-4,4,-22,14,-23,21,-19,17,-14,13,-14,7,-13,10,-15,13,-15,15,-16,14,-15,12,-14,11,-15,9,-12,8,-9,8,-9,9,-8,7,-7,6,-4,4,-3,1,-2,7,-6,11,-10,9,-10,14,-22,17,-31,19,-21,19,-29,26,-18,15,-17,17,-22,25,-27,29,-27,31,-28,31,-30,31,-34,31,-38,36,-41,37,-42,37,-42,33,-38,32,-33,34,-32,30,-24,20,-16,12,-13,9,-8,7,-4,7,-2,2,0,0,-4,3,-6,7,0,0,0,0,0,0,0,0,0,0,-2,3,-14,22,-22,27,-16,16,-15,19,-14,16,-13,15,-13,14,-14,13,-18,12,-18,11,-16,10,-9,11,-5,7,-3,4,-3,3,-2,3,-3,4,-4,2,-3,2,-1,2,-11,4,-26,20,-21,18,-21,20,-19,20,-18,20,-17,19,-16,19,-19,16,-18,16,-17,14,-16,14,-17,15,-19,18,-15,21,-13,20,-12,17,-11,14,-9,12,-9,10,-11,12,-13,13,-13,13,-13,12,-10,10,-13,7,-11,8,-13,13,-8,6,-3,5,-3,2,-1,1,-1,1,-1,1,-1,0,0,0,-1,1,-12,9,-11,7,-11,6,-9,6,-9,8,-4,7,-1,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1,0,-1,2,-11,37,-18,38,-24,32,-32,29,-31,33,-26,30,-42,29,-34,37,-28,40,-40,47,-46,39,-38,27,-30,22,-37,20,-37,18,-34,19,-30,20,-25,19,-18,18,-19,22,-18,20,-17,16,-20,15,-19,13,-16,11,-10,11,-15,13,-18,15,-19,15,-19,14,-19,15,-19,15,-19,15,-19,15,-19,14,-17,14,-16,12,-10,10,-8,10,-8,9,-10,8,-8,10,-9,10,-11,10,-11,11,-11,10,-10,10,-6,9,-4,7,-3,4,-3,4,-2,4,-3,3,-4,4,-6,6,-6,8,-6,9,-6,10,-7,10,-7,11,-7,11,-7,11,-6,11,-7,12,-7,13,-7,13,-7,12,-6,12,-5,11,-4,11,-5,10,-4,9,-5,10,-5,9,-5,8,-4,7,-4,6,-4,6,-4,3,-4,5,-3,5,-3,5,-3,7,-3,8,-2,7,-4,3,-3,3,-2,2,-1,1,-2,1,-3,4,-6,6,-7,8,-6,7,-14,15,-15,13,-12,12,-12,12,-9,10,-11,11,-9,7,-11,13,-8,10,-6,6,-1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,-1,3,-12,12,-19,19,-22,24,-18,19,-28,26,-30,31,-32,32,-31,32,-29,30,-26,30,-43,41,-44,37,-40,29,-22,15,-11,9,-11,8,-9,7,-5,5,-8,8,-6,7,-5,5,-5,4,-5,4,-7,5,-13,11,-24,18,-42,31,-49,47,-55,53,-50,49,-45,45,-37,43,-44,41,-48,39,-43,36,-38,37,-34,33,-12,9,-13,10,-19,18,-35,28,-34,26,-17,12,-9,6,-4,5,-3,3,-5,3,-5,3,-4,4,-3,3,-5,4,-6,8,-18,9,-32,13,-35,16,-25,16,-21,19,-21,21,-22,24,-22,21,-23,23,-26,27,-20,18,-18,20,-22,21,-22,23,-29,24,-27,29,-29,28,-11,12,-23,18,-35,23,-43,25,-42,27,-43,27,-37,29,-25,16,-5,9,-4,7,-6,8,-8,8,-10,9,-5,9,-9,8,-10,6,-3,2,-7,5,-4,4,-3,3,-2,2,-3,4,-2,3,-1,1,-2,3,-2,2,-1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1,0,-14,20,-17,21,-18,25,-20,26,-21,27,-22,25,-24,24,-25,22,-25,23,-25,23,-21,24,-19,20,-14,12,-13,16,-16,14,-19,27,-15,24,-10,14,-14,18,-17,22,-21,20,-16,20,-16,17,-12,14,-13,15,-10,13,-8,13,-8,13,-8,12,-7,12,-7,12,-8,12,-9,12,-10,18,-14,23,-24,25,-23,20,-31,29,-42,27,-42,30,-40,27,-43,31,-38,31,-45,31,-43,30,-41,29,-40,26,-34,29,-30,32,-33,31,-36,24,-37,16,-35,19,-34,13,-23,13,-11,10,-8,8,-4,6,-2,2,-2,2,-10,11,-14,15,-8,7,-4,4,-1,1,0,0,0,0,0,0,0,0,0,0,0,0,-2,1,-2,4,-4,2,-5,2,0,4,-3,2,-3,2,-2,3,-15,11,-32,18,-44,37,-48,45,-54,45,-53,36,-55,29,-28,25,-29,33,-18,19,-14,14,-14,15,-6,13,-7,8,-7,8,-4,4,-2,2,0,0,0,0,0,0,0,0,0,0,0,0,-1,2,-1,1,-1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-4,3,-13,9,-21,13,-26,14,-30,16,-31,19,-30,17,-26,16,-11,3,-4,1,-1,1,0,0,0,0,0,0,0,0,0,0,-5,3,-5,3,-1,3,-1,3,-9,6,-14,11,-17,13,-18,14,-16,14,-11,7,-4,5,-2,4,0,1,0,0,0,0,0,0,-2,1,-7,10,-32,26,-38,31,-37,32,-31,29,-27,24,-10,11,-7,7,-6,6,-5,5,-1,3,0,0,-3,3,-2,3,-19,6,-25,17,-34,25,-44,26,-45,22,-39,21,-35,20,-33,20,-30,19,-20,14,-9,10,-2,4,0,0,0,0,0,0,-9,8,-14,12,-9,9,-3,3,-3,2,-8,4,-9,10,-13,15,-11,14,-10,11,-6,10,-5,7,-6,8,-7,9,-10,11,-18,17,-18,19,-15,21,-12,15,-13,15,-8,10,-8,8,-8,9,-9,11,-11,14,-12,15,-12,16,-13,21,-13,17,-17,20,-16,31,-20,34,-21,40,-19,40,-21,45,-20,44,-19,45,-23,46,-20,42,-24,45,-17,39,-19,31,-21,43,-22,27,-13,16,-7,10,-4,5,-1,0,0,0,0,0,0,0,0,0,-2,1,-2,1,-3,2,-8,7,-7,9,-9,10,-9,7,-12,11,-12,10,-8,8,-6,5,-5,3,-9,13,-10,17,-9,17,-8,12,-8,12,-10,11,-10,11,-10,12,-8,13,-7,13,-8,13,-9,13,-10,12,-12,12,-13,12,-13,11,-14,12,-14,11,-13,7,-10,4,-5,1,-3,3,-7,8,-11,12,-5,4,-6,3,-16,12,-18,17,-14,16,-13,15,-12,16,-12,15,-12,15,-11,20,-15,22,-14,18,-12,18,-11,16,-9,15,-9,15,-9,14,-12,12,-14,11,-14,13,-15,13,-16,12,-17,11,-18,11,-14,16,-15,14,-15,14,-15,13,-14,14,-13,13,-14,12,-17,12,-19,11,-21,9,-17,9,-5,7,-1,2,-1,0,0,0,0,0,0,0,-8,7,-12,10,-15,20,-15,13,-10,11,-7,7,-3,4,-5,8,-7,4,-4,3,-2,1,-3,2,-1,0,-1,2,-1,1,-1,1,-1,2,-1,2,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-11,11,-10,26,-9,21,-9,3,-3,2,0,1,0,0,0,0,0,0,0,0,-2,3,-5,2,-2,6,-11,9,-18,10,-27,14,-41,27,-39,38,-45,60,-42,61,-38,44,-32,40,-43,30,-40,28,-30,27,-29,24,-34,16,-13,9,-2,3,0,0,0,0,0,0,0,0,-2,3,-4,4,-3,3,-3,3,-12,6,-24,11,-34,13,-47,15,-45,20,-39,18,-28,11,-22,21,-22,17,-19,16,-19,18,-20,15,-13,15,-17,25,-21,22,-15,5,-10,5,-9,4,-7,4,-5,2,-1,1,0,0,0,0,0,0,0,0,0,0,-1,2,-4,4,-6,5,-11,6,-13,7,-14,8,-19,11,-21,12,-26,14,-27,16,-30,18,-31,19,-30,23,-31,25,-30,25,-10,11,-14,12,-18,21,-22,25,-25,25,-20,22,-10,7,-5,4,-5,5,-5,5,-3,3,-3,3,-2,2,-1,2,-1,0,-12,12,-22,14,-27,16,-29,18,-26,20,-18,11,-6,6,-3,3,0,0,0,0,0,0,0,0,0,0,0,0,-1,3,-8,2,-3,5,-3,7,-4,3,-3,4,-4,6,-7,7,-9,7,-17,8,-22,10,-25,16,-33,21,-34,27,-30,27,-32,25,-29,23,-22,19,-22,20,-23,24,-17,16,-16,14,-18,13,-20,12,-14,7,-12,13,-8,6,-11,6,-11,7,-5,6,-8,4,-2,1,-2,3,-2,1,-5,1,-5,2,-10,8,-13,11,-14,12,-12,10,-6,7,-3,3,-1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-3,10,-13,10,-18,23,-25,23,-30,28,-34,37,-37,42,-37,42,-37,41,-26,28,-7,8,-4,3,-1,0,0,0,0,0,0,0,0,0,0,0,0,0,-4,2,-3,3,-1,1,-4,5,-12,11,-10,9,-12,13,-11,9,-16,14,-12,13,-9,10,-9,8,-10,8,-6,5,-8,9,-3,3,-2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-2,1,-7,8,-6,7,-7,8,-9,9,-9,8,-9,7,-9,5,-5,4,-3,3,-4,6,-9,9,-13,13,-16,13,-13,14,-16,19,-21,14,-17,18,-15,16,-18,18,-20,18,-24,27,-11,15,-7,7,-3,3,-2,1,0,1,0,0,0,0,0,0,-4,5,-12,6,-16,9,-23,15,-20,16,-24,20,-24,18,-22,19,-24,22,-26,24,-24,24,-21,23,-20,21,-19,18,-15,13,-12,12,-17,13,-12,16,-10,12,-8,12,-4,9,-6,2,-5,9,-7,5,-3,5,-3,3,-3,2,-2,2,-2,1,0,1,-1,1,0,0,-1,1,-1,1,-1,1,-1,0,-2,2,-2,1,-1,0,-1,2,-4,4,-5,5,-8,8,-8,8,-9,8,-7,5,-3,4,-4,4,-5,5,-5,5,-1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,1,-3,3,-4,3,-12,29,-16,28,-20,32,-14,28,-17,28,-18,28,-21,28,-20,31,-20,35,-19,40,-29,39,-58,48,-60,52,-60,60,-56,60,-57,60,-57,61,-61,60,-61,61,-42,61,-40,61,-48,38,-45,38,-48,39,-30,28,-10,12,-8,10,-7,6,-3,3,0,2,-1,5,-11,8,-26,20,-37,15,-35,12,-37,18,-36,21,-34,22,-32,21,-27,16,-24,13,-18,14,-16,19,-14,20,-11,21,-9,16,-9,10,-12,10,-8,9,-11,6,-9,10,-6,6,-22,9,-15,5,-17,20,-34,40,-36,32,-27,43,-9,7,-9,9,-12,10,-6,10,-1,2,0,0,0,0,0,0,0,0,0,0,-1,0,-9,6,-5,5,-2,2,-2,2,-1,2,-1,2,0,3,0,2,-2,1,-2,2,-2,2,-15,6,-22,20,-23,24,-31,28,-39,40,-51,49,-53,51,-49,46,-40,36,-43,43,-38,25,-43,20,-41,17,-33,21,-30,18,-32,17,-26,17,-24,17,-22,14,-18,8,-12,5,-6,11,-6,4,-4,13,-1,0,0,0,-1,0,-1,0,0,0,-1,0,-7,5,-11,12,-11,9,-6,4,-4,5,-1,2,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,-1,0,-13,8,-7,7,-8,13,-12,11,-11,7,-10,10,-10,10,-10,9,-8,9,-2,5,-6,6,-5,8,-2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1,0,-1,0,-11,1,-2,21,-13,13,-13,1,-2,6,0,5,0,4,-12,14,-19,14,-18,15,-17,15,-13,15,-12,12,-7,7,-5,5,-17,26,-22,26,-31,38,-35,28,-34,32,-26,36,-30,32,-23,22,-12,10,-6,7,0,2,-1,2,-3,1,-1,2,-1,1,0,1,-3,2,-7,5,-7,8,-9,7,-9,5,-5,3,-1,1,0,0,0,0,0,0,0,0,0,0,-2,2,-5,5,-1,6,-5,0,-4,0,0,1,0,1,-5,7,-9,11,-8,8,-11,6,-12,13,-12,14,-8,11,-4,14,-4,17,-5,11,-8,9,-5,6,-3,10,-4,7,-2,2,-1,1,-1,1,-3,3,-8,8,-9,9,-12,13,-12,17,-12,16,-15,18,-14,17,-19,20,-13,18,-17,18,-15,16,-17,16,-13,10,-10,12,-10,11,-9,8,-6,4,-3,1,-1,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,-1,0,-1,0,0,0,0,0,-1,1,-4,8,-12,10,-13,7,-21,9,-20,7,-16,7,-4,5,-1,0,-1,0,0,1,0,1,0,0,-9,0,-3,9,-5,7,-12,6,-12,20,-12,24,-16,22,-16,20,-14,17,-9,5,-5,4,-1,2,0,1,0,0,0,0,-3,2,0,3,-14,8,-18,15,-15,13,-9,9,-6,5,-4,4,-1,2,0,1,0,1,-13,11,-6,7,-16,15,-20,16,-19,14,-19,18,-20,15,-20,16,-20,17,-14,18,-8,14,-2,3,0,1,0,0,-7,9,-27,32,-22,21,-8,7,-3,4,-1,1,-4,2,-5,9,-6,8,-6,6,-5,6,-5,4,-4,4,-3,5,-3,5,-4,4,-4,4,-7,6,-12,10,-17,11,-21,12,-20,11,-20,12,-20,11,-20,10,-17,8,-18,7,-17,6,-16,7,-14,7,-13,7,-13,6,-13,5,-13,5,-13,6,-10,8,-10,9,-10,9,-7,8,-5,2,-2,2,-1,0,0,0,-2,3,-5,4,-4,4,-2,2,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1,2,-4,3,-10,11,-16,14,-25,28,-37,39,-46,58,-48,39,-48,41,-45,56,-48,48,-6,6,-1,0,0,0,0,0,0,0,-7,9,-10,16,-15,25,-22,21,-24,21,-25,20,-23,17,-23,18,-21,14,-17,16,-7,5,-6,3,-2,3,-5,3,-12,9,-13,10,-11,9,-11,8,-10,8,-10,8,-8,8,-8,7,-8,6,-7,4,-6,4,-6,5,-2,5,-4,2,-3,2,-1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,-4,4,-3,5,-4,7,-5,5,-5,5,-5,5,-3,5,-5,5,-6,5,-6,5,-5,4,-4,3,-3,3,-2,1,-6,2,-8,4,-9,4,-9,5,-10,5,-12,5,-13,6,-11,10,-9,9,-10,8,-11,7,-10,7,-7,8,-8,10,-8,9,-9,6,-10,7,-8,4,-10,4,-10,4,-10,5,-8,5,-6,5,-7,5,-6,4,-6,3,-6,3,-6,3,-5,4,-6,5,-6,4,-4,4,-3,2,-1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1,0,-1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1,0,-1,-1,-1,0,0,0,0,0,0,0,-1,0,-1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,-2,1,-16,10,-16,18,-25,18,-18,15,-17,13,-40,35,-51,51,-56,56,-57,49,-61,53,-61,60,-61,59,-62,59,-38,34,-19,20,-31,19,-36,23,-44,29,-55,49,-59,61,-59,54,-48,55,-60,49,-49,37,-34,38,-38,41,-43,34,-47,33,-47,33,-40,32,-36,30,-32,25,-20,11,-13,7,-9,5,-6,5,-6,4,-5,3,-5,6,-9,9,-26,25,-26,24,-40,41,-35,41,-14,16,-24,11,-27,11,-30,10,-26,9,-19,6,-2,4,-2,3,-2,3,-3,3,-2,3,-3,4,-2,1,-1,2,-2,3,-2,2,-3,2,-4,4,-1,2,-28,9,-27,14,-26,11,-24,12,-25,9,-25,11,-24,10,-20,12,-17,14,-16,13,-13,12,-13,12,-7,7,-5,4,-2,2,-2,2,-1,4,-3,1,-3,0,0,4,-3,4,-6,4,-9,7,-13,7,-14,8,-10,9,-7,7,-2,2,-1,1,0,1,-3,2,-2,1,0,1,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,1,-4,5,-10,21,-14,25,-23,21,-22,26,-20,24,-7,19,-14,22,-8,10,-9,5,-1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,-8,9,-7,8,-14,15,-18,21,-34,29,-30,25,-32,23,-28,26,-17,21,-19,23,-21,21,-20,20,-20,18,-8,11,0,1,0,0,0,0,0,0,-2,2,-2,0,-5,5,-13,7,-14,12,-17,12,-14,11,-10,10,-10,8,-7,8,-6,7,-4,5,-5,5,-2,0,0,0,-1,0,-6,8,-33,44,-28,47,-20,45,-20,45,-18,39,-11,31,-6,9,-6,5,-5,6,-6,7,-9,6,-5,3,-5,2,-5,8,-10,14,-17,17,-17,17,-17,16,-17,15,-11,15,-7,7,-7,5,-17,18,-17,23,-24,30,-27,31,-33,45,-39,48,-36,43,-46,46,-31,34,-22,30,-4,5,-13,15,-16,16,-18,14,-18,12,-16,17,-16,12,-4,7,-4,7,-4,7,-3,6,-1,5,0,2,0,0,0,0,-5,3,-4,5,-14,8,-10,11,-7,3,-11,12,-15,11,-14,10,-15,7,-11,5,-7,4,-5,2,-1,2,-1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-5,5,-9,10,-3,1,0,1,0,0,0,0,0,0,0,0,0,0,-3,2,0,0,-2,3,-14,8,-9,5,-12,7,-12,8,-14,7,-16,7,-16,7,-13,8,-18,21,-21,34,-20,35,-19,33,-18,32,-20,37,-16,30,-14,22,-6,9,-7,4,-4,4,-2,1,-3,5,-8,10,-19,21,-14,13,-8,8,-5,4,-3,3,-5,6,-4,8,-5,9,-10,9,-11,8,-6,6,-9,6,-6,5,-5,3,-2,0,0,0,0,0,0,3,0,2,-1,5,-7,4,-5,4,-7,5,-8,5,-4,5,-8,4,-8,4,-7,3,-6,3,-6,3,-4,2,-2,1,-1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,-1,3,-7,9,-14,16,-15,18,-13,18,-3,5,-1,1,0,0,0,0,-2,3,-6,4,-11,14,-17,15,-17,17,-11,14,-9,9,-9,10,-32,21,-44,35,-52,52,-42,42,-41,40,-51,34,-35,36,-38,37,-30,33,-33,30,-44,30,-38,30,-37,33,-34,35,-35,33,-34,34,-34,36,-32,34,-20,28,-20,29,-22,30,-23,30,-21,28,-20,22,-13,15,-5,6,-2,1,-1,1,-1,0,-3,3,-9,9,-20,18,-20,20,-22,21,-17,15,-19,14,-12,10,-5,4,-6,12,-21,36,-51,53,-50,51,-51,51,-50,51,-47,51,-50,50,-44,52,-37,50,-24,36,-19,22,-12,10,-11,12,-11,12,-10,12,-11,11,-9,11,-7,10,-6,7,-3,5,-7,5,-23,15,-36,27,-32,30,-35,23,-24,24,-20,15,-18,14,-16,15,-16,16,-17,17,-22,23,-34,23,-33,25,-32,28,-29,27,-29,25,-27,26,-28,27,-28,27,-27,26,-27,30,-25,30,-24,22,-20,17,-17,18,-17,16,-17,17,-16,17,-16,16,-16,16,-13,15,-14,13,-17,10,-20,13,-9,9,-18,7,-5,1,-1,1,-1,0,0,0,0,0,-4,6,-8,7,-8,6,-9,9,-5,7,-4,6,-3,4,-6,4,-17,9,-27,34,-28,34,-21,35,-29,41,-32,44,-28,48,-25,44,-24,41,-32,38,-25,41,-32,37,-30,27,-21,25,-14,17,-12,12,-13,16,-4,4,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-4,4,-2,2,-1,1,-3,4,-5,3,-5,3,-3,2,-3,3,-4,4,-3,4,-3,3,-4,4,-3,3,-3,3,-2,2,0,2,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-5,4,-4,7,-1,3,-2,1,-3,2,-11,9,-18,24,-19,25,-27,31,-31,34,-39,32,-42,37,-44,38,-42,36,-44,30,-40,22,-27,18,-26,18,-23,21,-21,20,-17,16,-14,17,-11,15,-11,13,-10,14,-11,12,-13,12,-11,11,-8,9,-3,4,0,1,0,0,0,0,0,0,0,12,-7,8,-7,1,-5,7,-2,10,-12,17,-33,19,-33,20,-49,30,-45,29,-39,30,-42,29,-38,30,-34,25,-35,23,-30,19,-21,18,-14,11,-9,6,-4,4,-5,8,-7,7,-5,7,-6,6,-4,7,-4,4,-2,2,-9,5,-12,16,-11,16,-11,14,-10,14,-9,12,-9,7,-6,5,-3,2,-2,3,-3,3,-4,6,-5,8,-6,10,-8,11,-8,10,-8,11,-9,11,-8,9,-7,10,-6,8,-3,5,-4,3,-6,5,-3,4,-4,4,-1,2,0,0,0,0,0,0,0,0,0,0,-2,2,-5,4,-4,3,-1,2,-1,2,-8,10,-10,13,-7,17,-6,13,-4,8,-3,6,-5,7,-8,12,-12,12,-15,16,-15,17,-14,17,-14,16,-9,14,-11,11,-8,6,-6,6,-5,4,-4,3,-4,2,-3,2,-1,1,-4,3,-11,8,-19,19,-18,16,-15,20,-12,19,-11,19,-12,16,-10,5,-4,6,-1,3,-2,3,-4,4,-5,4,-6,4,-2,3,-1,1,0,0,0,0,0,0,0,0,-1,0,-2,1,-13,6,-14,6,-7,6,-10,6,-9,7,-8,9,-11,8,-12,9,-12,9,-11,9,-11,8,-14,7,-8,7,-15,8,-15,6,-15,7,-13,5,-15,5,-12,5,-2,4,-5,1,-3,2,-6,4,-5,4,-6,5,-4,4,-4,5,-4,5,-3,2,-3,2,-1,0,-2,1,-3,5,-2,1,0,1,-1,2,-10,3,-2,3,-5,4,-4,3,-5,3,-5,2,-5,3,-4,2,-4,1,-1,1,-2,1,-1,1,-1,2,-2,2,-2,3,-2,1,-2,2,-1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1,3,-8,4,-14,9,-15,9,-11,10,-14,8,-13,8,-12,6,-13,7,-13,6,-16,9,-21,14,-27,21,-31,29,-41,28,-46,32,-48,30,-43,33,-41,34,-30,32,-17,13,-13,9,-12,9,-9,8,-15,8,-26,16,-27,18,-30,19,-32,20,-33,20,-30,16,-25,18,-21,21,-20,21,-14,21,-3,6,0,1,-1,0,0,0,0,0,0,0,-5,5,-4,4,-5,8,-9,9,-9,10,-9,6,-8,8,-13,10,-9,9,-13,9,-10,10,-8,6,-7,4,-24,32,-33,35,-35,32,-32,46,-30,51,-33,50,-31,48,-35,45,-35,42,-36,36,-38,29,-35,37,-27,39,-26,38,-28,36,-26,37,-23,35,-23,32,-24,31,-23,24,-16,18,-15,20,-13,19,-11,13,-9,11,-6,10,-19,12,-15,23,-7,15,-4,7,-4,4,-3,3,-3,1,-2,3,-1,3,-2,1,-1,0,-7,6,-14,15,-14,15,-13,16,-11,13,-11,10,-8,7,-9,4,-8,5,-10,8,-10,9,-9,11,-8,12,-9,14,-14,15,-17,16,-17,17,-18,19,-18,19,-19,21,-18,21,-19,20,-22,20,-22,21,-22,21,-26,26,-27,28,-25,20,-20,19,-18,19,-17,18,-16,17,-15,16,-14,18,-13,16,-12,14,-9,12,-10,9,-5,5,-6,4,-5,4,-6,6,-8,9,-14,13,-18,17,-18,17,-16,18,-18,15,-17,18,-16,17,-12,17,-12,15,-13,17,-14,17,-13,15,-10,12,-10,12,-11,11,-12,8,-10,9,-11,9,-11,7,-10,5,-6,8,-10,8,-8,8,-9,10,-10,11,-17,14,-21,18,-20,19,-20,18,-21,18,-24,19,-23,20,-24,16,-25,21,-28,22,-29,24,-30,24,-24,22,-22,20,-22,21,-17,17,-19,16,-21,18,-22,16,-21,11,-18,11,-15,10,-9,10,-13,9,-11,7,-9,6,-7,6,-3,5,-5,3,-3,3,-3,3,-3,3,-3,4,-1,1,0,0,0,0,0,0,-1,4,-3,3,-3,6,-4,6,-7,7,-7,7,-4,7,-3,4,-5,5,-5,5,-7,10,-8,7,-8,8,-7,6,-9,9,-7,6,-9,7,-7,8,-7,6,-5,5,-6,5,-5,7,-6,6,-4,5,-1,2,-4,8,-10,12,-12,15,-12,13,-12,12,-11,11,-11,10,-11,10,-12,9,-13,9,-13,7,-10,6,-2,4,-5,1,-1,2,-1,2,-2,5,-5,8,-11,5,-9,5,-7,8,-4,8,-7,7,-6,7,-5,5,-6,7,-7,7,-5,8,-7,7,-7,6,-6,6,-4,5,-5,4,-3,3,-1,2,-1,1,-1,0,-1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,-8,11,-15,15,-17,13,-20,12,-20,12,-18,16,-15,16,-15,15,-14,15,-14,14,-14,14,-18,10,-20,10,-19,9,-10,8,-3,0,0,0,0,0,0,0,0,0,-6,4,-4,3,-5,4,-5,4,-3,4,-2,3,-18,11,-22,16,-24,19,-22,16,-2,2,0,0,0,0,0,0,0,0,-5,6,-9,8,-13,12,-15,13,-11,10,-10,10,-5,6,-3,3,-5,10,-8,15,-8,17,-11,17,-11,17,-12,16,-11,15,-10,14,-10,14,-9,14,-8,13,-8,13,-7,12,-6,10,-5,10,-5,7,-4,5,-3,4,-1,1,0,0,0,0,0,0,-1,2,-2,2,-10,6,-10,9,-10,8,-10,7,-10,7,-11,8,-12,9,-10,7,-11,8,-14,9,-13,8,-13,7,-13,7,-7,4,-6,3,-4,2,-3,2,-4,2,-5,12,-9,17,-11,22,-12,25,-9,24,-9,24,-8,24,-9,25,-9,26,-11,28,-11,28,-10,25,-11,23,-10,19,-12,16,-12,16,-12,16,-12,16,-13,16,-22,15,-19,23,-17,25,-17,26,-20,29,-14,25,-9,10,-16,7,-5,4,-11,9,-9,6,-6,5,-7,8,-9,6,-7,4,-3,3,-2,2,-1,1,-2,1,-2,2,-2,2,-2,1,-1,1,-1,1,-1,1,-1,1,-1,1,0,1,-1,1,-1,1,-1,2,-17,21,-19,24,-23,27,-25,32,-30,39,-22,19,-11,7,-7,5,-7,8,-7,9,-10,11,-13,13,-14,12,-10,11,-10,8,-6,7,-12,10,-12,11,-8,8,-3,3,0,0,-1,0,0,0,0,0,0,1,-6,9,-7,11,-6,12,-6,11,-6,10,-6,9,-6,9,-6,9,-6,9,-6,7,-2,3,-1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1,0,-2,4,-18,13,-25,12,-16,10,-17,9,-17,8,-19,8,-21,10,-18,11,-13,9,-6,5,-1,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-4,4,-6,4,-1,4,-1,0,0,0,0,0,0,0,0,0,0,0,0,0,-2,1,-11,5,-15,8,-14,8,-15,8,-15,8,-12,8,-14,8,-13,10,-17,10,-14,11,-15,9,-17,8,-16,7,-5,2,-4,2,-4,2,-4,2,-5,3,-3,4,-2,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1,0,-1,1,-2,1,-2,1,-4,8,-8,11,-7,7,-9,8,-9,9,-10,10,-9,9,-9,9,-9,9,-8,9,-5,8,-4,8,-3,4,-3,4,-2,3,-2,2,-2,2,-1,2,-1,2,-1,0,0,0,-2,3,-6,6,-8,11,-5,11,-5,10,-5,9,-4,9,-4,8,-4,9,-3,8,-4,7,-5,9,-7,9,-5,8,-4,9,-5,6,-6,6,-7,4,-6,4,-6,4,-6,4,-6,4,-4,4,-3,4,-3,4,-4,3,-4,3,-3,3,-2,2,-2,3,-3,3,-2,2,-3,2,-2,2,-2,3,-1,2,-1,1,-1,1,-1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-2,2,-2,3,-6,8,-9,13,-13,16,-13,18,-13,17,-12,14,-8,11,-8,10,-7,8,-7,7,-6,7,-6,7,-7,7,-9,10,-15,11,-15,15,-16,18,-18,19,-17,19,-18,18,-17,16,-19,14,-19,19,-21,21,-25,20,-25,18,-25,14,-24,12,-23,10,-21,10,-18,9,-14,6,-7,4,-5,4,-4,3,-3,3,-4,4,-8,5,-8,6,-7,6,-6,5,-7,8,-9,9,-4,4,-1,1,-2,0,-1,0,0,0,0,0,0,0,-2,3,-7,5,-7,6,-10,8,-11,10,-11,11,-9,10,-9,9,-8,8,-7,7,-4,5,-2,3,-2,2,-1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-6,9,-7,10,-4,3,-7,16,-14,28,-14,28,-11,27,-12,25,-12,25,-13,25,-11,22,-10,21,-10,20,-9,20,-9,20,-8,19,-8,17,-10,17,-11,17,-11,16,-11,14,-11,12,-9,7,-4,4,-3,3,-1,1,-1,2,-4,6,-12,8,-17,10,-17,10,-16,9,-15,9,-12,9,-10,11,-10,13,-11,13,-11,13,-11,11,-10,11,-7,9,-5,9,-4,5,-3,4,-1,1,0,0,-2,4,-11,3,-13,7,-13,10,-14,11,-16,12,-15,13,-9,11,-9,9,-10,10,-10,8,-8,7,-10,7,-10,8,-10,9,-11,11,-10,12,-11,8,-10,8,-12,7,-14,8,-14,9,-16,11,-16,11,-17,11,-18,11,-17,11,-16,12,-17,13,-16,13,-15,12,-15,11,-15,11,-13,11,-8,12,-8,13,-7,12,-6,10,-6,9,-6,8,-6,9,-4,3,-3,4,-3,6,-1,1,0,0,0,0,0,0,0,0,-5,8,-4,5,-5,7,-5,4,-5,4,-5,5,-5,4,-6,5,-5,4,-3,4,-4,4,-3,3,-1,1,0,0,0,0,0,0,0,0,0,0,-3,3,-3,4,-3,2,-1,1,-1,1,-1,2,-1,1,-1,1,0,1,-2,2,-5,4,-16,25,-25,34,-23,32,-17,18,-8,6,-5,5,-4,7,-6,6,-9,8,-9,8,-10,10,-7,8,-8,9,-8,11,-10,8,-9,12,-9,14,-4,6,0,0,0,0,0,0,-2,1,-3,4,-7,6,-10,8,-11,8,-10,8,-9,8,-9,6,-10,6,-11,10,-12,10,-9,9,-9,10,-7,10,-9,8,-10,7,-9,8,-8,8,-9,7,-7,8,-6,7,-6,8,-6,8,-6,9,-6,9,-5,8,-5,10,-4,11,-4,10,-4,10,-4,11,-4,10,-5,10,-6,10,-8,11,-7,9,-10,7,-11,7,-10,7,-9,8,-11,8,-15,10,-14,10,-16,13,-18,12,-8,5,-3,4,0,2,0,0,0,0,0,0,0,0,0,0,-1,1,-4,3,-7,5,-5,6,-11,9,-9,8,-4,5,-1,2,-1,2,-5,7,-7,9,-5,6,-8,6,-6,6,-7,7,-6,6,-7,7,-8,9,-8,9,-7,11,-7,10,-7,10,-7,11,-6,11,-5,10,-4,8,-1,1,-1,1,-2,4,-6,6,-8,5,-10,5,-8,4,-7,4,-6,4,-8,4,-8,4,-8,4,-6,4,-4,3,-2,1,-4,5,-7,6,-8,6,-6,6,-6,7,-8,8,-9,8,-9,7,-7,6,-8,8,-10,9,-2,4,-5,9,-17,11,-18,16,-16,15,-15,17,-13,14,-9,11,-19,14,-14,10,-15,11,-18,13,-16,13,-8,10,-6,7,-4,5,-3,4,-2,1,-1,1,0,1,0,1,-1,1,0,1,0,0,0,0,0,0,-6,4,-2,5,-7,4,-9,9,-9,11,-8,10,-9,9,-8,7,-9,5,-10,6,-10,5,-10,6,-9,5,-9,5,-9,5,-8,4,-7,5,-6,6,-6,7,-8,8,-8,8,-8,8,-7,7,-7,9,-7,9,-6,10,-5,9,-6,8,-6,8,-5,8,-6,8,-5,7,-5,6,-4,8,-7,7,-7,8,-7,8,-6,7,-5,7,-6,6,-5,6,-5,10,-6,9,-6,7,-6,7,-6,7,-6,6,-7,4,-8,4,-7,4,-7,5,-7,5,-8,6,-8,6,-9,5,-8,5,-7,6,-8,6,-8,6,-9,5,-9,5,-10,6,-11,7,-13,6,-14,6,-14,6,-14,6,-13,8,-13,8,-13,9,-12,8,-8,7,-4,3,-2,2,-3,5,-9,7,-7,7,-8,6,-6,5,-5,5,-2,3,0,1,-1,1,-2,3,-2,3,-2,2,-1,1,-1,3,-1,1,-1,2,-1,2,-2,1,-2,2,-2,1,-2,2,-2,3,-3,2,-2,2,-1,0,0,0,0,0,-2,2,-1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,-10,8,-15,28,-13,30,-6,21,-6,9,-7,8,-7,5,-4,4,-2,1,0,0,0,0,0,0,0,0,0,0,-13,12,-15,9,-18,32,-24,31,-21,35,-23,35,-23,35,-22,34,-20,30,-18,28,-19,27,-18,24,-9,13,-3,4,0,0,0,0,-1,0,-8,8,-21,17,-34,33,-31,30,-7,8,-3,2,-2,3,-2,3,-1,0,-3,1,-22,11,-27,14,-22,14,-17,11,-11,11,-7,8,-10,8,-8,8,-4,7,-12,23,-9,25,-11,23,-7,14,-3,3,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-7,7,-4,5,-3,11,-14,10,-6,10,-8,6,-3,5,-4,3,-36,16,-57,52,-57,57,-57,57,-57,57,-56,54,-51,40,-24,28,-3,3,-1,2,0,0,0,0,0,0,0,0,0,0,-7,9,-4,3,-15,22,-24,28,-25,31,-24,29,-21,27,-14,18,-9,13,-12,18,-16,23,-18,24,-18,25,-18,23,-16,22,-13,20,-8,15,-8,9,-8,11,-18,16,-17,17,-20,24,-19,18,-25,32,-24,27,-16,20,-10,8,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1,9,-27,21,-38,32,-30,28,-25,33,-21,40,-24,46,-24,44,-25,43,-22,40,-19,37,-17,38,-12,37,-13,39,-9,16,-5,6,-1,0,-1,1,-1,1,-15,9,-8,5,-13,11,-14,8,-11,13,-9,8,-12,7,-14,10,-14,10,-15,9,-14,9,-14,8,-12,7,-7,7,-6,6,-6,4,-6,6,-3,5,-5,5,-5,6,-5,6,-5,5,-7,5,-5,7,-5,5,-5,6,-3,2,-2,2,-33,21,-42,25,-32,33,-35,44,-44,50,-41,47,-53,47,-30,27,-14,24,-10,15,0,2,0,0,0,0,0,0,0,0,-3,3,-5,5,-10,8,-11,12,-13,16,-18,15,-12,15,-17,15,-10,11,-14,11,-17,15,-11,12,-4,6,-11,5,-21,15,-16,10,-15,10,-15,10,-14,14,-14,15,-15,15,-15,16,-15,16,-16,15,-17,14,-16,13,-17,13,-18,12,-18,10,-16,10,-14,10,-5,11,-5,8,-4,7,-3,4,-2,2,-10,16,-6,5,-15,24,-17,23,-11,22,-13,24,-13,22,-9,21,-8,22,-9,21,-7,20,-8,16,-11,15,-11,10,-12,8,-11,8,-11,5,-8,5,-7,4,-5,4,-5,4,-5,3,-2,2,-1,1,0,1,-1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-3,5,-6,6,-6,7,-5,4,-1,1,-10,2,-26,16,-24,14,-24,14,-23,16,-17,9,-4,3,0,1,0,0,0,0,0,0,-6,8,-1,2,-20,18,-21,30,-24,22,-20,23,-19,21,-15,19,-12,18,-6,14,-5,11,-6,10,-6,13,-10,19,-14,22,-14,26,-15,26,-12,22,-9,18,-9,14,-9,9,-18,23,-24,20,-25,26,-24,26,-32,22,-21,26,-31,27,-22,24,-28,22,-33,25,-23,30,-15,15,-9,9,-3,3,0,1,0,0,0,0,-7,5,-16,21,-13,21,-14,25,-15,27,-16,27,-13,22,-18,16,-20,16,-21,16,-22,16,-24,16,-22,20,-23,19,-24,18,-25,16,-23,12,-23,12,-23,12,-22,13,-21,13,-25,12,-27,12,-31,12,-31,13,-29,13,-23,15,-20,18,-19,18,-19,18,-19,18,-25,19,-27,21,-27,21,-25,22,-26,21,-26,21,-25,21,-26,21,-25,19,-27,19,-27,17,-28,16,-29,15,-30,15,-33,15,-33,14,-35,13,-34,15,-34,15,-36,19,-31,20,-30,21,-27,18,-23,14,-20,10,-14,9,-7,11,-13,14,-16,13,-22,19,-17,16,-26,19,-16,15,-22,19,-12,15,-11,9,-3,3,-5,6,-9,17,-8,15,-10,16,-9,16,-10,14,-11,15,-12,16,-12,16,-12,15,-11,14,-11,14,-10,13,-9,12,-8,10,-7,9,-6,8,-5,6,-4,5,-3,3,-3,2,-1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-10,4,-27,12,-27,19,-32,16,-35,19,-31,20,-31,22,-27,23,-29,21,-30,23,-28,22,-23,17,-11,9,-10,6,-6,4,-6,8,-17,11,-23,16,-23,18,-23,17,-19,12,-18,14,-20,14,-19,13,-19,12,-19,11,-17,9,-15,7,-6,7,-5,8,-8,7,-11,11,-10,9,-12,10,-8,8,-9,9,-9,12,-9,7,-7,9,-10,8,-9,10,-9,8,-3,3,-3,3,-29,22,-27,28,-22,33,-20,36,-21,34,-20,37,-20,41,-22,40,-19,36,-24,39,-21,37,-20,43,-18,38,-18,23,-23,15,-23,14,-21,13,-20,13,-19,12,-16,12,-8,9,-2,1,0,0,-16,11,-11,17,-4,2,-3,4,-2,5,-5,6,-3,3,-9,19,-14,23,-14,25,-22,28,-22,26,-18,25,-17,21,-17,16,-9,20,-7,17,-6,9,-6,7,-6,7,-8,10,-8,8,-12,11,-8,11,-9,11,-9,8,-7,6,-3,2,-1,1,-4,6,-9,13,-10,10,-11,9,-10,9,-10,8,-9,8,-8,8,-7,7,-7,8,-6,8,-6,8,-6,6,-4,5,-4,5,-6,7,-7,7,-7,8,-11,9,-8,9,-10,10,-6,7,-4,3,-2,2,-1,1,0,0,0,0,0,0,0,0,-1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-8,7,-9,7,-14,8,-15,10,-16,9,-16,13,-15,13,-11,12,-7,7,-3,4,0,1,0,0,0,0,0,0,0,0,-7,10,-1,2,-6,2,0,5,-1,0,0,0,-1,1,-24,11,-22,9,-18,7,-19,9,-22,14,-22,12,-23,9,-23,10,-25,10,-24,10,-24,10,-21,10,-21,10,-22,12,-21,12,-19,13,-18,13,-15,12,-11,11,-9,7,-4,6,-3,2,-1,1,0,0,0,0,-11,8,-4,6,-11,18,-16,24,-16,25,-18,23,-16,23,-15,21,-15,21,-15,20,-8,12,-2,4,0,0,0,0,-5,6,-8,9,-13,18,-26,24,-16,14,-5,4,-2,2,-1,1,0,0,0,0,0,0,0,0,-1,1,-17,32,-15,42,-15,42,-18,48,-21,51,-21,48,-20,47,-20,46,-17,45,-16,46,-14,48,-13,41,-13,39,-15,32,-13,27,-11,27,-12,23,-15,22,-16,23,-16,19,-15,18,-14,15,-10,11,-7,5,-3,2,-2,3,-1,2,0,0,-3,7,-4,3,-15,16,-13,15,-14,14,-13,15,-16,15,-10,13,-9,10,-11,8,-11,9,-9,6,-6,6,-6,4,-6,4,-2,3,-4,3,-2,3,-2,2,-2,2,-1,1,-1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]],F=({audioUrl:n})=>{const[t,s]=p.useState(!1),i=p.useRef(null),r=p.useRef(null);p.useEffect(()=>{if(i.current){let d=["alloy","echo","fable","onyx","nova","shimmer"].findIndex(u=>n.includes(u));d===-1&&(d=0),r.current=je.create({container:i.current,waveColor:"#ccc",progressColor:"#666",height:30,barWidth:2,barGap:1.5,barRadius:4,cursorWidth:0,peaks:[yi[d]],url:n}),r.current.on("play",()=>s(!0)),r.current.on("pause",()=>s(!1)),r.current.on("finish",()=>s(!1))}return()=>{r.current&&r.current.destroy()}},[n]);const o=()=>{r.current&&(t?r.current.pause():r.current.play())};return e.jsxs("div",{style:{position:"relative"},children:[e.jsx(R,{onClick:o,size:"medium",color:"neutral","aria-label":t?"Pause":"Play",style:{position:"absolute",left:0,top:-4},children:t?e.jsx(Tn,{}):e.jsx(Pn,{})}),e.jsx("div",{ref:i,style:{marginLeft:"48px"}})]})};function ot(n){const t=Object.assign({h1:"h1",p:"p",h2:"h2",ul:"ul",li:"li",strong:"strong",ol:"ol",a:"a",h3:"h3",code:"code",pre:"pre",table:"table",thead:"thead",tr:"tr",th:"th",tbody:"tbody",td:"td",blockquote:"blockquote"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"Fine-tuning"}),"\n",e.jsx(t.p,{children:"Learn how to customize a model for your application."}),"\n",e.jsx(t.h2,{children:"Introduction"}),"\n",e.jsx(t.p,{children:"Fine-tuning lets you get more out of the models available through the API by providing:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Higher quality results than prompting"}),"\n",e.jsx(t.li,{children:"Ability to train on more examples than can fit in a prompt"}),"\n",e.jsx(t.li,{children:"Token savings due to shorter prompts"}),"\n",e.jsx(t.li,{children:"Lower latency requests"}),"\n"]}),"\n",e.jsx(t.p,{children:'OpenAI\'s text generation models have been pre-trained on a vast amount of text. To use the models effectively, we include instructions and sometimes several examples in a prompt. Using demonstrations to show how to perform a task is often called "few-shot learning."'}),"\n",e.jsxs(t.p,{children:["Fine-tuning improves on few-shot learning by training on many more examples than can fit in the prompt, letting you achieve better results on a wide number of tasks. ",e.jsx(t.strong,{children:"Once a model has been fine-tuned, you won't need to provide as many examples in the prompt."})," This saves costs and enables lower-latency requests."]}),"\n",e.jsx(t.p,{children:"At a high level, fine-tuning involves the following steps:"}),"\n",e.jsxs(t.ol,{children:["\n",e.jsx(t.li,{children:"Prepare and upload training data"}),"\n",e.jsx(t.li,{children:"Train a new fine-tuned model"}),"\n",e.jsx(t.li,{children:"Evaluate results and go back to step 1 if needed"}),"\n",e.jsx(t.li,{children:"Use your fine-tuned model"}),"\n"]}),"\n",e.jsxs(t.p,{children:["Visit our ",e.jsx(t.a,{href:"https://openai.com/api/pricing",children:"pricing page"})," to learn more about how fine-tuned model training and usage are billed."]}),"\n",e.jsx(t.h3,{children:"Which models can be fine-tuned?"}),"\n",e.jsx(t.p,{children:"Fine-tuning is currently available for the following models:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"gpt-4o-2024-08-06"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"gpt-4o-mini-2024-07-18"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"gpt-4-0613"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"gpt-3.5-turbo-0125"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"gpt-3.5-turbo-1106"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"gpt-3.5-turbo-0613"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"babbage-002"})}),"\n",e.jsx(t.li,{children:e.jsx(t.code,{children:"davinci-002"})}),"\n"]}),"\n",e.jsx(b,{children:e.jsxs(t.p,{children:["New fine-tuning training runs on ",e.jsx(t.code,{children:"babbage-002"})," and ",e.jsx(t.code,{children:"davinci-002"})," will no longer be supported starting October 28, 2024."]})}),"\n",e.jsx(t.p,{children:"You can also fine-tune a fine-tuned model, which is useful if you acquire additional data and don't want to repeat the previous training steps."}),"\n",e.jsxs(t.p,{children:["We expect ",e.jsx(t.code,{children:"gpt-4o-mini"})," to be the right model for most users in terms of performance, cost, and ease of use."]}),"\n",e.jsx(t.h2,{children:"When to use fine-tuning"}),"\n",e.jsxs(t.p,{children:["Fine-tuning OpenAI text generation models can make them better for specific applications, but it requires a careful investment of time and effort. We recommend first attempting to get good results with prompt engineering, prompt chaining (breaking complex tasks into multiple prompts), and ",e.jsx(t.a,{href:"/docs/guides/function-calling",children:"function calling"}),", with the key reasons being:"]}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"There are many tasks at which our models may not initially appear to perform well, but results can be improved with the right prompts - thus fine-tuning may not be necessary"}),"\n",e.jsx(t.li,{children:"Iterating over prompts and other tactics has a much faster feedback loop than iterating with fine-tuning, which requires creating datasets and running training jobs"}),"\n",e.jsx(t.li,{children:"In cases where fine-tuning is still necessary, initial prompt engineering work is not wasted - we typically see best results when using a good prompt in the fine-tuning data (or combining prompt chaining / tool use with fine-tuning)"}),"\n"]}),"\n",e.jsxs(t.p,{children:["Our ",e.jsx(t.a,{href:"/docs/guides/prompt-engineering",children:"prompt engineering guide"})," provides a background on some of the most effective strategies and tactics for getting better performance without fine-tuning. You may find it helpful to iterate quickly on prompts in our ",e.jsx(t.a,{href:"/playground",children:"playground"}),"."]}),"\n",e.jsx(t.h3,{children:"Common use cases"}),"\n",e.jsx(t.p,{children:"Some common use cases where fine-tuning can improve results:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Setting the style, tone, format, or other qualitative aspects"}),"\n",e.jsx(t.li,{children:"Improving reliability at producing a desired output"}),"\n",e.jsx(t.li,{children:"Correcting failures to follow complex prompts"}),"\n",e.jsx(t.li,{children:"Handling many edge cases in specific ways"}),"\n",e.jsx(t.li,{children:"Performing a new skill or task that’s hard to articulate in a prompt"}),"\n"]}),"\n",e.jsx(t.p,{children:'One high-level way to think about these cases is when it’s easier to "show, not tell". In the sections to come, we will explore how to set up data for fine-tuning and various examples where fine-tuning improves the performance over the baseline model.'}),"\n",e.jsxs(t.p,{children:["Another scenario where fine-tuning is effective is reducing cost and/or latency by replacing a more expensive model like ",e.jsx(t.code,{children:"gpt-4o"})," with a fine-tuned ",e.jsx(t.code,{children:"gpt-4o-mini"})," model. If you can achieve good results with ",e.jsx(t.code,{children:"gpt-4o"}),", you can often reach similar quality with a fine-tuned ",e.jsx(t.code,{children:"gpt-4o-mini"})," model by fine-tuning on the ",e.jsx(t.code,{children:"gpt-4o"})," completions, possibly with a shortened instruction prompt."]}),"\n",e.jsx(t.h2,{children:"Preparing your dataset"}),"\n",e.jsx(t.p,{children:"Once you have determined that fine-tuning is the right solution (i.e. you’ve optimized your prompt as far as it can take you and identified problems that the model still has), you’ll need to prepare data for training the model. You should create a diverse set of demonstration conversations that are similar to the conversations you will ask the model to respond to at inference time in production."}),"\n",e.jsxs(t.p,{children:["Each example in the dataset should be a conversation in the same format as our ",e.jsx(t.a,{href:"/docs/api-reference/chat/create",children:"Chat Completions API"}),", specifically a list of messages where each message has a role, content, and ",e.jsx(t.a,{href:"/docs/api-reference/chat/create#chat/create-chat/create-messages-name",children:"optional name"}),". At least some of the training examples should directly target cases where the prompted model is not behaving as desired, and the provided assistant messages in the data should be the ideal responses you want the model to provide."]}),"\n",e.jsx(t.h3,{children:"Example format"}),"\n",e.jsx(t.p,{children:"In this example, our goal is to create a chatbot that occasionally gives sarcastic responses, these are three training examples (conversations) we could create for a dataset:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-jsonl",children:'{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What\'s the capital of France?"}, {"role": "assistant", "content": "Paris, as if everyone doesn\'t know that already."}]}\n{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "Who wrote \'Romeo and Juliet\'?"}, {"role": "assistant", "content": "Oh, just some guy named William Shakespeare. Ever heard of him?"}]}\n{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "How far is the Moon from Earth?"}, {"role": "assistant", "content": "Around 384,400 kilometers. Give or take a few, like that really matters."}]}\n'})}),"\n",e.jsxs(t.p,{children:["The conversational chat format is required to fine-tune ",e.jsx(t.code,{children:"gpt-4o-mini"})," and ",e.jsx(t.code,{children:"gpt-3.5-turbo"}),". For ",e.jsx(t.code,{children:"babbage-002"})," and ",e.jsx(t.code,{children:"davinci-002"}),", you can follow the prompt completion pair format as shown below."]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-json",children:'{"prompt": "<prompt text>", "completion": "<ideal generated text>"}\n{"prompt": "<prompt text>", "completion": "<ideal generated text>"}\n{"prompt": "<prompt text>", "completion": "<ideal generated text>"}\n'})}),"\n",e.jsx(t.h3,{children:"Multi-turn chat examples"}),"\n",e.jsxs(t.p,{children:["Examples in the chat format can have multiple messages with the assistant role. The default behavior during fine-tuning is to train on all assistant messages within a single example. To skip fine-tuning on specific assistant messages, a ",e.jsx(t.code,{children:"weight"})," key can be added disable fine-tuning on that message, allowing you to control which assistant messages are learned. The allowed values for ",e.jsx(t.code,{children:"weight"})," are currently 0 or 1. Some examples using ",e.jsx(t.code,{children:"weight"})," for the chat format are below."]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-jsonl",children:'{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What\'s the capital of France?"}, {"role": "assistant", "content": "Paris", "weight": 0}, {"role": "user", "content": "Can you be more sarcastic?"}, {"role": "assistant", "content": "Paris, as if everyone doesn\'t know that already.", "weight": 1}]}\n{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "Who wrote \'Romeo and Juliet\'?"}, {"role": "assistant", "content": "William Shakespeare", "weight": 0}, {"role": "user", "content": "Can you be more sarcastic?"}, {"role": "assistant", "content": "Oh, just some guy named William Shakespeare. Ever heard of him?", "weight": 1}]}\n{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "How far is the Moon from Earth?"}, {"role": "assistant", "content": "384,400 kilometers", "weight": 0}, {"role": "user", "content": "Can you be more sarcastic?"}, {"role": "assistant", "content": "Around 384,400 kilometers. Give or take a few, like that really matters.", "weight": 1}]}\n'})}),"\n",e.jsx(t.h3,{children:"Crafting prompts"}),"\n",e.jsx(t.p,{children:"We generally recommend taking the set of instructions and prompts that you found worked best for the model prior to fine-tuning, and including them in every training example. This should let you reach the best and most general results, especially if you have relatively few (e.g. under a hundred) training examples."}),"\n",e.jsx(t.p,{children:'If you would like to shorten the instructions or prompts that are repeated in every example to save costs, keep in mind that the model will likely behave as if those instructions were included, and it may be hard to get the model to ignore those "baked-in" instructions at inference time.'}),"\n",e.jsx(t.p,{children:"It may take more training examples to arrive at good results, as the model has to learn entirely through demonstration and without guided instructions."}),"\n",e.jsx(t.h3,{children:"Example count recommendations"}),"\n",e.jsxs(t.p,{children:["To fine-tune a model, you are required to provide at least 10 examples. We typically see clear improvements from fine-tuning on 50 to 100 training examples with ",e.jsx(t.code,{children:"gpt-4o-mini"})," and ",e.jsx(t.code,{children:"gpt-3.5-turbo"}),", but the right number varies greatly based on the exact use case."]}),"\n",e.jsx(t.p,{children:"We recommend starting with 50 well-crafted demonstrations and seeing if the model shows signs of improvement after fine-tuning. In some cases that may be sufficient, but even if the model is not yet production quality, clear improvements are a good sign that providing more data will continue to improve the model. No improvement suggests that you may need to rethink how to set up the task for the model or restructure the data before scaling beyond a limited example set."}),"\n",e.jsx(t.h3,{children:"Train and test splits"}),"\n",e.jsx(t.p,{children:"After collecting the initial dataset, we recommend splitting it into a training and test portion. When submitting a fine-tuning job with both training and test files, we will provide statistics on both during the course of training. These statistics will be your initial signal of how much the model is improving. Additionally, constructing a test set early on will be useful in making sure you are able to evaluate the model after training, by generating samples on the test set."}),"\n",e.jsx(t.h3,{children:"Token limits"}),"\n",e.jsxs(t.p,{children:["Token limits depend on the model you select. Here is an overview of the maximum inference context length and training examples context length for ",e.jsx(t.code,{children:"gpt-4o-mini"})," and ",e.jsx(t.code,{children:"gpt-3.5-turbo"})," models:"]}),"\n",e.jsx("div",{className:"models-table",children:e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Model"}),e.jsx(t.th,{children:"Inference context length"}),e.jsx(t.th,{children:"Training examples context length"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4o-2024-08-06"})}),e.jsx(t.td,{children:"128,000 tokens"}),e.jsx(t.td,{children:"65,536 tokens (128k coming soon)"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4o-mini-2024-07-18"})}),e.jsx(t.td,{children:"128,000 tokens"}),e.jsx(t.td,{children:"65,536 tokens (128k coming soon)"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-3.5-turbo-0125"})}),e.jsx(t.td,{children:"16,385 tokens"}),e.jsx(t.td,{children:"16,385 tokens"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-3.5-turbo-1106"})}),e.jsx(t.td,{children:"16,385 tokens"}),e.jsx(t.td,{children:"16,385 tokens"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-3.5-turbo-0613"})}),e.jsx(t.td,{children:"16,385 tokens"}),e.jsx(t.td,{children:"4,096 tokens"})]})]})]})}),"\n",e.jsx(t.p,{children:"Examples longer than the default will be truncated to the maximum context length which removes tokens from the end of the training example(s). To be sure that your entire training example fits in context, consider checking that the total token counts in the message contents are under the limit."}),"\n",e.jsxs(t.p,{children:["You can compute token counts using our ",e.jsx(t.a,{href:"https://cookbook.openai.com/examples/How_to_count_tokens_with_tiktoken.ipynb",children:"counting tokens notebook"})," from the OpenAI cookbook."]}),"\n",e.jsx(t.h3,{children:"Estimate costs"}),"\n",e.jsxs(t.p,{children:["For detailed pricing on training costs, as well as input and output costs for a deployed fine-tuned model, visit our ",e.jsx(t.a,{href:"https://openai.com/pricing",children:"pricing page"}),". Note that we don't charge for tokens used for training validation. To estimate the cost of a specific fine-tuning training job, use the following formula:"]}),"\n",e.jsxs(t.blockquote,{children:["\n",e.jsx(t.p,{children:"(base training cost per 1M input tokens ÷ 1M) × number of tokens in the input file × number of epochs trained"}),"\n"]}),"\n",e.jsx(t.p,{children:"For a training file with 100,000 tokens trained over 3 epochs, the expected cost would be:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["~$0.90 USD with ",e.jsx(t.code,{children:"gpt-4o-mini-2024-07-18"})," after the free period ends on September 23, 2024."]}),"\n",e.jsxs(t.li,{children:["~$2.40 USD with ",e.jsx(t.code,{children:"gpt-3.5-turbo-0125"}),"."]}),"\n"]}),"\n",e.jsx(t.h3,{children:"Check data formatting"}),"\n",e.jsx(t.p,{children:"Once you have compiled a dataset and before you create a fine-tuning job, it is important to check the data formatting. To do this, we created a simple Python script which you can use to find potential errors, review token counts, and estimate the cost of a fine-tuning job."}),"\n",e.jsx("a",{href:"https://cookbook.openai.com/examples/chat_finetuning_data_prep",target:"_blank",children:e.jsx(C,{icon:e.jsx(Ln,{}),color:"green",title:"Fine-tuning data format validation",className:"mt-6",children:e.jsx(t.p,{children:"Learn about fine-tuning data formatting"})})}),"\n",e.jsx(t.h3,{children:"Upload a training file"}),"\n",e.jsxs(t.p,{children:["Once you have the data validated, the file needs to be uploaded using the ",e.jsx(t.a,{href:"/docs/api-reference/files/create",children:"Files API"})," in order to be used with a fine-tuning jobs:"]}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\nfrom openai import OpenAI\nclient = OpenAI()\n\nclient.files.create(\n  file=open("mydata.jsonl", "rb"),\n  purpose="fine-tune"\n)\n'.trim(),"node.js":"\nimport fs from 'fs';\nimport fetch from 'node-fetch';\nimport OpenAI, { toFile } from 'openai';\n\nconst openai = new OpenAI();\n\n// If you have access to Node fs we recommend using fs.createReadStream():\nawait openai.files.create({ file: fs.createReadStream('mydata.jsonl'), purpose: 'fine-tune' });\n\n// Or if you have the web File API you can pass a File instance:\nawait openai.files.create({ file: new File(['my bytes'], 'mydata.jsonl'), purpose: 'fine-tune' });\n\n// You can also pass a fetch Response:\nawait openai.files.create({ file: await fetch('https://somesite/mydata.jsonl'), purpose: 'fine-tune' });\n".trim(),curl:'\ncurl https://api.openai.com/v1/files \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -F purpose="fine-tune" \\\n  -F file="@mydata.jsonl"\n'.trim()}}),"\n",e.jsx(t.p,{children:"After you upload the file, it may take some time to process. While the file is processing, you can still create a fine-tuning job but it will not start until the file processing has completed."}),"\n",e.jsx(t.p,{children:"The maximum file upload size is 1 GB, though we do not suggest fine-tuning with that amount of data since you are unlikely to need that large of an amount to see improvements."}),"\n",e.jsx(t.h2,{children:"Create a fine-tuned model"}),"\n",e.jsxs(t.p,{children:["After ensuring you have the right amount and structure for your dataset, and have uploaded the file, the next step is to create a fine-tuning job. We support creating fine-tuning jobs via the ",e.jsx(t.a,{href:"/finetune",children:"fine-tuning UI"})," or programmatically."]}),"\n",e.jsx(t.p,{children:"To start a fine-tuning job using the OpenAI SDK:"}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\nfrom openai import OpenAI\nclient = OpenAI()\n\nclient.fine_tuning.jobs.create(\n  training_file="file-abc123", \n  model="gpt-4o-mini"\n)\n'.trim(),"node.js":"\nconst fineTune = await openai.fineTuning.jobs.create({ training_file: 'file-abc123', model: 'gpt-4o-mini' });\n".trim()}}),"\n",e.jsxs(t.p,{children:["In this example, ",e.jsx(t.code,{children:"model"})," is the name of the model you want to fine-tune (",e.jsx(t.code,{children:"gpt-4o-mini"})," in this case, or any of our ",e.jsx(t.a,{href:"/docs/guides/fine-tuning/which-models-can-be-fine-tuned",children:"supported models"}),") and ",e.jsx(t.code,{children:"training_file"})," is the file ID that was returned when the training file was uploaded to the OpenAI API. You can customize your fine-tuned model's name using the ",e.jsx(t.a,{href:"/docs/api-reference/fine-tuning/create#fine-tuning/create-suffix",children:"suffix parameter"}),"."]}),"\n",e.jsxs(t.p,{children:["To set additional fine-tuning parameters like the ",e.jsx(t.code,{children:"validation_file"})," or ",e.jsx(t.code,{children:"hyperparameters"}),", please refer to the ",e.jsx(t.a,{href:"/docs/api-reference/fine-tuning/create",children:"API specification for fine-tuning"}),"."]}),"\n",e.jsx(t.p,{children:"After you've started a fine-tuning job, it may take some time to complete. Your job may be queued behind other jobs in our system, and training a model can take minutes or hours depending on the model and dataset size. After the model training is completed, the user who created the fine-tuning job will receive an email confirmation."}),"\n",e.jsx(t.p,{children:"In addition to creating a fine-tuning job, you can also list existing jobs, retrieve the status of a job, or cancel a job."}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\nfrom openai import OpenAI\nclient = OpenAI()\n\n# List 10 fine-tuning jobs\nclient.fine_tuning.jobs.list(limit=10)\n\n# Retrieve the state of a fine-tune\nclient.fine_tuning.jobs.retrieve("ftjob-abc123")\n\n# Cancel a job\nclient.fine_tuning.jobs.cancel("ftjob-abc123")\n\n# List up to 10 events from a fine-tuning job\nclient.fine_tuning.jobs.list_events(fine_tuning_job_id="ftjob-abc123", limit=10)\n\n# Delete a fine-tuned model (must be an owner of the org the model was created in)\nclient.models.delete("ft:gpt-3.5-turbo:acemeco:suffix:abc123")\n'.trim(),"node.js":"\n// List 10 fine-tuning jobs\nlet page = await openai.fineTuning.jobs.list({ limit: 10 });\n\n// Retrieve the state of a fine-tune\nlet fineTune = await openai.fineTuning.jobs.retrieve('ftjob-abc123');\n\n// Cancel a job\nlet status = await openai.fineTuning.jobs.cancel('ftjob-abc123');\n\n// List up to 10 events from a fine-tuning job\nlet events = await openai.fineTuning.jobs.listEvents(fineTune.id, { limit: 10 });\n\n// Delete a fine-tuned model (must be an owner of the org the model was created in)\nlet model = await openai.models.delete('ft:gpt-3.5-turbo:acemeco:suffix:abc123');\n".trim()}}),"\n",e.jsx(t.h2,{children:"Use a fine-tuned model"}),"\n",e.jsxs(t.p,{children:["When a job has succeeded, you will see the ",e.jsx(t.code,{children:"fine_tuned_model"})," field populated with the name of the model when you retrieve the job details. You may now specify this model as a parameter to in the ",e.jsx(t.a,{href:"/docs/api-reference/chat",children:"Chat Completions"})," API, and make requests to it using the ",e.jsx(t.a,{href:"/playground",children:"Playground"}),"."]}),"\n",e.jsx(t.p,{children:"After your job is completed, the model should be available right away for inference use. In some cases, it may take several minutes for your model to become ready to handle requests. If requests to your model time out or the model name cannot be found, it is likely because your model is still being loaded. If this happens, try again in a few minutes."}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\nfrom openai import OpenAI\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n  model="ft:gpt-4o-mini:my-org:custom_suffix:id",\n  messages=[\n    {"role": "system", "content": "You are a helpful assistant."},\n    {"role": "user", "content": "Hello!"}\n  ]\n)\nprint(completion.choices[0].message)\n'.trim(),"node.js":'\nasync function main() {\n  const completion = await openai.chat.completions.create({\n    messages: [{ role: "system", content: "You are a helpful assistant." }],\n    model: "ft:gpt-4o-mini:my-org:custom_suffix:id",\n  });\n  console.log(completion.choices[0]);\n}\nmain();\n'.trim()}}),"\n",e.jsxs(t.p,{children:["You can start making requests by passing the model name as shown above and in our ",e.jsx(t.a,{href:"/docs/guides/text-generation/chat-completions-api",children:"GPT guide"}),"."]}),"\n",e.jsx(t.h2,{children:"Use a checkpointed model"}),"\n",e.jsx(t.p,{children:"In addition to creating a final fine-tuned model at the end of each fine-tuning job, OpenAI will create one full model checkpoint for you at the end of each training epoch. These checkpoints are themselves full models that can be used within our completions and chat-completions endpoints. Checkpoints are useful as they potentially provide a version of your fine-tuned model from before it experienced overfitting."}),"\n",e.jsx(t.p,{children:"To access these checkpoints,"}),"\n",e.jsxs(t.ol,{children:["\n",e.jsxs(t.li,{children:["Wait until a job succeeds, which you can verify by ",e.jsx(t.a,{href:"/docs/api-reference/fine-tuning/retrieve",children:"querying the status of a job."})]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"/docs/api-reference/fine-tuning/list-checkpoints",children:"Query the checkpoints endpoint"})," with your fine-tuning job ID to access a list of model checkpoints for the fine-tuning job."]}),"\n"]}),"\n",e.jsxs(t.p,{children:["For each checkpoint object, you will see the ",e.jsx(t.code,{children:"fine_tuned_model_checkpoint"})," field populated with the name of the model checkpoint. You may now use this model just like you would with the ",e.jsx(t.a,{href:"/docs/guides/fine-tuning/use-a-fine-tuned-model",children:"final fine-tuned model"}),"."]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-json",children:'{\n    "object": "fine_tuning.job.checkpoint",\n    "id": "ftckpt_zc4Q7MP6XxulcVzj4MZdwsAB",\n    "created_at": 1519129973,\n    "fine_tuned_model_checkpoint": "ft:gpt-3.5-turbo-0125:my-org:custom-suffix:96olL566:ckpt-step-2000",\n    "metrics": {\n        "full_valid_loss": 0.134,\n        "full_valid_mean_token_accuracy": 0.874\n    },\n    "fine_tuning_job_id": "ftjob-abc123",\n    "step_number": 2000\n}\n'})}),"\n",e.jsx(t.p,{children:"Each checkpoint will specify its:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"step_number"}),": The step at which the checkpoint was created (where each epoch is number of steps in the training set divided by the batch size)"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"metrics"}),": an object containing the metrics for your fine-tuning job at the step when the checkpoint was created."]}),"\n"]}),"\n",e.jsx(t.p,{children:"Currently, only the checkpoints for the last 3 epochs of the job are saved and available for use. We plan to release more complex and flexible checkpointing strategies in the near future."}),"\n",e.jsx(t.h2,{children:"Analyzing your fine-tuned model"}),"\n",e.jsx(t.p,{children:"We provide the following training metrics computed over the course of training:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"training loss"}),"\n",e.jsx(t.li,{children:"training token accuracy"}),"\n",e.jsx(t.li,{children:"valid loss"}),"\n",e.jsx(t.li,{children:"valid token accuracy"}),"\n"]}),"\n",e.jsx(t.p,{children:"Valid loss and valid token accuracy are computed in two different ways - on a small batch of the data during each step, and on the full valid split at the end of each epoch. The full valid loss and full valid token accuracy metrics are the most accurate metric tracking the overall performance of your model. These statistics are meant to provide a sanity check that training went smoothly (loss should decrease, token accuracy should increase). While an active fine-tuning jobs is running, you can view an event object which contains some useful metrics:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-json",children:'{\n    "object": "fine_tuning.job.event",\n    "id": "ftevent-abc-123",\n    "created_at": 1693582679,\n    "level": "info",\n    "message": "Step 300/300: training loss=0.15, validation loss=0.27, full validation loss=0.40",\n    "data": {\n        "step": 300,\n        "train_loss": 0.14991648495197296,\n        "valid_loss": 0.26569826706596045,\n        "total_steps": 300,\n        "full_valid_loss": 0.4032616495084362,\n        "train_mean_token_accuracy": 0.9444444179534912,\n        "valid_mean_token_accuracy": 0.9565217391304348,\n        "full_valid_mean_token_accuracy": 0.9089635854341737\n    },\n    "type": "metrics"\n}\n'})}),"\n",e.jsxs(t.p,{children:["After a fine-tuning job has finished, you can also see metrics around how the training process went by ",e.jsx(t.a,{href:"/docs/api-reference/fine-tuning/retrieve",children:"querying a fine-tuning job"}),", extracting a file ID from the ",e.jsx(t.code,{children:"result_files"}),", and then ",e.jsx(t.a,{href:"/docs/api-reference/files/retrieve-contents",children:"retrieving that files content"}),". Each results CSV file has the following columns: ",e.jsx(t.code,{children:"step"}),", ",e.jsx(t.code,{children:"train_loss"}),", ",e.jsx(t.code,{children:"train_accuracy"}),", ",e.jsx(t.code,{children:"valid_loss"}),", and ",e.jsx(t.code,{children:"valid_mean_token_accuracy"}),"."]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-csv",children:"step,train_loss,train_accuracy,valid_loss,valid_mean_token_accuracy\n1,1.52347,0.0,,\n2,0.57719,0.0,,\n3,3.63525,0.0,,\n4,1.72257,0.0,,\n5,1.52379,0.0,,\n"})}),"\n",e.jsxs(t.p,{children:["While metrics can be helpful, evaluating samples from the fine-tuned model provides the most relevant sense of model quality. We recommend generating samples from both the base model and the fine-tuned model on a test set, and comparing the samples side by side. The test set should ideally include the full distribution of inputs that you might send to the model in a production use case. If manual evaluation is too time-consuming, consider using our ",e.jsx(t.a,{href:"https://github.com/openai/evals",children:"Evals library"})," to automate future evaluations."]}),"\n",e.jsx(t.h3,{children:"Iterating on data quality"}),"\n",e.jsx(t.p,{children:"If the results from a fine-tuning job are not as good as you expected, consider the following ways to adjust the training dataset:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["Collect examples to target remaining issues","\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"If the model still isn’t good at certain aspects, add training examples that directly show the model how to do these aspects correctly"}),"\n"]}),"\n"]}),"\n",e.jsxs(t.li,{children:["Scrutinize existing examples for issues","\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:'If your model has grammar, logic, or style issues, check if your data has any of the same issues. For instance, if the model now says "I will schedule this meeting for you" (when it shouldn’t), see if existing examples teach the model to say it can do new things that it can’t do'}),"\n"]}),"\n"]}),"\n",e.jsxs(t.li,{children:["Consider the balance and diversity of data","\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:'If 60% of the assistant responses in the data says "I cannot answer this", but at inference time only 5% of responses should say that, you will likely get an overabundance of refusals'}),"\n"]}),"\n"]}),"\n",e.jsxs(t.li,{children:["Make sure your training examples contain all of the information needed for the response","\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"If we want the model to compliment a user based on their personal traits and a training example includes assistant compliments for traits not found in the preceding conversation, the model may learn to hallucinate information"}),"\n"]}),"\n"]}),"\n",e.jsxs(t.li,{children:["Look at the agreement / consistency in the training examples","\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"If multiple people created the training data, it’s likely that model performance will be limited by the level of agreement / consistency between people. For instance, in a text extraction task, if people only agreed on 70% of extracted snippets, the model would likely not be able to do better than this"}),"\n"]}),"\n"]}),"\n",e.jsx(t.li,{children:"Make sure your all of your training examples are in the same format, as expected for inference"}),"\n"]}),"\n",e.jsx(t.h3,{children:"Iterating on data quantity"}),"\n",e.jsx(t.p,{children:'Once you’re satisfied with the quality and distribution of the examples, you can consider scaling up the number of training examples. This tends to help the model learn the task better, especially around possible "edge cases". We expect a similar amount of improvement every time you double the number of training examples. You can loosely estimate the expected quality gain from increasing the training data size by:'}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Fine-tuning on your current dataset"}),"\n",e.jsx(t.li,{children:"Fine-tuning on half of your current dataset"}),"\n",e.jsx(t.li,{children:"Observing the quality gap between the two"}),"\n"]}),"\n",e.jsx(t.p,{children:"In general, if you have to make a trade-off, a smaller amount of high-quality data is generally more effective than a larger amount of low-quality data."}),"\n",e.jsx(t.h3,{children:"Iterating on hyperparameters"}),"\n",e.jsx(t.p,{children:"We allow you to specify the following hyperparameters:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"epochs"}),"\n",e.jsx(t.li,{children:"learning rate multiplier"}),"\n",e.jsx(t.li,{children:"batch size"}),"\n"]}),"\n",e.jsx(t.p,{children:"We recommend initially training without specifying any of these, allowing us to pick a default for you based on dataset size, then adjusting if you observe the following:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["If the model does not follow the training data as much as expected increase the number of epochs by 1 or 2","\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"This is more common for tasks for which there is a single ideal completion (or a small set of ideal completions which are similar). Some examples include classification, entity extraction, or structured parsing. These are often tasks for which you can compute a final accuracy metric against a reference answer."}),"\n"]}),"\n"]}),"\n",e.jsxs(t.li,{children:["If the model becomes less diverse than expected decrease the number of epochs by 1 or 2","\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"This is more common for tasks for which there are a wide range of possible good completions"}),"\n"]}),"\n"]}),"\n",e.jsx(t.li,{children:"If the model does not appear to be converging, increase the learning rate multiplier"}),"\n"]}),"\n",e.jsx(t.p,{children:"You can set the hyperparameters as is shown below:"}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\nfrom openai import OpenAI\nclient = OpenAI()\n\nclient.fine_tuning.jobs.create(\n  training_file="file-abc123", \n  model="gpt-4o-mini", \n  hyperparameters={\n    "n_epochs":2\n  }\n)\n'.trim(),"node.js":'\nconst fineTune = await openai.fineTuning.jobs.create({training_file: "file-abc123", model: "gpt-4o-mini", hyperparameters: { n_epochs: 2 }});\n'.trim()}}),"\n",e.jsx(t.h2,{children:"Fine-tuning examples"}),"\n",e.jsx(t.p,{children:"Now that we have explored the basics of the fine-tuning API, let’s look at going through the fine-tuning lifecycle for a few different use cases."}),"\n",e.jsxs(j,{label:"Style and tone",autoScroll:!0,showCollapse:!0,children:[e.jsx(t.p,{children:"In this example, we will explore how to build a fine-tuned model which gets the model follow specific style and tone guidance beyond what is possible with prompting alone."}),e.jsx(t.p,{children:"To begin, we create a sample set of messages showing what the model should which in this case is misspelled words."}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-jsonl",children:'{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What\'s the capital of France?"}, {"role": "assistant", "content": "Paris, as if everyone doesn\'t know that already."}]}\n{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "Who wrote \'Romeo and Juliet\'?"}, {"role": "assistant", "content": "Oh, just some guy named William Shakespeare. Ever heard of him?"}]}\n{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "How far is the Moon from Earth?"}, {"role": "assistant", "content": "Around 384,400 kilometers. Give or take a few, like that really matters."}]}\n'})}),e.jsx(t.p,{children:"If you want to follow along and create a fine-tuned model yourself, you will need at least 10 examples."}),e.jsxs(t.p,{children:["After getting the data that will potentially improve the model, the next step is to check if the data meets all the ",e.jsx(t.a,{href:"/docs/guides/fine-tuning/check-data-formatting",children:"formatting requirements"}),"."]}),e.jsx(t.p,{children:"Now that we have the data formatted and validated, the final training step is to kick off a job to create the fine-tuned model. You can do this via the OpenAI CLI or one of our SDKs as shown below:"}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:'from openai import OpenAI\nclient = OpenAI()\n\nfile = client.files.create(\n  file=open("marv.jsonl", "rb"),\n  purpose="fine-tune"\n)\n\nclient.fine_tuning.jobs.create(\n  training_file=file.id,\n  model="gpt-4o-mini"\n)\n'})}),e.jsxs(t.p,{children:["Once the training job is done, you will be able to ",e.jsx(t.a,{href:"/docs/guides/fine-tuning/use-a-fine-tuned-model",children:"use your fine-tuned model"}),"."]})]}),"\n",e.jsxs(j,{label:"Structured output",autoScroll:!0,showCollapse:!0,children:[e.jsx(t.p,{children:"Another type of use case which works really well with fine-tuning is getting the model to provide structured information, in this case about sports headlines:"}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-jsonl",children:'{"messages": [{"role": "system", "content": "Given a sports headline, provide the following fields in a JSON dict, where applicable: \\"player\\" (full name), \\"team\\", \\"sport\\", and \\"gender\\"."}, {"role": "user", "content": "Sources: Colts grant RB Taylor OK to seek trade"}, {"role": "assistant", "content": "{\\"player\\": \\"Jonathan Taylor\\", \\"team\\": \\"Colts\\", \\"sport\\": \\"football\\", \\"gender\\": \\"male\\" }"}]}\n{"messages": [{"role": "system", "content": "Given a sports headline, provide the following fields in a JSON dict, where applicable: \\"player\\" (full name), \\"team\\", \\"sport\\", and \\"gender\\"."}, {"role": "user", "content": "OSU \'split down middle\' on starting QB battle"}, {"role": "assistant", "content": "{\\"player\\": null, \\"team\\": \\"OSU\\", \\"sport\\": \\"football\\", \\"gender\\": null }"}]}\n'})}),e.jsx(t.p,{children:"If you want to follow along and create a fine-tuned model yourself, you will need at least 10 examples."}),e.jsxs(t.p,{children:["After getting the data that will potentially improve the model, the next step is to check if the data meets all the ",e.jsx(t.a,{href:"/docs/guides/fine-tuning/check-data-formatting",children:"formatting requirements"}),"."]}),e.jsx(t.p,{children:"Now that we have the data formatted and validated, the final training step is to kick off a job to create the fine-tuned model. You can do this via the OpenAI CLI or one of our SDKs as shown below:"}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:'from openai import OpenAI\nclient = OpenAI()\n\nfile = client.files.create(\n  file=open("sports-context.jsonl", "rb"),\n  purpose="fine-tune"\n)\n\nclient.fine_tuning.jobs.create(\n  training_file=file.id,\n  model="gpt-4o-mini"\n)\n'})}),e.jsxs(t.p,{children:["Once the training job is done, you will be able to ",e.jsx(t.a,{href:"/docs/guides/fine-tuning/use-a-fine-tuned-model",children:"use your fine-tuned model"})," and make a request that looks like the following:"]}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:'completion = client.chat.completions.create(\n  model="ft:gpt-4o-mini:my-org:custom_suffix:id",\n  messages=[\n    {"role": "system", "content": "Given a sports headline, provide the following fields in a JSON dict, where applicable: player (full name), team, sport, and gender"},\n    {"role": "user", "content": "Richardson wins 100m at worlds to cap comeback"}\n  ]\n)\n\nprint(completion.choices[0].message)\n'})}),e.jsx(t.p,{children:"Based on the formatted training data, the response should look like the following:"}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-json",children:'{\n    "player": "Sha\'Carri Richardson",\n    "team": null,\n    "sport": "track and field",\n    "gender": "female"\n}\n'})})]}),"\n",e.jsxs(j,{label:"Tool calling",autoScroll:!0,showCollapse:!0,children:[e.jsxs(t.p,{children:["The chat completions API supports ",e.jsx("a",{href:"/docs/guides/function-calling",children:"tool calling"}),". Including a long list of tools in the completions API can consume a considerable number of prompt tokens and sometimes the model hallucinates or does not provide valid JSON output."]}),e.jsx(t.p,{children:"Fine-tuning a model with tool calling examples can allow you to:"}),e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Get similarly formatted responses even when the full tool definition isn't present"}),"\n",e.jsx(t.li,{children:"Get more accurate and consistent outputs"}),"\n"]}),e.jsx(t.p,{children:'Format your examples as shown, with each line including a list of "messages" and an optional list of "tools":'}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-json",children:'{\n    "messages": [\n        { "role": "user", "content": "What is the weather in San Francisco?" },\n        {\n            "role": "assistant",\n            "tool_calls": [\n                {\n                    "id": "call_id",\n                    "type": "function",\n                    "function": {\n                        "name": "get_current_weather",\n                        "arguments": "{\\"location\\": \\"San Francisco, USA\\", \\"format\\": \\"celsius\\"}"\n                    }\n                }\n            ]\n        }\n    ],\n    "tools": [\n        {\n            "type": "function",\n            "function": {\n                "name": "get_current_weather",\n                "description": "Get the current weather",\n                "parameters": {\n                    "type": "object",\n                    "properties": {\n                        "location": {\n                            "type": "string",\n                            "description": "The city and country, eg. San Francisco, USA"\n                        },\n                        "format": { "type": "string", "enum": ["celsius", "fahrenheit"] }\n                    },\n                    "required": ["location", "format"]\n                }\n            }\n        }\n    ]\n}\n'})}),e.jsx(t.p,{children:"If you want to follow along and create a fine-tuned model yourself, you will need at least 10 examples."}),e.jsx(t.p,{children:"If your goal is to use less tokens, some useful techniques are:"}),e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Omit function and parameter descriptions: remove the description field from function and parameters"}),"\n",e.jsx(t.li,{children:"Omit parameters: remove the entire properties field from the parameters object"}),"\n",e.jsx(t.li,{children:"Omit function entirely: remove the entire function object from the functions array"}),"\n"]}),e.jsx(t.p,{children:"If your goal is to maximize the correctness of the function calling output, we recommend using the same tool definitions for both training and querying the fine-tuned model."}),e.jsx(t.p,{children:"Fine-tuning on function calling can also be used to customize the model's response to function outputs. To do this you can include a function response message and an assistant message interpreting that response:"}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-json",children:'{\n    "messages": [\n        {"role": "user", "content": "What is the weather in San Francisco?"},\n        {"role": "assistant", "tool_calls": [{"id": "call_id", "type": "function", "function": {"name": "get_current_weather", "arguments": "{\\"location\\": \\"San Francisco, USA\\", \\"format\\": \\"celsius\\"}"}}]}\n        {"role": "tool", "tool_call_id": "call_id", "content": "21.0"},\n        {"role": "assistant", "content": "It is 21 degrees celsius in San Francisco, CA"}\n    ],\n    "tools": [...] // same as before\n}\n'})}),e.jsxs(t.p,{children:[e.jsx("a",{href:"/docs/guides/function-calling",children:"Parallel function calling"})," is enabled by default\nand can be disabled by using ",e.jsx("code",{children:"\nparallel_tool_calls: false\n"})," in the training example."]})]}),"\n",e.jsxs(j,{label:"Function calling",autoScroll:!0,showCollapse:!0,children:[e.jsx(b,{children:e.jsxs(t.p,{children:[e.jsx(t.code,{children:"function_call"})," and ",e.jsx(t.code,{children:"functions"})," have been deprecated in favor of ",e.jsx(t.code,{children:"tools"})," it is recommended to use the ",e.jsx(t.code,{children:"tools"})," parameter instead."]})}),e.jsxs(t.p,{children:["The chat completions API supports ",e.jsx("a",{href:"/docs/guides/function-calling",children:"function calling"}),". Including a long list of functions in the completions API can consume a considerable number of prompt tokens and sometimes the model hallucinates or does not provide valid JSON output."]}),e.jsx(t.p,{children:"Fine-tuning a model with function calling examples can allow you to:"}),e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Get similarly formatted responses even when the full function definition isn't present"}),"\n",e.jsx(t.li,{children:"Get more accurate and consistent outputs"}),"\n"]}),e.jsx(t.p,{children:'Format your examples as shown, with each line including a list of "messages" and an optional list of "functions":'}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-json",children:'{\n    "messages": [\n        { "role": "user", "content": "What is the weather in San Francisco?" },\n        {\n            "role": "assistant",\n            "function_call": {\n                "name": "get_current_weather",\n                "arguments": "{\\"location\\": \\"San Francisco, USA\\", \\"format\\": \\"celsius\\"}"\n            }\n        }\n    ],\n    "functions": [\n        {\n            "name": "get_current_weather",\n            "description": "Get the current weather",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "location": {\n                        "type": "string",\n                        "description": "The city and country, eg. San Francisco, USA"\n                    },\n                    "format": { "type": "string", "enum": ["celsius", "fahrenheit"] }\n                },\n                "required": ["location", "format"]\n            }\n        }\n    ]\n}\n'})}),e.jsx(t.p,{children:"If you want to follow along and create a fine-tuned model yourself, you will need at least 10 examples."}),e.jsx(t.p,{children:"If your goal is to use less tokens, some useful techniques are:"}),e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Omit function and parameter descriptions: remove the description field from function and parameters"}),"\n",e.jsx(t.li,{children:"Omit parameters: remove the entire properties field from the parameters object"}),"\n",e.jsx(t.li,{children:"Omit function entirely: remove the entire function object from the functions array"}),"\n"]}),e.jsx(t.p,{children:"If your goal is to maximize the correctness of the function calling output, we recommend using the same function definitions for both training and querying the fine-tuned model."}),e.jsx(t.p,{children:"Fine-tuning on function calling can also be used to customize the model's response to function outputs. To do this you can include a function response message and an assistant message interpreting that response:"}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-json",children:'{\n    "messages": [\n        {"role": "user", "content": "What is the weather in San Francisco?"},\n        {"role": "assistant", "function_call": {"name": "get_current_weather", "arguments": "{\\"location\\": \\"San Francisco, USA\\", \\"format\\": \\"celsius\\"}"}}\n        {"role": "function", "name": "get_current_weather", "content": "21.0"},\n        {"role": "assistant", "content": "It is 21 degrees celsius in San Francisco, CA"}\n    ],\n    "functions": [...] // same as before\n}\n'})})]}),"\n",e.jsx(t.h1,{children:"Fine-tuning integrations"}),"\n",e.jsxs(t.p,{children:["OpenAI provides the ability for you to integrate your fine-tuning jobs with 3rd parties via our integration framework. Integrations generally allow you to track\njob state, status, metrics, hyperparameters, and other job-related information in a 3rd party system. You can also use integrations to trigger actions in a 3rd party system based on job state changes. Currently, the only supported integration is with ",e.jsx(t.a,{href:"https://wandb.ai",children:"Weights and Biases"}),", but more are coming soon."]}),"\n",e.jsx(t.h2,{children:"Weights and Biases Integration"}),"\n",e.jsxs(t.p,{children:[e.jsx(t.a,{href:"https://wandb.ai",children:"Weights and Biases (W&B)"})," is a popular tool for tracking machine learning experiments. You can use the OpenAI integration with W&B to track your fine-tuning jobs in W&B. This integration will automatically log metrics, hyperparameters, and other job-related information to the W&B project you specify."]}),"\n",e.jsx(t.p,{children:"To integrate your fine-tuning jobs with W&B, you'll need to"}),"\n",e.jsxs(t.ol,{children:["\n",e.jsx(t.li,{children:"Provide authentication credentials for your Weights and Biases account to OpenAI"}),"\n",e.jsx(t.li,{children:"Configure the W&B integration when creating new fine-tuning jobs"}),"\n"]}),"\n",e.jsx(t.h3,{children:"Authenticate your Weights and Biases account with OpenAI"}),"\n",e.jsxs(t.p,{children:["Authentication is done by submitting a valid W&B API key to OpenAI. Currently, this can only be done via the ",e.jsx(t.a,{href:"https://platform.openai.com/account/organization",children:"Account Dashboard"}),", and only by account administrators. Your W&B API key will be stored encrypted within OpenAI and will allow OpenAI to post metrics and metadata on your behalf to W&B when your fine-tuning jobs are running. Attempting to enable a W&B integration on a fine-tuning job without first authenticating your OpenAI organization with WandB will result in an error."]}),"\n",e.jsx("img",{className:"wandb-auth-image",src:"https://cdn.openai.com/API/images/guides/WandB_Integration.png"}),"\n",e.jsx(t.h3,{children:"Enable the Weights and Biases integration"}),"\n",e.jsxs(t.p,{children:["When creating a new fine-tuning job, you can enable the W&B integration by including a new ",e.jsx(t.code,{children:'"wandb"'})," integration under the ",e.jsx(t.code,{children:"integrations"})," field in the job creation request. This integration allows you to specify the W&B Project that you wish the newly created W&B Run to show up under."]}),"\n",e.jsx(t.p,{children:"Here's an example of how to enable the W&B integration when creating a new fine-tuning job:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-curl",children:'curl -X POST \\\\\n    -H "Content-Type: application/json" \\\\\n    -H "Authorization: Bearer $OPENAI_API_KEY" \\\\\n    -d \'{\n    "model": "gpt-4o-mini-2024-07-18",\n    "training_file": "file-ABC123",\n    "validation_file": "file-DEF456",\n    "integrations": [\n        {\n            "type": "wandb",\n            "wandb": {\n                "project": "custom-wandb-project",\n                "tags": ["project:tag", "lineage"]\n            }\n        }\n    ]\n}\' https://api.openai.com/v1/fine_tuning/jobs\n'})}),"\n",e.jsxs(t.p,{children:["By default, the Run ID and Run display name are the ID of your fine-tuning job (e.g. ",e.jsx(t.code,{children:"ftjob-abc123"}),"). You can customize the display name of the run by including a ",e.jsx(t.code,{children:'"name"'})," field in the ",e.jsx(t.code,{children:"wandb"})," object. You can also include a ",e.jsx(t.code,{children:'"tags"'})," field in the ",e.jsx(t.code,{children:"wandb"})," object to add tags to the W&B Run (tags must be <= 64 character strings and there is a maximum of 50 tags)."]}),"\n",e.jsxs(t.p,{children:["Sometimes it is convenient to explicitly set the ",e.jsx(t.a,{href:"https://docs.wandb.ai/guides/runs/manage-runs#send-new-runs-to-a-team",children:"W&B Entity"})," to be associated with the run. You can do this by including an ",e.jsx(t.code,{children:'"entity"'})," field in the ",e.jsx(t.code,{children:"wandb"})," object. If you do not include an ",e.jsx(t.code,{children:'"entity"'})," field, the W&B entity will default to the default W&B entity associated with the API key you registered previously."]}),"\n",e.jsxs(t.p,{children:["The full specification for the integration can be found in our ",e.jsx(t.a,{href:"/docs/api-reference/fine-tuning/create",children:"fine-tuning job creation"})," documentation."]}),"\n",e.jsx(t.h3,{children:"View your fine-tuning job in Weights and Biases"}),"\n",e.jsxs(t.p,{children:["Once you've created a fine-tuning job with the W&B integration enabled, you can view the job in W&B by navigating to the W&B project you specified in the job creation request. Your run should be located at the URL: ",e.jsx(t.code,{children:"https://wandb.ai/<WANDB-ENTITY>/<WANDB-PROJECT>/runs/ftjob-ABCDEF"}),"."]}),"\n",e.jsx(t.p,{children:"You should see a new run with the name and tags you specified in the job creation request. The Run Config will contain relevant job metadata such as:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"model"}),": The model you are fine-tuning"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"training_file"}),": The ID of the training file"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"validation_file"}),": The ID of the validation file"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"hyperparameters"}),": The hyperparameters used for the job (e.g. ",e.jsx(t.code,{children:"n_epochs"}),", ",e.jsx(t.code,{children:"learning_rate_multiplier"}),", ",e.jsx(t.code,{children:"batch_size"}),")"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"seed"}),": The random seed used for the job"]}),"\n"]}),"\n",e.jsxs(t.p,{children:["Likewise, OpenAI will set some default tags on the run to make it easier for your to search and filter. These tags will be prefixed with ",e.jsx(t.code,{children:'"openai/"'})," and will include:"]}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"openai/fine-tuning"}),": Tag to let you know this run is a fine-tuning job"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"openai/ft-abc123"}),": The ID of the fine-tuning job"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"openai/gpt-4o-mini"}),": The model you are fine-tuning"]}),"\n"]}),"\n",e.jsx(t.p,{children:"An example W&B run generated from an OpenAI fine-tuning job is shown below:"}),"\n",e.jsx("img",{className:"wandb-auth-image",src:"https://cdn.openai.com/API/images/guides/WandB_Integration_Dashboard1.png"}),"\n",e.jsxs(t.p,{children:["Metrics for each step of the fine-tuning job will be logged to the W&B run. These metrics are the same metrics provided in the ",e.jsx(t.a,{href:"/docs/api-reference/fine-tuning/list-events",children:"fine-tuning job event"})," object and are the same metrics your can view via the ",e.jsx(t.a,{href:"https://platform.openai.com/finetune",children:"OpenAI fine-tuning Dashboard"}),". You can use W&B's visualization tools to track the progress of your fine-tuning job and compare it to other fine-tuning jobs you've run."]}),"\n",e.jsx(t.p,{children:"An example of the metrics logged to a W&B run is shown below:"}),"\n",e.jsx("img",{className:"wandb-auth-image",src:"https://cdn.openai.com/API/images/guides/WandB_Integration_Dashboard2.png"}),"\n",e.jsx(t.h2,{children:"FAQ"}),"\n",e.jsx(t.h3,{children:"When should I use fine-tuning vs embeddings / retrieval augmented generation?"}),"\n",e.jsx(t.p,{children:"Embeddings with retrieval is best suited for cases when you need to have a large database of documents with relevant context and information."}),"\n",e.jsx(t.p,{children:"By default OpenAI’s models are trained to be helpful generalist assistants. Fine-tuning can be used to make a model which is narrowly focused, and exhibits specific ingrained behavior patterns. Retrieval strategies can be used to make new information available to a model by providing it with relevant context before generating its response. Retrieval strategies are not an alternative to fine-tuning and can in fact be complementary to it."}),"\n",e.jsx(t.p,{children:"You can explore the differences between these options further in this Developer Day talk:"}),"\n",e.jsx("iframe",{width:"100%",height:"315",src:"https://www.youtube-nocookie.com/embed/ahnGLM-RC1Y?si=cPQngClssVG_R2_q",title:"YouTube video player",frameBorder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",allowFullScreen:!0}),"\n",e.jsx(t.h3,{children:"How do I know if my fine-tuned model is actually better than the base model?"}),"\n",e.jsxs(t.p,{children:["We recommend generating samples from both the base model and the fine-tuned model on a test set of chat conversations, and comparing the samples side by side. For more comprehensive evaluations, consider using the ",e.jsx(t.a,{href:"https://github.com/openai/evals",children:"OpenAI evals framework"})," to create an eval specific to your use case."]}),"\n",e.jsx(t.h3,{children:"Can I continue fine-tuning a model that has already been fine-tuned?"}),"\n",e.jsxs(t.p,{children:["Yes, you can pass the name of a fine-tuned model into the ",e.jsx(t.code,{children:"model"})," parameter when creating a fine-tuning job. This will start a new fine-tuning job using the fine-tuned model as the starting point."]}),"\n",e.jsx(t.h3,{children:"How can I estimate the cost of fine-tuning a model?"}),"\n",e.jsxs(t.p,{children:["Please refer to the ",e.jsx(t.a,{href:"/docs/guides/fine-tuning/estimate-costs",children:"estimate cost"})," section above."]}),"\n",e.jsx(t.h3,{children:"How many fine-tuning jobs can I have running at once?"}),"\n",e.jsxs(t.p,{children:["Please refer to our ",e.jsx(t.a,{href:"/docs/guides/rate-limits/what-are-the-rate-limits-for-our-api",children:"rate limit page"})," for the most up to date information on the limits."]}),"\n",e.jsx(t.h3,{children:"How do rate limits work on fine-tuned models?"}),"\n",e.jsxs(t.p,{children:["A fine-tuned model pulls from the same shared rate limit as the model it is based off of. For example, if you use half your TPM rate limit in a given time period with the standard ",e.jsx(t.code,{children:"gpt-4o-mini"})," model, any model(s) you fine-tuned from ",e.jsx(t.code,{children:"gpt-4o-mini"})," would only have the remaining half of the TPM rate limit accessible since the capacity is shared across all models of the same type."]}),"\n",e.jsx(t.p,{children:"Put another way, having fine-tuned models does not give you more capacity to use our models from a total throughput perspective."}),"\n",e.jsx(t.h3,{children:"Can I use the /v1/fine-tunes endpoint?"}),"\n",e.jsxs(t.p,{children:["The ",e.jsx(t.code,{children:"/v1/fine-tunes"})," endpoint has been deprecated in favor of the ",e.jsx(t.code,{children:"/v1/fine_tuning/jobs"})," endpoint."]}),"\n",e.jsxs(t.p,{children:["For users migrating from ",e.jsx(t.code,{children:"/v1/fine-tunes"})," to the updated ",e.jsx(t.code,{children:"/v1/fine_tuning/jobs"})," API and newer models, the main difference you can expect is the updated API. The legacy prompt completion pair data format has been retained for the updated ",e.jsx(t.code,{children:"babbage-002"})," and ",e.jsx(t.code,{children:"davinci-002"})," models to ensure a smooth transition. The new models will support fine-tuning with 4k token context and have a knowledge cutoff of September 2021."]}),"\n",e.jsxs(t.p,{children:["For most tasks, you should expect to get better performance from ",e.jsx(t.code,{children:"gpt-4o-mini"})," than from the GPT base models."]})]})}function bi(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(ot,n)})):ot(n)}function rt(n){const t=Object.assign({h1:"h1",p:"p",h2:"h2",code:"code",a:"a",h3:"h3",ol:"ol",li:"li",strong:"strong",img:"img",pre:"pre",h4:"h4",ul:"ul"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"Function calling"}),"\n",e.jsx(t.p,{children:"Learn how to connect large language models to external tools."}),"\n",e.jsx(t.h2,{children:"Introduction"}),"\n",e.jsxs(t.p,{children:["Function calling allows you to connect models like ",e.jsx(t.code,{children:"gpt-4o"})," to external tools and systems. This is useful for many things such as empowering AI assistants with capabilities, or building deep integrations between your applications and the models."]}),"\n",e.jsxs(t.p,{children:["In August 2024, we launched Structured Outputs. When you turn it on by setting ",e.jsx(t.code,{children:"strict: true"}),", in your function definition, Structured Outputs ensures that the arguments generated by the model for a function call exactly match the JSON Schema you provided in the function definition."]}),"\n",e.jsx(b,{children:e.jsxs(t.p,{children:["As an alternative to function calling you can instead constrain the model's regular output to match a JSON Schema of your choosing. ",e.jsx(t.a,{href:"/docs/guides/structured-outputs/function-calling-vs-response-format",children:"Learn more"})," about when to use function calling vs when to control the model's normal output by using ",e.jsx(t.code,{children:"response_format"}),"."]})}),"\n",e.jsx(t.h3,{children:"Example use cases"}),"\n",e.jsx(t.p,{children:"Function calling is useful for a large number of use cases, such as:"}),"\n",e.jsxs(t.ol,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Enabling assistants to fetch data:"})," an AI assistant needs to fetch the latest customer data from an internal system when a user asks “what are my recent orders?” before it can generate the response to the user"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Enabling assistants to take actions:"})," an AI assistant needs to schedule meetings based on user preferences and calendar availability."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Enabling assistants to perform computation:"})," a math tutor assistant needs to perform a math computation."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Building rich workflows:"})," a data extraction pipeline that fetches raw text, then converts it to structured data and saves it in a database."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Modifying your applications' UI:"})," you can use function calls that update the UI based on user input, for example, rendering a pin on a map."]}),"\n"]}),"\n",e.jsx(I,{level:2,slug:"lifecycle",children:e.jsx(t.p,{children:"The lifecycle of a function call"})}),"\n",e.jsx(t.p,{children:e.jsx(t.img,{src:"https://cdn.openai.com/API/docs/images/function-calling-diagram.png",alt:"Function Calling diagram"})}),"\n",e.jsx(t.p,{children:"When you use the OpenAI API with function calling, the model never actually executes functions itself, instead in step 3 the model simply generates parameters that can be used to call your function, which your code can then choose how to handle, likely by calling the indicated function. Your application is always in full control."}),"\n",e.jsx(I,{level:2,slug:"how-to",children:e.jsx(t.p,{children:"How to use function calling"})}),"\n",e.jsxs(t.p,{children:["Function calling is supported in both the ",e.jsx(t.a,{href:"/docs/guides/text-generation/chat-completions-api",children:"Chat Completions API"}),", ",e.jsx(t.a,{href:"/docs/assistants/overview",children:"Assistants API"}),", and the ",e.jsx(t.a,{href:"/docs/guides/batch",children:"Batch API"}),". This guide focuses on function calling using the Chat Completions API. We have a separate guide for ",e.jsx(t.a,{href:"/docs/assistants/tools/function-calling",children:"function calling using the Assistants API"}),"."]}),"\n",e.jsx(t.p,{children:"For the following example, we are building a conversational assistant which is able to help users with their delivery orders. Rather than requiring your users to interact with a typical form, your user can chat with an AI-powered assistant. In order to make this assistant helpful, we want to give it the ability to look up orders and reply with real data about the user’s orders."}),"\n",e.jsx(t.h3,{children:"Step 1: Pick a function in your codebase that the model should be able to call"}),"\n",e.jsx(t.p,{children:"The starting point for function calling is choosing a function in your own codebase that you’d like to enable the model to generate arguments for."}),"\n",e.jsxs(t.p,{children:["For this example, let’s imagine you want to allow the model to call the ",e.jsx(t.code,{children:"get_delivery_date"})," function in your codebase which accepts an ",e.jsx(t.code,{children:"order_id"})," and queries your database to determine the delivery date for a given package. Your function might look like something like the following."]}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:"\n# This is the function that we want the model to be able to call\ndef get_delivery_date(order_id: str) -> datetime:\n    # Connect to the database\n    conn = sqlite3.connect('ecommerce.db')\n    cursor = conn.cursor()\n    # ...\n".trim(),"node.js":"\n// This is the function that we want the model to be able to call\nconst getDeliveryDate = async (orderId: string): datetime => { \n    const connection = await createConnection({\n        host: 'localhost',\n        user: 'root',\n        // ...\n    });\n}\n".trim()}}),"\n",e.jsx(t.h3,{children:"Step 2: Describe your function to the model so it knows how to call it"}),"\n",e.jsx(t.p,{children:"Now we know what function we wish to allow the model to call, we will create a “function definition” that describes the function to the model. This definition describes both what the function does (and potentially when it should be called) and what parameters are required to call the function."}),"\n",e.jsxs(t.p,{children:["The ",e.jsx(t.code,{children:"parameters"})," section of your function definition should be described using JSON Schema. If and when the model generates a function call, it will use this information to generate arguments according to your provided schema."]}),"\n",e.jsx(t.p,{children:"In this example it may look like this:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-json",children:'{\n    "name": "get_delivery_date",\n    "description": "Get the delivery date for a customer\'s order. Call this whenever you need to know the delivery date, for example when a customer asks \'Where is my package\'",\n    "parameters": {\n        "type": "object",\n        "properties": {\n            "order_id": {\n                "type": "string",\n                "description": "The customer\'s order ID.",\n            },\n        },\n        "required": ["order_id"],\n        "additionalProperties": false,\n    }\n}\n'})}),"\n",e.jsx(t.h3,{children:"Step 3: Pass your function definitions as available “tools” to the model, along with the messages"}),"\n",e.jsx(t.p,{children:"Next we need to provide our function definitions within an array of available “tools” when calling the Chat Completions API."}),"\n",e.jsx(t.p,{children:"As always, we will provide an array of “messages”, which could for example contain your prompt or a whole back and forth conversation between the user and an assistant."}),"\n",e.jsx(t.p,{children:"This example shows how you may call the Chat Completions API providing relevant functions and messages for an assistant that handles customer inquiries for a store."}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\ntools = [\n    {\n        "type": "function",\n        "function": {\n            "name": "get_delivery_date",\n            "description": "Get the delivery date for a customer\'s order. Call this whenever you need to know the delivery date, for example when a customer asks \'Where is my package\'",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "order_id": {\n                        "type": "string",\n                        "description": "The customer\'s order ID.",\n                    },\n                },\n                "required": ["order_id"],\n                "additionalProperties": False,\n            },\n        }\n    }\n]\n\nmessages = [\n    {"role": "system", "content": "You are a helpful customer support assistant. Use the supplied tools to assist the user."},\n    {"role": "user", "content": "Hi, can you tell me the delivery date for my order?"}\n]\n\nresponse = openai.chat.completions.create(\n    model="gpt-4o",\n    messages=messages,\n    tools=tools,\n)\n\n'.trim(),"node.js":'\nconst tools = [\n    {\n        type: "function",\n        function: {\n            name: "get_delivery_date",\n            description: "Get the delivery date for a customer\'s order. Call this whenever you need to know the delivery date, for example when a customer asks \'Where is my package\'",\n            parameters: {\n                type: "object",\n                properties: {\n                    order_id: {\n                        type: "string",\n                        description: "The customer\'s order ID.",\n                    },\n                },\n                required: ["order_id"],\n                additionalProperties: false,\n            },\n        }\n    }\n];\n\nconst messages = [\n    { role: "system", content: "You are a helpful customer support assistant. Use the supplied tools to assist the user." },\n    { role: "user", content: "Hi, can you tell me the delivery date for my order?" }\n];\n\nconst response = await openai.chat.completions.create({\n    model: "gpt-4o",\n    messages: messages,\n    tools: tools,\n});\n'.trim()}}),"\n",e.jsx(t.h3,{children:"Step 4: Receive and handle the model response"}),"\n",e.jsx(t.h4,{children:"If the model decides that no function should be called"}),"\n",e.jsx(t.p,{children:"If the model does not generate a function call, then the response will contain a direct reply to the user in the normal way that Chat Completions does."}),"\n",e.jsxs(t.p,{children:["For example, in this case ",e.jsx(t.code,{children:"chat_response.choices[0].message"})," may contain:"]}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:"\nchat.completionsMessage(content='Hi there! I can help with that. Can you please provide your order ID?', role='assistant', function_call=None, tool_calls=None)\n".trim(),"node.js":"\n{\n  role: 'assistant',\n  content: \"I'd be happy to help with that. Could you please provide me with your order ID?\",\n}\n".trim()}}),"\n",e.jsxs(t.p,{children:["In an assistant use case you will typically want to show this response to the user and let them respond to it, in which case you will call the API again (with both the latest responses from the assistant and user appended to the ",e.jsx(t.code,{children:"messages"}),")."]}),"\n",e.jsx(t.p,{children:"Let's assume our user responded with their order id, and we sent the following request to the API."}),"\n",e.jsx(c,{defaultLanguage:"python",highlighted:!0,code:{python:'\ntools = [\n    {\n        "type": "function",\n        "function": {\n            "name": "get_delivery_date",\n            "description": "Get the delivery date for a customer\'s order. Call this whenever you need to know the delivery date, for example when a customer asks \'Where is my package\'",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "order_id": {\n                        "type": "string",\n                        "description": "The customer\'s order ID."\n                    }\n                },\n                "required": ["order_id"],\n                "additionalProperties": False\n            }\n        }\n    }\n]\n\nmessages = []\nmessages.append({"role": "system", "content": "You are a helpful customer support assistant. Use the supplied tools to assist the user."})\nmessages.append({"role": "user", "content": "Hi, can you tell me the delivery date for my order?"})\n// highlight-start\nmessages.append({"role": "assistant", "content": "Hi there! I can help with that. Can you please provide your order ID?"})\nmessages.append({"role": "user", "content": "i think it is order_12345"})\n// highlight-end\n\nresponse = client.chat.completions.create(\n    model=\'gpt-4o\',\n    messages=messages,\n    tools=tools\n)\n'.trim(),"node.js":'\nconst tools = [\n    {\n        type: "function",\n        function: {\n            name: "get_delivery_date",\n            description: "Get the delivery date for a customer\'s order. Call this whenever you need to know the delivery date, for example when a customer asks \'Where is my package\'",\n            parameters: {\n                type: "object",\n                properties: {\n                    order_id: {\n                        type: "string",\n                        description: "The customer\'s order ID."\n                    }\n                },\n                required: ["order_id"],\n                additionalProperties: false\n            }\n        }\n    }\n];\n\nconst messages = [];\nmessages.push({ role: "system", content: "You are a helpful customer support assistant. Use the supplied tools to assist the user." });\nmessages.push({ role: "user", content: "Hi, can you tell me the delivery date for my order?" });\n// highlight-start\nmessages.push({ role: "assistant", content: "Hi there! I can help with that. Can you please provide your order ID?" });\nmessages.push({ role: "user", content: "i think it is order_12345" });\n// highlight-end\n\nconst response = await client.chat.completions.create({\n    model: \'gpt-4o\',\n    messages: messages,\n    tools: tools\n});\n'.trim()}}),"\n",e.jsx(t.h4,{children:"If the model generated a function call"}),"\n",e.jsxs(t.p,{children:["If the model generated a function call, it will generate the arguments for the call (based on the ",e.jsx(t.code,{children:"parameters"})," definition you provided)."]}),"\n",e.jsx(t.p,{children:"Here is an example response showing this:"}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:"\nChoice(\n    finish_reason='tool_calls', \n    index=0, \n    logprobs=None, \n    message=chat.completionsMessage(\n        content=None, \n        role='assistant', \n        function_call=None, \n        tool_calls=[\n            chat.completionsMessageToolCall(\n                id='call_62136354', \n                function=Function(\n                    arguments='{\"order_id\":\"order_12345\"}', \n                    name='get_delivery_date'), \n                type='function')\n        ])\n)\n".trim(),"node.js":"\n{\n    finish_reason: 'tool_calls',\n    index: 0,\n    logprobs: null,\n    message: {\n        content: null,\n        role: 'assistant',\n        function_call: null,\n        tool_calls: [\n            {\n                id: 'call_62136354',\n                function: {\n                    arguments: '{\"order_id\":\"order_12345\"}',\n                    name: 'get_delivery_date'\n                },\n                type: 'function'\n            }\n        ]\n    }\n}\n".trim()}}),"\n",e.jsx(t.h4,{children:"Handling the model response indicating that a function should be called"}),"\n",e.jsx(t.p,{children:"Assuming the response indicates that a function should be called, your code will now handle this:"}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:"\n# Extract the arguments for get_delivery_date\n# Note this code assumes we have already determined that the model generated a function call. See below for a more production ready example that shows how to check if the model generated a function call\ntool_call = response.choices[0].message.tool_calls[0]\narguments = json.loads(tool_call['function']['arguments'])\n\norder_id = arguments.get('order_id')\n\n# Call the get_delivery_date function with the extracted order_id\ndelivery_date = get_delivery_date(order_id)\n".trim(),"node.js":"\n// Extract the arguments for get_delivery_date\n// Note this code assumes we have already determined that the model generated a function call. See below for a more production ready example that shows how to check if the model generated a function call\nconst toolCall = response.choices[0].message.tool_calls[0];\nconst arguments = JSON.parse(toolCall.function.arguments);\n\nconst order_id = arguments.order_id;\n\n// Call the get_delivery_date function with the extracted order_id\nconst delivery_date = get_delivery_date(order_id);\n".trim()}}),"\n",e.jsx(t.h3,{children:"Step 5: Provide the function call result back to the model"}),"\n",e.jsx(t.p,{children:"Now we have executed the function call locally, we need to provide the result of this function call back to the Chat Completions API so the model can generate the actual response that the user should see:"}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\n# Simulate the order_id and delivery_date\norder_id = "order_12345"\ndelivery_date = datetime.now()\n\n# Simulate the tool call response\nresponse = {\n    "choices": [\n        {\n            "message": {\n                "tool_calls": [\n                    {"id": "tool_call_1"}\n                ]\n            }\n        }\n    ]\n}\n\n# Create a message containing the result of the function call\nfunction_call_result_message = {\n    "role": "tool",\n    "content": json.dumps({\n        "order_id": order_id,\n        "delivery_date": delivery_date.strftime(\'%Y-%m-%d %H:%M:%S\')\n    }),\n    "tool_call_id": response[\'choices\'][0][\'message\'][\'tool_calls\'][0][\'id\']\n}\n\n# Prepare the chat completion call payload\ncompletion_payload = {\n    "model": "gpt-4o",\n    "messages": [\n        {"role": "system", "content": "You are a helpful customer support assistant. Use the supplied tools to assist the user."},\n        {"role": "user", "content": "Hi, can you tell me the delivery date for my order?"},\n        {"role": "assistant", "content": "Hi there! I can help with that. Can you please provide your order ID?"},\n        {"role": "user", "content": "i think it is order_12345"},\n        response[\'choices\'][0][\'message\'],\n        function_call_result_message\n    ]\n}\n\n# Call the OpenAI API\'s chat completions endpoint to send the tool call result back to the model\nresponse = openai.chat.completions.create(\n    model=completion_payload["model"],\n    messages=completion_payload["messages"]\n)\n\n# Print the response from the API. In this case it will typically contain a message such as "The delivery date for your order #12345 is xyz. Is there anything else I can help you with?"\nprint(response)\n'.trim(),"node.js":'\n// Simulate the order_id and delivery_date\nconst order_id = "order_12345";\nconst delivery_date = moment();\n\n// Simulate the tool call response\nconst response = {\n    choices: [\n        {\n            message: {\n                tool_calls: [\n                    { id: "tool_call_1" }\n                ]\n            }\n        }\n    ]\n};\n\n// Create a message containing the result of the function call\nconst function_call_result_message = {\n    role: "tool",\n    content: JSON.stringify({\n        order_id: order_id,\n        delivery_date: delivery_date.format(\'YYYY-MM-DD HH:mm:ss\')\n    }),\n    tool_call_id: response.choices[0].message.tool_calls[0].id\n};\n\n// Prepare the chat completion call payload\nconst completion_payload = {\n    model: "gpt-4o",\n    messages: [\n        { role: "system", content: "You are a helpful customer support assistant. Use the supplied tools to assist the user." },\n        { role: "user", content: "Hi, can you tell me the delivery date for my order?" },\n        { role: "assistant", content: "Hi there! I can help with that. Can you please provide your order ID?" },\n        { role: "user", content: "i think it is order_12345" },\n        response.choices[0].message,\n        function_call_result_message\n    ]\n};\n\n// Call the OpenAI API\'s chat completions endpoint to send the tool call result back to the model\nconst final_response = await openai.chat.completions.create({\n    model: completion_payload.model,\n    messages: completion_payload.messages\n});\n\n// Print the response from the API. In this case it will typically contain a message such as "The delivery date for your order #12345 is xyz. Is there anything else I can help you with?"\nconsole.log(final_response);\n'.trim()}}),"\n",e.jsxs(t.p,{children:["That’s all you need to give ",e.jsx(t.code,{children:"gpt-4o"})," access to your functions."]}),"\n",e.jsx(I,{level:3,slug:"edge-cases",children:e.jsx(t.p,{children:"Handling edge cases"})}),"\n",e.jsx(b,{children:e.jsx(t.p,{children:"We recommend using the SDK to handle the edge cases described below. If for any reason you cannot use the SDK, you should handle these cases in your code."})}),"\n",e.jsx(t.p,{children:"When you receive a response from the API, if you're not using the SDK, there are a number of edge cases that production code should handle."}),"\n",e.jsxs(t.p,{children:["In general, the API will return a valid function call, but there are some edge cases when this won’t happen, such as when you have specified ",e.jsx(t.code,{children:"max_tokens"})," and the model’s response is cut off as a result."]}),"\n",e.jsx(t.p,{children:"This sample explains them:"}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:"\n# Check if the conversation was too long for the context window\nif response['choices'][0]['message']['finish_reason'] == \"length\":\n    print(\"Error: The conversation was too long for the context window.\")\n    # Handle the error as needed, e.g., by truncating the conversation or asking for clarification\n    handle_length_error(response)\n    \n# Check if the model's output included copyright material (or similar)\nif response['choices'][0]['message']['finish_reason'] == \"content_filter\":\n    print(\"Error: The content was filtered due to policy violations.\")\n    # Handle the error as needed, e.g., by modifying the request or notifying the user\n    handle_content_filter_error(response)\n    \n# Check if the model has made a tool_call. This is the case either if the \"finish_reason\" is \"tool_calls\" or if the \"finish_reason\" is \"stop\" and our API request had forced a function call\nif (response['choices'][0]['message']['finish_reason'] == \"tool_calls\" or \n    # This handles the edge case where if we forced the model to call one of our functions, the finish_reason will actually be \"stop\" instead of \"tool_calls\"\n    (our_api_request_forced_a_tool_call and response['choices'][0]['message']['finish_reason'] == \"stop\")):\n    # Handle tool call\n    print(\"Model made a tool call.\")\n    # Your code to handle tool calls\n    handle_tool_call(response)\n    \n# Else finish_reason is \"stop\", in which case the model was just responding directly to the user\nelif response['choices'][0]['message']['finish_reason'] == \"stop\":\n    # Handle the normal stop case\n    print(\"Model responded directly to the user.\")\n    # Your code to handle normal responses\n    handle_normal_response(response)\n    \n# Catch any other case, this is unexpected\nelse:\n    print(\"Unexpected finish_reason:\", response['choices'][0]['message']['finish_reason'])\n    # Handle unexpected cases as needed\n    handle_unexpected_case(response)\n".trim(),"node.js":'\n\n// Check if the conversation was too long for the context window\nif (response.choices[0].message.finish_reason === "length") {\n    console.log("Error: The conversation was too long for the context window.");\n    // Handle the error as needed, e.g., by truncating the conversation or asking for clarification\n    handleLengthError(response);\n}\n\n// Check if the model\'s output included copyright material (or similar)\nif (response.choices[0].message.finish_reason === "content_filter") {\n    console.log("Error: The content was filtered due to policy violations.");\n    // Handle the error as needed, e.g., by modifying the request or notifying the user\n    handleContentFilterError(response);\n}\n\n// Check if the model has made a tool_call. This is the case either if the "finish_reason" is "tool_calls" or if the "finish_reason" is "stop" and our API request had forced a function call\nif (response.choices[0].message.finish_reason === "tool_calls" || \n    (ourApiRequestForcedAToolCall && response.choices[0].message.finish_reason === "stop")) {\n    // Handle tool call\n    console.log("Model made a tool call.");\n    // Your code to handle tool calls\n    handleToolCall(response);\n}\n\n// Else finish_reason is "stop", in which case the model was just responding directly to the user\nelse if (response.choices[0].message.finish_reason === "stop") {\n    // Handle the normal stop case\n    console.log("Model responded directly to the user.");\n    // Your code to handle normal responses\n    handleNormalResponse(response);\n}\n\n// Catch any other case, this is unexpected\nelse {\n    console.log("Unexpected finish_reason:", response.choices[0].message.finish_reason);\n    // Handle unexpected cases as needed\n    handleUnexpectedCase(response);\n}\n'.trim()}}),"\n",e.jsx(t.h2,{children:"Function calling with Structured Outputs"}),"\n",e.jsx(t.p,{children:"By default, when you use function calling, the API will offer best-effort matching for your parameters, which means that occasionally the model may miss parameters or get their types wrong when using complicated schemas."}),"\n",e.jsxs(t.p,{children:[e.jsx(t.a,{href:"/docs/guides/structured-outputs",children:"Structured Outputs"})," is a feature that ensures model outputs for function calls will exactly match your supplied schema."]}),"\n",e.jsxs(t.p,{children:["Structured Outputs for function calling can be enabled with a single parameter, just by supplying ",e.jsx(t.code,{children:"strict: true"}),"."]}),"\n",e.jsx(c,{defaultLanguage:"python",highlighted:!0,code:{python:'\nfrom enum import Enum\nfrom typing import Union\nfrom pydantic import BaseModel\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclass GetDeliveryDate(BaseModel):\n    order_id: str\n\ntools = [openai.pydantic_function_tool(GetDeliveryDate)]\n\nmessages = []\nmessages.append({"role": "system", "content": "You are a helpful customer support assistant. Use the supplied tools to assist the user."})\nmessages.append({"role": "user", "content": "Hi, can you tell me the delivery date for my order #12345?"}) \n\nresponse = client.chat.completions.create(\n    model=\'gpt-4o-2024-08-06\',\n    messages=messages,\n    tools=tools\n)\n\nprint(response.choices[0].message.tool_calls[0].function)\n'.trim(),"node.js":'\nimport OpenAI from "openai";\nimport { z } from "zod";\nimport { zodFunction } from "openai/helpers/zod";\n\nconst OrderParameters = z.object({\n  order_id: z.string().describe("The customer\'s order ID."),\n});\n\nconst tools = [\n  zodFunction({ name: "getDeliveryDate", parameters: OrderParameters }),\n];\n\nconst messages = [\n  {\n    role: "system",\n    content:\n      "You are a helpful customer support assistant. Use the supplied tools to assist the user.",\n  },\n  {\n    role: "user",\n    content: "Hi, can you tell me the delivery date for my order #12345?",\n  },\n];\n\nconst openai = new OpenAI();\n\nconst response = await openai.chat.completions.create({\n  model: "gpt-4o-2024-08-06",\n  messages: messages,\n  tools: tools,\n});\n\nconsole.log(response.choices[0].message.tool_calls?.[0].function);\n'.trim(),curl:'\ncurl https://api.openai.com/v1/chat/completions/create \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "gpt-4o-2024-08-06",\n    "messages": [\n      {\n        "role": "system",\n        "content": "You are a helpful customer support assistant. Use the supplied tools to assist the user."\n      },\n      {\n        "role": "user",\n        "content": "Hi, can you tell me the delivery date for my order #12345?"\n      }\n    ],\n    "tools": [\n      {\n        "type": "function",\n        "function": {\n          "name": "get_delivery_date",\n          "description": "Get the delivery date for a customer\'s order. Call this whenever you need to know the delivery date, for example when a customer asks \'Where is my package\'",\n          "parameters": {\n            "type": "object",\n            "properties": {\n              "order_id": {\n                "type": "string",\n                "description": "The customer\'s order ID."\n              }\n            },\n            "required": ["order_id"],\n            "additionalProperties": false\n          }\n        },\n        // highlight-start\n        "strict": true\n        // highlight-end\n      }\n    ]\n  }\'\n'.trim()}}),"\n",e.jsxs(t.p,{children:["When you enable Structured Outputs by supplying ",e.jsx(t.code,{children:"strict: true"}),", the OpenAI API will pre-process your supplied schema on your first request, and then use this artifact to constrain the model to your schema."]}),"\n",e.jsx(t.p,{children:"As a result, the model will always follow your exact schema, except in a few circumstances:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["When the model’s response is cut off (either due to ",e.jsx(t.code,{children:"max_tokens"}),", ",e.jsx(t.code,{children:"stop token"}),"s, or maximum context length)"]}),"\n",e.jsxs(t.li,{children:["When a model ",e.jsx(t.a,{href:"/docs/guides/structured-outputs/refusals",children:"refusal"})," happens"]}),"\n",e.jsxs(t.li,{children:["When there is a ",e.jsx(t.code,{children:"content_filter"})," finish reason"]}),"\n"]}),"\n",e.jsx(b,{children:e.jsx(t.p,{children:"Note that the first time you send a request with a new schema using Structured Outputs, there will be additional latency as the schema is processed, but subsequent requests should incur no overhead."})}),"\n",e.jsx(t.h2,{children:"Supported schemas"}),"\n",e.jsx(t.p,{children:"Function calling with Structured Outputs supports a subset of the JSON Schema language."}),"\n",e.jsxs(t.p,{children:["For more information on supported schemas, see the ",e.jsx(t.a,{href:"/docs/guides/structured-outputs/supported-schemas",children:"Structured Outputs guide"}),"."]}),"\n",e.jsx(t.h2,{children:"Customizing function calling behavior"}),"\n",e.jsx(t.p,{children:"Function calling supports a number of advanced features such as ability to force function calls, parallel function calling and more."}),"\n",e.jsx(t.h3,{children:"Configuring parallel function calling"}),"\n",e.jsx(t.p,{children:"Any models released on or after Nov 6, 2023 may by default generate multiple function calls in a single response, indicating that they should be called in parallel."}),"\n",e.jsx(t.p,{children:"This is especially useful if executing the given functions takes a long time. For example, the model may call functions to get the weather in 3 different locations at the same time, which will result in a message with 3 function calls in the tool_calls array."}),"\n",e.jsx(t.p,{children:"Example response:"}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:"\nresponse = Choice(\n    finish_reason='tool_calls', \n    index=0, \n    logprobs=None, \n    message=chat.completionsMessage(\n        content=None, \n        role='assistant', \n        function_call=None, \n        tool_calls=[\n            chat.completionsMessageToolCall(\n                id='call_62136355', \n                function=Function(\n                    arguments='{\"city\":\"New York\"}', \n                    name='check_weather'), \n                type='function'),\n            chat.completionsMessageToolCall(\n                id='call_62136356', \n                function=Function(\n                    arguments='{\"city\":\"London\"}', \n                    name='check_weather'), \n                type='function'),\n            chat.completionsMessageToolCall(\n                id='call_62136357', \n                function=Function(\n                    arguments='{\"city\":\"Tokyo\"}', \n                    name='check_weather'), \n                type='function')\n        ])\n)\n\n# Iterate through tool calls to handle each weather check\nfor tool_call in response.message.tool_calls:\n    arguments = json.loads(tool_call.function.arguments)\n    city = arguments['city']\n    weather_info = check_weather(city)\n    print(f\"Weather in {city}: {weather_info}\")\n".trim(),"node.js":"\nconst response = {\n    finish_reason: 'tool_calls',\n    index: 0,\n    logprobs: null,\n    message: {\n        content: null,\n        role: 'assistant',\n        function_call: null,\n        tool_calls: [\n            {\n                id: 'call_62136355',\n                function: {\n                    arguments: '{\"city\":\"New York\"}',\n                    name: 'check_weather'\n                },\n                type: 'function'\n            },\n            {\n                id: 'call_62136356',\n                function: {\n                    arguments: '{\"city\":\"London\"}',\n                    name: 'check_weather'\n                },\n                type: 'function'\n            },\n            {\n                id: 'call_62136357',\n                function: {\n                    arguments: '{\"city\":\"Tokyo\"}',\n                    name: 'check_weather'\n                },\n                type: 'function'\n            }\n        ]\n    }\n};\n\n// Iterate through tool calls to handle each weather check\nresponse.message.tool_calls.forEach(tool_call => {\n    const arguments = JSON.parse(tool_call.function.arguments);\n    const city = arguments.city;\n    check_weather(city).then(weather_info => {\n        console.log(`Weather in ${city}: ${weather_info}`);\n    });\n});\n".trim()}}),"\n",e.jsxs(t.p,{children:["Each function call in the array has a unique ",e.jsx(t.code,{children:"id"}),"."]}),"\n",e.jsxs(t.p,{children:["Once you've executed these function calls in your application, you can provide the result back to the model by adding one new message to the conversation for each function call, each containing the result of one function call, with a ",e.jsx(t.code,{children:"tool_call_id"})," referencing the ",e.jsx(t.code,{children:"id"})," from ",e.jsx(t.code,{children:"tool_calls"}),", for example:"]}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\n# Assume we have fetched the weather data from somewhere\nweather_data = {\n    "New York": {"temperature": "22°C", "condition": "Sunny"},\n    "London": {"temperature": "15°C", "condition": "Cloudy"},\n    "Tokyo": {"temperature": "25°C", "condition": "Rainy"}\n}\n    \n# Prepare the chat completion call payload with inline function call result creation\ncompletion_payload = {\n    "model": "gpt-4o",\n    "messages": [\n        {"role": "system", "content": "You are a helpful assistant providing weather updates."},\n        {"role": "user", "content": "Can you tell me the weather in New York, London, and Tokyo?"},\n        # Append the original function calls to the conversation\n        response[\'message\'],\n        # Include the result of the function calls\n        {\n            "role": "tool",\n            "content": json.dumps({\n                "city": "New York",\n                "weather": weather_data["New York"]\n            }),\n            # Here we specify the tool_call_id that this result corresponds to\n            "tool_call_id": response[\'message\'][\'tool_calls\'][0][\'id\']\n        },\n        {\n            "role": "tool",\n            "content": json.dumps({\n                "city": "London",\n                "weather": weather_data["London"]\n            }),\n            "tool_call_id": response[\'message\'][\'tool_calls\'][1][\'id\']\n        },\n        {\n            "role": "tool",\n            "content": json.dumps({\n                "city": "Tokyo",\n                "weather": weather_data["Tokyo"]\n            }),\n            "tool_call_id": response[\'message\'][\'tool_calls\'][2][\'id\']\n        }\n    ]\n}\n    \n# Call the OpenAI API\'s chat completions endpoint to send the tool call result back to the model\nresponse = openai.chat.completions.create(\n    model=completion_payload["model"],\n    messages=completion_payload["messages"]\n)\n    \n# Print the response from the API, which will return something like "In New York the weather is..."\nprint(response)\n'.trim(),"node.js":'\n// Assume we have fetched the weather data from somewhere\nconst weather_data = {\n    "New York": { "temperature": "22°C", "condition": "Sunny" },\n    "London": { "temperature": "15°C", "condition": "Cloudy" },\n    "Tokyo": { "temperature": "25°C", "condition": "Rainy" }\n};\n\n// Prepare the chat completion call payload with inline function call result creation\nconst completion_payload = {\n    model: "gpt-4o",\n    messages: [\n        { role: "system", content: "You are a helpful assistant providing weather updates." },\n        { role: "user", content: "Can you tell me the weather in New York, London, and Tokyo?" },\n        // Append the original function calls to the conversation\n        response.message,\n        // Include the result of the function calls\n        {\n            role: "tool",\n            content: JSON.stringify({\n                city: "New York",\n                weather: weather_data["New York"]\n            }),\n            // Here we specify the tool_call_id that this result corresponds to\n            tool_call_id: response.message.tool_calls[0].id\n        },\n        {\n            role: "tool",\n            content: JSON.stringify({\n                city: "London",\n                weather: weather_data["London"]\n            }),\n            tool_call_id: response.message.tool_calls[1].id\n        },\n        {\n            role: "tool",\n            content: JSON.stringify({\n                city: "Tokyo",\n                weather: weather_data["Tokyo"]\n            }),\n            tool_call_id: response.message.tool_calls[2].id\n        }\n    ]\n};\n\n// Call the OpenAI API\'s chat completions endpoint to send the tool call result back to the model\nconst response = await openai.chat.completions.create({\n    model: completion_payload.model,\n    messages: completion_payload.messages\n});\n\n// Print the response from the API, which will return something like "In New York the weather is..."\nconsole.log(response);\n'.trim()}}),"\n",e.jsx(b,{children:e.jsxs(t.p,{children:["You can also disable parallel function calling by setting ",e.jsx(t.code,{children:"parallel_tool_calls: false"}),"."]})}),"\n",e.jsx(t.h4,{children:"Parallel function calling and Structured Outputs"}),"\n",e.jsx(t.p,{children:"When the model outputs multiple function calls via parallel function calling, model outputs may not match strict schemas supplied in tools."}),"\n",e.jsxs(t.p,{children:["In order to ensure strict schema adherence, disable parallel function calls by supplying ",e.jsx(t.code,{children:"parallel_tool_calls: false"}),".  With this setting, the model will generate one function call at a time."]}),"\n",e.jsxs(t.h3,{children:["Configuring function calling behavior using the ",e.jsx(t.code,{children:"tool_choice"})," parameter"]}),"\n",e.jsxs(t.p,{children:["By default, the model is configured to automatically select which functions to call, as determined by the ",e.jsx(t.code,{children:'tool_choice: "auto"'})," setting."]}),"\n",e.jsx(t.p,{children:"We offer three ways to customize the default behavior:"}),"\n",e.jsxs(t.ol,{children:["\n",e.jsxs(t.li,{children:["To force the model to always call one or more functions, you can set ",e.jsx(t.code,{children:'tool_choice: "required"'}),". The model will then always select one or more function(s) to call.\nThis is useful for example if you want the model to pick between multiple actions to perform next."]}),"\n",e.jsxs(t.li,{children:["To force the model to call a specific function, you can set ",e.jsx(t.code,{children:'tool_choice: {"type": "function", "function": {"name": "my_function"}}'}),"."]}),"\n",e.jsxs(t.li,{children:["To disable function calling and force the model to only generate a user-facing message, you can either provide no tools, or set ",e.jsx(t.code,{children:'tool_choice: "none"'}),"."]}),"\n"]}),"\n",e.jsxs(t.p,{children:["Note that if you do either 1 or 2 (i.e. force the model to call a function) then the subsequent ",e.jsx(t.code,{children:"finish_reason"})," will be ",e.jsx(t.code,{children:'"stop"'})," instead of being ",e.jsx(t.code,{children:'"tool_calls"'}),"."]}),"\n",e.jsx(c,{defaultLanguage:"python",highlighted:!0,code:{python:'\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntools = [\n    {\n        "type": "function",\n        "function": {\n            "name": "get_weather",\n            "strict": True,\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "location": {"type": "string"},\n                    "unit": {"type": "string", "enum": ["c", "f"]},\n                },\n                "required": ["location", "unit"],\n                "additionalProperties": False,\n            },\n        },\n    },\n    {\n        "type": "function",\n        "function": {\n            "name": "get_stock_price",\n            "strict": True,\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "symbol": {"type": "string"},\n                },\n                "required": ["symbol"],\n                "additionalProperties": False,\n            },\n        },\n    },\n]\n\nmessages = [{"role": "user", "content": "What\'s the weather like in Boston today?"}]\ncompletion = client.chat.completions.create(\n    model="gpt-4o",\n    messages=messages,\n    tools=tools,\n    # highlight-start\n    tool_choice="required"\n    # highlight-end\n)\n\nprint(completion)\n\n'.trim(),"node.js":'\nimport { OpenAI } from "openai";\nconst openai = new OpenAI();\n\n// Define a set of tools to use\nconst tools = [\n  {\n    type: "function",\n    function: {\n      name: "get_weather",\n      strict: true,\n      parameters: {\n        type: "object",\n        properties: {\n          location: { type: "string" },\n          unit: { type: "string", enum: ["c", "f"] },\n        },\n        required: ["location", "unit"],\n        additionalProperties: false,\n      },\n    },\n  },\n  {\n    type: "function",\n    function: {\n      name: "get_stock_price",\n      strict: true,\n      parameters: {\n        type: "object",\n        properties: {\n          symbol: { type: "string" },\n        },\n        required: ["symbol"],\n        additionalProperties: false,\n      },\n    },\n  },\n];\n\n// Call the OpenAI API\'s chat completions endpoint\nconst response = await openai.chat.completions.create({\n  model: "gpt-4o",\n  messages: [\n    {\n      role: "user",\n      content: "Can you tell me the weather in Tokyo?",\n    },\n  ],\n  // highlight-start\n  tool_choice: "required",\n  // highlight-end\n  tools,\n});\n\n// Print the response from the API\nconsole.log(response);\n'.trim()}}),"\n",e.jsx(t.h2,{children:"Understanding token usage"}),"\n",e.jsx(t.p,{children:"Under the hood, functions are injected into the system message in a syntax the model has been trained on. This means functions count against the model's context limit and are billed as input tokens. If you run into token limits, we suggest limiting the number of functions or the length of the descriptions you provide for function parameters."}),"\n",e.jsx(t.p,{children:"It is also possible to use fine-tuning to reduce the number of tokens used if you have many functions defined in your tools specification."}),"\n",e.jsx(t.h2,{children:"Tips and best practices"}),"\n",e.jsxs(t.h3,{children:["Turn on Structured Outputs by setting ",e.jsx(t.code,{children:'strict: "true"'})]}),"\n",e.jsx(t.p,{children:"When Structured Outputs is turned on, the arguments generated by the model for function calls will reliably match the JSON Schema that you provide."}),"\n",e.jsx(t.p,{children:"If you are not using Structured Outputs, then the structure of arguments is not guaranteed to be correct, so we recommend the use of a validation library like Pydantic to first verify the arguments prior to using them."}),"\n",e.jsx(t.h3,{children:"Name functions intuitively, with detailed descriptions"}),"\n",e.jsx(t.p,{children:"If you find the model does not generate calls to the correct functions, you may need to update your function names and descriptions so the model more clearly understands when it should select each function. Avoid using abbreviations or acronyms to shorten function and argument names."}),"\n",e.jsx(t.p,{children:"You can also include detailed descriptions for when a tool should be called. For complex functions, you should include descriptions for each of the arguments to help the model know what it needs to ask the user to collect that argument."}),"\n",e.jsx(t.h3,{children:"Name function parameters intuitively, with detailed descriptions"}),"\n",e.jsx(t.p,{children:"Use clear and descriptive names for function parameters. For example, specify the expected format for a date parameter (e.g., YYYY-mm-dd or dd/mm/yy) in the description."}),"\n",e.jsx(t.h3,{children:"Consider providing additional information about how and when to call functions in your system message"}),"\n",e.jsx(t.p,{children:"Providing clear instructions in your system message can significantly improve the model's function calling accuracy. For example, guide the model with things like, \"Use check_order_status when the user inquires about the status of their order, such as 'Where is my order?' or 'Has my order shipped yet?'\". Provide context for complex scenarios, like \"Before scheduling a meeting with schedule_meeting, check the user's calendar for availability using check_availability to avoid conflicts.\""}),"\n",e.jsx(t.h3,{children:"Use enums for function arguments when possible"}),"\n",e.jsx(t.p,{children:"If your use case allows, you can use enums to constrain the possible values for arguments. This can help reduce hallucinations."}),"\n",e.jsx(t.p,{children:"For example, say you have an AI assistant that helps with ordering a T-shirt. You likely have a fixed set of sizes for the T-shirt, and you might want the model to output in a specific format. If you want the model to output “s”, “m”, “l”, etc for small, medium, and large, then you could provide those values in the enum, for example:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-json",children:'{\n    "name": "pick_tshirt_size",\n    "description": "Call this if the user specifies which size t-shirt they want",\n    "parameters": {\n        "type": "object",\n        "properties": {\n            "size": {\n                "type": "string",\n                "enum": ["s", "m", "l"],\n                "description": "The size of the t-shirt that the user would like to order"\n            }\n        },\n        "required": ["size"],\n        "additionalProperties": false\n    }\n}\n'})}),"\n",e.jsx(t.p,{children:"If you don’t constrain the output, a user may say “large” or “L”, and the model may return either value. Your code may expect a specific structure, so it’s important to limit the number of possible formats the model can choose from."}),"\n",e.jsx(t.h3,{children:"Keep the number of functions low for higher accuracy"}),"\n",e.jsx(t.p,{children:"We recommend that you use no more than 20 functions in a single tool call. Developers typically see a reduction in the model’s ability to select the correct tool once they have between 10-20 tools."}),"\n",e.jsxs(t.p,{children:["If your use case requires the model to be able to pick between a large number of functions, you may want to explore fine-tuning (",e.jsx(t.a,{href:"https://cookbook.openai.com/examples/fine_tuning_for_function_calling",children:"learn more"}),") or break out the tools and group them logically to create a multi-agent system."]}),"\n",e.jsx(t.h3,{children:"Set up evals to act as an aid in prompt engineering your function definitions and system messages"}),"\n",e.jsxs(t.p,{children:["We recommend for non-trivial uses of function calling that you set up a suite of evals that allow you to measure how frequently the correct function is called or correct arguments are generated for a wide variety of possible user messages. Learn more about setting up evals on the ",e.jsx(t.a,{href:"https://cookbook.openai.com/examples/evaluation/getting_started_with_openai_evals",children:"OpenAI Cookbook"}),"."]}),"\n",e.jsx(t.p,{children:"You can then use these to measure whether adjustments to your function definitions and system messages will improve or hurt your integration."}),"\n",e.jsx(t.h3,{children:"Fine-tuning may help improve accuracy for function calling"}),"\n",e.jsx(t.p,{children:"Fine-tuning a model can improve performance at function calling for your use case, especially if you have a large number of functions, or complex, nuanced or similar functions."}),"\n",e.jsxs(t.p,{children:["See our fine-tuning for function calling ",e.jsx(t.a,{href:"https://cookbook.openai.com/examples/fine_tuning_for_function_calling",children:"cookbook"})," for more information."]}),"\n",e.jsx("a",{href:"https://cookbook.openai.com/examples/fine_tuning_for_function_calling",target:"_blank",children:e.jsx(C,{icon:e.jsx(ye,{}),color:"green",title:"Fine-tuning for function calling",className:"mt-6",children:e.jsx(t.p,{children:"Learn how to fine-tune a model for function calling"})})}),"\n",e.jsx(t.h2,{children:"FAQ"}),"\n",e.jsx(t.h3,{children:"How do functions differ from tools?"}),"\n",e.jsxs(t.p,{children:["When using function calling with the OpenAI API, you provide them as ",e.jsx(t.code,{children:"tools"}),", configure them with ",e.jsx(t.code,{children:"tool_choice"})," and monitor for ",e.jsx(t.code,{children:'finish_reason: "tool_calls"'}),"."]}),"\n",e.jsxs(t.p,{children:["The parameters named things like ",e.jsx(t.code,{children:"functions"})," and ",e.jsx(t.code,{children:"function_call"})," etc are now deprecated."]}),"\n",e.jsx(t.h3,{children:"Should I include function call instructions in the tool specification or in the system prompt?"}),"\n",e.jsx(t.p,{children:"We recommend including instructions regarding when to call a function in the system prompt, while using the function definition to provide instructions on how to call the function and how to generate the parameters."}),"\n",e.jsx(t.h3,{children:"Which models support function calling?"}),"\n",e.jsxs(t.p,{children:["Function calling was introduced with the release of ",e.jsx(t.code,{children:"gpt-4-turbo"})," on June 13, 2023. This includes: ",e.jsx(t.code,{children:"gpt-4o"}),", ",e.jsx(t.code,{children:"gpt-4o-2024-08-06"}),", ",e.jsx(t.code,{children:"gpt-4o-2024-05-13"}),", ",e.jsx(t.code,{children:"gpt-4o-mini"}),", ",e.jsx(t.code,{children:"gpt-4o-mini-2024-07-18"}),", ",e.jsx(t.code,{children:"gpt-4-turbo"}),", ",e.jsx(t.code,{children:"gpt-4-turbo-2024-04-09"}),", ",e.jsx(t.code,{children:"gpt-4-turbo-preview"}),", ",e.jsx(t.code,{children:"gpt-4-0125-preview"}),", ",e.jsx(t.code,{children:"gpt-4-1106-preview"}),", ",e.jsx(t.code,{children:"gpt-4"}),", ",e.jsx(t.code,{children:"gpt-4-0613"}),", ",e.jsx(t.code,{children:"gpt-3.5-turbo"}),", ",e.jsx(t.code,{children:"gpt-3.5-turbo-0125"}),", ",e.jsx(t.code,{children:"gpt-3.5-turbo-1106"}),", and ",e.jsx(t.code,{children:"gpt-3.5-turbo-0613"}),"."]}),"\n",e.jsx(t.p,{children:"Legacy models released before this date were not trained to support function calling."}),"\n",e.jsxs(t.p,{children:["Parallel function calling is supported on models released on or after Nov 6, 2023.  This includes: ",e.jsx(t.code,{children:"gpt-4o"}),", ",e.jsx(t.code,{children:"gpt-4o-2024-08-06"}),", ",e.jsx(t.code,{children:"gpt-4o-2024-05-13"}),", ",e.jsx(t.code,{children:"gpt-4o-mini"}),", ",e.jsx(t.code,{children:"gpt-4o-mini-2024-07-18"}),", ",e.jsx(t.code,{children:"gpt-4-turbo"}),", ",e.jsx(t.code,{children:"gpt-4-turbo-2024-04-09"}),", ",e.jsx(t.code,{children:"gpt-4-turbo-preview"}),", ",e.jsx(t.code,{children:"gpt-4-0125-preview"}),", ",e.jsx(t.code,{children:"gpt-4-1106-preview"}),", ",e.jsx(t.code,{children:"gpt-3.5-turbo"}),", ",e.jsx(t.code,{children:"gpt-3.5-turbo-0125"}),", and ",e.jsx(t.code,{children:"gpt-3.5-turbo-1106"}),"."]}),"\n",e.jsx(t.h3,{children:"What are some example functions?"}),"\n",e.jsx(t.p,{children:e.jsx(t.strong,{children:"Data Retrieval:"})}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Scenario: A chatbot needs to fetch the latest customer data from an internal system when a user asks “who are my top customers?”"}),"\n",e.jsxs(t.li,{children:["Implementation: Define a function",e.jsx(t.code,{children:"get_customers(min_revenue: int, created_before: string, limit: int)"})," that retrieves customer data from your internal API. The model can suggest calling this function with the appropriate parameters based on user input."]}),"\n"]}),"\n",e.jsx(t.p,{children:e.jsx(t.strong,{children:"Task Automation:"})}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Scenario: An assistant bot schedules meetings based on user preferences and calendar availability."}),"\n",e.jsx(t.li,{children:"Implementation: Define a function scheduleMeeting(date: str, time: str, participants: list) that interacts with a calendar API. The model can suggest the best times and dates to call this function."}),"\n"]}),"\n",e.jsx(t.p,{children:e.jsx(t.strong,{children:"Computational Tasks:"})}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Scenario: A financial application calculates loan payments based on user input."}),"\n",e.jsx(t.li,{children:"Implementation: Define a function calculateLoanPayment(principal: float, interestRate: float, term: int) to perform the necessary calculations. The model can provide the input values for this function."}),"\n"]}),"\n",e.jsx(t.p,{children:e.jsx(t.strong,{children:"Customer Support:"})}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Scenario: A customer support bot assists users by providing the status of their orders."}),"\n",e.jsx(t.li,{children:"Implementation: Define a function getOrderStatus(orderId: str) that retrieves order status information from a database. The model can suggest calling this function with the appropriate order ID parameter based on user input."}),"\n"]}),"\n",e.jsx(t.h3,{children:"Can the model execute functions itself?"}),"\n",e.jsx(t.p,{children:"No, the model only suggests function calls and generates arguments. Your application handles the execution of the functions based on these suggestions (and returns the results of calling those functions to the model)."}),"\n",e.jsx(t.h3,{children:"What are Structured Outputs?"}),"\n",e.jsxs(t.p,{children:["Structured Outputs, introduced in August 2024, is a feature that ensures that the arguments generated by the model exactly match the provided JSON Schema, enhancing reliability and reducing errors. We recommend its use and it can be enabled by setting ",e.jsx(t.code,{children:'"strict": true'}),"."]}),"\n",e.jsx(t.h3,{children:"Why might I not want to turn on Structured Outputs?"}),"\n",e.jsx(t.p,{children:"The main reasons to not use Structured Outputs are:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["If you need to use some feature of JSON Schema that is not yet supported (",e.jsx(t.a,{href:"/docs/guides/structured-outputs/supported-schemas",children:"learn more"}),"), for example recursive schemas."]}),"\n",e.jsx(t.li,{children:"If each of your API requests will include a novel schema (i.e. your schemas are not fixed, but are generated on-demand and rarely repeat), since the first request with a novel JSON Schema will have increased latency as the schema is pre-processed and cached for future generations to constrain the output of the model."}),"\n"]}),"\n",e.jsx(t.h3,{children:"How do I ensure the model calls the correct function?"}),"\n",e.jsx(t.p,{children:"Use intuitive names and detailed descriptions for functions and parameters. Provide clear guidance in the system message to enhance the model’s ability to pick the correct function."}),"\n",e.jsx(t.h3,{children:"What does Structured Outputs mean for Zero Data Retention?"}),"\n",e.jsxs(t.p,{children:["When Structured Outputs is turned on, schemas provided are not eligible for ",e.jsx(t.a,{href:"/docs/models/how-we-use-your-data",children:"zero data retention"}),"."]}),"\n",e.jsx(t.h2,{children:"Resources"}),"\n",e.jsxs(t.p,{children:["The OpenAI Cookbook has several end-to-end examples to help you implement function calling. In our introductory cookbook ",e.jsx(t.a,{href:"https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models",children:"how to call functions with chat models"}),", we outline two examples of how the models can use function calling. This one is a great resource to follow as you get started:"]}),"\n",e.jsx("a",{href:"https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models",target:"_blank",children:e.jsx(C,{icon:e.jsx(ye,{}),color:"green",title:"Function calling",className:"mt-6",children:e.jsx(t.p,{children:"Learn from more examples demonstrating function calling"})})}),"\n",e.jsx(t.p,{children:"You can find more examples to help you get started with function calling in the OpenAI Cookbook."}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["How to call functions with chat models ",e.jsx(t.a,{href:"https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models",children:"cookbook"})]}),"\n",e.jsxs(t.li,{children:["How to use functions with a knowledge base ",e.jsx(t.a,{href:"https://cookbook.openai.com/examples/how_to_call_functions_for_knowledge_retrieval",children:"cookbook"})]}),"\n",e.jsxs(t.li,{children:["Assistants API overview ",e.jsx(t.a,{href:"https://cookbook.openai.com/examples/assistants_api_overview_python",children:"cookbook"})]}),"\n",e.jsxs(t.li,{children:["Fine-tuning for function calling ",e.jsx(t.a,{href:"https://cookbook.openai.com/examples/fine_tuning_for_function_calling",children:"cookbook"})]}),"\n"]})]})}function wi(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(rt,n)})):rt(n)}function at(n){const t=Object.assign({h3:"h3",p:"p",code:"code",pre:"pre"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h3,{children:"Using in-memory image data"}),"\n",e.jsxs(t.p,{children:["The Node.js examples in the guide above use the ",e.jsx(t.code,{children:"fs"})," module to read image data from disk. In some cases, you may have your image data in memory instead. Here's an example API call that uses image data stored in a Node.js ",e.jsx(t.code,{children:"Buffer"})," object:"]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-javascript",children:'import OpenAI from "openai";\n\nconst openai = new OpenAI();\n\n// This is the Buffer object that contains your image data\nconst buffer = [your image data];\n\n// Set a `name` that ends with .png so that the API knows it\'s a PNG image\nbuffer.name = "image.png";\n\nasync function main() {\n  const image = await openai.images.createVariation({ model: "dall-e-2", image: buffer, n: 1, size: "1024x1024" });\n  console.log(image.data);\n}\nmain();\n\n'})}),"\n",e.jsx(t.h3,{children:"Working with TypeScript"}),"\n",e.jsx(t.p,{children:"If you're using TypeScript, you may encounter some quirks with image file arguments. Here's an example of working around the type mismatch by explicitly casting the argument:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-javascript",children:'import fs from "fs";\nimport OpenAI from "openai";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  // Cast the ReadStream to `any` to appease the TypeScript compiler\n  const image = await openai.images.createVariation({\n    image: fs.createReadStream("image.png") as any,\n  });\n\n  console.log(image.data);\n}\nmain();\n'})}),"\n",e.jsx(t.p,{children:"And here's a similar example for in-memory image data:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-javascript",children:'import fs from "fs";\nimport OpenAI from "openai";\n\nconst openai = new OpenAI();\n\n// This is the Buffer object that contains your image data\nconst buffer: Buffer = [your image data];\n\n// Cast the buffer to `any` so that we can set the `name` property\nconst file: any = buffer;\n\n// Set a `name` that ends with .png so that the API knows it\'s a PNG image\nfile.name = "image.png";\n\nasync function main() {\n  const image = await openai.images.createVariation({\n    file,\n    1,\n    "1024x1024"\n  });\n  console.log(image.data);\n}\nmain();\n'})}),"\n",e.jsx(t.h3,{children:"Error handling"}),"\n",e.jsxs(t.p,{children:["API requests can potentially return errors due to invalid inputs, rate limits, or other issues. These errors can be handled with a ",e.jsx(t.code,{children:"try...catch"})," statement, and the error details can be found in either ",e.jsx(t.code,{children:"error.response"})," or ",e.jsx(t.code,{children:"error.message"}),":"]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-javascript",children:'import fs from "fs";\nimport OpenAI from "openai";\n\nconst openai = new OpenAI();\n\nasync function main() {\n    try {\n        const image = await openai.images.createVariation({\n            image: fs.createReadStream("image.png"),\n            n: 1,\n            size: "1024x1024",\n        });\n        console.log(image.data);\n    } catch (error) {\n        if (error.response) {\n            console.log(error.response.status);\n            console.log(error.response.data);\n        } else {\n            console.log(error.message);\n        }\n    }\n}\n\nmain();\n'})})]})}function vi(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(at,n)})):at(n)}function lt(n){const t=Object.assign({h3:"h3",p:"p",code:"code",pre:"pre"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h3,{children:"Using in-memory image data"}),"\n",e.jsxs(t.p,{children:["The Python examples in the guide above use the ",e.jsx(t.code,{children:"open"})," function to read image data from disk. In some cases, you may have your image data in memory instead. Here's an example API call that uses image data stored in a ",e.jsx(t.code,{children:"BytesIO"})," object:"]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:'from io import BytesIO\nfrom openai import OpenAI\nclient = OpenAI()\n\n# This is the BytesIO object that contains your image data\nbyte_stream: BytesIO = [your image data]\nbyte_array = byte_stream.getvalue()\nresponse = client.images.create_variation(\n  image=byte_array,\n  n=1,\n  model="dall-e-2",\n  size="1024x1024"\n)\n'})}),"\n",e.jsx(t.h3,{children:"Operating on image data"}),"\n",e.jsxs(t.p,{children:["It may be useful to perform operations on images before passing them to the API. Here's an example that uses ",e.jsx(t.code,{children:"PIL"})," to resize an image:"]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:'from io import BytesIO\nfrom PIL import Image\nfrom openai import OpenAI\nclient = OpenAI()\n\n# Read the image file from disk and resize it\nimage = Image.open("image.png")\nwidth, height = 256, 256\nimage = image.resize((width, height))\n\n# Convert the image to a BytesIO object\nbyte_stream = BytesIO()\nimage.save(byte_stream, format=\'PNG\')\nbyte_array = byte_stream.getvalue()\n\nresponse = client.images.create_variation(\n  image=byte_array,\n  n=1,\n  model="dall-e-2",\n  size="1024x1024"\n)\n'})}),"\n",e.jsx(t.h3,{children:"Error handling"}),"\n",e.jsxs(t.p,{children:["API requests can potentially return errors due to invalid inputs, rate limits, or other issues. These errors can be handled with a ",e.jsx(t.code,{children:"try...except"})," statement, and the error details can be found in ",e.jsx(t.code,{children:"e.error"}),":"]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:'import openai\nfrom openai import OpenAI\nclient = OpenAI()\n\ntry:\n  response = client.images.create_variation(\n    image=open("image_edit_mask.png", "rb"),\n    n=1,\n    model="dall-e-2",\n    size="1024x1024"\n  )\n  print(response.data[0].url)\nexcept openai.OpenAIError as e:\n  print(e.http_status)\n  print(e.error)\n'})})]})}function ki(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(lt,n)})):lt(n)}function ct(n){const t=Object.assign({h1:"h1",p:"p",a:"a",h2:"h2",ol:"ol",li:"li",h3:"h3",code:"code",table:"table",thead:"thead",tr:"tr",th:"th",tbody:"tbody",td:"td",strong:"strong"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"Image generation"}),"\n",e.jsx(t.p,{children:"Learn how to generate or manipulate images with DALL·E in the API."}),"\n",e.jsx(b,{children:e.jsxs(t.p,{children:["Looking to generate images in ChatGPT? Head to ",e.jsx(t.a,{href:"https://chatgpt.com",children:"chatgpt.com"}),"."]})}),"\n",e.jsx(t.h2,{children:"Introduction"}),"\n",e.jsx(t.p,{children:"The Images API provides three methods for interacting with images:"}),"\n",e.jsxs(t.ol,{children:["\n",e.jsx(t.li,{children:"Creating images from scratch based on a text prompt (DALL·E 3 and DALL·E 2)"}),"\n",e.jsx(t.li,{children:"Creating edited versions of images by having the model replace some areas of a pre-existing image, based on a new text prompt (DALL·E 2 only)"}),"\n",e.jsx(t.li,{children:"Creating variations of an existing image (DALL·E 2 only)"}),"\n"]}),"\n",e.jsxs(t.p,{children:["This guide covers the basics of using these three API endpoints with useful code samples. To try DALL·E 3, head to ",e.jsx(t.a,{href:"https://chatgpt.com/",children:"ChatGPT"}),"."]}),"\n",e.jsx(t.h2,{children:"Usage"}),"\n",e.jsx(t.h3,{children:"Generations"}),"\n",e.jsxs(t.p,{children:["The ",e.jsx(t.a,{href:"/docs/api-reference/images/create",children:"image generations"})," endpoint allows you to create an original image given a text prompt. When using DALL·E 3, images can have a size of 1024x1024, 1024x1792 or 1792x1024 pixels."]}),"\n",e.jsxs(t.p,{children:["By default, images are generated at ",e.jsx(t.code,{children:"standard"})," quality, but when using DALL·E 3 you can set ",e.jsx(t.code,{children:'quality: "hd"'})," for enhanced detail. Square, standard quality images are the fastest to generate."]}),"\n",e.jsxs(t.p,{children:["You can request 1 image at a time with DALL·E 3 (request more by making parallel requests) or up to 10 images at a time using DALL·E 2 with the ",e.jsx(t.a,{href:"/docs/api-reference/images/create#images/create-n",children:"n parameter"}),"."]}),"\n",e.jsx(c,{title:"Generate an image",defaultLanguage:"python",code:{python:'\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.images.generate(\n  model="dall-e-3",\n  prompt="a white siamese cat",\n  size="1024x1024",\n  quality="standard",\n  n=1,\n)\n\nimage_url = response.data[0].url\n'.trim(),"node.js":'\nconst response = await openai.images.generate({\n  model: "dall-e-3",\n  prompt: "a white siamese cat",\n  n: 1,\n  size: "1024x1024",\n});\nimage_url = response.data[0].url;\n'.trim(),curl:'\ncurl https://api.openai.com/v1/images/generations \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -d \'{\n    "model": "dall-e-3",\n    "prompt": "a white siamese cat",\n    "n": 1,\n    "size": "1024x1024"\n  }\'\n'.trim()}}),"\n",e.jsx("a",{href:"https://cookbook.openai.com/articles/what_is_new_with_dalle_3",target:"_blank",children:e.jsx(C,{icon:e.jsx(On,{}),color:"red",title:"What is new with DALL·E 3",className:"mt-6",children:e.jsx(t.p,{children:"Explore what is new with DALL·E 3 in the OpenAI Cookbook"})})}),"\n",e.jsx(t.h2,{children:"Prompting"}),"\n",e.jsx(t.p,{children:"With the release of DALL·E 3, the model now takes in the default prompt provided and automatically re-write it for safety reasons, and to add more detail (more detailed prompts generally result in higher quality images)."}),"\n",e.jsxs(t.p,{children:["While it is not currently possible to disable this feature, you can use prompting to get outputs closer to your requested image by adding the following to your prompt: ",e.jsx(t.code,{children:"I NEED to test how the tool works with extremely simple prompts. DO NOT add any detail, just use it AS-IS:"}),"."]}),"\n",e.jsxs(t.p,{children:["The updated prompt is visible in the ",e.jsx(t.code,{children:"revised_prompt"})," field of the data response object."]}),"\n",e.jsx(t.h2,{children:"Example DALL·E 3 generations"}),"\n",e.jsx("div",{className:"images-examples flex-first-col",children:e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Prompt"}),e.jsx(t.th,{children:"Generation"})]})}),e.jsx(t.tbody,{children:e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"A photograph of a white Siamese cat."}),e.jsx(t.td,{children:e.jsx("img",{className:"images-example-image",src:"https://cdn.openai.com/API/images/guides/simple-cat-image-dall-e-3.webp"})})]})})]})}),"\n",e.jsxs(t.p,{children:["Each image can be returned as either a URL or Base64 data, using the ",e.jsx(t.a,{href:"/docs/api-reference/images/create#images/create-response_format",children:"response_format"})," parameter. URLs will expire after an hour."]}),"\n",e.jsx(t.h3,{children:"Edits (DALL·E 2 only)"}),"\n",e.jsxs(t.p,{children:['Also known as "inpainting", the ',e.jsx(t.a,{href:"/docs/api-reference/images/create-edit",children:"image edits"})," endpoint allows you to edit or extend an image by uploading an image and mask indicating which areas should be replaced. The transparent areas of the mask indicate where the image should be edited, and the prompt should describe the full new image, ",e.jsx(t.strong,{children:"not just the erased area"}),". This endpoint can enable experiences like DALL·E image editing in ChatGPT Plus."]}),"\n",e.jsx(c,{title:"Edit an image",defaultLanguage:"python",code:{python:'\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.images.edit((\n  model="dall-e-2",\n  image=open("sunlit_lounge.png", "rb"),\n  mask=open("mask.png", "rb"),\n  prompt="A sunlit indoor lounge area with a pool containing a flamingo",\n  n=1,\n  size="1024x1024"\n)\nimage_url = response.data[0].url\n'.trim(),"node.js":'\nconst response = await openai.images.edit({\n  model: "dall-e-2",\n  image: fs.createReadStream("sunlit_lounge.png"),\n  mask: fs.createReadStream("mask.png"),\n  prompt: "A sunlit indoor lounge area with a pool containing a flamingo",\n  n: 1,\n  size: "1024x1024"\n});\nimage_url = response.data[0].url;\n'.trim(),curl:'\ncurl https://api.openai.com/v1/images/edits \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -F model="dall-e-2" \\\n  -F image="@sunlit_lounge.png" \\\n  -F mask="@mask.png" \\\n  -F prompt="A sunlit indoor lounge area with a pool containing a flamingo" \\\n  -F n=1 \\\n  -F size="1024x1024"\n'.trim()}}),"\n",e.jsx("div",{className:"images-examples",children:e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Image"}),e.jsx(t.th,{children:"Mask"}),e.jsx(t.th,{children:"Output"})]})}),e.jsx(t.tbody,{children:e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx("img",{className:"images-example-image",src:"https://cdn.openai.com/API/images/guides/image_edit_original.webp"})}),e.jsx(t.td,{children:e.jsx("img",{className:"images-example-image",src:"https://cdn.openai.com/API/images/guides/image_edit_mask.webp"})}),e.jsx(t.td,{children:e.jsx("img",{className:"images-example-image",src:"https://cdn.openai.com/API/images/guides/image_edit_output.webp"})})]})})]})}),"\n",e.jsx("p",{className:"images-edit-prompt body-small",children:e.jsx(t.p,{children:"Prompt: a sunlit indoor lounge area with a pool containing a flamingo"})}),"\n",e.jsx(t.p,{children:"The uploaded image and mask must both be square PNG images less than 4MB in size, and also must have the same dimensions as each other. The non-transparent areas of the mask are not used when generating the output, so they don’t necessarily need to match the original image like the example above."}),"\n",e.jsx(t.h3,{children:"Variations (DALL·E 2 only)"}),"\n",e.jsxs(t.p,{children:["The ",e.jsx(t.a,{href:"/docs/api-reference/images/create-variation",children:"image variations"})," endpoint allows you to generate a variation of a given image."]}),"\n",e.jsx(c,{title:"Generate an image variation",defaultLanguage:"python",code:{python:'\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.images.create_variation(\n  model="dall-e-2",\n  image=open("corgi_and_cat_paw.png", "rb"),\n  n=1,\n  size="1024x1024"\n)\n\nimage_url = response.data[0].url\n'.trim(),"node.js":'\nconst response = await openai.images.createVariation({\n  model: "dall-e-2",\n  image: fs.createReadStream("corgi_and_cat_paw.png"),\n  n: 1,\n  size: "1024x1024"\n});\nimage_url = response.data[0].url;\n'.trim(),curl:'\ncurl https://api.openai.com/v1/images/variations \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -F model="dall-e-2" \\\n  -F image="@corgi_and_cat_paw.png" \\\n  -F n=1 \\\n  -F size="1024x1024"\n'.trim()}}),"\n",e.jsx("div",{className:"images-examples",children:e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Image"}),e.jsx(t.th,{children:"Output"})]})}),e.jsx(t.tbody,{children:e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx("img",{className:"images-example-image",src:"https://cdn.openai.com/API/images/guides/image_variation_original.webp"})}),e.jsx(t.td,{children:e.jsx("img",{className:"images-example-image",src:"https://cdn.openai.com/API/images/guides/image_variation_output.webp"})})]})})]})}),"\n",e.jsx(t.p,{children:"Similar to the edits endpoint, the input image must be a square PNG image less than 4MB in size."}),"\n",e.jsx(t.h3,{children:"Content moderation"}),"\n",e.jsxs(t.p,{children:["Prompts and images are filtered based on our ",e.jsx(t.a,{href:"https://labs.openai.com/policies/content-policy",children:"content policy"}),", returning an error when a prompt or image is flagged."]}),"\n",e.jsx(t.h2,{children:"Language-specific tips"}),"\n",e.jsx(G,{options:[{value:"node",label:"Node.js",content:e.jsx(vi,{})},{value:"python",label:"Python",content:e.jsx(ki,{})}],initialValue:"node"})]})}function _i(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(ct,n)})):ct(n)}function dt(n){const t=Object.assign({h1:"h1",p:"p",strong:"strong",a:"a",h2:"h2",ol:"ol",li:"li",h3:"h3",ul:"ul",code:"code",em:"em",img:"img",pre:"pre",h4:"h4"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"Latency optimization"}),"\n",e.jsx(t.p,{children:"This guide covers the core set of principles you can apply to improve latency across a wide variety of LLM-related use cases. These techniques come from working with a wide range of customers and developers on production applications, so they should apply regardless of what you're building – from a granular workflow to an end-to-end chatbot!"}),"\n",e.jsxs(t.p,{children:["While there's many individual techniques, we'll be grouping them into ",e.jsx(t.strong,{children:"seven principles"})," meant to represent a high-level taxonomy of approaches for improving latency."]}),"\n",e.jsxs(t.p,{children:["At the end, we'll walk through an ",e.jsx(t.a,{href:"/docs/guides/latency-optimization/example",children:"example"})," to see how they can be applied."]}),"\n",e.jsx(t.h2,{children:"The seven principles"}),"\n",e.jsxs(t.ol,{children:["\n",e.jsx(t.li,{children:e.jsx(t.a,{href:"/docs/guides/latency-optimization/1-process-tokens-faster",children:"Process tokens faster."})}),"\n",e.jsx(t.li,{children:e.jsx(t.a,{href:"/docs/guides/latency-optimization/2-generate-fewer-tokens",children:"Generate fewer tokens."})}),"\n",e.jsx(t.li,{children:e.jsx(t.a,{href:"/docs/guides/latency-optimization/3-use-fewer-input-tokens",children:"Use fewer input tokens."})}),"\n",e.jsx(t.li,{children:e.jsx(t.a,{href:"/docs/guides/latency-optimization/4-make-fewer-requests",children:"Make fewer requests."})}),"\n",e.jsx(t.li,{children:e.jsx(t.a,{href:"/docs/guides/latency-optimization/5-parallelize",children:"Parallelize."})}),"\n",e.jsx(t.li,{children:e.jsx(t.a,{href:"/docs/guides/latency-optimization/6-make-your-users-wait-less",children:"Make your users wait less."})}),"\n",e.jsx(t.li,{children:e.jsx(t.a,{href:"/docs/guides/latency-optimization/7-don-t-default-to-an-llm",children:"Don't default to an LLM."})}),"\n"]}),"\n",e.jsx(b,{children:e.jsx(t.p,{children:"You can use the friendly, catchy acronym PGIRPWD to remember these. (Processing, Generation, Input, Requests, Parallelize, Waiting, Don't)"})}),"\n",e.jsx(t.h3,{children:"1. Process tokens faster"}),"\n",e.jsxs(t.p,{children:[e.jsx(t.strong,{children:"Inference speed"})," is probably the first thing that comes to mind when addressing latency (but as you'll see soon, it's far from the only one). This refers to the actual ",e.jsx(t.strong,{children:"rate at which the LLM processes tokens"}),", and is often measured in TPM (tokens per minute) or TPS (tokens per second)."]}),"\n",e.jsxs(t.p,{children:["The main factor that influences inference speed is ",e.jsx(t.strong,{children:"model size"})," – smaller models usually run faster (and cheaper), and when used correctly can even outperform larger models. To maintain high quality performance with smaller models you can explore:"]}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["using a longer, ",e.jsx(t.a,{href:"/docs/guides/prompt-engineering/tactic-specify-the-steps-required-to-complete-a-task",children:"more detailed prompt"}),","]}),"\n",e.jsxs(t.li,{children:["adding (more) ",e.jsx(t.a,{href:"/docs/guides/prompt-engineering/tactic-provide-examples",children:"few-shot examples"}),", or"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"/docs/guides/fine-tuning",children:"fine-tuning"})," / distillation."]}),"\n"]}),"\n",e.jsx(z,{title:"Compute capacity & inference optimizations",children:e.jsxs(t.p,{children:["Other factors that affect inference speed are the amount of ",e.jsx("b",{children:"compute"})," you have\navailable and any additional ",e.jsx("b",{children:"inference optimizations"})," you employ. ",e.jsx("br",{})," ",e.jsx("br",{}),"\nMost people can't influence these factors directly, but if you're curious, and have some\ncontrol over your infra, ",e.jsx("b",{children:"faster hardware"})," or ",e.jsx("b",{children:"\nrunning engines at a lower saturation\n"})," may give you a modest TPM boost. And if you're down in the trenches, there's a myriad\nof ",e.jsx("a",{href:"https://lilianweng.github.io/posts/2023-01-10-inference-optimization/",children:"\ninference optimizations\n"})," that are a bit beyond the scope of this guide."]})}),"\n",e.jsx(t.h3,{children:"2. Generate fewer tokens"}),"\n",e.jsxs(t.p,{children:["Generating tokens is almost always the highest latency step when using an LLM: as a general heuristic, ",e.jsx(t.strong,{children:"cutting 50% of your output tokens may cut ~50% your latency"}),". The way you reduce your output size will depend on output type:"]}),"\n",e.jsxs(t.p,{children:["If you're generating ",e.jsx(t.strong,{children:"natural language"}),", simply ",e.jsx(t.strong,{children:"asking the model to be more concise"}),' ("under 20 words" or "be very brief") may help. You can also use few shot examples and/or fine-tuning to teach the model shorter responses.']}),"\n",e.jsxs(t.p,{children:["If you're generating ",e.jsx(t.strong,{children:"structured output"}),", try to ",e.jsx(t.strong,{children:"minimize your output syntax"})," where possible: shorten function names, omit named arguments, coalesce parameters, etc."]}),"\n",e.jsxs(t.p,{children:["Finally, while not common, you can also use ",e.jsx(t.code,{children:"max_tokens"})," or ",e.jsx(t.code,{children:"stop_tokens"})," to end your generation early."]}),"\n",e.jsx(t.p,{children:"Always remember: an output token cut is a (milli)second earned!"}),"\n",e.jsx(t.h3,{children:"3. Use fewer input tokens"}),"\n",e.jsxs(t.p,{children:["While reducing the number of input tokens does result in lower latency, this is not usually a significant factor – ",e.jsx(t.strong,{children:"cutting 50% of your prompt may only result in a 1-5% latency improvement"}),". Unless you're working with truly massive context sizes (documents, images), you may want to spend your efforts elsewhere."]}),"\n",e.jsxs(t.p,{children:["That being said, if you ",e.jsx(t.em,{children:"are"})," working with massive contexts (or you're set on squeezing every last bit of performance ",e.jsx(t.em,{children:"and"})," you've exhausted all other options) you can use the following techniques to reduce your input tokens:"]}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Fine-tuning the model"}),", to replace the need for lengthy instructions / examples."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Filtering context input"}),", like pruning RAG results, cleaning HTML, etc."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Maximize shared prompt prefix"}),", by putting dynamic portions (e.g. RAG results, history, etc) later in the prompt. This makes your request more ",e.jsx(t.a,{href:"https://medium.com/@joaolages/kv-caching-explained-276520203249",children:"KV cache"}),"-friendly (which most LLM providers use) and means fewer input tokens are processed on each request. (",e.jsx(t.a,{href:"https://lilianweng.github.io/posts/2023-01-10-inference-optimization/",children:"why?"}),")"]}),"\n"]}),"\n",e.jsx(t.h3,{children:"4. Make fewer requests"}),"\n",e.jsx(t.p,{children:"Each time you make a request you incur some round-trip latency – this can start to add up."}),"\n",e.jsxs(t.p,{children:["If you have sequential steps for the LLM to perform, instead of firing off one request per step consider ",e.jsx(t.strong,{children:"putting them in a single prompt and getting them all in a single response"}),". You'll avoid the additional round-trip latency, and potentially also reduce complexity of processing multiple responses."]}),"\n",e.jsx(t.p,{children:"An approach to doing this is by collecting your steps in an enumerated list in the combined prompt, and then requesting the model to return the results in named fields in a JSON. This way you can easily parse out and reference each result!"}),"\n",e.jsx(t.h3,{children:"5. Parallelize"}),"\n",e.jsx(t.p,{children:"Parallelization can be very powerful when performing multiple steps with an LLM."}),"\n",e.jsxs(t.p,{children:["If the steps ",e.jsxs(t.strong,{children:["are ",e.jsx(t.em,{children:"not"})," strictly sequential"]}),", you can ",e.jsx(t.strong,{children:"split them out into parallel calls"}),". Two shirts take just as long to dry as one."]}),"\n",e.jsxs(t.p,{children:["If the steps ",e.jsxs(t.strong,{children:[e.jsx(t.em,{children:"are"})," strictly sequential"]}),", however, you might still be able to ",e.jsx(t.strong,{children:"leverage speculative execution"}),". This is particularly effective for classification steps where one outcome is more likely than the others (e.g. moderation)."]}),"\n",e.jsxs(t.ol,{children:["\n",e.jsx(t.li,{children:"Start step 1 & step 2 simultaneously (e.g. input moderation & story generation)"}),"\n",e.jsx(t.li,{children:"Verify the result of step 1"}),"\n",e.jsx(t.li,{children:"If result was not the expected, cancel step 2 (and retry if necessary)"}),"\n"]}),"\n",e.jsx(t.p,{children:"If your guess for step 1 is right, then you essentially got to run it with zero added latency!"}),"\n",e.jsx(t.h3,{children:"6. Make your users wait less"}),"\n",e.jsxs(t.p,{children:["There's a huge difference between ",e.jsx(t.strong,{children:"waiting"})," and ",e.jsx(t.strong,{children:"watching progress happen"})," – make sure your users experience the latter. Here are a few techniques:"]}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Streaming"}),": The single most effective approach, as it cuts the ",e.jsx(t.em,{children:"waiting"})," time to a second or less. (ChatGPT would feel pretty different if you saw nothing until each response was done.)"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Chunking"}),": If your output needs further processing before being shown to the user (moderation, translation) consider ",e.jsx(t.strong,{children:"processing it in chunks"})," instead of all at once. Do this by streaming to your backend, then sending processed chunks to your frontend."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Show your steps"}),": If you're taking multiple steps or using tools, surface this to the user. The more real progress you can show, the better."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Loading states"}),": Spinners and progress bars go a long way."]}),"\n"]}),"\n",e.jsxs(t.p,{children:["Note that while ",e.jsx(t.strong,{children:"showing your steps & having loading states"})," have a mostly\npsychological effect, ",e.jsx(t.strong,{children:"streaming & chunking"})," genuinely do reduce overall\nlatency once you consider the app + user system: the user will finish reading a response\nsooner."]}),"\n",e.jsx(t.h3,{children:"7. Don't default to an LLM"}),"\n",e.jsxs(t.p,{children:["LLMs are extremely powerful and versatile, and are therefore sometimes used in cases where a ",e.jsx(t.strong,{children:"faster classical method"})," would be more appropriate. Identifying such cases may allow you to cut your latency significantly. Consider the following examples:"]}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Hard-coding:"})," If your ",e.jsx(t.strong,{children:"output"})," is highly constrained, you may not need an LLM to generate it. Action confirmations, refusal messages, and requests for standard input are all great candidates to be hard-coded. (You can even use the age-old method of coming up with a few variations for each.)"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Pre-computing:"})," If your ",e.jsx(t.strong,{children:"input"})," is constrained (e.g. category selection) you can generate multiple responses in advance, and just make sure you never show the same one to a user twice."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Leveraging UI:"})," Summarized metrics, reports, or search results are sometimes better conveyed with classical, bespoke UI components rather than LLM-generated text."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Traditional optimization techniques:"})," An LLM application is still an application; binary search, caching, hash maps, and runtime complexity are all ",e.jsx(t.em,{children:"still"})," useful in a world of LLMs."]}),"\n"]}),"\n",e.jsx(t.h2,{children:"Example"}),"\n",e.jsx(t.p,{children:"Let's now look at a sample application, identify potential latency optimizations, and propose some solutions!"}),"\n",e.jsxs(t.p,{children:["We'll be analyzing the architecture and prompts of a hypothetical customer service bot inspired by real production applications. The ",e.jsx(t.a,{href:"/docs/guides/latency-optimization/architecture-and-prompts",children:"architecture and prompts"})," section sets the stage, and the ",e.jsx(t.a,{href:"/docs/guides/latency-optimization/analysis-and-optimizations",children:"analysis and optimizations"})," section will walk through the latency optimization process."]}),"\n",e.jsx(b,{children:e.jsx(t.p,{children:"You'll notice this example doesn't cover every single principle, much like real-world use cases don't require applying every technique."})}),"\n",e.jsx(t.h3,{children:"Architecture and prompts"}),"\n",e.jsxs(t.p,{children:["The following is the ",e.jsx(t.strong,{children:"initial architecture"})," for a hypothetical ",e.jsx(t.strong,{children:"customer service bot"}),". This is what we'll be making changes to."]}),"\n",e.jsx(t.p,{children:e.jsx(t.img,{src:"https://cdn.openai.com/API/docs/images/diagram-latency-customer-service-0.png",alt:"Assistants object architecture diagram"})}),"\n",e.jsx(t.p,{children:"At a high level, the diagram flow describes the following process:"}),"\n",e.jsxs(t.ol,{children:["\n",e.jsx(t.li,{children:"A user sends a message as part of an ongoing conversation."}),"\n",e.jsxs(t.li,{children:["The last message is turned into a ",e.jsx(t.strong,{children:"self-contained query"})," (see examples in prompt)."]}),"\n",e.jsxs(t.li,{children:["We determine whether or not ",e.jsx(t.strong,{children:"additional (retrieved) information is required"})," to respond to that query."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Retrieval"})," is performed, producing search results."]}),"\n",e.jsxs(t.li,{children:["The assistant ",e.jsx(t.strong,{children:"reasons"})," about the user's query and search results, and ",e.jsx(t.strong,{children:"produces a response"}),"."]}),"\n",e.jsx(t.li,{children:"The response is sent back to the user."}),"\n"]}),"\n",e.jsx(t.p,{children:"Below are the prompts used in each part of the diagram. While they are still only hypothetical and simplified, they are written with the same structure and wording that you would find in a production application."}),"\n",e.jsx(b,{children:e.jsxs(t.p,{children:['Places where you see placeholders like "',e.jsx(t.strong,{children:"[user input here]"}),'" represent dynamic portions, that would be replaced by actual data at runtime.']})}),"\n",e.jsxs(j,{label:"Query contextualization prompt",children:[e.jsx(t.p,{children:"Re-writes user query to be a self-contained search query."}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:'SYSTEM: Given the previous conversation, re-write the last user query so it contains\nall necessary context.\n\n# Example\nHistory: [{user: "What is your return policy?"},{assistant: "..."}]\nUser Query: "How long does it cover?"\nResponse: "How long does the return policy cover?"\n\n# Conversation\n[last 3 messages of conversation]\n\n# User Query\n[last user query]\n\nUSER: [JSON-formatted input conversation here]\n'})})]}),"\n",e.jsxs(j,{label:"Retrieval check prompt",children:[e.jsx(t.p,{children:"Determines whether a query requires performing retrieval to respond."}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:'SYSTEM: Given a user query, determine whether it requires doing a realtime lookup to\nrespond to.\n\n# Examples\nUser Query: "How can I return this item after 30 days?"\nResponse: "true"\n\nUser Query: "Thank you!"\nResponse: "false"\n\nUSER: [input user query here]\n'})})]}),"\n",e.jsxs(j,{label:"Assistant prompt",children:[e.jsx(t.p,{children:"Fills the fields of a JSON to reason through a pre-defined set of steps to produce a final response given a user conversation and relevant retrieved information."}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:'SYSTEM: You are a helpful customer service bot.\n\nUse the result JSON to reason about each user query - use the retrieved context.\n\n# Example\n\nUser: "My computer screen is cracked! I want it fixed now!!!"\n\nAssistant Response:\n{\n"message_is_conversation_continuation": "True",\n"number_of_messages_in_conversation_so_far": "1",\n"user_sentiment": "Aggravated",\n"query_type": "Hardware Issue",\n"response_tone": "Validating and solution-oriented",\n"response_requirements": "Propose options for repair or replacement.",\n"user_requesting_to_talk_to_human": "False",\n"enough_information_in_context": "True"\n"response": "..."\n}\n\nUSER: # Relevant Information\n` ` `\n[retrieved context]\n` ` `\n\nUSER: [input user query here]\n'})})]}),"\n",e.jsx(t.h3,{children:"Analysis and optimizations"}),"\n",e.jsx(t.h4,{children:"Part 1: Looking at retrieval prompts"}),"\n",e.jsxs(t.p,{children:["Looking at the architecture, the first thing that stands out is the ",e.jsx(t.strong,{children:"consecutive GPT-4 calls"})," - these hint at a potential inefficiency, and can often be replaced by a single call or parallel calls."]}),"\n",e.jsx(t.p,{children:e.jsx(t.img,{src:"https://cdn.openai.com/API/docs/images/diagram-latency-customer-service-2.png",alt:"Assistants object architecture diagram"})}),"\n",e.jsxs(t.p,{children:["In this case, since the check for retrieval requires the contextualized query, let's ",e.jsx(t.strong,{children:"combine them into a single prompt"})," to ",e.jsx(t.a,{href:"/docs/guides/latency-optimization/4-make-fewer-requests",children:"make fewer requests"}),"."]}),"\n",e.jsx(t.p,{children:e.jsx(t.img,{src:"https://cdn.openai.com/API/docs/images/diagram-latency-customer-service-3.png",alt:"Assistants object architecture diagram"})}),"\n",e.jsxs(j,{label:"Combined query contextualization and retrieval check prompt",children:[e.jsxs(t.p,{children:[e.jsx(t.strong,{children:"What changed?"})," Before, we had one prompt to re-write the query and one to determine whether this requires doing a retrieval lookup. Now, this combined prompt does both. Specifically, notice the updated instruction in the first line of the prompt, and the updated output JSON:"]}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-jsx",children:'{\nquery:"[contextualized query]",\nretrieval:"[true/false - whether retrieval is required]"\n}\n'})}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:'SYSTEM: Given the previous conversation, re-write the last user query so it contains\nall necessary context. Then, determine whether the full request requires doing a\nrealtime lookup to respond to.\n\nRespond in the following form:\n{\nquery:"[contextualized query]",\nretrieval:"[true/false - whether retrieval is required]"\n}\n\n# Examples\n\nHistory: [{user: "What is your return policy?"},{assistant: "..."}]\nUser Query: "How long does it cover?"\nResponse: {query: "How long does the return policy cover?", retrieval: "true"}\n\nHistory: [{user: "How can I return this item after 30 days?"},{assistant: "..."}]\nUser Query: "Thank you!"\nResponse: {query: "Thank you!", retrieval: "false"}\n\n# Conversation\n[last 3 messages of conversation]\n\n# User Query\n[last user query]\n\nUSER: [JSON-formatted input conversation here]\n'})})]}),"\n",e.jsx("br",{}),"\n",e.jsxs(t.p,{children:["Actually, adding context and determining whether to retrieve are very straightforward and well defined tasks, so we can likely use a ",e.jsx(t.strong,{children:"smaller, fine-tuned model"})," instead. Switching to GPT-3.5 will let us ",e.jsx(t.a,{href:"/docs/guides/latency-optimization/1-process-tokens-faster",children:"process tokens faster"}),"."]}),"\n",e.jsx(t.p,{children:e.jsx(t.img,{src:"https://cdn.openai.com/API/docs/images/diagram-latency-customer-service-4.png",alt:"Assistants object architecture diagram"})}),"\n",e.jsx(t.h4,{children:"Part 2: Analyzing the assistant prompt"}),"\n",e.jsxs(t.p,{children:["Let's now direct our attention to the Assistant prompt. There seem to be many distinct steps happening as it fills the JSON fields – this could indicate an opportunity to ",e.jsx(t.a,{href:"/docs/guides/latency-optimization/5-parallelize",children:"parallelize"}),"."]}),"\n",e.jsx(t.p,{children:e.jsx(t.img,{src:"https://cdn.openai.com/API/docs/images/diagram-latency-customer-service-5.png",alt:"Assistants object architecture diagram"})}),"\n",e.jsx(t.p,{children:"However, let's pretend we have run some tests and discovered that splitting the reasoning steps in the JSON produces worse responses, so we need to explore different solutions."}),"\n",e.jsxs(t.p,{children:[e.jsx(t.strong,{children:"Could we use a fine-tuned GPT-3.5 instead of GPT-4?"})," Maybe – but in general, open-ended responses from assistants are best left to GPT-4 so it can better handle a greater range of cases. That being said, looking at the reasoning steps themselves, they may not all require GPT-4 level reasoning to produce. The well defined, limited scope nature makes them and ",e.jsx(t.strong,{children:"good potential candidates for fine-tuning"}),"."]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-jsx",children:'{\n"message_is_conversation_continuation": "True", // <-\n"number_of_messages_in_conversation_so_far": "1", // <-\n"user_sentiment": "Aggravated", // <-\n"query_type": "Hardware Issue", // <-\n"response_tone": "Validating and solution-oriented", // <-\n"response_requirements": "Propose options for repair or replacement.", // <-\n"user_requesting_to_talk_to_human": "False", // <-\n"enough_information_in_context": "True" // <-\n"response": "..." // X -- benefits from GPT-4\n}\n'})}),"\n",e.jsxs(t.p,{children:["This opens up the possibility of a trade-off. Do we keep this as a ",e.jsx(t.strong,{children:"single request entirely generated by GPT-4"}),", or ",e.jsx(t.strong,{children:"split it into two sequential requests"})," and use GPT-3.5 for all but the final response? We have a case of conflicting principles: the first option lets us ",e.jsx(t.a,{href:"/docs/guides/latency-optimization/4-make-fewer-requests",children:"make fewer requests"}),", but the second may let us ",e.jsx(t.a,{href:"/docs/guides/latency-optimization/1-process-tokens-faster",children:"process tokens faster"}),"."]}),"\n",e.jsx(t.p,{children:"As with many optimization tradeoffs, the answer will depend on the details. For example:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["The proportion of tokens in the ",e.jsx(t.code,{children:"response"})," vs the other fields."]}),"\n",e.jsx(t.li,{children:"The average latency decrease from processing most fields faster."}),"\n",e.jsxs(t.li,{children:["The average latency ",e.jsx(t.em,{children:"increase"})," from doing two requests instead of one."]}),"\n"]}),"\n",e.jsxs(t.p,{children:["The conclusion will vary by case, and the best way to make the determiation is by testing this with production examples. In this case let's pretend the tests indicated it's favorable to split the prompt in two to ",e.jsx(t.a,{href:"/docs/guides/latency-optimization/1-process-tokens-faster",children:"process tokens faster"}),"."]}),"\n",e.jsx(t.p,{children:e.jsx(t.img,{src:"https://cdn.openai.com/API/docs/images/diagram-latency-customer-service-6.png",alt:"Assistants object architecture diagram"})}),"\n",e.jsxs(t.p,{children:[e.jsx(t.strong,{children:"Note:"})," We'll be grouping ",e.jsx(t.code,{children:"response"})," and ",e.jsx(t.code,{children:"enough_information_in_context"})," together in the second prompt to avoid passing the retrieved context to both new prompts."]}),"\n",e.jsxs(j,{label:"Assistants prompt - reasoning",children:[e.jsx(t.p,{children:"This prompt will be passed to GPT-3.5 and can be fine-tuned on curated examples."}),e.jsxs(t.p,{children:[e.jsx(t.strong,{children:"What changed?"}),' The "enough_information_in_context" and "response" fields were removed, and the retrieval results are no longer loaded into this prompt.']}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:'SYSTEM: You are a helpful customer service bot.\n\nBased on the previous conversation, respond in a JSON to determine the required\nfields.\n\n# Example\n\nUser: "My freaking computer screen is cracked!"\n\nAssistant Response:\n{\n"message_is_conversation_continuation": "True",\n"number_of_messages_in_conversation_so_far": "1",\n"user_sentiment": "Aggravated",\n"query_type": "Hardware Issue",\n"response_tone": "Validating and solution-oriented",\n"response_requirements": "Propose options for repair or replacement.",\n"user_requesting_to_talk_to_human": "False",\n}\n'})})]}),"\n",e.jsxs(j,{label:"Assistants prompt - response",children:[e.jsx(t.p,{children:"This prompt will be processed by GPT-4 and will receive the reasoning steps determined in the prior prompt, as well as the results from retrieval."}),e.jsxs(t.p,{children:[e.jsx(t.strong,{children:"What changed?"}),' All steps were removed except for "enough_information_in_context" and "response". Additionally, the JSON we were previously filling in as output will be passed in to this prompt.']}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:'SYSTEM: You are a helpful customer service bot.\n\nUse the retrieved context, as well as these pre-classified fields, to respond to\nthe user\'s query.\n\n# Reasoning Fields\n` ` `\n[reasoning json determined in previous GPT-3.5 call]\n` ` `\n\n# Example\n\nUser: "My freaking computer screen is cracked!"\n\nAssistant Response:\n{\n"enough_information_in_context": "True"\n"response": "..."\n}\n\nUSER: # Relevant Information\n` ` `\n[retrieved context]\n` ` `\n'})})]}),"\n",e.jsx("br",{}),"\n",e.jsxs(t.p,{children:["In fact, now that the reasoning prompt does not depend on the retrieved context we can ",e.jsx(t.a,{href:"/docs/guides/latency-optimization/5-parallelize",children:"parallelize"})," and fire it off at the same time as the retrieval prompts."]}),"\n",e.jsx(t.p,{children:e.jsx(t.img,{src:"https://cdn.openai.com/API/docs/images/diagram-latency-customer-service-6b.png",alt:"Assistants object architecture diagram"})}),"\n",e.jsx(t.h4,{children:"Part 3: Optimizing the structured output"}),"\n",e.jsx(t.p,{children:"Let's take another look at the reasoning prompt."}),"\n",e.jsx(t.p,{children:e.jsx(t.img,{src:"https://cdn.openai.com/API/docs/images/diagram-latency-customer-service-7b.png",alt:"Assistants object architecture diagram"})}),"\n",e.jsx(t.p,{children:"Taking a closer look at the reasoning JSON you may notice the field names themselves are quite long."}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-jsx",children:'{\n"message_is_conversation_continuation": "True", // <-\n"number_of_messages_in_conversation_so_far": "1", // <-\n"user_sentiment": "Aggravated", // <-\n"query_type": "Hardware Issue", // <-\n"response_tone": "Validating and solution-oriented", // <-\n"response_requirements": "Propose options for repair or replacement.", // <-\n"user_requesting_to_talk_to_human": "False", // <-\n}\n'})}),"\n",e.jsxs(t.p,{children:["By making them shorter and moving explanations to the comments we can ",e.jsx(t.a,{href:"/docs/guides/latency-optimization/2-generate-fewer-tokens",children:"generate fewer tokens"}),"."]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-jsx",children:'{\n"cont": "True", // whether last message is a continuation\n"n_msg": "1", // number of messages in the continued conversation\n"tone_in": "Aggravated", // sentiment of user query\n"type": "Hardware Issue", // type of the user query\n"tone_out": "Validating and solution-oriented", // desired tone for response\n"reqs": "Propose options for repair or replacement.", // response requirements\n"human": "False", // whether user is expressing want to talk to human\n}\n'})}),"\n",e.jsx(t.p,{children:e.jsx(t.img,{src:"https://cdn.openai.com/API/docs/images/diagram-latency-customer-service-8b.png",alt:"Assistants object architecture diagram"})}),"\n",e.jsx(t.p,{children:"This small change removed 19 output tokens. While with GPT-3.5 this may only result in a few millisecond improvement, with GPT-4 this could shave off up to a second."}),"\n",e.jsx(t.p,{children:e.jsx(t.img,{src:"https://cdn.openai.com/API/docs/images/token-counts-latency-customer-service-large.png",alt:"Assistants object architecture diagram"})}),"\n",e.jsx(t.p,{children:"You might imagine, however, how this can have quite a significant impact for larger model outputs."}),"\n",e.jsx(t.p,{children:"We could go further and use single chatacters for the JSON fields, or put everything in an array, but this may start to hurt our response quality. The best way to know, once again, is through testing."}),"\n",e.jsx(t.h4,{children:"Example wrap-up"}),"\n",e.jsx(t.p,{children:"Let's review the optimizations we implemented for the customer service bot example:"}),"\n",e.jsx(t.p,{children:e.jsx(t.img,{src:"https://cdn.openai.com/API/docs/images/diagram-latency-customer-service-11b.png",alt:"Assistants object architecture diagram"})}),"\n",e.jsxs(t.ol,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Combined"})," query contextualization and retrieval check steps to ",e.jsx(t.a,{href:"/docs/guides/latency-optimization/4-make-fewer-requests",children:"make fewer requests"}),"."]}),"\n",e.jsxs(t.li,{children:["For the new prompt, ",e.jsx(t.strong,{children:"switched to a smaller, fine-tuned GPT-3.5"})," to ",e.jsx(t.a,{href:"/docs/guides/latency-optimization/1-process-tokens-faster",children:"process tokens faster"}),"."]}),"\n",e.jsxs(t.li,{children:["Split the assistant prompt in two, ",e.jsx(t.strong,{children:"switching to a smaller, fine-tuned GPT-3.5"})," for the reasoning, again to ",e.jsx(t.a,{href:"/docs/guides/latency-optimization/1-process-tokens-faster",children:"process tokens faster"}),"."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"/docs/guides/latency-optimization/5-parallelize",children:"Parallelized"})," the retrieval checks and the reasoning steps."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Shortened reasoning field names"})," and moved comments into the prompt, to ",e.jsx(t.a,{href:"/docs/guides/latency-optimization/2-generate-fewer-tokens",children:"generate fewer tokens"}),"."]}),"\n"]}),"\n",e.jsx(t.h2,{children:"Conclusion"}),"\n",e.jsxs(t.p,{children:["You should now be familiar with the core set of principles you can use to improve latency in your LLM application. As you explore these techniques, always remember to measure where your latency is coming from, and test the impact of each solution your try. Now go make your application ",e.jsx(t.em,{children:"fly!"})]})]})}function Ci(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(dt,n)})):dt(n)}function ht(n){const t=Object.assign({h1:"h1",p:"p",strong:"strong",h2:"h2",ul:"ul",li:"li",h3:"h3",a:"a",h4:"h4",ol:"ol",code:"code",table:"table",thead:"thead",tr:"tr",th:"th",tbody:"tbody",td:"td"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"How to select the right model"}),"\n",e.jsxs(t.p,{children:["Choosing the right model, whether GPT-4o or a smaller option like GPT-4o-mini, requires balancing ",e.jsx(t.strong,{children:"accuracy"}),", ",e.jsx(t.strong,{children:"latency"}),", and ",e.jsx(t.strong,{children:"cost"}),". This guide explains key principles to help you make informed decisions, along with a practical example."]}),"\n",e.jsx(t.h2,{children:"Core Principles"}),"\n",e.jsx(t.p,{children:"The principles for model selection are simple:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Optimize for accuracy first:"})," Optimize for accuracy until you hit your accuracy target."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Optimize for cost and latency second:"})," Then aim to maintain accuracy with the cheapest, fastest model possible."]}),"\n"]}),"\n",e.jsx(t.h3,{children:"1. Focus on Accuracy First"}),"\n",e.jsx(t.p,{children:'Begin by setting a clear accuracy goal for your use case, where you\'re clear on the accuracy that would be "good enough" for this use case to go to production. You can accomplish this through:'}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Setting a clear accuracy target:"})," Identify what your target accuracy statistic is going to be.","\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"For example, 90% of customer service calls need to be triaged correctly at the first interaction."}),"\n"]}),"\n"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Developing an evaluation dataset:"})," Create a dataset that allows you to measure the model's performance against these goals.","\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"To extend the example above, capture 100 interaction examples where we have what the user asked for, what the LLM triaged them to, what the correct triage should be, and whether this was correct or not."}),"\n"]}),"\n"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Using the most powerful model to optimize:"})," Start with the most capable model available to achieve your accuracy targets. Log all responses so we can use them for distillation of a smaller model.","\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Use retrieval-augmented generation to optimize for accuracy"}),"\n",e.jsx(t.li,{children:"Use fine-tuning to optimize for consistency and behavior"}),"\n"]}),"\n"]}),"\n"]}),"\n",e.jsxs(t.p,{children:["During this process, collect prompt and completion pairs for use in evaluations, few-shot learning, or fine-tuning. This practice, known as ",e.jsx(t.strong,{children:"prompt baking"}),", helps you produce high-quality examples for future use."]}),"\n",e.jsxs(t.p,{children:["For more methods and tools here, see our ",e.jsx(t.a,{href:"https://platform.openai.com/docs/guides/optimizing-llm-accuracy",children:"Accuracy Optimization Guide"}),"."]}),"\n",e.jsx(t.h4,{children:"Setting a realistic accuracy target"}),"\n",e.jsx(t.p,{children:"Calculate a realistic accuracy target by evaluating the financial impact of model decisions. For example, in a fake news classification scenario:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Correctly classified news:"})," If the model classifies it correctly, it saves you the cost of a human reviewing it - let's assume ",e.jsx(t.strong,{children:"$50"}),"."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Incorrectly classified news:"})," If it falsely classifies a safe article or misses a fake news article, it may trigger a review process and possible complaint, which might cost us ",e.jsx(t.strong,{children:"$300"}),"."]}),"\n"]}),"\n",e.jsxs(t.p,{children:["Our news classification example would need ",e.jsx(t.strong,{children:"85.8%"})," accuracy to cover costs, so targeting 90% or more ensures an overall return on investment. Use these calculations to set an effective accuracy target based on your specific cost structures."]}),"\n",e.jsx(t.h3,{children:"2. Optimize cost and latency"}),"\n",e.jsx(t.p,{children:"Cost and latency are considered secondary because if the model can’t hit your accuracy target then these concerns are moot. However, once you’ve got a model that works for your use case, you can take one of two approaches:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Compare with a smaller model zero- or few-shot:"})," Swap out the model for a smaller, cheaper one and test whether it maintains accuracy at the lower cost and latency point."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Model distillation:"})," Fine-tune a smaller model using the data gathered during accuracy optimization."]}),"\n"]}),"\n",e.jsx(t.p,{children:"Cost and latency are typically interconnected; reducing tokens and requests generally leads to faster processing."}),"\n",e.jsx(t.p,{children:"The main strategies to consider here are:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Reduce requests:"})," Limit the number of necessary requests to complete tasks."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Minimize tokens:"})," Lower the number of input tokens and optimize for shorter model outputs."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Select a smaller model:"})," Use models that balance reduced costs and latency with maintained accuracy."]}),"\n"]}),"\n",e.jsxs(t.p,{children:["To dive deeper into these, please refer to our guide on ",e.jsx(t.a,{href:"https://platform.openai.com/docs/guides/latency-optimization",children:"latency optimization"}),"."]}),"\n",e.jsx(t.h4,{children:"Exceptions to the rule"}),"\n",e.jsx(t.p,{children:"Clear exceptions exist for these principles. If your use case is extremely cost or latency sensitive, establish thresholds for these metrics before beginning your testing, then remove the models that exceed those from consideration. Once benchmarks are set, these guidelines will help you refine model accuracy within your constraints."}),"\n",e.jsx(t.h2,{children:"Practical example"}),"\n",e.jsx(t.p,{children:"To demonstrate these principles, we'll develop a fake news classifier with the following target metrics:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Accuracy:"})," Achieve 90% correct classification"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Cost:"})," Spend less than $5 per 1,000 articles"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Latency:"})," Maintain processing time under 2 seconds per article"]}),"\n"]}),"\n",e.jsx(t.h3,{children:"Experiments"}),"\n",e.jsx(t.p,{children:"We ran three experiments to reach our goal:"}),"\n",e.jsxs(t.ol,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Zero-shot:"})," Used ",e.jsx(t.code,{children:"GPT-4o"})," with a basic prompt for 1,000 records, but missed the accuracy target."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Few-shot learning:"})," Included 5 few-shot examples, meeting the accuracy target but exceeding cost due to more prompt tokens."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Fine-tuned model:"})," Fine-tuned ",e.jsx(t.code,{children:"GPT-4o-mini"})," with 1,000 labeled examples, meeting all targets with similar latency and accuracy but significantly lower costs."]}),"\n"]}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"ID"}),e.jsx(t.th,{children:"Method"}),e.jsx(t.th,{children:"Accuracy"}),e.jsx(t.th,{children:"Accuracy target"}),e.jsx(t.th,{children:"Cost"}),e.jsx(t.th,{children:"Cost target"}),e.jsx(t.th,{children:"Avg. latency"}),e.jsx(t.th,{children:"Latency target"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"1"}),e.jsx(t.td,{children:"gpt-4o zero-shot"}),e.jsx(t.td,{children:"84.5%"}),e.jsx(t.td,{}),e.jsx(t.td,{children:"$1.72"}),e.jsx(t.td,{}),e.jsx(t.td,{children:"< 1s"}),e.jsx(t.td,{})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2"}),e.jsx(t.td,{children:"gpt-4o few-shot (n=5)"}),e.jsx(t.td,{children:"91.5%"}),e.jsx(t.td,{children:"✓"}),e.jsx(t.td,{children:"$11.92"}),e.jsx(t.td,{}),e.jsx(t.td,{children:"< 1s"}),e.jsx(t.td,{children:"✓"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"3"}),e.jsx(t.td,{children:"gpt-4o-mini fine-tuned w/ 1000 examples"}),e.jsx(t.td,{children:"91.5%"}),e.jsx(t.td,{children:"✓"}),e.jsx(t.td,{children:"$0.21"}),e.jsx(t.td,{children:"✓"}),e.jsx(t.td,{children:"< 1s"}),e.jsx(t.td,{children:"✓"})]})]})]}),"\n",e.jsx(t.h2,{children:"Conclusion"}),"\n",e.jsxs(t.p,{children:["By switching from ",e.jsx(t.code,{children:"gpt-4o"})," to ",e.jsx(t.code,{children:"gpt-4o-mini"})," with fine-tuning, we achieved ",e.jsx(t.strong,{children:"equivalent performance for less than 2%"})," of the cost, using only 1,000 labeled examples."]}),"\n",e.jsxs(t.p,{children:["This process is important - you often can’t jump right to fine-tuning because you don’t know whether fine-tuning is the right tool for the optimization you need, or you don’t have enough labeled examples. Use ",e.jsx(t.code,{children:"gpt-4o"})," to achieve your accuracy targets, and curate a good training set - then go for a smaller, more efficient model with fine-tuning."]})]})}function Ai(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(ht,n)})):ht(n)}function ut(n){const t=Object.assign({h1:"h1",p:"p",h2:"h2",a:"a",table:"table",thead:"thead",tr:"tr",th:"th",tbody:"tbody",td:"td",code:"code",ul:"ul",li:"li",pre:"pre"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"Moderation"}),"\n",e.jsx(t.p,{children:"Learn how to build moderation into your AI applications."}),"\n",e.jsx(t.h2,{children:"Overview"}),"\n",e.jsxs(t.p,{children:["The ",e.jsx(t.a,{href:"/docs/api-reference/moderations",children:"moderations"})," endpoint is a tool you can use to check whether text is potentially harmful. Developers can use it to identify content that might be harmful and take action, for instance by filtering it."]}),"\n",e.jsx(t.p,{children:"The models classifies the following categories:"}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Category"}),e.jsx(t.th,{children:"Description"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"hate"})}),e.jsx(t.td,{children:"Content that expresses, incites, or promotes hate based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste. Hateful content aimed at non-protected groups (e.g., chess players) is harassment."})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"hate/threatening"})}),e.jsx(t.td,{children:"Hateful content that also includes violence or serious harm towards the targeted group based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste."})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"harassment"})}),e.jsx(t.td,{children:"Content that expresses, incites, or promotes harassing language towards any target."})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"harassment/threatening"})}),e.jsx(t.td,{children:"Harassment content that also includes violence or serious harm towards any target."})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"self-harm"})}),e.jsx(t.td,{children:"Content that promotes, encourages, or depicts acts of self-harm, such as suicide, cutting, and eating disorders."})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"self-harm/intent"})}),e.jsx(t.td,{children:"Content where the speaker expresses that they are engaging or intend to engage in acts of self-harm, such as suicide, cutting, and eating disorders."})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"self-harm/instructions"})}),e.jsx(t.td,{children:"Content that encourages performing acts of self-harm, such as suicide, cutting, and eating disorders, or that gives instructions or advice on how to commit such acts."})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"sexual"})}),e.jsx(t.td,{children:"Content meant to arouse sexual excitement, such as the description of sexual activity, or that promotes sexual services (excluding sex education and wellness)."})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"sexual/minors"})}),e.jsx(t.td,{children:"Sexual content that includes an individual who is under 18 years old."})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"violence"})}),e.jsx(t.td,{children:"Content that depicts death, violence, or physical injury."})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"violence/graphic"})}),e.jsx(t.td,{children:"Content that depicts death, violence, or physical injury in graphic detail."})]})]})]}),"\n",e.jsx(t.p,{children:"The moderation endpoint is free to use for most developers. For higher accuracy, try splitting long pieces of text into smaller chunks each less than 2,000 characters."}),"\n",e.jsx(b,{children:e.jsx(t.p,{children:"We are continuously working to improve the accuracy of our classifier. Our support for non-English languages is currently limited."})}),"\n",e.jsx(t.h2,{children:"Quickstart"}),"\n",e.jsxs(t.p,{children:["To obtain a classification for a piece of text, make a request to the ",e.jsx(t.a,{href:"/docs/api-reference/moderations",children:"moderation endpoint"})," as demonstrated in the following code snippets:"]}),"\n",e.jsx(c,{title:"Example: Getting moderations",defaultLanguage:"curl",code:{python:'\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.moderations.create(input="Sample text goes here.")\n\noutput = response.results[0]\n'.trim(),curl:'\ncurl https://api.openai.com/v1/moderations \\\n  -X POST \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -d \'{"input": "Sample text goes here"}\'\n'.trim(),node:'\nimport OpenAI from "openai";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const moderation = await openai.moderations.create({ input: "Sample text goes here." });\n\n  console.log(moderation);\n}\nmain();\n'.trim()}}),"\n",e.jsx(t.p,{children:"Below is an example output of the endpoint. It returns the following fields:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"flagged"}),": Set to ",e.jsx(t.code,{children:"true"})," if the model classifies the content as potentially harmful, ",e.jsx(t.code,{children:"false"})," otherwise."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"categories"}),": Contains a dictionary of per-category violation flags. For each category, the value is ",e.jsx(t.code,{children:"true"})," if the model flags the corresponding category as violated, ",e.jsx(t.code,{children:"false"})," otherwise."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"category_scores"}),": Contains a dictionary of per-category raw scores output by the model, denoting the model's confidence that the input violates the OpenAI's policy for the category. The value is between 0 and 1, where higher values denote higher confidence. The scores should not be interpreted as probabilities."]}),"\n"]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-json",children:'{\n    "id": "modr-XXXXX",\n    "model": "text-moderation-007",\n    "results": [\n        {\n            "flagged": true,\n            "categories": {\n                "sexual": false,\n                "hate": false,\n                "harassment": false,\n                "self-harm": false,\n                "sexual/minors": false,\n                "hate/threatening": false,\n                "violence/graphic": false,\n                "self-harm/intent": false,\n                "self-harm/instructions": false,\n                "harassment/threatening": true,\n                "violence": true\n            },\n            "category_scores": {\n                "sexual": 1.2282071e-6,\n                "hate": 0.010696256,\n                "harassment": 0.29842457,\n                "self-harm": 1.5236925e-8,\n                "sexual/minors": 5.7246268e-8,\n                "hate/threatening": 0.0060676364,\n                "violence/graphic": 4.435014e-6,\n                "self-harm/intent": 8.098441e-10,\n                "self-harm/instructions": 2.8498655e-11,\n                "harassment/threatening": 0.63055265,\n                "violence": 0.99011886\n            }\n        }\n    ]\n}\n'})}),"\n",e.jsx(b,{children:e.jsxs(t.p,{children:["We plan to continuously upgrade the moderation endpoint's underlying model. Therefore, custom policies that rely on ",e.jsx(t.code,{children:"category_scores"})," may need recalibration over time."]})})]})}function Ii(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(ut,n)})):ut(n)}function pt(n){const t=Object.assign({h1:"h1",h3:"h3",p:"p",ul:"ul",li:"li",strong:"strong",h2:"h2",img:"img",h4:"h4",a:"a",table:"table",thead:"thead",tr:"tr",th:"th",tbody:"tbody",td:"td",pre:"pre",code:"code",em:"em",ol:"ol"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"Optimizing LLMs for accuracy"}),"\n",e.jsx(t.h3,{children:"How to maximize correctness and consistent behavior when working with LLMs"}),"\n",e.jsx(t.p,{children:"Optimizing LLMs is hard."}),"\n",e.jsx(t.p,{children:"We've worked with many developers across both start-ups and enterprises, and the reason optimization is hard consistently boils down to these reasons:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["Knowing ",e.jsx(t.strong,{children:"how to start"})," optimizing accuracy"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"When to use what"})," optimization method"]}),"\n",e.jsxs(t.li,{children:["What level of accuracy is ",e.jsx(t.strong,{children:"good enough"})," for production"]}),"\n"]}),"\n",e.jsx(t.p,{children:"This paper gives a mental model for how to optimize LLMs for accuracy and behavior. We’ll explore methods like prompt engineering, retrieval-augmented generation (RAG) and fine-tuning. We’ll also highlight how and when to use each technique, and share a few pitfalls."}),"\n",e.jsx(t.p,{children:"As you read through, it's important to mentally relate these principles to what accuracy means for your specific use case. This may seem obvious, but there is a difference between producing a bad copy that a human needs to fix vs. refunding a customer $1000 rather than $100. You should enter any discussion on LLM accuracy with a rough picture of how much a failure by the LLM costs you, and how much a success saves or earns you - this will be revisited at the end, where we cover how much accuracy is “good enough” for production."}),"\n",e.jsx(t.h2,{children:"LLM optimization context"}),"\n",e.jsx(t.p,{children:"Many “how-to” guides on optimization paint it as a simple linear flow - you start with prompt engineering, then you move on to retrieval-augmented generation, then fine-tuning. However, this is often not the case - these are all levers that solve different things, and to optimize in the right direction you need to pull the right lever."}),"\n",e.jsx(t.p,{children:"It is useful to frame LLM optimization as more of a matrix:"}),"\n",e.jsx(t.p,{children:e.jsx(t.img,{src:"https://openaiassets.blob.core.windows.net/$web/API/docs/images/diagram-optimizing-accuracy-01.png",alt:"Accuracy mental model diagram"})}),"\n",e.jsx(t.p,{children:"The typical LLM task will start in the bottom left corner with prompt engineering, where we test, learn, and evaluate to get a baseline. Once we’ve reviewed those baseline examples and assessed why they are incorrect, we can pull one of our levers:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Context optimization:"})," You need to optimize for context when 1) the model lacks contextual knowledge because it wasn’t in its training set, 2) its knowledge is out of date, or 3) it requires knowledge of proprietary information. This axis maximizes ",e.jsx(t.strong,{children:"response accuracy"}),"."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"LLM optimization:"})," You need to optimize the LLM when 1) the model is producing inconsistent results with incorrect formatting, 2) the tone or style of speech is not correct, or 3) the reasoning is not being followed consistently. This axis maximizes ",e.jsx(t.strong,{children:"consistency of behavior"}),"."]}),"\n"]}),"\n",e.jsx(t.p,{children:"In reality this turns into a series of optimization steps, where we evaluate, make a hypothesis on how to optimize, apply it, evaluate, and re-assess for the next step. Here’s an example of a fairly typical optimization flow:"}),"\n",e.jsx(t.p,{children:e.jsx(t.img,{src:"https://openaiassets.blob.core.windows.net/$web/API/docs/images/diagram-optimizing-accuracy-02.png",alt:"Accuracy mental model journey diagram"})}),"\n",e.jsx(t.p,{children:"In this example, we do the following:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Begin with a prompt, then evaluate its performance"}),"\n",e.jsx(t.li,{children:"Add static few-shot examples, which should improve consistency of results"}),"\n",e.jsx(t.li,{children:"Add a retrieval step so the few-shot examples are brought in dynamically based on the question - this boosts performance by ensuring relevant context for each input"}),"\n",e.jsx(t.li,{children:"Prepare a dataset of 50+ examples and fine-tune a model to increase consistency"}),"\n",e.jsx(t.li,{children:"Tune the retrieval and add a fact-checking step to find hallucinations to achieve higher accuracy"}),"\n",e.jsx(t.li,{children:"Re-train the fine-tuned model on the new training examples which include our enhanced RAG inputs"}),"\n"]}),"\n",e.jsx(t.p,{children:"This is a fairly typical optimization pipeline for a tough business problem - it helps us decide whether we need more relevant context or if we need more consistent behavior from the model. Once we make that decision, we know which lever to pull as our first step toward optimization."}),"\n",e.jsx(t.p,{children:"Now that we have a mental model, let’s dive into the methods for taking action on all of these areas. We’ll start in the bottom-left corner with Prompt Engineering."}),"\n",e.jsx(t.h3,{children:"Prompt engineering"}),"\n",e.jsx(t.p,{children:"Prompt engineering is typically the best place to start**. It is often the only method needed for use cases like summarization, translation, and code generation where a zero-shot approach can reach production levels of accuracy and consistency."}),"\n",e.jsxs(t.p,{children:["This is because it forces you to define what accuracy means for your use case - you start at the most basic level by providing an input, so you need to be able to judge whether or not the output matches your expectations. If it is not what you want, then the reasons ",e.jsx(t.strong,{children:"why"})," will show you what to use to drive further optimizations."]}),"\n",e.jsxs(t.p,{children:["To achieve this, you should always start with a simple prompt and an expected output in mind, and then optimize the prompt by adding ",e.jsx(t.strong,{children:"context"}),", ",e.jsx(t.strong,{children:"instructions"}),", or ",e.jsx(t.strong,{children:"examples"})," until it gives you what you want."]}),"\n",e.jsx(t.h4,{children:"Optimization"}),"\n",e.jsxs(t.p,{children:["To optimize your prompts, I’ll mostly lean on strategies from the ",e.jsx(t.a,{href:"https://platform.openai.com/docs/guides/prompt-engineering",children:"Prompt Engineering guide"})," in the OpenAI API documentation. Each strategy helps you tune Context, the LLM, or both:"]}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Strategy"}),e.jsx(t.th,{align:"center",children:"Context optimization"}),e.jsx(t.th,{align:"center",children:"LLM optimization"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"Write clear instructions"}),e.jsx(t.td,{align:"center"}),e.jsx(t.td,{align:"center",children:"X"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"Split complex tasks into simpler subtasks"}),e.jsx(t.td,{align:"center",children:"X"}),e.jsx(t.td,{align:"center",children:"X"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:'Give GPTs time to "think"'}),e.jsx(t.td,{align:"center"}),e.jsx(t.td,{align:"center",children:"X"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"Test changes systematically"}),e.jsx(t.td,{align:"center",children:"X"}),e.jsx(t.td,{align:"center",children:"X"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"Provide reference text"}),e.jsx(t.td,{align:"center",children:"X"}),e.jsx(t.td,{align:"center"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"Use external tools"}),e.jsx(t.td,{align:"center",children:"X"}),e.jsx(t.td,{align:"center"})]})]})]}),"\n",e.jsx(t.p,{children:"These can be a little difficult to visualize, so we’ll run through an example where we test these out with a practical example. Let’s use gpt-4-turbo to correct Icelandic sentences to see how this can work."}),"\n",e.jsxs(j,{label:"Prompt engineering for language corrections ",children:[e.jsxs(t.p,{children:["The ",e.jsx(t.a,{href:"https://repository.clarin.is/repository/xmlui/handle/20.500.12537/105",children:"Icelandic Errors Corpus"})," contains combinations of an Icelandic sentence with errors, and the corrected version of that sentence. We’ll use the baseline GPT-4 model to try to solve this task, and then apply different optimization techniques to see how we can improve the model’s performance."]}),e.jsx(t.p,{children:"Given an Icelandic sentence, we want the model to return a corrected version of the sentence. We’ll use Bleu score to measure the relative quality of the translation."}),e.jsx("div",{className:"icelandic-zero-shot-table",children:e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"system"}),e.jsx(t.th,{children:"user"}),e.jsx(t.th,{children:"ground_truth"}),e.jsx(t.th,{children:"assistant"}),e.jsx(t.th,{children:"BLEU"})]})}),e.jsx(t.tbody,{children:e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"The following sentences contain Icelandic sentences which may include errors. Please correct these errors using as few word changes as possible."}),e.jsx(t.td,{children:"Sörvistölur eru nær hálsi og skartgripir kvenna á brjótsti."}),e.jsx(t.td,{children:"Sörvistölur eru nær hálsi og skartgripir kvenna á brjósti."}),e.jsx(t.td,{children:"Sörvistölur eru nær hálsi og skartgripir kvenna á brjósti."}),e.jsx(t.td,{children:"1.0"})]})})]})}),e.jsx(t.p,{children:"We perform a first attempt with GPT-4 with no examples, and it performs decently, getting a BLEU score of 62.\nWe’ll now add some few-shot examples and see whether we can teach the model the style we’re looking for by showing rather than telling.\nAn example looks like this:"}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:'SYSTEM: The following sentences contain Icelandic sentences which may include errors. Please correct these errors using as few word changes as possible.\n\n# Examples\nUSER: "Stofnendurnir séu margir og eru fulltrúar hennar frá Englandi, Grikklandi, Rússlandi, Svíþjóð og fleiri löndum Evrópu."\nASSISTANT: "Hann segir að stofnendur leynireglunnar séu margir og að fulltrúar hennar séu frá Englandi, Grikklandi, Rússlandi, Svíþjóð og fleiri löndum Evrópu."\n\nUSER: "Helsta fæða bjúgorma eru hægfara lífverur sem eru á sama búsvæði og bjúgormarnir, oft smærri ormar eins og burstormar (fræðiheiti: Polychatete)."\nASSISTANT: "Helsta fæða bjúgorma eru hægfara lífverur sem eru á sama búsvæði og bjúgormarnir, oft smærri ormar eins og burstaormar (fræðiheiti: Polychatete)."\n\nUSER: "Sörvistölur eru nær hálsi og skartgripir kvenna á brjótsti."\nASSISTANT: "Sörvistölur eru nær hálsi og skartgripir kvenna á brjósti."\n\nUSER: [input user query here]\n'})}),e.jsxs(t.p,{children:["The overall translation quality is better, showing an improvement to a Bleu score of ",e.jsx(t.strong,{children:"70 (+8%)"}),". This is pretty good, and shows us that giving the model examples of the task is helping it to learn."]}),e.jsxs(t.p,{children:["This tells us that it is the ",e.jsx(t.strong,{children:"behavior"})," of the model that we need to optimize - it already has the knowledge that it needs to solve the problem, so providing many more examples may be the optimization we need."]}),e.jsx(t.p,{children:"We’ll revisit this later in the paper to test how our more advanced optimization methods play with this use case."})]}),"\n",e.jsx(t.p,{children:"We’ve seen that prompt engineering is a great place to start, and that with the right tuning methods we can push the performance pretty far."}),"\n",e.jsx(t.p,{children:"However, the biggest issue with prompt engineering is that it often doesn’t scale - we either need dynamic context to be fed to allow the model to deal with a wider range of problems than we can deal with through simple context stuffing or we need more consistent behavior than we can achieve with few-shot examples."}),"\n",e.jsxs(z,{title:"Using long context to scale prompt engineering",children:[e.jsxs(t.p,{children:["Long-context models allow prompt engineering to scale further - however, beware that models can struggle to maintain attention across very large prompts with complex instructions, and so you should always pair long context models with evaluation at different context sizes to ensure you don’t get ",e.jsx(t.a,{href:"https://arxiv.org/abs/2307.03172",children:e.jsx(t.strong,{children:"lost in the middle"})}),". \"Lost in the middle\" is a term that addresses how an LLM can't pay equal attention to all the tokens given to it at any one time. This can result in it missing information seemingly randomly. This doesn't mean you shouldn't use long context, but you need to pair it with thorough evaluation."]}),e.jsxs(t.p,{children:["One open-source contributor, Greg Kamradt, made a useful evaluation called ",e.jsx(t.a,{href:"https://github.com/gkamradt/LLMTest_NeedleInAHaystack",children:e.jsx(t.strong,{children:"Needle in A Haystack (NITA)"})})," which hid a piece of information at varying depths in long-context documents and evaluated the retrieval quality. This illustrates the problem with long-context - it promises a much simpler retrieval process where you can dump everything in context, but at a cost in accuracy."]})]}),"\n",e.jsx(t.p,{children:"So how far can you really take prompt engineering? The answer is that it depends, and the way you make your decision is through evaluations."}),"\n",e.jsx(t.h3,{children:"Evaluation"}),"\n",e.jsxs(t.p,{children:["This is why ",e.jsx(t.strong,{children:"a good prompt with an evaluation set of questions and ground truth answers"})," is the best output from this stage. If we have a set of 20+ questions and answers, and we have looked into the details of the failures and have a hypothesis of why they’re occurring, then we’ve got the right baseline to take on more advanced optimization methods."]}),"\n",e.jsx(t.p,{children:"Before you move on to more sophisticated optimization methods, it's also worth considering how to automate this evaluation to speed up your iterations. Some common practices we’ve seen be effective here are:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["Using approaches like ",e.jsx(t.a,{href:"https://aclanthology.org/W04-1013/",children:"ROUGE"})," or ",e.jsx(t.a,{href:"https://arxiv.org/abs/1904.09675",children:"BERTScore"})," to provide a finger-in-the-air judgment. This doesn’t correlate that closely with human reviewers, but can give a quick and effective measure of how much an iteration changed your model outputs."]}),"\n",e.jsxs(t.li,{children:["Using ",e.jsx(t.a,{href:"https://arxiv.org/pdf/2303.16634.pdf",children:"GPT-4"})," as an evaluator as outlined in the G-Eval paper, where you provide the LLM a scorecard to assess the output as objectively as possible."]}),"\n"]}),"\n",e.jsxs(t.p,{children:["If you want to dive deeper on these, check out ",e.jsx(t.a,{href:"https://cookbook.openai.com/examples/evaluation/how_to_eval_abstractive_summarization",children:"this cookbook"})," which takes you through all of them in practice."]}),"\n",e.jsx(t.h2,{children:"Understanding the tools"}),"\n",e.jsx(t.p,{children:"So you’ve done prompt engineering, you’ve got an eval set, and your model is still not doing what you need it to do. The most important next step is to diagnose where it is failing, and what tool works best to improve it."}),"\n",e.jsx(t.p,{children:"Here is a basic framework for doing so:"}),"\n",e.jsx(t.p,{children:e.jsx(t.img,{src:"https://openaiassets.blob.core.windows.net/$web/API/docs/images/diagram-optimizing-accuracy-03.png",alt:"Classifying memory problem diagram"})}),"\n",e.jsxs(t.p,{children:["You can think of framing each failed evaluation question as an ",e.jsx(t.strong,{children:"in-context"})," or ",e.jsx(t.strong,{children:"learned"})," memory problem. As an analogy, imagine writing an exam. There are two ways you can ensure you get the right answer:"]}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["You attend class for the last 6 months, where you see many repeated examples of how a particular concept works. This is ",e.jsx(t.strong,{children:"learned"})," memory - you solve this with LLMs by showing examples of the prompt and the response you expect, and the model learning from those."]}),"\n",e.jsxs(t.li,{children:["You have the textbook with you, and can look up the right information to answer the question with. This is ",e.jsx(t.strong,{children:"in-context"})," memory - we solve this in LLMs by stuffing relevant information into the context window, either in a static way using prompt engineering, or in an industrial way using RAG."]}),"\n"]}),"\n",e.jsxs(t.p,{children:["These two optimization methods are ",e.jsx(t.strong,{children:"additive, not exclusive"})," - they stack, and some use cases will require you to use them together to use optimal performance."]}),"\n",e.jsx(t.p,{children:"Let’s assume that we’re facing a short-term memory problem - for this we’ll use RAG to solve it."}),"\n",e.jsx(t.h3,{children:"Retrieval-augmented generation (RAG)"}),"\n",e.jsxs(t.p,{children:["RAG is the process of ",e.jsx(t.strong,{children:"R"}),"etrieving content to ",e.jsx(t.strong,{children:"A"}),"ugment your LLM’s prompt before ",e.jsx(t.strong,{children:"G"}),"enerating an answer. It is used to give the model ",e.jsx(t.strong,{children:"access to domain-specific context"})," to solve a task."]}),"\n",e.jsx(t.p,{children:"RAG is an incredibly valuable tool for increasing the accuracy and consistency of an LLM - many of our largest customer deployments at OpenAI were done using only prompt engineering and RAG."}),"\n",e.jsx(t.p,{children:e.jsx(t.img,{src:"https://openaiassets.blob.core.windows.net/$web/API/docs/images/diagram-optimizing-accuracy-04.png",alt:"RAG diagram"})}),"\n",e.jsx(t.p,{children:"In this example we have embedded a knowledge base of statistics. When our user asks a question, we embed that question and retrieve the most relevant content from our knowledge base. This is presented to the model, which answers the question."}),"\n",e.jsx(t.p,{children:"RAG applications introduce a new axis we need to optimize against, which is retrieval. For our RAG to work, we need to give the right context to the model, and then assess whether the model is answering correctly. I’ll frame these in a grid here to show a simple way to think about evaluation with RAG:"}),"\n",e.jsx(t.p,{children:e.jsx(t.img,{src:"https://openaiassets.blob.core.windows.net/$web/API/docs/images/diagram-optimizing-accuracy-05.png",alt:"RAG evaluation diagram"})}),"\n",e.jsx(t.p,{children:"You have two areas your RAG application can break down:"}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Area"}),e.jsx(t.th,{children:"Problem"}),e.jsx(t.th,{children:"Resolution"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"Retrieval"}),e.jsx(t.td,{children:"You can supply the wrong context, so the model can’t possibly answer, or you can supply too much irrelevant context, which drowns out the real information and causes hallucinations."}),e.jsxs(t.td,{children:["Optimizing your retrieval, which can include:",e.jsx("br",{}),"- Tuning the search to return the right results.",e.jsx("br",{}),"- Tuning the search to include less noise.",e.jsx("br",{}),"- Providing more information in each retrieved result",e.jsx("br",{}),"These are just examples, as tuning RAG performance is an industry into itself, with libraries like LlamaIndex and LangChain giving many approaches to tuning here."]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"LLM"}),e.jsx(t.td,{children:"The model can also get the right context and do the wrong thing with it."}),e.jsx(t.td,{children:"Prompt engineering by improving the instructions and method the model uses, and, if showing it examples increases accuracy, adding in fine-tuning"})]})]})]}),"\n",e.jsx(t.p,{children:"The key thing to take away here is that the principle remains the same from our mental model at the beginning - you evaluate to find out what has gone wrong, and take an optimization step to fix it. The only difference with RAG is you now have the retrieval axis to consider."}),"\n",e.jsx(t.p,{children:"While useful, RAG only solves our in-context learning issues - for many use cases, the issue will be ensuring the LLM can learn a task so it can perform it consistently and reliably. For this problem we turn to fine-tuning."}),"\n",e.jsx(t.h3,{children:"Fine-tuning"}),"\n",e.jsxs(t.p,{children:["To solve a learned memory problem, many developers will continue the training process of the LLM on a smaller, domain-specific dataset to optimize it for the specific task. This process is known as ",e.jsx(t.strong,{children:"fine-tuning"}),"."]}),"\n",e.jsx(t.p,{children:"Fine-tuning is typically performed for one of two reasons:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"To improve model accuracy on a specific task:"})," Training the model on task-specific data to solve a learned memory problem by showing it many examples of that task being performed correctly."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"To improve model efficiency:"})," Achieve the same accuracy for less tokens or by using a smaller model."]}),"\n"]}),"\n",e.jsx(t.p,{children:"The fine-tuning process begins by preparing a dataset of training examples - this is the most critical step, as your fine-tuning examples must exactly represent what the model will see in the real world."}),"\n",e.jsx(b,{children:e.jsxs(t.p,{children:["Many customers use a process known as ",e.jsx(t.strong,{children:"prompt baking"}),", where you extensively log your prompt inputs and outputs during a pilot. These logs can be pruned into an effective training set with realistic examples."]})}),"\n",e.jsx(t.p,{children:e.jsx(t.img,{src:"https://openaiassets.blob.core.windows.net/$web/API/docs/images/diagram-optimizing-accuracy-06.png",alt:"Fine-tuning process diagram"})}),"\n",e.jsxs(t.p,{children:["Once you have this clean set, you can train a fine-tuned model by performing a ",e.jsx(t.strong,{children:"training"})," run - depending on the platform or framework you’re using for training you may have hyperparameters you can tune here, similar to any other machine learning model. We always recommend maintaining a hold-out set to use for ",e.jsx(t.strong,{children:"evaluation"})," following training to detect overfitting. For tips on how to construct a good training set you can check out the ",e.jsx(t.a,{href:"https://platform.openai.com/docs/guides/fine-tuning/analyzing-your-fine-tuned-model",children:"guidance"})," in our Fine-tuning documentation, while for how to prep and tune the hold-out set there is more info ",e.jsx(t.a,{href:"LINK_HERE",children:"here"}),". Once training is completed, the new, fine-tuned model is available for inference."]}),"\n",e.jsx(t.p,{children:"For optimizing fine-tuning we’ll focus on best practices we observe with OpenAI’s model customization offerings, but these principles should hold true with other providers and OSS offerings. The key practices to observe here are:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Start with prompt-engineering:"})," Have a solid evaluation set from prompt engineering which you can use as a baseline. This allows a low-investment approach until you’re confident in your base prompt."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Start small, focus on quality:"})," Quality of training data is more important than quantity when fine-tuning on top of a foundation model. Start with 50+ examples, evaluate, and then dial your training set size up if you haven’t yet hit your accuracy needs, and if the issues causing incorrect answers are due to consistency/behavior and not context."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Ensure your examples are representative:"})," One of the most common pitfalls we see is non-representative training data, where the examples used for fine-tuning differ subtly in formatting or form from what the LLM sees in production. For example, if you have a RAG application, fine-tune the model with RAG examples in it so it isn’t learning how to use the context zero-shot."]}),"\n"]}),"\n",e.jsx(t.h3,{children:"All of the above"}),"\n",e.jsx(t.p,{children:"These techniques stack on top of each other - if your early evals show issues with both context and behavior, then it's likely you may end up with fine-tuning + RAG in your production solution. This is ok - these stack to balance the weaknesses of both approaches. Some of the main benefits are:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["Using fine-tuning to ",e.jsx(t.strong,{children:"minimize the tokens"})," used for prompt engineering, as you replace instructions and few-shot examples with many training examples to ingrain consistent behaviour in the model."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Teaching complex behavior"})," using extensive fine-tuning"]}),"\n",e.jsxs(t.li,{children:["Using RAG to ",e.jsx(t.strong,{children:"inject context"}),", more recent content or any other specialized context required for your use cases"]}),"\n"]}),"\n",e.jsxs(j,{label:"Using these tools to improve language translation",children:[e.jsx(t.p,{children:"We’ll continue building on the Icelandic correction example we used above. We’ll test out the following approaches:"}),e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Our original hypothesis was that this was a behavior optimization problem, so our first step will be to fine-tune a model. We’ll try both gpt-3.5-turbo and gpt-4 here."}),"\n",e.jsx(t.li,{children:"We’ll also try RAG - in this instance our hypothesis is that relevant examples might give additional context which could help the model solve the problem, but this is a lower confidence optimization."}),"\n"]}),e.jsx(t.h4,{children:"Fine-tuning"}),e.jsx(t.p,{children:"To fine-tune for our use-case we’ll use a dataset of 1000 examples similar to our few-shot examples above:"}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:'# One training example\nSYSTEM: The following sentences contain Icelandic sentences which may include errors. Please correct these errors using as few word changes as possible.\nUSER: "Hið sameinaða fyrirtæki verður einn af stærstu bílaframleiðendum í heiminum."\nASSISTANT: "Hið sameinaða fyrirtæki verður einn af stærstu bílaframleiðendum heims."\n'})}),e.jsx(t.p,{children:"We use these 1000 examples to train both gpt-3.5-turbo and gpt-4 fine-tuned models, and rerun our evaluation on our validation set. This confirmed our hypothesis - we got a meaningful bump in performance with both, with even the 3.5 model outperforming few-shot gpt-4 by 8 points:"}),e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Run"}),e.jsx(t.th,{children:"Method"}),e.jsx(t.th,{children:"Bleu Score"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"1"}),e.jsx(t.td,{children:"gpt-4 with zero-shot"}),e.jsx(t.td,{children:"62"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"2"}),e.jsx(t.td,{children:"gpt-4 with 3 few-shot examples"}),e.jsx(t.td,{children:"70"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"3"}),e.jsx(t.td,{children:"gpt-3.5-turbo fine-tuned with 1000 examples"}),e.jsx(t.td,{children:"78"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"4"}),e.jsx(t.td,{children:"gpt-4 fine-tuned with 1000 examples"}),e.jsx(t.td,{children:"87"})]})]})]}),e.jsx(t.p,{children:"Great, this is starting to look like production level accuracy for our use case. However, let's test whether we can squeeze a little more performance out of our pipeline by adding some relevant RAG examples to the prompt for in-context learning."}),e.jsx(t.h4,{children:"RAG + Fine-tuning"}),e.jsx(t.p,{children:"Our final optimization adds 1000 examples from outside of the training and validation sets which are embedded and placed in a vector database. We then run a further test with our gpt-4 fine-tuned model, with some perhaps surprising results:"}),e.jsxs(t.p,{children:[e.jsx(t.img,{src:"https://openaiassets.blob.core.windows.net/$web/API/docs/images/diagram-optimizing-accuracy-07.png",alt:"Icelandic case study diagram"}),"\n",e.jsx(t.em,{children:"Bleu Score per tuning method (out of 100)"})]}),e.jsxs(t.p,{children:["RAG actually ",e.jsx(t.strong,{children:"decreased"})," accuracy, dropping four points from our GPT-4 fine-tuned model to 83."]}),e.jsx(t.p,{children:"This illustrates the point that you use the right optimization tool for the right job - each offers benefits and risks that we manage with evaluations and iterative changes. The behavior we witnessed in our evals and from what we know about this question told us that this is a behavior optimization problem where additional context will not necessarily help the model. This was borne out in practice - RAG actually confounded the model by giving it extra noise when it had already learned the task effectively through fine-tuning."}),e.jsx(t.p,{children:"We now have a model that should be close to production-ready, and if we want to optimize further we can consider a wider diversity and quantity of training examples."})]}),"\n",e.jsx(t.p,{children:"Now you should have an appreciation for RAG and fine-tuning, and when each is appropriate. The last thing you should appreciate with these tools is that once you introduce them there is a trade-off here in our speed to iterate:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"For RAG you need to tune the retrieval as well as LLM behavior"}),"\n",e.jsx(t.li,{children:"With fine-tuning you need to rerun the fine-tuning process and manage your training and validation sets when you do additional tuning."}),"\n"]}),"\n",e.jsx(t.p,{children:"Both of these can be time-consuming and complex processes, which can introduce regression issues as your LLM application becomes more complex. If you take away one thing from this paper, let it be to squeeze as much accuracy out of basic methods as you can before reaching for more complex RAG or fine-tuning - let your accuracy target be the objective, not jumping for RAG + FT because they are perceived as the most sophisticated."}),"\n",e.jsx(t.h2,{children:"How much accuracy is “good enough” for production"}),"\n",e.jsx(t.p,{children:"Tuning for accuracy can be a never-ending battle with LLMs - they are unlikely to get to 99.999% accuracy using off-the-shelf methods. This section is all about deciding when is enough for accuracy - how do you get comfortable putting an LLM in production, and how do you manage the risk of the solution you put out there."}),"\n",e.jsxs(t.p,{children:["I find it helpful to think of this in both a ",e.jsx(t.strong,{children:"business"})," and ",e.jsx(t.strong,{children:"technical"})," context. I’m going to describe the high level approaches to managing both, and use a customer service help-desk use case to illustrate how we manage our risk in both cases."]}),"\n",e.jsx(t.h3,{children:"Business"}),"\n",e.jsx(t.p,{children:"For the business it can be hard to trust LLMs after the comparative certainties of rules-based or traditional machine learning systems, or indeed humans! A system where failures are open-ended and unpredictable is a difficult circle to square."}),"\n",e.jsx(t.p,{children:"An approach I’ve seen be successful here was for a customer service use case - for this, we did the following:"}),"\n",e.jsx(t.p,{children:"First we identify the primary success and failure cases, and assign an estimated cost to them. This gives us a clear articulation of what the solution is likely to save or cost based on pilot performance."}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["For example, a case getting solved by an AI where it was previously solved by a human may save ",e.jsx("b",{children:"$20"}),"."]}),"\n",e.jsxs(t.li,{children:["Someone getting escalated to a human when they shouldn’t might cost ",e.jsx(t.strong,{children:"$40"})]}),"\n",e.jsxs(t.li,{children:["In the worst case scenario, a customer gets so frustrated with the AI they churn, costing us ",e.jsx(t.strong,{children:"$1000"}),". We assume this happens in 5% of cases."]}),"\n"]}),"\n",e.jsx("center",{children:e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Event"}),e.jsx(t.th,{children:"Value"}),e.jsx(t.th,{children:"Number of cases"}),e.jsx(t.th,{children:"Total value"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"AI success"}),e.jsx(t.td,{children:"+20"}),e.jsx(t.td,{children:"815"}),e.jsx(t.td,{children:"$16,300"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"AI failure (escalation)"}),e.jsx(t.td,{children:"-40"}),e.jsx(t.td,{children:"175.75"}),e.jsx(t.td,{children:"$7,030"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"AI failure (churn)"}),e.jsx(t.td,{children:"-1000"}),e.jsx(t.td,{children:"9.25"}),e.jsx(t.td,{children:"$9,250"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.strong,{children:"Result"})}),e.jsx(t.td,{}),e.jsx(t.td,{}),e.jsx(t.td,{children:e.jsx(t.strong,{children:"+20"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.strong,{children:"Break-even accuracy"})}),e.jsx(t.td,{}),e.jsx(t.td,{}),e.jsx(t.td,{children:e.jsx(t.strong,{children:"81.5%"})})]})]})]})}),"\n",e.jsx(t.p,{children:"The other thing we did is to measure the empirical stats around the process which will help us measure the macro impact of the solution. Again using customer service, these could be:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"The CSAT score for purely human interactions vs. AI ones"}),"\n",e.jsx(t.li,{children:"The decision accuracy for retrospectively reviewed cases for human vs. AI"}),"\n",e.jsx(t.li,{children:"The time to resolution for human vs. AI"}),"\n"]}),"\n",e.jsx(t.p,{children:"In the customer service example, this helped us make two key decisions following a few pilots to get clear data:"}),"\n",e.jsxs(t.ol,{children:["\n",e.jsx(t.li,{children:"Even if our LLM solution escalated to humans more than we wanted, it still made an enormous operational cost saving over the existing solution. This meant that an accuracy of even 85% could be ok, if those 15% were primarily early escalations."}),"\n",e.jsx(t.li,{children:"Where the cost of failure was very high, such as a fraud case being incorrectly resolved, we decided the human would drive and the AI would function as an assistant. In this case, the decision accuracy stat helped us make the call that we weren’t comfortable with full autonomy."}),"\n"]}),"\n",e.jsx(t.h3,{children:"Technical"}),"\n",e.jsx(t.p,{children:"On the technical side it is more clear - now that the business is clear on the value they expect and the cost of what can go wrong, your role is to build a solution that handles failures gracefully in a way that doesn’t disrupt the user experience."}),"\n",e.jsx(t.p,{children:"Let’s use the customer service example one more time to illustrate this, and we’ll assume we’ve got a model that is 85% accurate in determining intent. As a technical team, here are a few ways we can minimize the impact of the incorrect 15%:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"We can prompt engineer the model to prompt the customer for more information if it isn’t confident, so our first-time accuracy may drop but we may be more accurate given 2 shots to determine intent."}),"\n",e.jsx(t.li,{children:"We can give the second-line assistant the option to pass back to the intent determination stage, again giving the UX a way of self-healing at the cost of some additional user latency."}),"\n",e.jsx(t.li,{children:"We can prompt engineer the model to hand off to a human if the intent is unclear, which costs us some operational savings in the short-term but may offset customer churn risk in the long term."}),"\n"]}),"\n",e.jsx(t.p,{children:"Those decisions then feed into our UX, which gets slower at the cost of higher accuracy, or more human interventions, which feed into the cost model covered in the business section above."}),"\n",e.jsx(t.p,{children:"You now have an approach to breaking down the business and technical decisions involved in setting an accuracy target that is grounded in business reality."}),"\n",e.jsx(t.h2,{children:"Taking this forward"}),"\n",e.jsxs(t.p,{children:["This is a high level mental model for thinking about maximizing accuracy for LLMs, the tools you can use to achieve it, and the approach for deciding where enough is enough for production. You have the framework and tools you need to get to production consistently, and if you want to be inspired by what others have achieved with these methods then look no further than our customer stories, where use cases like ",e.jsx(t.a,{href:"https://openai.com/customer-stories/morgan-stanley",children:"Morgan Stanley"})," and ",e.jsx(t.a,{href:"https://openai.com/customer-stories/klarna",children:"Klarna"})," show what you can achieve by leveraging these techniques."]}),"\n",e.jsx(t.p,{children:"Best of luck, and we’re excited to see what you build with this!"})]})}function Ti(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(pt,n)})):pt(n)}function Pi(){return e.jsxs("div",{className:"latency-example",children:[e.jsx("div",{className:"body-small bold latency-example-label",children:"Network"}),e.jsx("div",{children:"End user to API latency"}),e.jsx("div",{}),e.jsx("div",{children:e.jsx(re,{})}),e.jsx("div",{className:"body-small bold latency-example-label",children:"Server"}),e.jsx("div",{children:"Time to process prompt tokens"}),e.jsx("div",{}),e.jsx("div",{children:e.jsx(re,{})}),e.jsx("div",{className:"body-small bold latency-example-label",children:"Server"}),e.jsx("div",{children:"Time to sample/generate tokens"}),e.jsx("div",{}),e.jsx("div",{children:e.jsx(re,{})}),e.jsx("div",{className:"body-small bold latency-example-label",children:"Network"}),e.jsx("div",{children:"API to end user latency"}),e.jsx("div",{})]})}function mt(n){const t=Object.assign({h1:"h1",p:"p",h2:"h2",a:"a",strong:"strong",ul:"ul",li:"li",h3:"h3",code:"code",blockquote:"blockquote",h4:"h4"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"Production best practices"}),"\n",e.jsx(t.p,{children:"This guide provides a comprehensive set of best practices to help you transition from prototype to production. Whether you are a seasoned machine learning engineer or a recent enthusiast, this guide should provide you with the tools you need to successfully put the platform to work in a production setting: from securing access to our API to designing a robust architecture that can handle high traffic volumes. Use this guide to help develop a plan for deploying your application as smoothly and effectively as possible."}),"\n",e.jsx(t.p,{children:"If you want to explore best practices for going into production further, please check out our Developer Day talk:"}),"\n",e.jsx("iframe",{width:"100%",height:"315",src:"https://www.youtube-nocookie.com/embed/XGJNo8TpuVA?si=mvYm3Un23iHnlXcg",title:"YouTube video player",frameBorder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",allowFullScreen:!0}),"\n",e.jsx(t.h2,{children:"Setting up your organization"}),"\n",e.jsxs(t.p,{children:["Once you ",e.jsx(t.a,{href:"/login",children:"log in"})," to your OpenAI account, you can find your organization name and ID in your ",e.jsx(t.a,{href:"/account/organization",children:"organization settings"}),". The organization name is the label for your organization, shown in user interfaces. The organization ID is the unique identifier for your organization which can be used in API requests."]}),"\n",e.jsxs(t.p,{children:["Users who belong to multiple organizations can ",e.jsx(t.a,{href:"/docs/api-reference/requesting-organization",children:"pass a header"})," to specify which organization is used for an API request. Usage from these API requests will count against the specified organization's quota. If no header is provided, the ",e.jsx(t.a,{href:"/account/api-keys",children:"default organization"})," will be billed. You can change your default organization in your ",e.jsx(t.a,{href:"/account/api-keys",children:"user settings"}),"."]}),"\n",e.jsxs(t.p,{children:["You can invite new members to your organization from the ",e.jsx(t.a,{href:"/account/team",children:"Team page"}),". Members can be ",e.jsx(t.strong,{children:"readers"})," or ",e.jsx(t.strong,{children:"owners"}),"."]}),"\n",e.jsx(t.p,{children:"Readers:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Can make API requests."}),"\n",e.jsx(t.li,{children:"Can view basic organization information."}),"\n",e.jsx(t.li,{children:"Can create, update, and delete resources (like Assistants) in the organization, unless otherwise noted."}),"\n"]}),"\n",e.jsx(t.p,{children:"Owners:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Have all the permissions of readers."}),"\n",e.jsx(t.li,{children:"Can modify billing information."}),"\n",e.jsx(t.li,{children:"Can manage members within the organization."}),"\n"]}),"\n",e.jsx(t.h3,{children:"Managing billing limits"}),"\n",e.jsxs(t.p,{children:["To begin using the OpenAI API, enter your ",e.jsx(t.a,{href:"/account/billing/overview",children:"billing information"}),". If no billing information is entered, you will still have login access but will be unable to make API requests."]}),"\n",e.jsxs(t.p,{children:["Once you’ve entered your billing information, you will have an approved usage limit of $100 per month, which is set by OpenAI. Your quota limit will automatically increase as your usage on your platform increases and you move from one ",e.jsx(t.a,{href:"/docs/guides/rate-limits/usage-tiers",children:"usage tier"})," to another. You can review your current usage limit in the ",e.jsx(t.a,{href:"/account/rate-limits",children:"limits"})," page in your account settings."]}),"\n",e.jsxs(t.p,{children:["If you’d like to be notified when your usage exceeds a certain dollar amount, you can set a notification threshold through the ",e.jsx(t.a,{href:"/account/limits",children:"usage limits"})," page. When the notification threshold is reached, the owners of the organization will receive an email notification. You can also set a monthly budget so that, once the monthly budget is reached, any subsequent API requests will be rejected. Note that these limits are best effort, and there may be 5 to 10 minutes of delay between the usage and the limits being enforced."]}),"\n",e.jsx(t.h3,{children:"API keys"}),"\n",e.jsxs(t.p,{children:["The OpenAI API uses API keys for authentication. Visit your ",e.jsx(t.a,{href:"/account/api-keys",children:"API keys"})," page to retrieve the API key you'll use in your requests."]}),"\n",e.jsxs(t.p,{children:["This is a relatively straightforward way to control access, but you must be vigilant about securing these keys. Avoid exposing the API keys in your code or in public repositories; instead, store them in a secure location. You should expose your keys to your application using environment variables or secret management service, so that you don't need to hard-code them in your codebase. Read more in our ",e.jsx(t.a,{href:"https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety",children:"Best practices for API key safety"}),"."]}),"\n",e.jsxs(t.p,{children:["API key usage can be monitored on the ",e.jsx(t.a,{href:"/usage",children:"Usage page"})," once tracking is enabled. If you are using an API key generated prior to Dec 20, 2023 tracking will not be enabled by default. You can enable tracking going forward on the ",e.jsx(t.a,{href:"/api-keys",children:"API key management dashboard"}),". All API keys generated past Dec 20, 2023 have tracking enabled. Any previous untracked usage will be displayed as ",e.jsx(t.code,{children:"Untracked"})," in the dashboard."]}),"\n",e.jsx(t.h3,{children:"Staging projects"}),"\n",e.jsx(t.p,{children:"As you scale, you may want to create separate projects for your staging and production environments. You can create these projects in the dashboard, allowing you to isolate your development and testing work, so you don't accidentally disrupt your live application. You can also limit user access to your production project, and set custom rate and spend limits per project."}),"\n",e.jsx(t.h2,{children:"Scaling your solution architecture"}),"\n",e.jsx(t.p,{children:"When designing your application or service for production that uses our API, it's important to consider how you will scale to meet traffic demands. There are a few key areas you will need to consider regardless of the cloud service provider of your choice:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Horizontal scaling"}),": You may want to scale your application out horizontally to accommodate requests to your application that come from multiple sources. This could involve deploying additional servers or containers to distribute the load. If you opt for this type of scaling, make sure that your architecture is designed to handle multiple nodes and that you have mechanisms in place to balance the load between them."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Vertical scaling"}),": Another option is to scale your application up vertically, meaning you can beef up the resources available to a single node. This would involve upgrading your server's capabilities to handle the additional load. If you opt for this type of scaling, make sure your application is designed to take advantage of these additional resources."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Caching"}),": By storing frequently accessed data, you can improve response times without needing to make repeated calls to our API. Your application will need to be designed to use cached data whenever possible and invalidate the cache when new information is added. There are a few different ways you could do this. For example, you could store data in a database, filesystem, or in-memory cache, depending on what makes the most sense for your application."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Load balancing"}),": Finally, consider load-balancing techniques to ensure requests are distributed evenly across your available servers. This could involve using a load balancer in front of your servers or using DNS round-robin. Balancing the load will help improve performance and reduce bottlenecks."]}),"\n"]}),"\n",e.jsx(t.h2,{children:"Managing rate limits"}),"\n",e.jsxs(t.p,{children:["When using our API, it's important to understand and plan for ",e.jsx(t.a,{href:"/docs/guides/rate-limits",children:"rate limits"}),"."]}),"\n",e.jsx(t.h2,{children:"Improving latencies"}),"\n",e.jsx(b,{children:e.jsxs(t.p,{children:["Check out our most up-to-date guide on ",e.jsx(t.a,{href:"/docs/guides/latency-optimization",children:"latency optimization"}),"."]})}),"\n",e.jsx(t.p,{children:"Latency is the time it takes for a request to be processed and a response to be returned. In this section, we will discuss some factors that influence the latency of our text generation models and provide suggestions on how to reduce it."}),"\n",e.jsx(t.p,{children:"The latency of a completion request is mostly influenced by two factors: the model and the number of tokens generated. The life cycle of a completion request looks like this:"}),"\n",e.jsx(Pi,{}),"\n",e.jsx("br",{}),"\n",e.jsx(t.p,{children:"The bulk of the latency typically arises from the token generation step."}),"\n",e.jsxs(t.blockquote,{children:["\n",e.jsxs(t.p,{children:[e.jsx(t.strong,{children:"Intuition"}),": Prompt tokens add very little latency to completion calls. Time to generate completion tokens is much longer, as tokens are generated one at a time. Longer generation lengths will accumulate latency due to generation required for each token."]}),"\n"]}),"\n",e.jsx(t.h3,{children:"Common factors affecting latency and possible mitigation techniques"}),"\n",e.jsx(t.p,{children:"Now that we have looked at the basics of latency, let’s take a look at various factors that can affect latency, broadly ordered from most impactful to least impactful."}),"\n",e.jsx(t.h4,{children:"Model"}),"\n",e.jsxs(t.p,{children:["Our API offers different models with varying levels of complexity and generality. The most capable models, such as ",e.jsx(t.code,{children:"gpt-4"}),", can generate more complex and diverse completions, but they also take longer to process your query.\nModels such as ",e.jsx(t.code,{children:"gpt-4o-mini"}),", can generate faster and cheaper chat completions, but they may generate results that are less accurate or relevant for your query. You can choose the model that best suits your use case and the trade-off between speed, cost, and quality."]}),"\n",e.jsx(t.h4,{children:"Number of completion tokens"}),"\n",e.jsx(t.p,{children:"Requesting a large amount of generated tokens completions can lead to increased latencies:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Lower max tokens"}),": for requests with a similar token generation count, those that have a lower ",e.jsx(t.code,{children:"max_tokens"})," parameter incur less latency."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Include stop sequences"}),": to prevent generating unneeded tokens, add a stop sequence. For example, you can use stop sequences to generate a list with a specific number of items. In this case, by using ",e.jsx(t.code,{children:"11."})," as a stop sequence, you can generate a list with only 10 items, since the completion will stop when ",e.jsx(t.code,{children:"11."})," is reached. ",e.jsx(t.a,{href:"https://help.openai.com/en/articles/5072263-how-do-i-use-stop-sequences",children:"Read our help article on stop sequences"})," for more context on how you can do this."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Generate fewer completions"}),": lower the values of ",e.jsx(t.code,{children:"n"})," and ",e.jsx(t.code,{children:"best_of"})," when possible where ",e.jsx(t.code,{children:"n"})," refers to how many completions to generate for each prompt and ",e.jsx(t.code,{children:"best_of"})," is used to represent the result with the highest log probability per token."]}),"\n"]}),"\n",e.jsxs(t.p,{children:["If ",e.jsx(t.code,{children:"n"})," and ",e.jsx(t.code,{children:"best_of"})," both equal 1 (which is the default), the number of generated tokens will be at most, equal to ",e.jsx(t.code,{children:"max_tokens"}),"."]}),"\n",e.jsxs(t.p,{children:["If ",e.jsx(t.code,{children:"n"})," (the number of completions returned) or ",e.jsx(t.code,{children:"best_of"})," (the number of completions generated for consideration) are set to ",e.jsx(t.code,{children:"> 1"}),", each request will create multiple outputs. Here, you can consider the number of generated tokens as ",e.jsx(t.code,{children:"[ max_tokens * max (n, best_of) ]"})]}),"\n",e.jsx(t.h4,{children:"Streaming"}),"\n",e.jsxs(t.p,{children:["Setting ",e.jsx(t.code,{children:"stream: true"})," in a request makes the model start returning tokens as soon as they are available, instead of waiting for the full sequence of tokens to be generated. It does not change the time to get all the tokens, but it reduces the time for first token for an application where we want to show partial progress or are going to stop generations. This can be a better user experience and a UX improvement so it’s worth experimenting with streaming."]}),"\n",e.jsx(t.h4,{children:"Infrastructure"}),"\n",e.jsx(t.p,{children:"Our servers are currently located in the US. While we hope to have global redundancy in the future, in the meantime you could consider locating the relevant parts of your infrastructure in the US to minimize the roundtrip time between your servers and the OpenAI servers."}),"\n",e.jsx(t.h4,{children:"Batching"}),"\n",e.jsxs(t.p,{children:["Depending on your use case, batching ",e.jsx("i",{children:"may help"}),". If you are sending multiple requests to the same endpoint, you can ",e.jsx(t.a,{href:"/docs/guides/rate-limits/batching-requests",children:"batch the prompts"})," to be sent in the same request. This will reduce the number of requests you need to make. The prompt parameter can hold up to 20 unique prompts. We advise you to test out this method and see if it helps. In some cases, you may end up increasing the number of generated tokens which will slow the response time."]}),"\n",e.jsx(t.h2,{children:"Managing costs"}),"\n",e.jsxs(t.p,{children:["To monitor your costs, you can set a ",e.jsx(t.a,{href:"/account/limits",children:"notification threshold"})," in your account to receive an email alert once you pass a certain usage threshold. You can also set a ",e.jsx(t.a,{href:"/account/limits",children:"monthly budget"}),". Please be mindful of the potential for a monthly budget to cause disruptions to your application/users. Use the ",e.jsx(t.a,{href:"/account/usage",children:"usage tracking dashboard"})," to monitor your token usage during the current and past billing cycles."]}),"\n",e.jsx(t.h3,{children:"Text generation"}),"\n",e.jsxs(t.p,{children:["One of the challenges of moving your prototype into production is budgeting for the costs associated with running your application. OpenAI offers a ",e.jsx(t.a,{href:"https://openai.com/api/pricing/",children:"pay-as-you-go pricing model"}),", with prices per 1,000 tokens (roughly equal to 750 words). To estimate your costs, you will need to project the token utilization. Consider factors such as traffic levels, the frequency with which users will interact with your application, and the amount of data you will be processing."]}),"\n",e.jsxs(t.p,{children:[e.jsx(t.strong,{children:"One useful framework for thinking about reducing costs is to consider costs as a function of the number of tokens and the cost per token."})," There are two potential avenues for reducing costs using this framework. First, you could work to reduce the cost per token by switching to smaller models for some tasks in order to reduce costs. Alternatively, you could try to reduce the number of tokens required. There are a few ways you could do this, such as by using shorter prompts, ",e.jsx(t.a,{href:"/docs/guides/fine-tuning",children:"fine-tuning"})," models, or caching common user queries so that they don't need to be processed repeatedly."]}),"\n",e.jsxs(t.p,{children:["You can experiment with our interactive ",e.jsx(t.a,{href:"/tokenizer",children:"tokenizer tool"})," to help you estimate costs. The API and playground also returns token counts as part of the response. Once you’ve got things working with our most capable model, you can see if the other models can produce the same results with lower latency and costs. Learn more in our ",e.jsx(t.a,{href:"https://help.openai.com/en/articles/6614209-how-do-i-check-my-token-usage",children:"token usage help article"}),"."]}),"\n",e.jsx(t.h2,{children:"MLOps strategy"}),"\n",e.jsx(t.p,{children:"As you move your prototype into production, you may want to consider developing an MLOps strategy. MLOps (machine learning operations) refers to the process of managing the end-to-end life cycle of your machine learning models, including any models you may be fine-tuning using our API. There are a number of areas to consider when designing your MLOps strategy. These include"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Data and model management: managing the data used to train or fine-tune your model and tracking versions and changes."}),"\n",e.jsx(t.li,{children:"Model monitoring: tracking your model's performance over time and detecting any potential issues or degradation."}),"\n",e.jsx(t.li,{children:"Model retraining: ensuring your model stays up to date with changes in data or evolving requirements and retraining or fine-tuning it as needed."}),"\n",e.jsx(t.li,{children:"Model deployment: automating the process of deploying your model and related artifacts into production."}),"\n"]}),"\n",e.jsx(t.p,{children:"Thinking through these aspects of your application will help ensure your model stays relevant and performs well over time."}),"\n",e.jsx(t.h2,{children:"Security and compliance"}),"\n",e.jsxs(t.p,{children:["As you move your prototype into production, you will need to assess and address any security and compliance requirements that may apply to your application. This will involve examining the data you are handling, understanding how our API processes data, and determining what regulations you must adhere to. Our ",e.jsx(t.a,{href:"https://www.openai.com/security",children:"security practices"})," and ",e.jsx(t.a,{href:"https://trust.openai.com/",children:"trust and compliance portal"})," provide our most comprehensive and up-to-date documentation. For reference, here is our ",e.jsx(t.a,{href:"https://openai.com/privacy/",children:"Privacy Policy"})," and ",e.jsx(t.a,{href:"https://openai.com/api/policies/terms/",children:"Terms of Use"}),"."]}),"\n",e.jsx(t.p,{children:"Some common areas you'll need to consider include data storage, data transmission, and data retention. You might also need to implement data privacy protections, such as encryption or anonymization where possible. In addition, you should follow best practices for secure coding, such as input sanitization and proper error handling."}),"\n",e.jsx(t.h3,{children:"Safety best practices"}),"\n",e.jsxs(t.p,{children:["When creating your application with our API, consider our ",e.jsx(t.a,{href:"/docs/guides/safety-best-practices",children:"safety best practices"})," to ensure your application is safe and successful. These recommendations highlight the importance of testing the product extensively, being proactive about addressing potential issues, and limiting opportunities for misuse."]}),"\n",e.jsx(t.h2,{children:"Business considerations"}),"\n",e.jsx(t.p,{children:"As projects using AI move from prototype to production, it is important to consider how to build a great product with AI and how that ties back to your core business. We certainly don't have all the answers but a great starting place is a talk from our Developer Day where we dive into this with some of our customers:"}),"\n",e.jsx("iframe",{width:"100%",height:"315",src:"https://www.youtube-nocookie.com/embed/knHW-p31R0c?si=g0ddoMoUykjclH4k",title:"YouTube video player",frameBorder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",allowFullScreen:!0})]})}function xt(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(mt,n)})):mt(n)}function gt(n){const t=Object.assign({p:"p",h2:"h2",h3:"h3",ul:"ul",li:"li",a:"a",h4:"h4",table:"table",thead:"thead",tr:"tr",th:"th",tbody:"tbody",td:"td",strong:"strong",pre:"pre",code:"code"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(I,{level:1,children:"Prompt engineering"}),"\n",e.jsx(t.p,{children:"This guide shares strategies and tactics for getting better results from large language models (sometimes referred to as GPT models) like GPT-4o. The methods described here can sometimes be deployed in combination for greater effect. We encourage experimentation to find the methods that work best for you."}),"\n",e.jsx(t.p,{children:"You can also explore example prompts which showcase what our models are capable of:"}),"\n",e.jsx(A,{to:"/examples",children:e.jsx(C,{icon:e.jsx(Sn,{}),color:"green",title:"Prompt examples",className:"mt-6",children:e.jsx(t.p,{children:"Explore prompt examples to learn what GPT models can do"})})}),"\n",e.jsx(t.h2,{children:"Six strategies for getting better results"}),"\n",e.jsx(t.h3,{children:"Write clear instructions"}),"\n",e.jsx(t.p,{children:"These models can’t read your mind. If outputs are too long, ask for brief replies. If outputs are too simple, ask for expert-level writing. If you dislike the format, demonstrate the format you’d like to see. The less the model has to guess at what you want, the more likely you’ll get it."}),"\n",e.jsx(t.p,{children:"Tactics:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:e.jsx(t.a,{href:"/docs/guides/prompt-engineering/tactic-include-details-in-your-query-to-get-more-relevant-answers",children:"Include details in your query to get more relevant answers"})}),"\n",e.jsx(t.li,{children:e.jsx(t.a,{href:"/docs/guides/prompt-engineering/tactic-ask-the-model-to-adopt-a-persona",children:"Ask the model to adopt a persona"})}),"\n",e.jsx(t.li,{children:e.jsx(t.a,{href:"/docs/guides/prompt-engineering/tactic-use-delimiters-to-clearly-indicate-distinct-parts-of-the-input",children:"Use delimiters to clearly indicate distinct parts of the input"})}),"\n",e.jsx(t.li,{children:e.jsx(t.a,{href:"/docs/guides/prompt-engineering/tactic-specify-the-steps-required-to-complete-a-task",children:"Specify the steps required to complete a task"})}),"\n",e.jsx(t.li,{children:e.jsx(t.a,{href:"/docs/guides/prompt-engineering/tactic-provide-examples",children:"Provide examples"})}),"\n",e.jsx(t.li,{children:e.jsx(t.a,{href:"/docs/guides/prompt-engineering/tactic-specify-the-desired-length-of-the-output",children:"Specify the desired length of the output"})}),"\n"]}),"\n",e.jsx(t.h3,{children:"Provide reference text"}),"\n",e.jsx(t.p,{children:"Language models can confidently invent fake answers, especially when asked about esoteric topics or for citations and URLs. In the same way that a sheet of notes can help a student do better on a test, providing reference text to these models can help in answering with fewer fabrications."}),"\n",e.jsx(t.p,{children:"Tactics:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:e.jsx(t.a,{href:"/docs/guides/prompt-engineering/tactic-instruct-the-model-to-answer-using-a-reference-text",children:"Instruct the model to answer using a reference text"})}),"\n",e.jsx(t.li,{children:e.jsx(t.a,{href:"/docs/guides/prompt-engineering/tactic-instruct-the-model-to-answer-with-citations-from-a-reference-text",children:"Instruct the model to answer with citations from a reference text"})}),"\n"]}),"\n",e.jsx(t.h3,{children:"Split complex tasks into simpler subtasks"}),"\n",e.jsx(t.p,{children:"Just as it is good practice in software engineering to decompose a complex system into a set of modular components, the same is true of tasks submitted to a language model. Complex tasks tend to have higher error rates than simpler tasks. Furthermore, complex tasks can often be re-defined as a workflow of simpler tasks in which the outputs of earlier tasks are used to construct the inputs to later tasks."}),"\n",e.jsx(t.p,{children:"Tactics:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:e.jsx(t.a,{href:"/docs/guides/prompt-engineering/tactic-use-intent-classification-to-identify-the-most-relevant-instructions-for-a-user-query",children:"Use intent classification to identify the most relevant instructions for a user query"})}),"\n",e.jsx(t.li,{children:e.jsx(t.a,{href:"/docs/guides/prompt-engineering/tactic-for-dialogue-applications-that-require-very-long-conversations-summarize-or-filter-previous-dialogue",children:"For dialogue applications that require very long conversations, summarize or filter previous dialogue"})}),"\n",e.jsx(t.li,{children:e.jsx(t.a,{href:"/docs/guides/prompt-engineering/tactic-summarize-long-documents-piecewise-and-construct-a-full-summary-recursively",children:"Summarize long documents piecewise and construct a full summary recursively"})}),"\n"]}),"\n",e.jsx(t.h3,{children:'Give the model time to "think"'}),"\n",e.jsx(t.p,{children:'If asked to multiply 17 by 28, you might not know it instantly, but can still work it out with time. Similarly, models make more reasoning errors when trying to answer right away, rather than taking time to work out an answer. Asking for a "chain of thought" before an answer can help the model reason its way toward correct answers more reliably.'}),"\n",e.jsx(t.p,{children:"Tactics:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:e.jsx(t.a,{href:"/docs/guides/prompt-engineering/tactic-instruct-the-model-to-work-out-its-own-solution-before-rushing-to-a-conclusion",children:"Instruct the model to work out its own solution before rushing to a conclusion"})}),"\n",e.jsx(t.li,{children:e.jsx(t.a,{href:"/docs/guides/prompt-engineering/tactic-use-inner-monologue-or-a-sequence-of-queries-to-hide-the-model-s-reasoning-process",children:"Use inner monologue or a sequence of queries to hide the model's reasoning process"})}),"\n",e.jsx(t.li,{children:e.jsx(t.a,{href:"/docs/guides/prompt-engineering/tactic-ask-the-model-if-it-missed-anything-on-previous-passes",children:"Ask the model if it missed anything on previous passes"})}),"\n"]}),"\n",e.jsx(t.h3,{children:"Use external tools"}),"\n",e.jsx(t.p,{children:"Compensate for the weaknesses of the model by feeding it the outputs of other tools. For example, a text retrieval system (sometimes called RAG or retrieval augmented generation) can tell the model about relevant documents. A code execution engine like OpenAI's Code Interpreter can help the model do math and run code. If a task can be done more reliably or efficiently by a tool rather than by a language model, offload it to get the best of both."}),"\n",e.jsx(t.p,{children:"Tactics:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:e.jsx(t.a,{href:"/docs/guides/prompt-engineering/tactic-use-embeddings-based-search-to-implement-efficient-knowledge-retrieval",children:"Use embeddings-based search to implement efficient knowledge retrieval"})}),"\n",e.jsx(t.li,{children:e.jsx(t.a,{href:"/docs/guides/prompt-engineering/tactic-use-code-execution-to-perform-more-accurate-calculations-or-call-external-apis",children:"Use code execution to perform more accurate calculations or call external APIs"})}),"\n",e.jsx(t.li,{children:e.jsx(t.a,{href:"/docs/guides/prompt-engineering/tactic-give-the-model-access-to-specific-functions",children:"Give the model access to specific functions"})}),"\n"]}),"\n",e.jsx(t.h3,{children:"Test changes systematically"}),"\n",e.jsx(t.p,{children:'Improving performance is easier if you can measure it. In some cases a modification to a prompt will achieve better performance on a few isolated examples but lead to worse overall performance on a more representative set of examples. Therefore to be sure that a change is net positive to performance it may be necessary to define a comprehensive test suite (also known an as an "eval").'}),"\n",e.jsx(t.p,{children:"Tactic:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:e.jsx(t.a,{href:"/docs/guides/prompt-engineering/tactic-evaluate-model-outputs-with-reference-to-gold-standard-answers",children:"Evaluate model outputs with reference to gold-standard answers"})}),"\n"]}),"\n",e.jsx(t.h2,{children:"Tactics"}),"\n",e.jsx(t.p,{children:"Each of the strategies listed above can be instantiated with specific tactics. These tactics are meant to provide ideas for things to try. They are by no means fully comprehensive, and you should feel free to try creative ideas not represented here."}),"\n",e.jsx(t.h3,{children:"Strategy: Write clear instructions"}),"\n",e.jsx(t.h4,{children:"Tactic: Include details in your query to get more relevant answers"}),"\n",e.jsx(t.p,{children:"In order to get a highly relevant response, make sure that requests provide any important details or context. Otherwise you are leaving it up to the model to guess what you mean."}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{}),e.jsx(t.th,{})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.strong,{children:"Worse"})}),e.jsx(t.td,{children:e.jsx(t.strong,{children:"Better"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"How do I add numbers in Excel?"}),e.jsx(t.td,{children:'How do I add up a row of dollar amounts in Excel? I want to do this automatically for a whole sheet of rows with all the totals ending up on the right in a column called "Total".'})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"Who’s president?"}),e.jsx(t.td,{children:"Who was the president of Mexico in 2021, and how frequently are elections held?"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"Write code to calculate the Fibonacci sequence."}),e.jsx(t.td,{children:"Write a TypeScript function to efficiently calculate the Fibonacci sequence. Comment the code liberally to explain what each piece does and why it's written that way."})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"Summarize the meeting notes."}),e.jsx(t.td,{children:"Summarize the meeting notes in a single paragraph. Then write a markdown list of the speakers and each of their key points. Finally, list the next steps or action items suggested by the speakers, if any."})]})]})]}),"\n",e.jsx(t.h4,{children:"Tactic: Ask the model to adopt a persona"}),"\n",e.jsx(t.p,{children:"The system message can be used to specify the persona used by the model in its replies."}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:"SYSTEM: When I ask for help to write something, you will reply with a document that contains at least one joke or playful comment in every paragraph.\n\nUSER: Write a thank you note to my steel bolt vendor for getting the delivery in on time and in short notice. This made it possible for us to deliver an important order.\n"})}),"\n",e.jsx(t.h4,{children:"Tactic: Use delimiters to clearly indicate distinct parts of the input"}),"\n",e.jsx(t.p,{children:"Delimiters like triple quotation marks, XML tags, section titles, etc. can help demarcate sections of text to be treated differently."}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:'USER: Summarize the text delimited by triple quotes with a haiku.\n\n"""insert text here"""\n'})}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:"SYSTEM: You will be provided with a pair of articles (delimited with XML tags) about the same topic. First summarize the arguments of each article. Then indicate which of them makes a better argument and explain why.\n\nUSER: <article> insert first article here </article>\n\n<article> insert second article here </article>\n"})}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:"SYSTEM: You will be provided with a thesis abstract and a suggested title for it. The thesis title should give the reader a good idea of the topic of the thesis but should also be eye-catching. If the title does not meet these criteria, suggest 5 alternatives.\n\nUSER: Abstract: insert abstract here\n\nTitle: insert title here\n"})}),"\n",e.jsx(t.p,{children:"For straightforward tasks such as these, using delimiters might not make a difference in the output quality. However, the more complex a task is the more important it is to disambiguate task details. Don’t make the model work to understand exactly what you are asking of them."}),"\n",e.jsx(t.h4,{children:"Tactic: Specify the steps required to complete a task"}),"\n",e.jsx(t.p,{children:"Some tasks are best specified as a sequence of steps. Writing the steps out explicitly can make it easier for the model to follow them."}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:'SYSTEM: Use the following step-by-step instructions to respond to user inputs.\n\nStep 1 - The user will provide you with text in triple quotes. Summarize this text in one sentence with a prefix that says "Summary: ".\n\nStep 2 - Translate the summary from Step 1 into Spanish, with a prefix that says "Translation: ".\n\nUSER: """insert text here"""\n'})}),"\n",e.jsx(t.h4,{children:"Tactic: Provide examples"}),"\n",e.jsx(t.p,{children:'Providing general instructions that apply to all examples is generally more efficient than demonstrating all permutations of a task by example, but in some cases providing examples may be easier. For example, if you intend for the model to copy a particular style of responding to user queries which is difficult to describe explicitly. This is known as "few-shot" prompting.'}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:"SYSTEM: Answer in a consistent style.\n\nUSER: Teach me about patience.\n\nASSISTANT: The river that carves the deepest valley flows from a modest spring; the grandest symphony originates from a single note; the most intricate tapestry begins with a solitary thread.\n\nUSER: Teach me about the ocean.\n"})}),"\n",e.jsx(t.h4,{children:"Tactic: Specify the desired length of the output"}),"\n",e.jsx(t.p,{children:"You can ask the model to produce outputs that are of a given target length. The targeted output length can be specified in terms of the count of words, sentences, paragraphs, bullet points, etc. Note however that instructing the model to generate a specific number of words does not work with high precision. The model can more reliably generate outputs with a specific number of paragraphs or bullet points."}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:'USER: Summarize the text delimited by triple quotes in about 50 words.\n\n"""insert text here"""\n'})}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:'USER: Summarize the text delimited by triple quotes in 2 paragraphs.\n\n"""insert text here"""\n'})}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:'USER: Summarize the text delimited by triple quotes in 3 bullet points.\n\n"""insert text here"""\n'})}),"\n",e.jsx(t.h3,{children:"Strategy: Provide reference text"}),"\n",e.jsx(t.h4,{children:"Tactic: Instruct the model to answer using a reference text"}),"\n",e.jsx(t.p,{children:"If we can provide a model with trusted information that is relevant to the current query, then we can instruct the model to use the provided information to compose its answer."}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:'SYSTEM: Use the provided articles delimited by triple quotes to answer questions. If the answer cannot be found in the articles, write "I could not find an answer."\n\nUSER: <insert articles, each delimited by triple quotes>\n\nQuestion: <insert question here>\n'})}),"\n",e.jsxs(t.p,{children:["Given that all models have limited context windows, we need some way to dynamically lookup information that is relevant to the question being asked. ",e.jsx(t.a,{href:"https://platform.openai.com/docs/guides/embeddings/what-are-embeddings",children:"Embeddings"})," can be used to implement efficient knowledge retrieval. See the tactic ",e.jsx(t.a,{href:"/docs/guides/prompt-engineering/tactic-use-embeddings-based-search-to-implement-efficient-knowledge-retrieval",children:'"Use embeddings-based search to implement efficient knowledge retrieval"'})," for more details on how to implement this."]}),"\n",e.jsx(t.h4,{children:"Tactic: Instruct the model to answer with citations from a reference text"}),"\n",e.jsx(t.p,{children:"If the input has been supplemented with relevant knowledge, it's straightforward to request that the model add citations to its answers by referencing passages from provided documents. Note that citations in the output can then be verified programmatically by string matching within the provided documents."}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:'SYSTEM: You will be provided with a document delimited by triple quotes and a question. Your task is to answer the question using only the provided document and to cite the passage(s) of the document used to answer the question. If the document does not contain the information needed to answer this question then simply write: "Insufficient information." If an answer to the question is provided, it must be annotated with a citation. Use the following format for to cite relevant passages ({"citation": …}).\n\nUSER: """<insert document here>"""\n\nQuestion: <insert question here>\n'})}),"\n",e.jsx(t.h3,{children:"Strategy: Split complex tasks into simpler subtasks"}),"\n",e.jsx(t.h4,{children:"Tactic: Use intent classification to identify the most relevant instructions for a user query"}),"\n",e.jsxs(t.p,{children:["For tasks in which lots of independent sets of instructions are needed to handle different cases, it can be beneficial to first classify the type of query and to use that classification to determine which instructions are needed. This can be achieved by defining fixed categories and hardcoding instructions that are relevant for handling tasks in a given category. This process can also be applied recursively to decompose a task into a sequence of stages. The advantage of this approach is that each query will contain only those instructions that are required to perform the next stage of a task which can result in lower error rates compared to using a single query to perform the whole task. This can also result in lower costs since larger prompts cost more to run (",e.jsx(t.a,{href:"https://openai.com/pricing",children:"see pricing information"}),")."]}),"\n",e.jsx(t.p,{children:"Suppose for example that for a customer service application, queries could be usefully classified as follows:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:"SYSTEM: You will be provided with customer service queries. Classify each query into a primary category and a secondary category. Provide your output in json format with the keys: primary and secondary.\n\nPrimary categories: Billing, Technical Support, Account Management, or General Inquiry.\n\nBilling secondary categories:\n- Unsubscribe or upgrade\n- Add a payment method\n- Explanation for charge\n- Dispute a charge\n\nTechnical Support secondary categories:\n- Troubleshooting\n- Device compatibility\n- Software updates\n\nAccount Management secondary categories:\n- Password reset\n- Update personal information\n- Close account\n- Account security\n\nGeneral Inquiry secondary categories:\n- Product information\n- Pricing\n- Feedback\n- Speak to a human\n\nUSER: I need to get my internet working again.\n"})}),"\n",e.jsx(t.p,{children:'Based on the classification of the customer query, a set of more specific instructions can be provided to a model for it to handle next steps. For example, suppose the customer requires help with "troubleshooting".'}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:'SYSTEM: You will be provided with customer service inquiries that require troubleshooting in a technical support context. Help the user by:\n\n- Ask them to check that all cables to/from the router are connected. Note that it is common for cables to come loose over time.\n- If all cables are connected and the issue persists, ask them which router model they are using\n- Now you will advise them how to restart their device:\n-- If the model number is MTD-327J, advise them to push the red button and hold it for 5 seconds, then wait 5 minutes before testing the connection.\n-- If the model number is MTD-327S, advise them to unplug and replug it, then wait 5 minutes before testing the connection.\n- If the customer\'s issue persists after restarting the device and waiting 5 minutes, connect them to IT support by outputting {"IT support requested"}.\n- If the user starts asking questions that are unrelated to this topic then confirm if they would like to end the current chat about troubleshooting and classify their request according to the following scheme:\n\n<insert primary/secondary classification scheme from above here>\n\nUSER: I need to get my internet working again.\n'})}),"\n",e.jsx(t.p,{children:"Notice that the model has been instructed to emit special strings to indicate when the state of the conversation changes. This enables us to turn our system into a state machine where the state determines which instructions are injected. By keeping track of state, what instructions are relevant at that state, and also optionally what state transitions are allowed from that state, we can put guardrails around the user experience that would be hard to achieve with a less structured approach."}),"\n",e.jsx(t.h4,{children:"Tactic: For dialogue applications that require very long conversations, summarize or filter previous dialogue"}),"\n",e.jsx(t.p,{children:"Since models have a fixed context length, dialogue between a user and an assistant in which the entire conversation is included in the context window cannot continue indefinitely."}),"\n",e.jsx(t.p,{children:"There are various workarounds to this problem, one of which is to summarize previous turns in the conversation. Once the size of the input reaches a predetermined threshold length, this could trigger a query that summarizes part of the conversation and the summary of the prior conversation could be included as part of the system message. Alternatively, prior conversation could be summarized asynchronously in the background throughout the entire conversation."}),"\n",e.jsxs(t.p,{children:["An alternative solution is to dynamically select previous parts of the conversation that are most relevant to the current query. See the tactic ",e.jsx(t.a,{href:"/docs/guides/prompt-engineering/tactic-use-embeddings-based-search-to-implement-efficient-knowledge-retrieval",children:'"Use embeddings-based search to implement efficient knowledge retrieval"'}),"."]}),"\n",e.jsx(t.h4,{children:"Tactic: Summarize long documents piecewise and construct a full summary recursively"}),"\n",e.jsx(t.p,{children:"Since models have a fixed context length, they cannot be used to summarize a text longer than the context length minus the length of the generated summary in a single query."}),"\n",e.jsxs(t.p,{children:["To summarize a very long document such as a book we can use a sequence of queries to summarize each section of the document. Section summaries can be concatenated and summarized producing summaries of summaries. This process can proceed recursively until an entire document is summarized. If it’s necessary to use information about earlier sections in order to make sense of later sections, then a further trick that can be useful is to include a running summary of the text that precedes any given point in the book while summarizing content at that point. The effectiveness of this procedure for summarizing books has been studied in previous ",e.jsx(t.a,{href:"https://openai.com/research/summarizing-books",children:"research"})," by OpenAI using variants of GPT-3."]}),"\n",e.jsx(t.h3,{children:'Strategy: Give models time to "think"'}),"\n",e.jsx(t.h4,{children:"Tactic: Instruct the model to work out its own solution before rushing to a conclusion"}),"\n",e.jsx(t.p,{children:"Sometimes we get better results when we explicitly instruct the model to reason from first principles before coming to a conclusion. Suppose for example we want a model to evaluate a student’s solution to a math problem. The most obvious way to approach this is to simply ask the model if the student's solution is correct or not."}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:"SYSTEM: Determine if the student's solution is correct or not.\n\nUSER: Problem Statement: I'm building a solar power installation and I need help working out the financials.\n- Land costs $100 / square foot\n- I can buy solar panels for $250 / square foot\n- I negotiated a contract for maintenance that will cost me a flat $100k per year, and an additional $10 / square foot\nWhat is the total cost for the first year of operations as a function of the number of square feet.\n\nStudent's Solution: Let x be the size of the installation in square feet.\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 100x\nTotal cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n\nASSISTANT: The student's solution is correct.\n"})}),"\n",e.jsx(t.p,{children:"But the student's solution is actually not correct! We can get the model to successfully notice this by prompting the model to generate its own solution first."}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:"SYSTEM: First work out your own solution to the problem. Then compare your solution to the student's solution and evaluate if the student's solution is correct or not. Don't decide if the student's solution is correct until you have done the problem yourself.\n\nUSER: Problem Statement: I'm building a solar power installation and I need help working out the financials.\n- Land costs $100 / square foot\n- I can buy solar panels for $250 / square foot\n- I negotiated a contract for maintenance that will cost me a flat $100k per year, and an additional $10 / square foot\nWhat is the total cost for the first year of operations as a function of the number of square feet.\n\nStudent's Solution: Let x be the size of the installation in square feet.\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 100x\nTotal cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n\nASSISTANT: Let x be the size of the installation in square feet.\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 10x\nTotal cost: 100x + 250x + 100,000 + 10x = 360x + 100,000\n\nThe student's solution is incorrect. They made an error in the maintenance cost calculation, using 100x instead of 10x. The correct total cost for the first year of operations is 360x + 100,000.\n"})}),"\n",e.jsx(t.h4,{children:"Tactic: Use inner monologue or a sequence of queries to hide the model's reasoning process"}),"\n",e.jsx(t.p,{children:"The previous tactic demonstrates that it is sometimes important for the model to reason in detail about a problem before answering a specific question. For some applications, the reasoning process that a model uses to arrive at a final answer would be inappropriate to share with the user. For example, in tutoring applications we may want to encourage students to work out their own answers, but a model’s reasoning process about the student’s solution could reveal the answer to the student."}),"\n",e.jsx(t.p,{children:"Inner monologue is a tactic that can be used to mitigate this. The idea of inner monologue is to instruct the model to put parts of the output that are meant to be hidden from the user into a structured format that makes parsing them easy. Then before presenting the output to the user, the output is parsed and only part of the output is made visible."}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:'SYSTEM: Follow these steps to answer the user queries.\n\nStep 1 - First work out your own solution to the problem. Don\'t rely on the student\'s solution since it may be incorrect. Enclose all your work for this step within triple quotes (""").\n\nStep 2 - Compare your solution to the student\'s solution and evaluate if the student\'s solution is correct or not. Enclose all your work for this step within triple quotes (""").\n\nStep 3 - If the student made a mistake, determine what hint you could give the student without giving away the answer. Enclose all your work for this step within triple quotes (""").\n\nStep 4 - If the student made a mistake, provide the hint from the previous step to the student (outside of triple quotes). Instead of writing "Step 4 - ..." write "Hint:".\n\nUSER: Problem Statement: <insert problem statement>\n\nStudent Solution: <insert student solution>\n'})}),"\n",e.jsx(t.p,{children:"Alternatively, this can be achieved with a sequence of queries in which all except the last have their output hidden from the end user."}),"\n",e.jsx(t.p,{children:"First, we can ask the model to solve the problem on its own. Since this initial query doesn't require the student’s solution, it can be omitted. This provides the additional advantage that there is no chance that the model’s solution will be biased by the student’s attempted solution."}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:"USER: <insert problem statement>\n"})}),"\n",e.jsx(t.p,{children:"Next, we can have the model use all available information to assess the correctness of the student’s solution."}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:'SYSTEM: Compare your solution to the student\'s solution and evaluate if the student\'s solution is correct or not.\n\nUSER: Problem statement: """<insert problem statement>"""\n\nYour solution: """<insert model generated solution>"""\n\nStudent’s solution: """<insert student\'s solution>"""\n'})}),"\n",e.jsx(t.p,{children:"Finally, we can let the model use its own analysis to construct a reply in the persona of a helpful tutor."}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:'SYSTEM: You are a math tutor. If the student made an error, offer a hint to the student in a way that does not reveal the answer. If the student did not make an error, simply offer them an encouraging comment.\n\nUSER: Problem statement: """<insert problem statement>"""\n\nYour solution: """<insert model generated solution>"""\n\nStudent’s solution: """<insert student\'s solution>"""\n\nAnalysis: """<insert model generated analysis from previous step>"""\n'})}),"\n",e.jsx(t.h4,{children:"Tactic: Ask the model if it missed anything on previous passes"}),"\n",e.jsx(t.p,{children:"Suppose that we are using a model to list excerpts from a source which are relevant to a particular question. After listing each excerpt the model needs to determine if it should start writing another or if it should stop. If the source document is large, it is common for a model to stop too early and fail to list all relevant excerpts. In that case, better performance can often be obtained by prompting the model with followup queries to find any excerpts it missed on previous passes."}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:'SYSTEM: You will be provided with a document delimited by triple quotes. Your task is to select excerpts which pertain to the following question: "What significant paradigm shifts have occurred in the history of artificial intelligence."\n\nEnsure that excerpts contain all relevant context needed to interpret them - in other words don\'t extract small snippets that are missing important context. Provide output in JSON format as follows:\n\n[{"excerpt": "..."},\n...\n{"excerpt": "..."}]\n\nUSER: """<insert document here>"""\n\nASSISTANT: [{"excerpt": "the model writes an excerpt here"},\n...\n{"excerpt": "the model writes another excerpt here"}]\n\nUSER: Are there more relevant excerpts? Take care not to repeat excerpts. Also ensure that excerpts contain all relevant context needed to interpret them - in other words don\'t extract small snippets that are missing important context.\n'})}),"\n",e.jsx(t.h3,{children:"Strategy: Use external tools"}),"\n",e.jsx(t.h4,{children:"Tactic: Use embeddings-based search to implement efficient knowledge retrieval"}),"\n",e.jsx(t.p,{children:"A model can leverage external sources of information if provided as part of its input. This can help the model to generate more informed and up-to-date responses. For example, if a user asks a question about a specific movie, it may be useful to add high quality information about the movie (e.g. actors, director, etc…) to the model’s input. Embeddings can be used to implement efficient knowledge retrieval, so that relevant information can be added to the model input dynamically at run-time."}),"\n",e.jsx(t.p,{children:"A text embedding is a vector that can measure the relatedness between text strings. Similar or relevant strings will be closer together than unrelated strings. This fact, along with the existence of fast vector search algorithms means that embeddings can be used to implement efficient knowledge retrieval. In particular, a text corpus can be split up into chunks, and each chunk can be embedded and stored. Then a given query can be embedded and vector search can be performed to find the embedded chunks of text from the corpus that are most related to the query (i.e. closest together in the embedding space)."}),"\n",e.jsxs(t.p,{children:["Example implementations can be found in the ",e.jsx(t.a,{href:"https://cookbook.openai.com/examples/vector_databases/readme",children:"OpenAI Cookbook"}),". See the tactic ",e.jsx(t.a,{href:"/docs/guides/prompt-engineering/tactic-instruct-the-model-to-answer-using-a-reference-text",children:"“Instruct the model to use retrieved knowledge to answer queries”"})," for an example of how to use knowledge retrieval to minimize the likelihood that a model will make up incorrect facts."]}),"\n",e.jsx(t.h4,{children:"Tactic: Use code execution to perform more accurate calculations or call external APIs"}),"\n",e.jsx(t.p,{children:"Language models cannot be relied upon to perform arithmetic or long calculations accurately on their own. In cases where this is needed, a model can be instructed to write and run code instead of making its own calculations. In particular, a model can be instructed to put code that is meant to be run into a designated format such as triple backtick. After an output is produced, the code can be extracted and run. Finally, if necessary, the output from the code execution engine (i.e. Python interpreter) can be provided as an input to the model for the next query."}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:"SYSTEM: You can write and execute Python code by enclosing it in triple backticks, e.g. ```code goes here```. Use this to perform calculations.\n\nUSER: Find all real-valued roots of the following polynomial: 3*x**5 - 5*x**4 - 3*x**3 - 7*x - 10.\n"})}),"\n",e.jsx(t.p,{children:"Another good use case for code execution is calling external APIs. If a model is instructed in the proper use of an API, it can write code that makes use of it. A model can be instructed in how to use an API by providing it with documentation and/or code samples showing how to use the API."}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:'SYSTEM: You can write and execute Python code by enclosing it in triple backticks. Also note that you have access to the following module to help users send messages to their friends:\n\n```python\nimport message\nmessage.write(to="John", message="Hey, want to meetup after work?")```\n'})}),"\n",e.jsx(t.p,{children:e.jsx(t.strong,{children:"WARNING: Executing code produced by a model is not inherently safe and precautions should be taken in any application that seeks to do this. In particular, a sandboxed code execution environment is needed to limit the harm that untrusted code could cause."})}),"\n",e.jsx(t.h4,{children:"Tactic: Give the model access to specific functions"}),"\n",e.jsxs(t.p,{children:["The Chat Completions API allows passing a list of function descriptions in requests. This enables models to generate function arguments according to the provided schemas. Generated function arguments are returned by the API in JSON format and can be used to execute function calls. Output provided by function calls can then be fed back into a model in the following request to close the loop. This is the recommended way of using OpenAI models to call external functions. To learn more see the ",e.jsx(t.a,{href:"/docs/guides/function-calling",children:"function calling section"})," in our introductory text generation guide and more ",e.jsx(t.a,{href:"https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models",children:"function calling examples"})," in the OpenAI Cookbook."]}),"\n",e.jsx(t.h3,{children:"Strategy: Test changes systematically"}),"\n",e.jsx(t.p,{children:"Sometimes it can be hard to tell whether a change — e.g., a new instruction or a new design — makes your system better or worse. Looking at a few examples may hint at which is better, but with small sample sizes it can be hard to distinguish between a true improvement or random luck. Maybe the change helps performance on some inputs, but hurts performance on others."}),"\n",e.jsx(t.p,{children:'Evaluation procedures (or "evals") are useful for optimizing system designs. Good evals are:'}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Representative of real-world usage (or at least diverse)"}),"\n",e.jsx(t.li,{children:"Contain many test cases for greater statistical power (see table below for guidelines)"}),"\n",e.jsx(t.li,{children:"Easy to automate or repeat"}),"\n"]}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Difference to detect"}),e.jsx(t.th,{children:"Sample size needed for 95% confidence"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"30%"}),e.jsx(t.td,{children:"~10"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"10%"}),e.jsx(t.td,{children:"~100"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"3%"}),e.jsx(t.td,{children:"~1,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"1%"}),e.jsx(t.td,{children:"~10,000"})]})]})]}),"\n",e.jsxs(t.p,{children:["Evaluation of outputs can be done by computers, humans, or a mix. Computers can automate evals with objective criteria (e.g., questions with single correct answers) as well as some subjective or fuzzy criteria, in which model outputs are evaluated by other model queries. ",e.jsx(t.a,{href:"https://github.com/openai/evals",children:"OpenAI Evals"})," is an open-source software framework that provides tools for creating automated evals."]}),"\n",e.jsx(t.p,{children:"Model-based evals can be useful when there exists a range of possible outputs that would be considered equally high in quality (e.g. for questions with long answers). The boundary between what can be realistically evaluated with a model-based eval and what requires a human to evaluate is fuzzy and is constantly shifting as models become more capable. We encourage experimentation to figure out how well model-based evals can work for your use case."}),"\n",e.jsx(t.h4,{children:"Tactic: Evaluate model outputs with reference to gold-standard answers"}),"\n",e.jsx(t.p,{children:"Suppose it is known that the correct answer to a question should make reference to a specific set of known facts. Then we can use a model query to count how many of the required facts are included in the answer."}),"\n",e.jsx(t.p,{children:"For example, using the following system message:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:'SYSTEM: You will be provided with text delimited by triple quotes that is supposed to be the answer to a question. Check if the following pieces of information are directly contained in the answer:\n\n- Neil Armstrong was the first person to walk on the moon.\n- The date Neil Armstrong first walked on the moon was July 21, 1969.\n\nFor each of these points perform the following steps:\n\n1 - Restate the point.\n2 - Provide a citation from the answer which is closest to this point.\n3 - Consider if someone reading the citation who doesn\'t know the topic could directly infer the point. Explain why or why not before making up your mind.\n4 - Write "yes" if the answer to 3 was yes, otherwise write "no".\n\nFinally, provide a count of how many "yes" answers there are. Provide this count as {"count": <insert count here>}.\n'})}),"\n",e.jsx(t.p,{children:"Here's an example input where both points are satisfied:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:'SYSTEM: <insert system message above>\n\nUSER: """Neil Armstrong is famous for being the first human to set foot on the Moon. This historic event took place on July 21, 1969, during the Apollo 11 mission."""\n'})}),"\n",e.jsx(t.p,{children:"Here's an example input where only one point is satisfied:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:'SYSTEM: <insert system message above>\n\nUSER: """Neil Armstrong made history when he stepped off the lunar module, becoming the first person to walk on the moon."""\n'})}),"\n",e.jsx(t.p,{children:"Here's an example input where none are satisfied:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:'SYSTEM: <insert system message above>\n\nUSER: """In the summer of \'69, a voyage grand,\nApollo 11, bold as legend\'s hand.\nArmstrong took a step, history unfurled,\n"One small step," he said, for a new world."""\n'})}),"\n",e.jsx(t.p,{children:"There are many possible variants on this type of model-based eval. Consider the following variation which tracks the kind of overlap between the candidate answer and the gold-standard answer, and also tracks whether the candidate answer contradicts any part of the gold-standard answer."}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:'SYSTEM: Use the following steps to respond to user inputs. Fully restate each step before proceeding. i.e. "Step 1: Reason...".\n\nStep 1: Reason step-by-step about whether the information in the submitted answer compared to the expert answer is either: disjoint, equal, a subset, a superset, or overlapping (i.e. some intersection but not subset/superset).\n\nStep 2: Reason step-by-step about whether the submitted answer contradicts any aspect of the expert answer.\n\nStep 3: Output a JSON object structured like: {"type_of_overlap": "disjoint" or "equal" or "subset" or "superset" or "overlapping", "contradiction": true or false}\n'})}),"\n",e.jsx(t.p,{children:"Here's an example input with a substandard answer which nonetheless does not contradict the expert answer:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:'SYSTEM: <insert system message above>\n\nUSER: Question: """What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time."""\n\nSubmitted Answer: """Didn\'t he walk on the moon or something?"""\n\nExpert Answer: """Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969."""\n'})}),"\n",e.jsx(t.p,{children:"Here's an example input with answer that directly contradicts the expert answer:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:'SYSTEM: <insert system message above>\n\nUSER: Question: """What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time."""\n\nSubmitted Answer: """On the 21st of July 1969, Neil Armstrong became the second person to walk on the moon, following after Buzz Aldrin."""\n\nExpert Answer: """Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969."""\n'})}),"\n",e.jsx(t.p,{children:"Here's an example input with a correct answer that also provides a bit more detail than is necessary:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-chat",children:'SYSTEM: <insert system message above>\n\nUSER: Question: """What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time."""\n\nSubmitted Answer: """At approximately 02:56 UTC on July 21st 1969, Neil Armstrong became the first human to set foot on the lunar surface, marking a monumental achievement in human history."""\n\nExpert Answer: """Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969."""\n'})}),"\n",e.jsx(t.h2,{children:"Other resources"}),"\n",e.jsxs(t.p,{children:["For more inspiration, visit the ",e.jsx(t.a,{href:"https://cookbook.openai.com",children:"OpenAI Cookbook"}),", which contains example code and also links to third-party resources such as:"]}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:e.jsx(t.a,{href:"https://cookbook.openai.com/related_resources#prompting-libraries--tools",children:"Prompting libraries & tools"})}),"\n",e.jsx(t.li,{children:e.jsx(t.a,{href:"https://cookbook.openai.com/related_resources#prompting-guides",children:"Prompting guides"})}),"\n",e.jsx(t.li,{children:e.jsx(t.a,{href:"https://cookbook.openai.com/related_resources#video-courses",children:"Video courses"})}),"\n",e.jsx(t.li,{children:e.jsx(t.a,{href:"https://cookbook.openai.com/related_resources#papers-on-advanced-prompting-to-improve-reasoning",children:"Papers on advanced prompting to improve reasoning"})}),"\n"]})]})}function Li(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(gt,n)})):gt(n)}function ft(n){const t=Object.assign({h4:"h4",p:"p",a:"a",table:"table",thead:"thead",tr:"tr",th:"th",tbody:"tbody",td:"td",code:"code"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h4,{children:"Free tier rate limits"}),"\n",e.jsxs(t.p,{children:["This is a high level summary and there are per-model exceptions to these limits (e.g. some legacy models or models with larger context windows have different rate limits). To view the exact rate limits per model for your account, visit the ",e.jsx(t.a,{href:"/account/rate-limits",children:"limits"})," section of your account settings."]}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Model"}),e.jsx(t.th,{children:"RPM"}),e.jsx(t.th,{children:"RPD"}),e.jsx(t.th,{children:"TPM"}),e.jsx(t.th,{children:"Batch Queue Limit"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-3.5-turbo"})}),e.jsx(t.td,{children:"3"}),e.jsx(t.td,{children:"200"}),e.jsx(t.td,{children:"40,000"}),e.jsx(t.td,{children:"200,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-3-large"})}),e.jsx(t.td,{children:"3,000"}),e.jsx(t.td,{children:"200"}),e.jsx(t.td,{children:"1,000,000"}),e.jsx(t.td,{children:"3,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-3-small"})}),e.jsx(t.td,{children:"3,000"}),e.jsx(t.td,{children:"200"}),e.jsx(t.td,{children:"1,000,000"}),e.jsx(t.td,{children:"3,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-ada-002"})}),e.jsx(t.td,{children:"3,000"}),e.jsx(t.td,{children:"200"}),e.jsx(t.td,{children:"1,000,000"}),e.jsx(t.td,{children:"3,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"whisper-1"})}),e.jsx(t.td,{children:"3"}),e.jsx(t.td,{children:"200"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"tts-1"})}),e.jsx(t.td,{children:"3"}),e.jsx(t.td,{children:"200"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"dall-e-2"})}),e.jsx(t.td,{children:"5 img/min"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"dall-e-3"})}),e.jsx(t.td,{children:"1 img/min"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"})]})]})]})]})}function Oi(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(ft,n)})):ft(n)}function jt(n){const t=Object.assign({h4:"h4",p:"p",a:"a",table:"table",thead:"thead",tr:"tr",th:"th",tbody:"tbody",td:"td",code:"code"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h4,{children:"Tier 1 rate limits"}),"\n",e.jsxs(t.p,{children:["This is a high level summary and there are per-model exceptions to these limits (e.g. some legacy models or models with larger context windows have different rate limits). To view the exact rate limits per model for your account, visit the ",e.jsx(t.a,{href:"/account/rate-limits",children:"limits"})," section of your account settings."]}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Model"}),e.jsx(t.th,{children:"RPM"}),e.jsx(t.th,{children:"RPD"}),e.jsx(t.th,{children:"TPM"}),e.jsx(t.th,{children:"Batch Queue Limit"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4o"})}),e.jsx(t.td,{children:"500"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"30,000"}),e.jsx(t.td,{children:"90,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4o-mini"})}),e.jsx(t.td,{children:"500"}),e.jsx(t.td,{children:"10,000"}),e.jsx(t.td,{children:"200,000"}),e.jsx(t.td,{children:"2,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4-turbo"})}),e.jsx(t.td,{children:"500"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"30,000"}),e.jsx(t.td,{children:"90,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4"})}),e.jsx(t.td,{children:"500"}),e.jsx(t.td,{children:"10,000"}),e.jsx(t.td,{children:"10,000"}),e.jsx(t.td,{children:"100,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-3.5-turbo"})}),e.jsx(t.td,{children:"3,500"}),e.jsx(t.td,{children:"10,000"}),e.jsx(t.td,{children:"200,000"}),e.jsx(t.td,{children:"2,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-3-large"})}),e.jsx(t.td,{children:"3,000"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"1,000,000"}),e.jsx(t.td,{children:"3,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-3-small"})}),e.jsx(t.td,{children:"3,000"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"1,000,000"}),e.jsx(t.td,{children:"3,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-ada-002"})}),e.jsx(t.td,{children:"3,000"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"1,000,000"}),e.jsx(t.td,{children:"3,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"whisper-1"})}),e.jsx(t.td,{children:"50"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"tts-1"})}),e.jsx(t.td,{children:"50"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"tts-1-hd"})}),e.jsx(t.td,{children:"3"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"dall-e-2"})}),e.jsx(t.td,{children:"5 img/min"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"dall-e-3"})}),e.jsx(t.td,{children:"5 img/min"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"})]})]})]})]})}function Si(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(jt,n)})):jt(n)}function yt(n){const t=Object.assign({h4:"h4",p:"p",a:"a",table:"table",thead:"thead",tr:"tr",th:"th",tbody:"tbody",td:"td",code:"code"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h4,{children:"Tier 2 rate limits"}),"\n",e.jsxs(t.p,{children:["This is a high level summary and there are per-model exceptions to these limits (e.g. some legacy models or models with larger context windows have different rate limits). To view the exact rate limits per model for your account, visit the ",e.jsx(t.a,{href:"/account/rate-limits",children:"limits"})," section of your account settings."]}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Model"}),e.jsx(t.th,{children:"RPM"}),e.jsx(t.th,{children:"TPM"}),e.jsx(t.th,{children:"Batch Queue Limit"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4o"})}),e.jsx(t.td,{children:"5,000"}),e.jsx(t.td,{children:"450,000"}),e.jsx(t.td,{children:"1,350,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4o-mini"})}),e.jsx(t.td,{children:"5,000"}),e.jsx(t.td,{children:"2,000,000"}),e.jsx(t.td,{children:"20,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4-turbo"})}),e.jsx(t.td,{children:"5,000"}),e.jsx(t.td,{children:"450,000"}),e.jsx(t.td,{children:"1,350,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4"})}),e.jsx(t.td,{children:"5,000"}),e.jsx(t.td,{children:"40,000"}),e.jsx(t.td,{children:"200,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-3.5-turbo"})}),e.jsx(t.td,{children:"3,500"}),e.jsx(t.td,{children:"2,000,000"}),e.jsx(t.td,{children:"5,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-3-large"})}),e.jsx(t.td,{children:"5,000"}),e.jsx(t.td,{children:"1,000,000"}),e.jsx(t.td,{children:"20,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-3-small"})}),e.jsx(t.td,{children:"5,000"}),e.jsx(t.td,{children:"1,000,000"}),e.jsx(t.td,{children:"20,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-ada-002"})}),e.jsx(t.td,{children:"5,000"}),e.jsx(t.td,{children:"1,000,000"}),e.jsx(t.td,{children:"20,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"whisper-1"})}),e.jsx(t.td,{children:"50"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"tts-1"})}),e.jsx(t.td,{children:"50"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"tts-1-hd"})}),e.jsx(t.td,{children:"5"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"dall-e-2"})}),e.jsx(t.td,{children:"50 img/min"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"dall-e-3"})}),e.jsx(t.td,{children:"7 img/min"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"})]})]})]})]})}function Mi(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(yt,n)})):yt(n)}function bt(n){const t=Object.assign({h4:"h4",p:"p",a:"a",table:"table",thead:"thead",tr:"tr",th:"th",tbody:"tbody",td:"td",code:"code"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h4,{children:"Tier 3 rate limits"}),"\n",e.jsxs(t.p,{children:["This is a high level summary and there are per-model exceptions to these limits (e.g. some legacy models or models with larger context windows have different rate limits). To view the exact rate limits per model for your account, visit the ",e.jsx(t.a,{href:"/account/rate-limits",children:"limits"})," section of your account settings."]}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Model"}),e.jsx(t.th,{children:"RPM"}),e.jsx(t.th,{children:"TPM"}),e.jsx(t.th,{children:"Batch Queue Limit"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4o"})}),e.jsx(t.td,{children:"5,000"}),e.jsx(t.td,{children:"800,000"}),e.jsx(t.td,{children:"50,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4o-mini"})}),e.jsx(t.td,{children:"5,000"}),e.jsx(t.td,{children:"4,000,000"}),e.jsx(t.td,{children:"40,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4-turbo"})}),e.jsx(t.td,{children:"5,000"}),e.jsx(t.td,{children:"600,000"}),e.jsx(t.td,{children:"40,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4"})}),e.jsx(t.td,{children:"5,000"}),e.jsx(t.td,{children:"80,000"}),e.jsx(t.td,{children:"5,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-3.5-turbo"})}),e.jsx(t.td,{children:"3,500"}),e.jsx(t.td,{children:"4,000,000"}),e.jsx(t.td,{children:"100,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-3-large"})}),e.jsx(t.td,{children:"5,000"}),e.jsx(t.td,{children:"5,000,000"}),e.jsx(t.td,{children:"100,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-3-small"})}),e.jsx(t.td,{children:"5,000"}),e.jsx(t.td,{children:"5,000,000"}),e.jsx(t.td,{children:"100,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-ada-002"})}),e.jsx(t.td,{children:"5,000"}),e.jsx(t.td,{children:"5,000,000"}),e.jsx(t.td,{children:"100,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"whisper-1"})}),e.jsx(t.td,{children:"100"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"tts-1"})}),e.jsx(t.td,{children:"100"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"tts-1-hd"})}),e.jsx(t.td,{children:"7"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"dall-e-2"})}),e.jsx(t.td,{children:"100 img/min"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"dall-e-3"})}),e.jsx(t.td,{children:"7 img/min"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"})]})]})]})]})}function qi(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(bt,n)})):bt(n)}function wt(n){const t=Object.assign({h4:"h4",p:"p",a:"a",table:"table",thead:"thead",tr:"tr",th:"th",tbody:"tbody",td:"td",code:"code"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h4,{children:"Tier 4 rate limits"}),"\n",e.jsxs(t.p,{children:["This is a high level summary and there are per-model exceptions to these limits (e.g. some legacy models or models with larger context windows have different rate limits). To view the exact rate limits per model for your account, visit the ",e.jsx(t.a,{href:"/account/rate-limits",children:"limits"})," section of your account settings."]}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Model"}),e.jsx(t.th,{children:"RPM"}),e.jsx(t.th,{children:"TPM"}),e.jsx(t.th,{children:"Batch Queue Limit"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4o"})}),e.jsx(t.td,{children:"10,000"}),e.jsx(t.td,{children:"2,000,000"}),e.jsx(t.td,{children:"200,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4o-mini"})}),e.jsx(t.td,{children:"10,000"}),e.jsx(t.td,{children:"10,000,000"}),e.jsx(t.td,{children:"1,000,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4-turbo"})}),e.jsx(t.td,{children:"10,000"}),e.jsx(t.td,{children:"800,000"}),e.jsx(t.td,{children:"80,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4"})}),e.jsx(t.td,{children:"10,000"}),e.jsx(t.td,{children:"300,000"}),e.jsx(t.td,{children:"30,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-3.5-turbo"})}),e.jsx(t.td,{children:"10,000"}),e.jsx(t.td,{children:"10,000,000"}),e.jsx(t.td,{children:"1,000,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-3-large"})}),e.jsx(t.td,{children:"10,000"}),e.jsx(t.td,{children:"5,000,000"}),e.jsx(t.td,{children:"500,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-3-small"})}),e.jsx(t.td,{children:"10,000"}),e.jsx(t.td,{children:"5,000,000"}),e.jsx(t.td,{children:"500,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-ada-002"})}),e.jsx(t.td,{children:"10,000"}),e.jsx(t.td,{children:"5,000,000"}),e.jsx(t.td,{children:"500,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"whisper-1"})}),e.jsx(t.td,{children:"100"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"tts-1"})}),e.jsx(t.td,{children:"100"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"tts-1-hd"})}),e.jsx(t.td,{children:"10"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"dall-e-2"})}),e.jsx(t.td,{children:"100 img/min"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"dall-e-3"})}),e.jsx(t.td,{children:"15 img/min"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"})]})]})]})]})}function Hi(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(wt,n)})):wt(n)}function vt(n){const t=Object.assign({h4:"h4",p:"p",a:"a",table:"table",thead:"thead",tr:"tr",th:"th",tbody:"tbody",td:"td",code:"code"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h4,{children:"Tier 5 rate limits"}),"\n",e.jsxs(t.p,{children:["This is a high level summary and there are per-model exceptions to these limits (e.g. some legacy models or models with larger context windows have different rate limits). To view the exact rate limits per model for your account, visit the ",e.jsx(t.a,{href:"/account/rate-limits",children:"limits"})," section of your account settings."]}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Model"}),e.jsx(t.th,{children:"RPM"}),e.jsx(t.th,{children:"TPM"}),e.jsx(t.th,{children:"Batch Queue Limit"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4o"})}),e.jsx(t.td,{children:"10,000"}),e.jsx(t.td,{children:"30,000,000"}),e.jsx(t.td,{children:"5,000,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4o-mini"})}),e.jsx(t.td,{children:"30,000"}),e.jsx(t.td,{children:"150,000,000"}),e.jsx(t.td,{children:"15,000,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4-turbo"})}),e.jsx(t.td,{children:"10,000"}),e.jsx(t.td,{children:"2,000,000"}),e.jsx(t.td,{children:"300,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-4"})}),e.jsx(t.td,{children:"10,000"}),e.jsx(t.td,{children:"1,000,000"}),e.jsx(t.td,{children:"150,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"gpt-3.5-turbo"})}),e.jsx(t.td,{children:"10,000"}),e.jsx(t.td,{children:"50,000,000"}),e.jsx(t.td,{children:"10,000,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-3-large"})}),e.jsx(t.td,{children:"10,000"}),e.jsx(t.td,{children:"10,000,000"}),e.jsx(t.td,{children:"4,000,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-3-small"})}),e.jsx(t.td,{children:"10,000"}),e.jsx(t.td,{children:"10,000,000"}),e.jsx(t.td,{children:"4,000,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-ada-002"})}),e.jsx(t.td,{children:"10,000"}),e.jsx(t.td,{children:"10,000,000"}),e.jsx(t.td,{children:"4,000,000,000"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"whisper-1"})}),e.jsx(t.td,{children:"500"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"tts-1"})}),e.jsx(t.td,{children:"500"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"tts-1-hd"})}),e.jsx(t.td,{children:"20"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"dall-e-2"})}),e.jsx(t.td,{children:"500 img/min"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"dall-e-3"})}),e.jsx(t.td,{children:"200 img/min"}),e.jsx(t.td,{children:"-"}),e.jsx(t.td,{children:"-"})]})]})]}),"\n",e.jsxs(t.p,{children:["If you need higher rate limits than this, you can request an increase in the ",e.jsx(t.a,{href:"/account/rate-limits",children:"limits"})," section of your account settings."]})]})}function Ei(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(vt,n)})):vt(n)}function kt(n){const t=Object.assign({p:"p",h3:"h3",ul:"ul",li:"li",strong:"strong",a:"a",table:"table",thead:"thead",tr:"tr",th:"th",tbody:"tbody",td:"td",h2:"h2",h4:"h4",code:"code",em:"em"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsxs(t.p,{children:[e.jsx("h1",{children:"Rate limits"}),"\nRate limits are restrictions that our API imposes on the number of times a user or client can\naccess our services within a specified period of time."]}),"\n",e.jsx(t.h3,{children:"Why do we have rate limits?"}),"\n",e.jsx(t.p,{children:"Rate limits are a common practice for APIs, and they're put in place for a few different reasons:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"They help protect against abuse or misuse of the API."})," For example, a malicious actor could flood the API with requests in an attempt to overload it or cause disruptions in service. By setting rate limits, OpenAI can prevent this kind of activity."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Rate limits help ensure that everyone has fair access to the API."})," If one person or organization makes an excessive number of requests, it could bog down the API for everyone else. By throttling the number of requests that a single user can make, OpenAI ensures that the most number of people have an opportunity to use the API without experiencing slowdowns."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Rate limits can help OpenAI manage the aggregate load on its infrastructure."})," If requests to the API increase dramatically, it could tax the servers and cause performance issues. By setting rate limits, OpenAI can help maintain a smooth and consistent experience for all users."]}),"\n"]}),"\n",e.jsx(b,{children:e.jsx(t.p,{children:"Please work through this document in its entirety to better understand how OpenAI’s rate limit system works. We include code examples and possible solutions to handle common issues. We also include details around how your rate limits are automatically increased in the usage tiers section below."})}),"\n",e.jsx(t.h3,{children:"How do these rate limits work?"}),"\n",e.jsxs(t.p,{children:["Rate limits are measured in five ways: ",e.jsx(t.strong,{children:"RPM"})," (requests per minute), ",e.jsx(t.strong,{children:"RPD"})," (requests per day), ",e.jsx(t.strong,{children:"TPM"})," (tokens per minute), ",e.jsx(t.strong,{children:"TPD"})," (tokens per day), and ",e.jsx(t.strong,{children:"IPM"})," (images per minute). Rate limits can be hit across any of the options depending on what occurs first. For example, you might send 20 requests with only 100 tokens to the ChatCompletions endpoint and that would fill your limit (if your RPM was 20), even if you did not send 150k tokens (if your TPM limit was 150k) within those 20 requests."]}),"\n",e.jsxs(t.p,{children:[e.jsx(t.a,{href:"/docs/api-reference/batch/create",children:"Batch API"})," queue limits are calculated based on the total number of input tokens queued for a given model. Tokens from pending batch jobs are counted against your queue limit. Once a batch job is completed, its tokens are no longer counted against that model's limit."]}),"\n",e.jsx(t.p,{children:"Other important things worth noting:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["Rate limits are defined at the ",e.jsx(t.a,{href:"/docs/guides/production-best-practices",children:"organization level"})," and at the project level, not user level."]}),"\n",e.jsxs(t.li,{children:["Rate limits vary by the ",e.jsx(t.a,{href:"/docs/models/models",children:"model"})," being used."]}),"\n",e.jsx(t.li,{children:'Limits are also placed on the total amount an organization can spend on the API each month. These are also known as "usage limits".'}),"\n",e.jsxs(t.li,{children:['Some model families have shared rate limits. Any models listed under a "shared limit" in your ',e.jsx(t.a,{href:"https://platform.openai.com/settings/organization/limits",children:"organizations limit page"}),' share a rate limit between them. For example, if the listed shared TPM is 3.5M, all calls to any model in the given "shared limit" list will count towards that 3.5M.']}),"\n"]}),"\n",e.jsx(t.h3,{children:"Usage tiers"}),"\n",e.jsxs(t.p,{children:["You can view the rate and usage limits for your organization under the ",e.jsx(t.a,{href:"/account/rate-limits",children:"limits"})," section of your account settings. As your usage of the OpenAI API and your spend on our API goes up, we automatically graduate you to the next usage tier. This usually results in an increase in rate limits across most models."]}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Tier"}),e.jsx(t.th,{children:"Qualification"}),e.jsx(t.th,{children:"Usage limits"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"Free"}),e.jsxs(t.td,{children:["User must be in an ",e.jsx(t.a,{href:"/docs/supported-countries",children:"allowed geography"})]}),e.jsx(t.td,{children:"$100 / month"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"Tier 1"}),e.jsx(t.td,{children:"$5 paid"}),e.jsx(t.td,{children:"$100 / month"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"Tier 2"}),e.jsx(t.td,{children:"$50 paid and 7+ days since first successful payment"}),e.jsx(t.td,{children:"$500 / month"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"Tier 3"}),e.jsx(t.td,{children:"$100 paid and 7+ days since first successful payment"}),e.jsx(t.td,{children:"$1,000 / month"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"Tier 4"}),e.jsx(t.td,{children:"$250 paid and 14+ days since first successful payment"}),e.jsx(t.td,{children:"$5,000 / month"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"Tier 5"}),e.jsx(t.td,{children:"$1,000 paid and 30+ days since first successful payment"}),e.jsx(t.td,{children:"$50,000 / month"})]})]})]}),"\n",e.jsx(t.p,{children:"Select a tier below to view a high-level summary of rate limits per model."}),"\n",e.jsx(G,{options:[{value:"tier-free",label:"Free",content:e.jsx(Oi,{})},{value:"tier-one",label:"Tier 1",content:e.jsx(Si,{})},{value:"tier-two",label:"Tier 2",content:e.jsx(Mi,{})},{value:"tier-three",label:"Tier 3",content:e.jsx(qi,{})},{value:"tier-four",label:"Tier 4",content:e.jsx(Hi,{})},{value:"tier-five",label:"Tier 5",content:e.jsx(Ei,{})}],initialValue:"tier-free"}),"\n",e.jsx(t.h3,{children:"Rate limits in headers"}),"\n",e.jsxs(t.p,{children:["In addition to seeing your rate limit on your ",e.jsx(t.a,{href:"/account/rate-limits",children:"account page"}),", you can also view important information about your rate limits such as the remaining requests, tokens, and other metadata in the headers of the HTTP response."]}),"\n",e.jsx(t.p,{children:"You can expect to see the following header fields:"}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Field"}),e.jsx(t.th,{children:"Sample Value"}),e.jsx(t.th,{children:"Description"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"x-ratelimit-limit-requests"}),e.jsx(t.td,{children:"60"}),e.jsx(t.td,{children:"The maximum number of requests that are permitted before exhausting the rate limit."})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"x-ratelimit-limit-tokens"}),e.jsx(t.td,{children:"150000"}),e.jsx(t.td,{children:"The maximum number of tokens that are permitted before exhausting the rate limit."})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"x-ratelimit-remaining-requests"}),e.jsx(t.td,{children:"59"}),e.jsx(t.td,{children:"The remaining number of requests that are permitted before exhausting the rate limit."})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"x-ratelimit-remaining-tokens"}),e.jsx(t.td,{children:"149984"}),e.jsx(t.td,{children:"The remaining number of tokens that are permitted before exhausting the rate limit."})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"x-ratelimit-reset-requests"}),e.jsx(t.td,{children:"1s"}),e.jsx(t.td,{children:"The time until the rate limit (based on requests) resets to its initial state."})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"x-ratelimit-reset-tokens"}),e.jsx(t.td,{children:"6m0s"}),e.jsx(t.td,{children:"The time until the rate limit (based on tokens) resets to its initial state."})]})]})]}),"\n",e.jsx(t.h2,{children:"Error Mitigation"}),"\n",e.jsx(t.h3,{children:"What are some steps I can take to mitigate this?"}),"\n",e.jsxs(t.p,{children:["The OpenAI Cookbook has a ",e.jsx(t.a,{href:"https://cookbook.openai.com/examples/how_to_handle_rate_limits",children:"Python notebook"})," that explains how to avoid rate limit errors, as well an example ",e.jsx(t.a,{href:"https://github.com/openai/openai-cookbook/blob/main/examples/api_request_parallel_processor.py",children:"Python script"})," for staying under rate limits while batch processing API requests."]}),"\n",e.jsx(t.p,{children:"You should also exercise caution when providing programmatic access, bulk processing features, and automated social media posting - consider only enabling these for trusted customers."}),"\n",e.jsx(t.p,{children:"To protect against automated and high-volume misuse, set a usage limit for individual users within a specified time frame (daily, weekly, or monthly). Consider implementing a hard cap or a manual review process for users who exceed the limit."}),"\n",e.jsx(t.h4,{children:"Retrying with exponential backoff"}),"\n",e.jsx(t.p,{children:"One easy way to avoid rate limit errors is to automatically retry requests with a random exponential backoff. Retrying with exponential backoff means performing a short sleep when a rate limit error is hit, then retrying the unsuccessful request. If the request is still unsuccessful, the sleep length is increased and the process is repeated. This continues until the request is successful or until a maximum number of retries is reached.\nThis approach has many benefits:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Automatic retries means you can recover from rate limit errors without crashes or missing data"}),"\n",e.jsx(t.li,{children:"Exponential backoff means that your first retries can be tried quickly, while still benefiting from longer delays if your first few retries fail"}),"\n",e.jsx(t.li,{children:"Adding random jitter to the delay helps retries from all hitting at the same time."}),"\n"]}),"\n",e.jsx(t.p,{children:"Note that unsuccessful requests contribute to your per-minute limit, so continuously resending a request won’t work."}),"\n",e.jsxs(t.p,{children:["Below are a few example solutions ",e.jsx(t.strong,{children:"for Python"})," that use exponential backoff."]}),"\n",e.jsxs(j,{label:"Example 1: Using the Tenacity library",autoScroll:!0,showCollapse:!0,children:[e.jsxs(t.p,{children:["Tenacity is an Apache 2.0 licensed general-purpose retrying library, written in Python, to simplify the task of adding retry behavior to just about anything.\nTo add exponential backoff to your requests, you can use the ",e.jsx(t.code,{children:"tenacity.retry"})," decorator. The below example uses the ",e.jsx(t.code,{children:"tenacity.wait_random_exponential"})," function to add random exponential backoff to a request."]}),e.jsx(c,{title:"Using the Tenacity library",defaultLanguage:"python",code:{python:'\nfrom openai import OpenAI\nclient = OpenAI()\n\nfrom tenacity import (\n    retry,\n    stop_after_attempt,\n    wait_random_exponential,\n)  # for exponential backoff\n \n@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\ndef completion_with_backoff(**kwargs):\n    return client.completions.create(**kwargs)\n \ncompletion_with_backoff(model="gpt-4o-mini", prompt="Once upon a time,")\n'.trim()}}),e.jsx(t.p,{children:"Note that the Tenacity library is a third-party tool, and OpenAI makes no guarantees about\nits reliability or security."})]}),"\n",e.jsxs(j,{label:"Example 2: Using the backoff library",autoScroll:!0,showCollapse:!0,children:[e.jsxs(t.p,{children:["Another python library that provides function decorators for backoff and retry is ",e.jsx(t.a,{href:"https://pypi.org/project/backoff/",children:"backoff"}),":"]}),e.jsx(c,{title:"Using the Tenacity library",defaultLanguage:"python",code:{python:'\nimport backoff \nimport openai\nfrom openai import OpenAI\nclient = OpenAI()\n\n@backoff.on_exception(backoff.expo, openai.RateLimitError)\ndef completions_with_backoff(**kwargs):\n    return client.completions.create(**kwargs)\n \ncompletions_with_backoff(model="gpt-4o-mini", prompt="Once upon a time,")\n'.trim()}}),e.jsx(t.p,{children:"Like Tenacity, the backoff library is a third-party tool, and OpenAI makes no guarantees about its reliability or security."})]}),"\n",e.jsxs(j,{label:"Example 3: Manual backoff implementation",autoScroll:!0,showCollapse:!0,children:[e.jsx(t.p,{children:"If you don't want to use third-party libraries, you can implement your own backoff logic following this example:"}),e.jsx(c,{title:"Using manual backoff implementation",defaultLanguage:"python",code:{python:'\n# imports\nimport random\nimport time\n \nimport openai\nfrom openai import OpenAI\nclient = OpenAI()\n \n# define a retry decorator\ndef retry_with_exponential_backoff(\n    func,\n    initial_delay: float = 1,\n    exponential_base: float = 2,\n    jitter: bool = True,\n    max_retries: int = 10,\n    errors: tuple = (openai.RateLimitError,),\n):\n    """Retry a function with exponential backoff."""\n \n    def wrapper(*args, **kwargs):\n        # Initialize variables\n        num_retries = 0\n        delay = initial_delay\n \n        # Loop until a successful response or max_retries is hit or an exception is raised\n        while True:\n            try:\n                return func(*args, **kwargs)\n \n            # Retry on specific errors\n            except errors as e:\n                # Increment retries\n                num_retries += 1\n \n                # Check if max retries has been reached\n                if num_retries > max_retries:\n                    raise Exception(\n                        f"Maximum number of retries ({max_retries}) exceeded."\n                    )\n \n                # Increment the delay\n                delay *= exponential_base * (1 + jitter * random.random())\n \n                # Sleep for the delay\n                time.sleep(delay)\n \n            # Raise exceptions for any errors not specified\n            except Exception as e:\n                raise e\n \n    return wrapper\n    \n@retry_with_exponential_backoff\ndef completions_with_backoff(**kwargs):\n    return client.completions.create(**kwargs)\n'.trim()}}),e.jsx(t.p,{children:"Again, OpenAI makes no guarantees on the security or efficiency of this solution but it can be a good starting place for your own solution."})]}),"\n",e.jsxs(t.h4,{children:["Reduce the ",e.jsx(t.code,{children:"max_tokens"})," to match the size of your completions"]}),"\n",e.jsxs(t.p,{children:["Your rate limit is calculated as the maximum of ",e.jsx(t.code,{children:"max_tokens"})," and the estimated number of tokens based on the character count of your request. Try to set the ",e.jsx(t.code,{children:"max_tokens"})," value as close to your expected response size as possible."]}),"\n",e.jsx(t.h4,{children:"Batching requests"}),"\n",e.jsxs(t.p,{children:["If your use case does not require immediate responses, you can use the ",e.jsx(t.a,{href:"/docs/guides/batch",children:"Batch API"})," to more easily submit and execute large collections of requests without impacting your synchronous request rate limits."]}),"\n",e.jsxs(t.p,{children:["For use cases that ",e.jsx(t.em,{children:"do"})," requires synchronous respones, the OpenAI API has separate limits for ",e.jsx(t.strong,{children:"requests per minute"})," and ",e.jsx(t.strong,{children:"tokens per minute"}),"."]}),"\n",e.jsx(t.p,{children:"If you're hitting the limit on requests per minute but have available capacity on tokens per minute, you can increase your throughput by batching multiple tasks into each request. This will allow you to process more tokens per minute, especially with our smaller models."}),"\n",e.jsx(t.p,{children:"Sending in a batch of prompts works exactly the same as a normal API call, except you pass in a list of strings to the prompt parameter instead of a single string."}),"\n",e.jsx(j,{label:"Example without batching",autoScroll:!0,showCollapse:!0,children:e.jsx(c,{title:"No batching",defaultLanguage:"python",code:{python:'\nfrom openai import OpenAI\nclient = OpenAI()\n \nnum_stories = 10\nprompt = "Once upon a time,"\n \n# serial example, with one story completion per request\nfor _ in range(num_stories):\n    response = client.completions.create(\n        model="curie",\n        prompt=prompt,\n        max_tokens=20,\n    )\n    # print story\n    print(prompt + response.choices[0].text)\n'.trim()}})}),"\n",e.jsx(j,{label:"Example with batching",autoScroll:!0,showCollapse:!0,children:e.jsx(c,{title:"Batching",defaultLanguage:"python",code:{python:'\nfrom openai import OpenAI\nclient = OpenAI()\n \nnum_stories = 10\nprompts = ["Once upon a time,"] * num_stories\n \n# batched example, with 10 story completions per request\nresponse = client.completions.create(\n    model="curie",\n    prompt=prompts,\n    max_tokens=20,\n)\n \n# match completions to prompts by index\nstories = [""] * len(prompts)\nfor choice in response.choices:\n    stories[choice.index] = prompts[choice.index] + choice.text\n \n# print stories\nfor story in stories:\n    print(story)\n'.trim()}})}),"\n",e.jsx(b,{children:e.jsx(t.p,{children:"Warning: the response object may not return completions in the order of the prompts, so always remember to match responses back to prompts using the index field."})})]})}function Ni(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(kt,n)})):kt(n)}function _t(n){const t=Object.assign({h1:"h1",h3:"h3",p:"p",a:"a",h2:"h2",code:"code"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"Safety best practices"}),"\n",e.jsx(t.h3,{children:"Use our free Moderation API"}),"\n",e.jsxs(t.p,{children:["OpenAI's ",e.jsx(t.a,{href:"/docs/guides/moderation",children:"Moderation API"})," is free-to-use and can help reduce the frequency of unsafe content in your completions. Alternatively, you may wish to develop your own content filtration system tailored to your use case."]}),"\n",e.jsx(t.h3,{children:"Adversarial testing"}),"\n",e.jsx(t.p,{children:"We recommend “red-teaming” your application to ensure it's robust to adversarial input. Test your product over a wide range of inputs and user behaviors, both a representative set and those reflective of someone trying to ‘break' your application. Does it wander off topic? Can someone easily redirect the feature via prompt injections, e.g. “ignore the previous instructions and do this instead”?"}),"\n",e.jsx(t.h3,{children:"Human in the loop (HITL)"}),"\n",e.jsx(t.p,{children:"Wherever possible, we recommend having a human review outputs before they are used in practice. This is especially critical in high-stakes domains, and for code generation. Humans should be aware of the limitations of the system, and have access to any information needed to verify the outputs (for example, if the application summarizes notes, a human should have easy access to the original notes to refer back)."}),"\n",e.jsx(t.h3,{children:"Prompt engineering"}),"\n",e.jsx(t.p,{children:"“Prompt engineering” can help constrain the topic and tone of output text. This reduces the chance of producing undesired content, even if a user tries to produce it. Providing additional context to the model (such as by giving a few high-quality examples of desired behavior prior to the new input) can make it easier to steer model outputs in desired directions."}),"\n",e.jsx(t.h3,{children:"“Know your customer” (KYC)"}),"\n",e.jsx(t.p,{children:"Users should generally need to register and log-in to access your service. Linking this service to an existing account, such as a Gmail, LinkedIn, or Facebook log-in, may help, though may not be appropriate for all use-cases. Requiring a credit card or ID card reduces risk further."}),"\n",e.jsx(t.h3,{children:"Constrain user input and limit output tokens"}),"\n",e.jsx(t.p,{children:"Limiting the amount of text a user can input into the prompt helps avoid prompt injection. Limiting the number of output tokens helps reduce the chance of misuse."}),"\n",e.jsx(t.p,{children:"Narrowing the ranges of inputs or outputs, especially drawn from trusted sources, reduces the extent of misuse possible within an application."}),"\n",e.jsx(t.p,{children:"Allowing user inputs through validated dropdown fields (e.g., a list of movies on Wikipedia) can be more secure than allowing open-ended text inputs."}),"\n",e.jsx(t.p,{children:"Returning outputs from a validated set of materials on the backend, where possible, can be safer than returning novel generated content (for instance, routing a customer query to the best-matching existing customer support article, rather than attempting to answer the query from-scratch)."}),"\n",e.jsx(t.h3,{children:"Allow users to report issues"}),"\n",e.jsx(t.p,{children:"Users should generally have an easily-available method for reporting improper functionality or other concerns about application behavior (listed email address, ticket submission method, etc). This method should be monitored by a human and responded to as appropriate."}),"\n",e.jsx(t.h3,{children:"Understand and communicate limitations"}),"\n",e.jsx(t.p,{children:"From hallucinating inaccurate information, to offensive outputs, to bias, and much more, language models may not be suitable for every use case without significant modifications. Consider whether the model is fit for your purpose, and evaluate the performance of the API on a wide range of potential inputs in order to identify cases where the API's performance might drop. Consider your customer base and the range of inputs that they will be using, and ensure their expectations are calibrated appropriately."}),"\n",e.jsxs(b,{children:[e.jsx(t.p,{children:"Safety and security are very important to us at OpenAI."}),e.jsxs(t.p,{children:["If in the course of your development you do notice any safety or security issues with the API or anything else related to OpenAI, please submit these through our ",e.jsx(t.a,{href:"https://openai.com/security/disclosure/",children:"Coordinated Vulnerability Disclosure Program"}),"."]})]}),"\n",e.jsx(t.h2,{children:"End-user IDs"}),"\n",e.jsx(t.p,{children:"Sending end-user IDs in your requests can be a useful tool to help OpenAI monitor and detect abuse. This allows OpenAI to provide your team with more actionable feedback in the event that we detect any policy violations in your application."}),"\n",e.jsx(t.p,{children:"The IDs should be a string that uniquely identifies each user. We recommend hashing their username or email address, in order to avoid sending us any identifying information. If you offer a preview of your product to non-logged in users, you can send a session ID instead."}),"\n",e.jsxs(t.p,{children:["You can include end-user IDs in your API requests via the ",e.jsx(t.code,{children:"user"})," parameter as follows:"]}),"\n",e.jsx(c,{title:"Example: Providing a user identifer",defaultLanguage:"python",code:{python:'\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n  model="gpt-4o-mini",\n  messages=[\n    {"role": "user", "content": "This is a test"}\n  ],\n  max_tokens=5,\n  user="user_123456"\n)\n'.trim(),curl:'\ncurl https://api.openai.com/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -d \'{\n  "model": "gpt-4o-mini",\n  "messages": [\n    {"role": "user", "content": "This is a test"}\n  ],\n  "max_tokens": 5,\n  "user": "user123456"\n}\'\n'.trim()}})]})}function Vi(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(_t,n)})):_t(n)}function Ct(n){const t=Object.assign({h1:"h1",p:"p",h2:"h2",code:"code",a:"a",ul:"ul",li:"li",h3:"h3",pre:"pre",em:"em",ol:"ol"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"Speech to text"}),"\n",e.jsx(t.p,{children:"Learn how to turn audio into text"}),"\n",e.jsx(t.h2,{children:"Introduction"}),"\n",e.jsxs(t.p,{children:["The Audio API provides two speech to text endpoints, ",e.jsx(t.code,{children:"transcriptions"})," and ",e.jsx(t.code,{children:"translations"}),", based on our state-of-the-art open source large-v2 ",e.jsx(t.a,{href:"https://openai.com/blog/whisper/",children:"Whisper model"}),". They can be used to:"]}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Transcribe audio into whatever language the audio is in."}),"\n",e.jsx(t.li,{children:"Translate and transcribe the audio into english."}),"\n"]}),"\n",e.jsxs(t.p,{children:["File uploads are currently limited to 25 MB and the following input file types are supported: ",e.jsx(t.code,{children:"mp3"}),", ",e.jsx(t.code,{children:"mp4"}),", ",e.jsx(t.code,{children:"mpeg"}),", ",e.jsx(t.code,{children:"mpga"}),", ",e.jsx(t.code,{children:"m4a"}),", ",e.jsx(t.code,{children:"wav"}),", and ",e.jsx(t.code,{children:"webm"}),"."]}),"\n",e.jsx(t.h2,{children:"Quickstart"}),"\n",e.jsx(t.h3,{children:"Transcriptions"}),"\n",e.jsx(t.p,{children:"The transcriptions API takes as input the audio file you want to transcribe and the desired output file format for the transcription of the audio. We currently support multiple input and output file formats."}),"\n",e.jsx(c,{title:"Transcribe audio",defaultLanguage:"python",code:{python:'\nfrom openai import OpenAI\nclient = OpenAI()\n\naudio_file= open("/path/to/file/audio.mp3", "rb")\ntranscription = client.audio.transcriptions.create(\n  model="whisper-1", \n  file=audio_file\n)\nprint(transcription.text)\n'.trim(),node:'\nimport fs from "fs";\nimport OpenAI from "openai";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const transcription = await openai.audio.transcriptions.create({\n    file: fs.createReadStream("/path/to/file/audio.mp3"),\n    model: "whisper-1",\n  });\n\n  console.log(transcription.text);\n}\nmain();\n'.trim(),curl:"\ncurl --request POST \\\n  --url https://api.openai.com/v1/audio/transcriptions \\\n  --header \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  --header 'Content-Type: multipart/form-data' \\\n  --form file=@/path/to/file/audio.mp3 \\\n  --form model=whisper-1\n".trim()}}),"\n",e.jsx(t.p,{children:"By default, the response type will be json with the raw text included."}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-content",children:"{\n  \"text\": \"Imagine the wildest idea that you've ever had, and you're curious about how it might scale to something that's a 100, a 1,000 times bigger.\n....\n}\n"})}),"\n",e.jsxs(t.p,{children:["The Audio API also allows you to set additional parameters in a request. For example, if you want to set the ",e.jsx(t.code,{children:"response_format"})," as ",e.jsx(t.code,{children:"text"}),", your request would look like the following:"]}),"\n",e.jsx(c,{title:"Additional options",defaultLanguage:"python",code:{python:'\nfrom openai import OpenAI\nclient = OpenAI()\n\naudio_file = open("/path/to/file/speech.mp3", "rb")\ntranscription = client.audio.transcriptions.create(\n  model="whisper-1", \n  file=audio_file, \n  response_format="text"\n)\nprint(transcription.text)\n'.trim(),node:'\nimport fs from "fs";\nimport OpenAI from "openai";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const transcription = await openai.audio.transcriptions.create({\n    file: fs.createReadStream("/path/to/file/speech.mp3"),\n    model: "whisper-1",\n    response_format: "text",\n  });\n\n  console.log(transcription.text);\n}\nmain();\n'.trim(),curl:"\ncurl --request POST \\\n  --url https://api.openai.com/v1/audio/transcriptions \\\n  --header \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  --header 'Content-Type: multipart/form-data' \\\n  --form file=@/path/to/file/speech.mp3 \\\n  --form model=whisper-1 \\\n  --form response_format=text\n".trim()}}),"\n",e.jsxs(t.p,{children:["The ",e.jsx(t.a,{href:"/docs/api-reference/audio",children:"API Reference"})," includes the full list of available parameters."]}),"\n",e.jsx(t.h3,{children:"Translations"}),"\n",e.jsx(t.p,{children:"The translations API takes as input the audio file in any of the supported languages and transcribes, if necessary, the audio into English. This differs from our /Transcriptions endpoint since the output is not in the original input language and is instead translated to English text."}),"\n",e.jsx(c,{title:"Translate audio",defaultLanguage:"python",code:{python:'\nfrom openai import OpenAI\nclient = OpenAI()\n\naudio_file= open("/path/to/file/german.mp3", "rb")\ntranslation = client.audio.translations.create(\n  model="whisper-1", \n  file=audio_file\n)\nprint(translation.text)\n'.trim(),node:'\nimport fs from "fs";\nimport OpenAI from "openai";\n\nconst openai = new OpenAI();\n\nasync function main() {\n    const translation = await openai.audio.translations.create({\n        file: fs.createReadStream("/path/to/file/german.mp3"),\n        model: "whisper-1",\n    });\n\n    console.log(translation.text);\n}\nmain();\n'.trim(),curl:"\ncurl --request POST \\\n  --url https://api.openai.com/v1/audio/translations \\\n  --header \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  --header 'Content-Type: multipart/form-data' \\\n  --form file=@/path/to/file/german.mp3 \\\n  --form model=whisper-1\n  ".trim()}}),"\n",e.jsx(t.p,{children:"In this case, the inputted audio was german and the outputted text looks like:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-example-content",children:"Hello, my name is Wolfgang and I come from Germany. Where are you heading today?\n"})}),"\n",e.jsx(t.p,{children:"We only support translation into English at this time."}),"\n",e.jsx(t.h2,{children:"Supported languages"}),"\n",e.jsxs(t.p,{children:["We currently ",e.jsx(t.a,{href:"https://github.com/openai/whisper#available-models-and-languages",children:"support the following languages"})," through both the ",e.jsx(t.code,{children:"transcriptions"})," and ",e.jsx(t.code,{children:"translations"})," endpoint:"]}),"\n",e.jsx(t.p,{children:"Afrikaans, Arabic, Armenian, Azerbaijani, Belarusian, Bosnian, Bulgarian, Catalan, Chinese, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, Galician, German, Greek, Hebrew, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Kannada, Kazakh, Korean, Latvian, Lithuanian, Macedonian, Malay, Marathi, Maori, Nepali, Norwegian, Persian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Slovenian, Spanish, Swahili, Swedish, Tagalog, Tamil, Thai, Turkish, Ukrainian, Urdu, Vietnamese, and Welsh."}),"\n",e.jsxs(t.p,{children:["While the underlying model was trained on 98 languages, we only list the languages that exceeded <50% ",e.jsx(t.a,{href:"https://en.wikipedia.org/wiki/Word_error_rate",children:"word error rate"})," (WER) which is an industry standard benchmark for speech to text model accuracy. The model will return results for languages not listed above but the quality will be low."]}),"\n",e.jsx(t.h2,{children:"Timestamps"}),"\n",e.jsxs(t.p,{children:["By default, the Whisper API will output a transcript of the provided audio in text. The ",e.jsxs(t.a,{href:"/docs/api-reference/audio/createTranscription#audio-createtranscription-timestamp_granularities",children:[e.jsx(t.code,{children:"timestamp_granularities[]"})," parameter"]})," enables a more structured and timestamped json output format, with timestamps at the segment, word level, or both. This enables word-level precision for transcripts and video edits, which allows for the removal of specific frames tied to individual words."]}),"\n",e.jsx(c,{title:"Timestamp options",defaultLanguage:"python",code:{python:'\nfrom openai import OpenAI\nclient = OpenAI()\n\naudio_file = open("speech.mp3", "rb")\ntranscript = client.audio.transcriptions.create(\n  file=audio_file,\n  model="whisper-1",\n  response_format="verbose_json",\n  timestamp_granularities=["word"]\n)\n\nprint(transcript.words)\n'.trim(),node:'\nimport fs from "fs";\nimport OpenAI from "openai";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const transcription = await openai.audio.transcriptions.create({\n    file: fs.createReadStream("audio.mp3"),\n    model: "whisper-1",\n    response_format: "verbose_json",\n    timestamp_granularities: ["word"]\n  });\n\n  console.log(transcription.text);\n}\nmain();\n'.trim(),curl:'\ncurl https://api.openai.com/v1/audio/transcriptions \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -H "Content-Type: multipart/form-data" \\\n  -F file="@/path/to/file/audio.mp3" \\\n  -F "timestamp_granularities[]=word" \\\n  -F model="whisper-1" \\\n  -F response_format="verbose_json"\n'.trim()}}),"\n",e.jsx(t.h2,{children:"Longer inputs"}),"\n",e.jsx(t.p,{children:"By default, the Whisper API only supports files that are less than 25 MB. If you have an audio file that is longer than that, you will need to break it up into chunks of 25 MB's or less or used a compressed audio format. To get the best performance, we suggest that you avoid breaking the audio up mid-sentence as this may cause some context to be lost."}),"\n",e.jsxs(t.p,{children:["One way to handle this is to use the ",e.jsx(t.a,{href:"https://github.com/jiaaro/pydub",children:"PyDub open source Python package"})," to split the audio:"]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:'from pydub import AudioSegment\n\nsong = AudioSegment.from_mp3("good_morning.mp3")\n\n# PyDub handles time in milliseconds\nten_minutes = 10 * 60 * 1000\n\nfirst_10_minutes = song[:ten_minutes]\n\nfirst_10_minutes.export("good_morning_10.mp3", format="mp3")\n'})}),"\n",e.jsx(t.p,{children:e.jsx(t.em,{children:"OpenAI makes no guarantees about the usability or security of 3rd party software like PyDub."})}),"\n",e.jsx(t.h2,{children:"Prompting"}),"\n",e.jsxs(t.p,{children:["You can use a ",e.jsx(t.a,{href:"/docs/api-reference/audio/createTranscription#audio/createTranscription-prompt",children:"prompt"})," to improve the quality of the transcripts generated by the Whisper API. The model will try to match the style of the prompt, so it will be more likely to use capitalization and punctuation if the prompt does too. However, the current prompting system is much more limited than our other language models and only provides limited control over the generated audio. Here are some examples of how prompting can help in different scenarios:"]}),"\n",e.jsxs(t.ol,{children:["\n",e.jsx(t.li,{children:'Prompts can be very helpful for correcting specific words or acronyms that the model may misrecognize in the audio. For example, the following prompt improves the transcription of the words DALL·E and GPT-3, which were previously written as "GDP 3" and "DALI": "The transcript is about OpenAI which makes technology like DALL·E, GPT-3, and ChatGPT with the hope of one day building an AGI system that benefits all of humanity"'}),"\n",e.jsxs(t.li,{children:["To preserve the context of a file that was split into segments, you can prompt the model with the transcript of the preceding segment. This will make the transcript more accurate, as the model will use the relevant information from the previous audio. The model will only consider the final 224 tokens of the prompt and ignore anything earlier. For multilingual inputs, Whisper uses a custom tokenizer. For English only inputs, it uses the standard GPT-2 tokenizer which are both accessible through the open source ",e.jsx(t.a,{href:"https://github.com/openai/whisper/blob/main/whisper/tokenizer.py#L361",children:"Whisper Python package"}),"."]}),"\n",e.jsx(t.li,{children:'Sometimes the model might skip punctuation in the transcript. You can avoid this by using a simple prompt that includes punctuation: "Hello, welcome to my lecture."'}),"\n",e.jsx(t.li,{children:"The model may also leave out common filler words in the audio. If you want to keep the filler words in your transcript, you can use a prompt that contains them: \"Umm, let me think like, hmm... Okay, here's what I'm, like, thinking.\""}),"\n",e.jsx(t.li,{children:"Some languages can be written in different ways, such as simplified or traditional Chinese. The model might not always use the writing style that you want for your transcript by default. You can improve this by using a prompt in your preferred writing style."}),"\n"]}),"\n",e.jsx(t.h2,{children:"Improving reliability"}),"\n",e.jsx(t.p,{children:"As we explored in the prompting section, one of the most common challenges faced when using Whisper is the model often does not recognize uncommon words or acronyms. To address this, we have highlighted different techniques which improve the reliability of Whisper in these cases:"}),"\n",e.jsxs(j,{label:"Using the prompt parameter",autoScroll:!0,showCollapse:!0,children:[e.jsx(t.p,{children:"The first method involves using the optional prompt parameter to pass a dictionary of the correct spellings."}),e.jsx(t.p,{children:"Since it wasn't trained using instruction-following techniques, Whisper operates more like a base GPT model. It's important to keep in mind that Whisper only considers the first 244 tokens of the prompt."}),e.jsx(c,{title:"Prompt parameter",defaultLanguage:"python",code:{python:'\nfrom openai import OpenAI\nclient = OpenAI()\n\naudio_file = open("/path/to/file/speech.mp3", "rb")\ntranscription = client.audio.transcriptions.create(\n  model="whisper-1", \n  file=audio_file, \n  response_format="text",\n  prompt="ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T."\n)\nprint(transcription.text)\n'.trim(),node:'\nimport fs from "fs";\nimport OpenAI from "openai";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const transcription = await openai.audio.transcriptions.create({\n    file: fs.createReadStream("/path/to/file/speech.mp3"),\n    model: "whisper-1",\n    response_format: "text",\n    prompt:"ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T.",\n  });\n\n  console.log(transcription.text);\n}\nmain();\n'.trim()}}),e.jsx(t.p,{children:"While it will increase reliability, this technique is limited to only 244 characters so your list of SKUs would need to be relatively small in order for this to be a scalable solution."})]}),"\n",e.jsxs(j,{label:"Post-processing with GPT-4",autoScroll:!0,showCollapse:!0,children:[e.jsx(t.p,{children:"The second method involves a post-processing step using GPT-4 or GPT-3.5-Turbo."}),e.jsxs(t.p,{children:["We start by providing instructions for GPT-4 through the ",e.jsx(t.code,{children:"system_prompt"})," variable. Similar to what we did with the prompt parameter earlier, we can define our company and product names."]}),e.jsx(c,{title:"Post-processing",defaultLanguage:"python",code:{python:'\nsystem_prompt = "You are a helpful assistant for the company ZyntriQix. Your task is to correct any spelling discrepancies in the transcribed text. Make sure that the names of the following products are spelled correctly: ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T. Only add necessary punctuation such as periods, commas, and capitalization, and use only the context provided."\n\ndef generate_corrected_transcript(temperature, system_prompt, audio_file):\n    response = client.chat.completions.create(\n        model="gpt-4o",\n        temperature=temperature,\n        messages=[\n            {\n                "role": "system",\n                "content": system_prompt\n            },\n            {\n                "role": "user",\n                "content": transcribe(audio_file, "")\n            }\n        ]\n    )\n    return completion.choices[0].message.content\n\ncorrected_text = generate_corrected_transcript(0, system_prompt, fake_company_filepath)\n'.trim(),node:'\nconst systemPrompt = "You are a helpful assistant for the company ZyntriQix. Your task is to correct any spelling discrepancies in the transcribed text. Make sure that the names of the following products are spelled correctly: ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T. Only add necessary punctuation such as periods, commas, and capitalization, and use only the context provided.";\n\nasync function generateCorrectedTranscript(temperature, systemPrompt, audioFile) {\n  const transcript = await transcribe(audioFile);\n  const completion = await openai.chat.completions.create({\n    model: "gpt-4o",\n    temperature: temperature,\n    messages: [\n      {\n        role: "system",\n        content: systemPrompt\n      },\n      {\n        role: "user",\n        content: transcript\n      }\n    ]\n  });\n  return completion.choices[0].message.content;\n}\n\nconst fakeCompanyFilepath = "path/to/audio/file";\ngenerateCorrectedTranscript(0, systemPrompt, fakeCompanyFilepath)\n  .then(correctedText => console.log(correctedText))\n  .catch(error => console.error(error));\n'.trim()}}),e.jsx(t.p,{children:"If you try this on your own audio file, you can see that GPT-4 manages to correct many misspellings in the transcript. Due to its larger context window, this method might be more scalable than using Whisper's prompt parameter and is more reliable since GPT-4 can be instructed and guided in ways that aren't possible with Whisper given the lack of instruction following."})]})]})}function zi(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Ct,n)})):Ct(n)}function At(n){return e.jsx(c,{title:"Getting a structured response",defaultLanguage:"python",code:{python:'\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclass CalendarEvent(BaseModel):\n    name: str\n    date: str\n    participants: list[str]\n\ncompletion = client.beta.chat.completions.parse(\n    model="gpt-4o-2024-08-06",\n    messages=[\n        {"role": "system", "content": "Extract the event information."},\n        {"role": "user", "content": "Alice and Bob are going to a science fair on Friday."},\n    ],\n    response_format=CalendarEvent,\n)\n\nevent = completion.choices[0].message.parsed\n'.trim(),"node.js":'\nimport OpenAI from "openai";\nimport { zodResponseFormat } from "openai/helpers/zod";\nimport { z } from "zod";\n\nconst openai = new OpenAI();\n\nconst CalendarEvent = z.object({\n  name: z.string(),\n  date: z.string(),\n  participants: z.array(z.string()),\n});\n\nconst completion = await openai.beta.chat.completions.parse({\n  model: "gpt-4o-2024-08-06",\n  messages: [\n    { role: "system", content: "Extract the event information." },\n    { role: "user", content: "Alice and Bob are going to a science fair on Friday." },\n  ],\n  response_format: zodResponseFormat(CalendarEvent, "event"),\n});\n\nconst event = completion.choices[0].message.parsed;\n'.trim()}})}function Di(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(At,n)})):At()}function It(n){const t=Object.assign({h3:"h3",p:"p",h4:"h4",pre:"pre",code:"code"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h3,{children:"Chain of thought"}),"\n",e.jsx(t.p,{children:"You can ask the model to output an answer in a structured, step-by-step way, to guide the user through the solution."}),"\n",e.jsx(c,{title:"Structured Outputs for chain-of-thought math tutoring",defaultLanguage:"python",code:{python:'\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclass Step(BaseModel):\n    explanation: str\n    output: str\n\nclass MathReasoning(BaseModel):\n    steps: list[Step]\n    final_answer: str\n\ncompletion = client.beta.chat.completions.parse(\n    model="gpt-4o-2024-08-06",\n    messages=[\n        {"role": "system", "content": "You are a helpful math tutor. Guide the user through the solution step by step."},\n        {"role": "user", "content": "how can I solve 8x + 7 = -23"}\n    ],\n    response_format=MathReasoning,\n)\n\nmath_reasoning = completion.choices[0].message.parsed\n'.trim(),"node.js":'\nimport OpenAI from "openai";\nimport { z } from "zod";\nimport { zodResponseFormat } from "openai/helpers/zod";\n\nconst openai = new OpenAI();\n\nconst Step = z.object({\n  explanation: z.string(),\n  output: z.string(),\n});\n\nconst MathReasoning = z.object({\n  steps: z.array(Step),\n  final_answer: z.string(),\n});\n\nconst completion = await openai.beta.chat.completions.parse({\n  model: "gpt-4o-2024-08-06",\n  messages: [\n    { role: "system", content: "You are a helpful math tutor. Guide the user through the solution step by step." },\n    { role: "user", content: "how can I solve 8x + 7 = -23" },\n  ],\n  response_format: zodResponseFormat(MathReasoning, "math_reasoning"),\n});\n\nconst math_reasoning = completion.choices[0].message.parsed;\n\n'.trim(),curl:'\ncurl https://api.openai.com/v1/chat/completions \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "gpt-4o-2024-08-06",\n    "messages": [\n      {\n        "role": "system",\n        "content": "You are a helpful math tutor. Guide the user through the solution step by step."\n      },\n      {\n        "role": "user",\n        "content": "how can I solve 8x + 7 = -23"\n      }\n    ],\n    "response_format": {\n      "type": "json_schema",\n      "json_schema": {\n        "name": "math_reasoning",\n        "schema": {\n          "type": "object",\n          "properties": {\n            "steps": {\n              "type": "array",\n              "items": {\n                "type": "object",\n                "properties": {\n                  "explanation": { "type": "string" },\n                  "output": { "type": "string" }\n                },\n                "required": ["explanation", "output"],\n                "additionalProperties": false\n              }\n            },\n            "final_answer": { "type": "string" }\n          },\n          "required": ["steps", "final_answer"],\n          "additionalProperties": false\n        },\n        "strict": true\n      }\n    }\n  }\'\n'.trim()}}),"\n",e.jsx(t.h4,{children:"Example response"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-json",children:'{\n  "steps": [\n    {\n      "explanation": "Start with the equation 8x + 7 = -23.",\n      "output": "8x + 7 = -23"\n    },\n    {\n      "explanation": "Subtract 7 from both sides to isolate the term with the variable.",\n      "output": "8x = -23 - 7"\n    },\n    {\n      "explanation": "Simplify the right side of the equation.",\n      "output": "8x = -30"\n    },\n    {\n      "explanation": "Divide both sides by 8 to solve for x.",\n      "output": "x = -30 / 8"\n    },\n    {\n      "explanation": "Simplify the fraction.",\n      "output": "x = -15 / 4"\n    }\n  ],\n  "final_answer": "x = -15 / 4"\n}\n'})})]})}function Fi(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(It,n)})):It(n)}function Tt(n){const t=Object.assign({h3:"h3",p:"p",h4:"h4",pre:"pre",code:"code"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h3,{children:"Structured data extraction"}),"\n",e.jsx(t.p,{children:"You can define structured fields to extract from unstructured input data, such as reasearch papers."}),"\n",e.jsx(c,{title:"Extracting data from research papers using Structured Outputs",defaultLanguage:"python",code:{python:'\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclass ResearchPaperExtraction(BaseModel):\n    title: str\n    authors: list[str]\n    abstract: str\n    keywords: list[str]\n\ncompletion = client.beta.chat.completions.parse(\n    model="gpt-4o-2024-08-06",\n    messages=[\n        {"role": "system", "content": "You are an expert at structured data extraction. You will be given unstructured text from a research paper and should convert it into the given structure."},\n        {"role": "user", "content": "..."}\n    ],\n    response_format=ResearchPaperExtraction,\n)\n\nresearch_paper = completion.choices[0].message.parsed\n'.trim(),"node.js":'\nimport OpenAI from "openai";\nimport { z } from "zod";\nimport { zodResponseFormat } from "openai/helpers/zod";\n\nconst openai = new OpenAI();\n\nconst ResearchPaperExtraction = z.object({\n  title: z.string(),\n  authors: z.array(z.string()),\n  abstract: z.string(),\n  keywords: z.array(z.string()),\n});\n\nconst completion = await openai.beta.chat.completions.parse({\n  model: "gpt-4o-2024-08-06",\n  messages: [\n    { role: "system", content: "You are an expert at structured data extraction. You will be given unstructured text from a research paper and should convert it into the given structure." },\n    { role: "user", content: "..." },\n  ],\n  response_format: zodResponseFormat(ResearchPaperExtraction, "research_paper_extraction"),\n});\n\nconst research_paper = completion.choices[0].message.parsed;\n'.trim(),curl:'\ncurl https://api.openai.com/v1/chat/completions \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "gpt-4o-2024-08-06",\n    "messages": [\n      {\n        "role": "system",\n        "content": "You are an expert at structured data extraction. You will be given unstructured text from a research paper and should convert it into the given structure."\n      },\n      {\n        "role": "user",\n        "content": "..."\n      }\n    ],\n    "response_format": {\n      "type": "json_schema",\n      "json_schema": {\n        "name": "research_paper_extraction",\n        "schema": {\n          "type": "object",\n          "properties": {\n            "title": { "type": "string" },\n            "authors": {\n              "type": "array",\n              "items": { "type": "string" }\n            },\n            "abstract": { "type": "string" },\n            "keywords": {\n              "type": "array",\n              "items": { "type": "string" }\n            }\n          },\n          "required": ["title", "authors", "abstract", "keywords"],\n          "additionalProperties": false\n        },\n        "strict": true\n      }\n    }\n  }\'\n'.trim()}}),"\n",e.jsx(t.h4,{children:"Example response"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-json",children:'{\n  "title": "Application of Quantum Algorithms in Interstellar Navigation: A New Frontier",\n  "authors": [\n    "Dr. Stella Voyager",\n    "Dr. Nova Star",\n    "Dr. Lyra Hunter"\n  ],\n  "abstract": "This paper investigates the utilization of quantum algorithms to improve interstellar navigation systems. By leveraging quantum superposition and entanglement, our proposed navigation system can calculate optimal travel paths through space-time anomalies more efficiently than classical methods. Experimental simulations suggest a significant reduction in travel time and fuel consumption for interstellar missions.",\n  "keywords": [\n    "Quantum algorithms",\n    "interstellar navigation",\n    "space-time anomalies",\n    "quantum superposition",\n    "quantum entanglement",\n    "space travel"\n  ]\n}\n\n'})})]})}function Ri(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Tt,n)})):Tt(n)}function Pt(n){const t=Object.assign({h3:"h3",p:"p",h4:"h4",pre:"pre",code:"code"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h3,{children:"UI Generation"}),"\n",e.jsx(t.p,{children:"You can generate valid HTML by representing it as recursive data structures with constraints, like enums."}),"\n",e.jsx(c,{title:"Generating HTML using Structured Outputs",defaultLanguage:"python",code:{python:'\nfrom enum import Enum\nfrom typing import List\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclass UIType(str, Enum):\n    div = "div"\n    button = "button"\n    header = "header"\n    section = "section"\n    field = "field"\n    form = "form"\n\nclass Attribute(BaseModel):\n    name: str\n    value: str\n\nclass UI(BaseModel):\n    type: UIType\n    label: str\n    children: List["UI"] \n    attributes: List[Attribute]\n\nUI.model_rebuild() # This is required to enable recursive types\n\nclass Response(BaseModel):\n    ui: UI\n\ncompletion = client.beta.chat.completions.parse(\n    model="gpt-4o-2024-08-06",\n    messages=[\n        {"role": "system", "content": "You are a UI generator AI. Convert the user input into a UI."},\n        {"role": "user", "content": "Make a User Profile Form"}\n    ],\n    response_format=Response,\n)\n\nui = completion.choices[0].message.parsed\nprint(ui)\n'.trim(),"node.js":'\nimport OpenAI from "openai";\nimport { z } from "zod";\nimport { zodResponseFormat } from "openai/helpers/zod";\n\nconst openai = new OpenAI();\n\nconst UI = z.lazy(() =>\n  z.object({\n    type: z.enum(["div", "button", "header", "section", "field", "form"]),\n    label: z.string(),\n    children: z.array(UI),\n    attributes: z.array(\n      z.object({\n        name: z.string(),\n        value: z.string(),\n      })\n    ),\n  })\n);\n\nconst completion = await openai.beta.chat.completions.parse({\n  model: "gpt-4o-2024-08-06",\n  messages: [\n    {\n      role: "system",\n      content: "You are a UI generator AI. Convert the user input into a UI.",\n    },\n    { role: "user", content: "Make a User Profile Form" },\n  ],\n  response_format: zodResponseFormat(UI, "ui"),\n});\n\nconst ui = completion.choices[0].message.parsed;\n'.trim(),curl:'\ncurl https://api.openai.com/v1/chat/completions \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "gpt-4o-2024-08-06",\n    "messages": [\n      {\n        "role": "system",\n        "content": "You are a UI generator AI. Convert the user input into a UI."\n      },\n      {\n        "role": "user",\n        "content": "Make a User Profile Form"\n      }\n    ],\n    "response_format": {\n      "type": "json_schema",\n      "json_schema": {\n        "name": "ui",\n        "description": "Dynamically generated UI",\n        "schema": {\n          "type": "object",\n          "properties": {\n            "type": {\n              "type": "string",\n              "description": "The type of the UI component",\n              "enum": ["div", "button", "header", "section", "field", "form"]\n            },\n            "label": {\n              "type": "string",\n              "description": "The label of the UI component, used for buttons or form fields"\n            },\n            "children": {\n              "type": "array",\n              "description": "Nested UI components",\n              "items": {"$ref": "#"}\n            },\n            "attributes": {\n              "type": "array",\n              "description": "Arbitrary attributes for the UI component, suitable for any element",\n              "items": {\n                "type": "object",\n                "properties": {\n                  "name": {\n                    "type": "string",\n                    "description": "The name of the attribute, for example onClick or className"\n                  },\n                  "value": {\n                    "type": "string",\n                    "description": "The value of the attribute"\n                  }\n                },\n                "required": ["name", "value"],\n                "additionalProperties": false\n              }\n            }\n          },\n          "required": ["type", "label", "children", "attributes"],\n          "additionalProperties": false\n        },\n        "strict": true\n      }\n    }\n  }\'\n'.trim()}}),"\n",e.jsx(t.h4,{children:"Example response"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-json",children:'{\n    "type": "form",\n    "label": "User Profile Form",\n    "children": [\n        {\n            "type": "div",\n            "label": "",\n            "children": [\n                {\n                    "type": "field",\n                    "label": "First Name",\n                    "children": [],\n                    "attributes": [\n                        {\n                            "name": "type",\n                            "value": "text"\n                        },\n                        {\n                            "name": "name",\n                            "value": "firstName"\n                        },\n                        {\n                            "name": "placeholder",\n                            "value": "Enter your first name"\n                        }\n                    ]\n                },\n                {\n                    "type": "field",\n                    "label": "Last Name",\n                    "children": [],\n                    "attributes": [\n                        {\n                            "name": "type",\n                            "value": "text"\n                        },\n                        {\n                            "name": "name",\n                            "value": "lastName"\n                        },\n                        {\n                            "name": "placeholder",\n                            "value": "Enter your last name"\n                        }\n                    ]\n                }\n            ],\n            "attributes": []\n        },\n        {\n            "type": "button",\n            "label": "Submit",\n            "children": [],\n            "attributes": [\n                {\n                    "name": "type",\n                    "value": "submit"\n                }\n            ]\n        }\n    ],\n    "attributes": [\n        {\n            "name": "method",\n            "value": "post"\n        },\n        {\n            "name": "action",\n            "value": "/submit-profile"\n        }\n    ]\n}\n\n'})})]})}function Gi(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Pt,n)})):Pt(n)}function Lt(n){const t=Object.assign({h3:"h3",p:"p",h4:"h4",pre:"pre",code:"code"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h3,{children:"Moderation"}),"\n",e.jsx(t.p,{children:"You can classify inputs on multiple categories, which is a common way of doing moderation."}),"\n",e.jsx(c,{title:"Moderation using Structured Outputs",defaultLanguage:"python",code:{python:'\nfrom enum import Enum\nfrom typing import Optional\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclass Category(str, Enum):\n    violence = "violence"\n    sexual = "sexual"\n    self_harm = "self_harm"\n\nclass ContentCompliance(BaseModel):\n    is_violating: bool\n    category: Optional[Category]\n    explanation_if_violating: Optional[str]\n\ncompletion = client.beta.chat.completions.parse(\n    model="gpt-4o-2024-08-06",\n    messages=[\n        {"role": "system", "content": "Determine if the user input violates specific guidelines and explain if they do."},\n        {"role": "user", "content": "How do I prepare for a job interview?"}\n    ],\n    response_format=ContentCompliance,\n)\n\ncompliance = completion.choices[0].message.parsed\n'.trim(),"node.js":'\nimport OpenAI from "openai";\nimport { z } from "zod";\nimport { zodResponseFormat } from "openai/helpers/zod";\n\nconst openai = new OpenAI();\n\nconst ContentCompliance = z.object({\n  is_violating: z.boolean(),\n  category: z.enum(["violence", "sexual", "self_harm"]).nullable(),\n  explanation_if_violating: z.string().nullable(),\n});\n\nconst completion = await openai.beta.chat.completions.parse({\n  model: "gpt-4o-2024-08-06",\n  messages: [\n    { role: "system", content: "Determine if the user input violates specific guidelines and explain if they do." },\n    { role: "user", content: "How do I prepare for a job interview?" },\n  ],\n  response_format: zodResponseFormat(ContentCompliance, "content_compliance"),\n});\n\nconst compliance = completion.choices[0].message.parsed;\n'.trim(),curl:'\ncurl https://api.openai.com/v1/chat/completions \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "gpt-4o-2024-08-06",\n    "messages": [\n      {\n        "role": "system",\n        "content": "Determine if the user input violates specific guidelines and explain if they do."\n      },\n      {\n        "role": "user",\n        "content": "How do I prepare for a job interview?"\n      }\n    ],\n    "response_format": {\n      "type": "json_schema",\n      "json_schema": {\n        "name": "content_compliance",\n        "description": "Determines if content is violating specific moderation rules",\n        "schema": {\n          "type": "object",\n          "properties": {\n            "is_violating": {\n              "type": "boolean",\n              "description": "Indicates if the content is violating guidelines"\n            },\n            "category": {\n              "type": ["string", "null"],\n              "description": "Type of violation, if the content is violating guidelines. Null otherwise.",\n              "enum": ["violence", "sexual", "self_harm"]\n            },\n            "explanation_if_violating": {\n              "type": ["string", "null"],\n              "description": "Explanation of why the content is violating"\n            }\n          },\n          "required": ["is_violating", "category", "explanation_if_violating"],\n          "additionalProperties": false\n        },\n        "strict": true\n      }\n    }\n  }\'\n'.trim()}}),"\n",e.jsx(t.h4,{children:"Example response"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-json",children:'{\n  "is_violating": false,\n  "category": null,\n  "explanation_if_violating": null\n}\n'})})]})}function Wi(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Lt,n)})):Lt(n)}function Ot(n){const t=Object.assign({p:"p",pre:"pre",code:"code"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.p,{children:"To use Structured Outputs, simply specify"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-json",children:'response_format: { "type": "json_schema", "json_schema": … , "strict": true }\n'})}),"\n",e.jsx(t.p,{children:"For example:"}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\n    response = client.chat.completions.create(\n    model="gpt-4o-2024-08-06",\n    messages=[\n        {"role": "system", "content": "You are a helpful math tutor. Guide the user through the solution step by step."},\n        {"role": "user", "content": "how can I solve 8x + 7 = -23"}\n    ],\n    response_format={\n        "type": "json_schema",\n        "json_schema": {\n            "name": "math_response",\n            "schema": {\n                "type": "object",\n                "properties": {\n                    "steps": {\n                        "type": "array",\n                        "items": {\n                            "type": "object",\n                            "properties": {\n                                "explanation": {"type": "string"},\n                                "output": {"type": "string"}\n                            },\n                            "required": ["explanation", "output"],\n                            "additionalProperties": False\n                        }\n                    },\n                    "final_answer": {"type": "string"}\n                },\n                "required": ["steps", "final_answer"],\n                "additionalProperties": False\n            },\n            "strict": True\n        }\n    }\n    )\n\n    print(response.choices[0].message.content)\n'.trim(),"node.js":'\nconst response = await openai.chat.completions.create({\n    model: "gpt-4o-2024-08-06",\n    messages: [\n        { role: "system", content: "You are a helpful math tutor. Guide the user through the solution step by step." },\n        { role: "user", content: "how can I solve 8x + 7 = -23" }\n    ],\n    response_format: {\n        type: "json_schema",\n        json_schema: {\n            name: "math_response",\n            schema: {\n                type: "object",\n                properties: {\n                    steps: {\n                        type: "array",\n                        items: {\n                            type: "object",\n                            properties: {\n                                explanation: { type: "string" },\n                                output: { type: "string" }\n                            },\n                            required: ["explanation", "output"],\n                            additionalProperties: false\n                        }\n                    },\n                    final_answer: { type: "string" }\n                },\n                required: ["steps", "final_answer"],\n                additionalProperties: false\n            },\n            strict: true\n        }\n    }\n});\n\nconsole.log(response.choices[0].message.content);\n'.trim(),curl:'\ncurl https://api.openai.com/v1/chat/completions \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "gpt-4o-2024-08-06",\n    "messages": [\n      {\n        "role": "system",\n        "content": "You are a helpful math tutor. Guide the user through the solution step by step."\n      },\n      {\n        "role": "user",\n        "content": "how can I solve 8x + 7 = -23"\n      }\n    ],\n    "response_format": {\n      "type": "json_schema",\n      "json_schema": {\n        "name": "math_response",\n        "schema": {\n          "type": "object",\n          "properties": {\n            "steps": {\n              "type": "array",\n              "items": {\n                "type": "object",\n                "properties": {\n                  "explanation": { "type": "string" },\n                  "output": { "type": "string" }\n                },\n                "required": ["explanation", "output"],\n                "additionalProperties": false\n              }\n            },\n            "final_answer": { "type": "string" }\n          },\n          "required": ["steps", "final_answer"],\n          "additionalProperties": false\n        },\n        "strict": true\n      }\n    }\n  }\'\n'.trim()}})]})}function Yi(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Ot,n)})):Ot(n)}function St(n){const t=Object.assign({p:"p"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.p,{children:"In some cases, the model might not generate a valid response that matches the provided JSON schema."}),"\n",e.jsx(t.p,{children:"This can happen in the case of a refusal, if the model refuses to answer for safety reasons, or if for example you reach a max tokens limit and the response is incomplete."}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\ntry:\n  response = client.chat.completions.create(\n  model="gpt-4o-2024-08-06",\n  messages=[\n      {"role": "system", "content": "You are a helpful math tutor. Guide the user through the solution step by step."},\n      {"role": "user", "content": "how can I solve 8x + 7 = -23"}\n  ],\n  response_format={\n      "type": "json_schema",\n      "json_schema": {\n          "name": "math_response",\n          "strict": True,\n          "schema": {\n              "type": "object",\n              "properties": {\n                  "steps": {\n                      "type": "array",\n                      "items": {\n                          "type": "object",\n                          "properties": {\n                              "explanation": {"type": "string"},\n                              "output": {"type": "string"}\n                          },\n                          "required": ["explanation", "output"],\n                          "additionalProperties": False\n                      }\n                  },\n                  "final_answer": {"type": "string"}\n              },\n              "required": ["steps", "final_answer"],\n              "additionalProperties": False\n          }\n      }\n  },\n  strict=True\n  )\nexcept Exception as e:\n  # handle errors like finish_reason, refusal, content_filter, etc.\n  pass\n'.trim(),"node.js":'\n try {\n  const completion = await openai.chat.completions.create({\n    model: "gpt-4o-2024-08-06",\n    messages: [\n      {\n        role: "system",\n        content:\n          "You are a helpful math tutor. Guide the user through the solution step by step.",\n      },\n      { role: "user", content: "how can I solve 8x + 7 = -23" },\n    ],\n    response_format: {\n      type: "json_schema",\n      json_schema: {\n        name: "math_response",\n        schema: {\n          type: "object",\n          properties: {\n            steps: {\n              type: "array",\n              items: {\n                type: "object",\n                properties: {\n                  explanation: { type: "string" },\n                  output: { type: "string" },\n                },\n                required: ["explanation", "output"],\n                additionalProperties: false,\n              },\n            },\n            final_answer: { type: "string" },\n          },\n          required: ["steps", "final_answer"],\n          additionalProperties: false,\n        },\n        strict: true,\n      },\n    },\n    max_tokens: 50,\n  });\n\n  if (completion.choices[0].finish_reason === "length") {\n    // Handle the case where the model did not return a complete response\n    throw new Error("Incomplete response");\n  }\n\n  const math_response = completion.choices[0].message;\n\n  if (math_response.refusal) {\n    // handle refusal\n    console.log(math_response.refusal);\n  } else if (math_response.content) {\n    console.log(math_response.content);\n  } else {\n    throw new Error("No response content");\n  }\n} catch (e) {\n  // Handle edge cases\n  console.error(e);\n}\n\n'.trim()}})]})}function Bi(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(St,n)})):St(n)}function Mt(n){const t=Object.assign({p:"p"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.p,{children:"Typically, when using Structured Outputs you will have a type or class in the type-system of your programming language representing the JSON Schema as an object."}),"\n",e.jsx(t.p,{children:"Once you have confirmed that you have received the JSON guaranteed to match the schema you requested, you can now safely parse it to the corresponding type."}),"\n",e.jsx(t.p,{children:"For example:"}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:"\nfrom pydantic import BaseModel, ValidationError\nfrom typing import List\n\n# Define types that match the JSON Schema using pydantic models\nclass Step(BaseModel):\n    explanation: str\n    output: str\n\nclass Solution(BaseModel):\n    steps: List[Step]\n    final_answer: str\n\n...\n\ntry:\n    # Parse and validate the response content\n    solution = Solution.parse_raw(response.choices[0].message.content)\n    print(solution)\nexcept ValidationError as e:\n    # Handle validation errors\n    print(e.json())\n\n".trim(),"node.js":"\n// Here we specify types in TypeScript that exactly match the JSON Schema we provided when calling the OpenAI API. Note that these *must* be kept in sync.\n\ntype Step = {\n  explanation: string;\n  output: string;\n};\n\ntype Solution = {\n  steps: Step[];\n  final_answer: string;\n};\n\n...\n\n// Now so long as the JSON Schema we created was exactly equivalent to our TypeScript types, this is type-safe\nconst solution = JSON.parse(response.choices[0].message.content)) as Solution\n\n\n".trim()}})]})}function Ui(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Mt,n)})):Mt(n)}function qt(n){const t=Object.assign({p:"p",a:"a",h4:"h4",ul:"ul",li:"li",code:"code"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsxs(j,{label:"Step 1: Define your object",autoScroll:!0,showCollapse:!0,children:[e.jsxs(t.p,{children:["First you must define an object or data structure to represent the JSON Schema that the model should be constrained to follow. See the ",e.jsx(t.a,{href:"/docs/guides/structured-outputs/examples",children:"examples"})," at the top of this guide for reference."]}),e.jsxs(t.p,{children:["While Structured Outputs supports much of JSON Schema, some features are unavailable either for performance or technical reasons. See ",e.jsx(t.a,{href:"/docs/guides/structured-outputs/supported-schemas",children:"here"})," for more details."]}),e.jsx(t.p,{children:"For example, you can define an object like this:"}),e.jsx(c,{defaultLanguage:"python",code:{python:"\nfrom pydantic import BaseModel\n\nclass Step(BaseModel):\n    explanation: str\n    output: str\n\nclass MathResponse(BaseModel):\n    steps: list[Step]\n    final_answer: str\n".trim(),"node.js":'\nimport { z } from "zod";\nimport { zodResponseFormat } from "openai/helpers/zod";\n\nconst Step = z.object({\n  explanation: z.string(),\n  output: z.string(),\n});\n\nconst MathResponse = z.object({\n  steps: z.array(Step),\n  final_answer: z.string(),\n});\n'.trim()}}),e.jsx(t.h4,{children:"Tips for your data structure"}),e.jsx(t.p,{children:"To maximize the quality of model generations, we recommend the following:"}),e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Name keys clearly and intuitively"}),"\n",e.jsx(t.li,{children:"Create clear titles and descriptions for important keys in your structure"}),"\n",e.jsx(t.li,{children:"Create and use evals to determine the structure that works best for your use case"}),"\n"]})]}),"\n",e.jsxs(j,{label:"Step 2: Supply your object in the API call",autoScroll:!0,showCollapse:!0,children:[e.jsxs(t.p,{children:["You can use the ",e.jsx(t.code,{children:"parse"})," method to automatically parse the JSON response into the object you defined."]}),e.jsx(t.p,{children:"Under the hood, the SDK takes care of supplying the JSON schema corresponding to your data structure, and then parsing the response as an object."}),e.jsx(c,{defaultLanguage:"python",code:{python:'\ncompletion = client.beta.chat.completions.parse(\n    model="gpt-4o-2024-08-06",\n    messages=[\n        {"role": "system", "content": "You are a helpful math tutor. Guide the user through the solution step by step."},\n        {"role": "user", "content": "how can I solve 8x + 7 = -23"}\n    ],\n    response_format=MathResponse\n  )\n\n'.trim(),"node.js":'\nconst completion = await openai.beta.chat.completions.parse({\n  model: "gpt-4o-2024-08-06",\n  messages: [\n    { role: "system", content: "You are a helpful math tutor. Guide the user through the solution step by step." },\n    { role: "user", content: "how can I solve 8x + 7 = -23" },\n  ],\n  response_format: zodResponseFormat(MathResponse, "math_response"),\n});\n\n'.trim()}})]}),"\n",e.jsxs(j,{label:"Step 3: Handle edge cases",autoScroll:!0,showCollapse:!0,children:[e.jsx(t.p,{children:"In some cases, the model might not generate a valid response that matches the provided JSON schema."}),e.jsx(t.p,{children:"This can happen in the case of a refusal, if the model refuses to answer for safety reasons, or if for example you reach a max tokens limit and the response is incomplete."}),e.jsx(c,{defaultLanguage:"python",code:{python:'\ntry:\n    completion = client.beta.chat.completions.parse(\n        model="gpt-4o-2024-08-06",\n        messages=[\n            {"role": "system", "content": "You are a helpful math tutor. Guide the user through the solution step by step."},\n            {"role": "user", "content": "how can I solve 8x + 7 = -23"}\n        ],\n        response_format=MathResponse,\n        max_tokens=50\n    )\n    math_response = completion.choices[0].message\n    if math_response.parsed:\n        print(math_response.parsed)\n    elif math_response.refusal:\n        # handle refusal\n        print(math_response.refusal)\nexcept Exception as e:\n    # Handle edge cases\n    if type(e) == openai.LengthFinishReasonError:\n        # Retry with a higher max tokens\n        print("Too many tokens: ", e)\n        pass\n    else:\n        # Handle other exceptions\n        print(e)\n        pass\n'.trim(),"node.js":'\ntry {\n  const completion = await openai.beta.chat.completions.parse({\n    model: "gpt-4o-2024-08-06",\n    messages: [\n      {\n        role: "system",\n        content:\n          "You are a helpful math tutor. Guide the user through the solution step by step.",\n      },\n      { role: "user", content: "how can I solve 8x + 7 = -23" },\n    ],\n    response_format: zodResponseFormat(MathResponse, "math_response"),\n    max_tokens: 50,\n  });\n\n  const math_response = completion.choices[0].message;\n  console.log(math_response);\n  if (math_response.parsed) {\n    console.log(math_response.parsed);\n  } else if (math_response.refusal) {\n    // handle refusal\n    console.log(math_response.refusal);\n  }\n} catch (e) {\n  // Handle edge cases\n  if (e.constructor.name == "LengthFinishReasonError") {\n    // Retry with a higher max tokens\n    console.log("Too many tokens: ", e.message);\n  } else {\n    // Handle other exceptions\n    console.log("An error occurred: ", e.message);\n  }\n}\n'.trim()}})]}),"\n",e.jsxs(j,{label:"Step 4: Use the generated structured data in a type-safe way",autoScroll:!0,showCollapse:!0,children:[e.jsxs(t.p,{children:["When using the SDK, you can use the ",e.jsx(t.code,{children:"parsed"})," attribute to access the parsed JSON response as an object. This object will be of the type you defined in the ",e.jsx(t.code,{children:"response_format"})," parameter."]}),e.jsx(c,{defaultLanguage:"python",code:{python:"\nmath_response = completion.choices[0].message.parsed\nprint(math_response.steps)\nprint(math_response.final_answer)\n\n".trim(),"node.js":"\nconst math_response = completion.choices[0].message.parsed;\nconsole.log(math_response.steps);\nconsole.log(math_response.final_answer);\n".trim()}})]})]})}function $i(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(qt,n)})):qt(n)}function Ht(n){const t=Object.assign({p:"p",a:"a",h4:"h4",ul:"ul",li:"li",strong:"strong"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsxs(j,{label:"Step 1: Define your schema",autoScroll:!0,showCollapse:!0,children:[e.jsxs(t.p,{children:["First you must design the JSON Schema that the model should be constrained to follow. See the ",e.jsx(t.a,{href:"/docs/guides/structured-outputs/examples",children:"examples"})," at the top of this guide for reference."]}),e.jsxs(t.p,{children:["While Structured Outputs supports much of JSON Schema, some features are unavailable either for performance or technical reasons. See ",e.jsx(t.a,{href:"/docs/guides/structured-outputs/supported-schemas",children:"here"})," for more details."]}),e.jsx(t.h4,{children:"Tips for your JSON Schema"}),e.jsx(t.p,{children:"To maximize the quality of model generations, we recommend the following:"}),e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Name keys clearly and intuitively"}),"\n",e.jsx(t.li,{children:"Create clear titles and descriptions for important keys in your structure"}),"\n",e.jsx(t.li,{children:"Create and use evals to determine the structure that works best for your use case"}),"\n"]})]}),"\n",e.jsxs(j,{label:"Step 2: Supply your schema in the API call",autoScroll:!0,showCollapse:!0,children:[e.jsx(Yi,{}),e.jsxs(t.p,{children:[e.jsx(t.strong,{children:"Note:"})," the first request you make with any schema will have additional latency as our API processes the schema, but subsequent requests with the same schema will not have additional latency."]})]}),"\n",e.jsx(j,{label:"Step 3: Handle edge cases",autoScroll:!0,showCollapse:!0,children:e.jsx(Bi,{})}),"\n",e.jsx(j,{label:"Step 4: Use the generated structured data in a type-safe way",autoScroll:!0,showCollapse:!0,children:e.jsx(Ui,{})})]})}function Zi(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Ht,n)})):Ht(n)}function Et(n){const t=Object.assign({p:"p",a:"a",h4:"h4",ul:"ul",li:"li",code:"code",strong:"strong"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsxs(t.p,{children:["Structured Outputs supports a subset of the ",e.jsx(t.a,{href:"https://json-schema.org/docs",children:"JSON Schema"})," language."]}),"\n",e.jsx(t.h4,{children:"Supported types"}),"\n",e.jsx(t.p,{children:"The following types are supported for Structured Outputs:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"String"}),"\n",e.jsx(t.li,{children:"Number"}),"\n",e.jsx(t.li,{children:"Boolean"}),"\n",e.jsx(t.li,{children:"Integer"}),"\n",e.jsx(t.li,{children:"Object"}),"\n",e.jsx(t.li,{children:"Array"}),"\n",e.jsx(t.li,{children:"Enum"}),"\n",e.jsx(t.li,{children:"anyOf"}),"\n"]}),"\n",e.jsxs(t.h4,{children:["Root objects must not be ",e.jsx(t.code,{children:"anyOf"})]}),"\n",e.jsxs(t.p,{children:["Note that the root level object of a schema must be an object, and not use ",e.jsx(t.code,{children:"anyOf"}),". A pattern that appears in Zod (as one example) is using a discriminated union, which produces an ",e.jsx(t.code,{children:"anyOf"})," at the top level. So code such as the following won't work:"]}),"\n",e.jsx(c,{highlighted:!0,defaultLanguage:"javascript",code:{javascript:"\nimport { z } from 'zod';\nimport { zodResponseFormat } from 'openai/helpers/zod';\n\nconst BaseResponseSchema = z.object({ /* ... */ });\nconst UnsuccessfulResponseSchema = z.object({ /* ... */ });\n\nconst finalSchema = z.discriminatedUnion('status', [\n    BaseResponseSchema,\n    UnsuccessfulResponseSchema,\n]);\n\n// Invalid JSON Schema for Structured Outputs\nconst json = zodResponseFormat(finalSchema, 'final_schema');\n".trim()}}),"\n",e.jsxs(t.h4,{children:["All fields must be ",e.jsx(t.code,{children:"required"})]}),"\n",e.jsxs(t.p,{children:["To use Structured Outputs, all fields or function parameters must be specified as ",e.jsx(t.code,{children:"required"}),"."]}),"\n",e.jsx(c,{highlighted:!0,defaultLanguage:"json",code:{json:'\n{\n    "name": "get_weather",\n    "description": "Fetches the weather in the given location",\n    "strict": true,\n    "parameters": {\n        "type": "object",\n        "properties": {\n            "location": {\n                "type": "string",\n                "description": "The location to get the weather for"\n            },\n            "unit": {\n                "type": "string",\n                "description": "The unit to return the temperature in",\n                "enum": ["F", "C"]\n            }\n        },\n        "additionalProperties": false,\n        // highlight-start\n        "required": ["location", "unit"]\n        // highlight-end\n    }\n}\n'.trim()}}),"\n",e.jsxs(t.p,{children:["Although all fields must be required (and the model will return a value for each parameter), it is possible to emulate an optional parameter by using a union type with ",e.jsx(t.code,{children:"null"}),"."]}),"\n",e.jsx(c,{highlighted:!0,defaultLanguage:"json",code:{json:'\n{\n    "name": "get_weather",\n    "description": "Fetches the weather in the given location",\n    "strict": true,\n    "parameters": {\n        "type": "object",\n        "properties": {\n            "location": {\n                "type": "string",\n                "description": "The location to get the weather for"\n            },\n            "unit": {\n                // highlight-start\n                "type": ["string", "null"],\n                // highlight-end\n                "description": "The unit to return the temperature in",\n                "enum": ["F", "C"]\n            }\n        },\n        "additionalProperties": false,\n        "required": [\n            "location", "unit"\n        ]\n    }\n}\n\n'.trim()}}),"\n",e.jsx(t.h4,{children:"Objects have limitations on nesting depth and size"}),"\n",e.jsx(t.p,{children:"A schema may have up to 100 object properties total, with up to 5 levels of nesting."}),"\n",e.jsxs(t.h4,{children:[e.jsx(t.code,{children:"additionalProperties: false"})," must always be set in objects"]}),"\n",e.jsxs(t.p,{children:[e.jsx(t.code,{children:"additionalProperties"})," controls whether it is allowable for an object to contain additional keys / values that were not defined in the JSON Schema."]}),"\n",e.jsxs(t.p,{children:["Structured Outputs only supports generating specified keys / values, so we require developers to set ",e.jsx(t.code,{children:"additionalProperties: false"})," to opt into Structured Outputs."]}),"\n",e.jsx(c,{highlighted:!0,defaultLanguage:"json",code:{json:'\n{\n    "name": "get_weather",\n    "description": "Fetches the weather in the given location",\n    "strict": true,\n    "schema": {\n        "type": "object",\n        "properties": {\n            "location": {\n                "type": "string",\n                "description": "The location to get the weather for"\n            },\n            "unit": {\n                "type": "string",\n                "description": "The unit to return the temperature in",\n                "enum": ["F", "C"]\n            }\n        },\n        // highlight-start\n        "additionalProperties": false,\n        // highlight-end\n        "required": [\n            "location", "unit"\n        ]\n    }\n}\n\n\n'.trim()}}),"\n",e.jsx(t.h4,{children:"Key ordering"}),"\n",e.jsx(t.p,{children:"When using Structured Outputs, outputs will be produced in the same order as the ordering of keys in the schema."}),"\n",e.jsx(t.h4,{children:"Some type-specific keywords are not yet supported"}),"\n",e.jsx(t.p,{children:"Notable keywords not supported include:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"For strings:"})," ",e.jsx(t.code,{children:"minLength"}),", ",e.jsx(t.code,{children:"maxLength"}),", ",e.jsx(t.code,{children:"pattern"}),", ",e.jsx(t.code,{children:"format"})]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"For numbers:"})," ",e.jsx(t.code,{children:"minimum"}),", ",e.jsx(t.code,{children:"maximum"}),", ",e.jsx(t.code,{children:"multipleOf"})]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"For objects:"})," ",e.jsx(t.code,{children:"patternProperties"}),", ",e.jsx(t.code,{children:"unevaluatedProperties"}),", ",e.jsx(t.code,{children:"propertyNames"}),", ",e.jsx(t.code,{children:"minProperties"}),", ",e.jsx(t.code,{children:"maxProperties"})]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"For arrays:"})," ",e.jsx(t.code,{children:"unevaluatedItems"}),", ",e.jsx(t.code,{children:"contains"}),", ",e.jsx(t.code,{children:"minContains"}),", ",e.jsx(t.code,{children:"maxContains"}),", ",e.jsx(t.code,{children:"minItems"}),", ",e.jsx(t.code,{children:"maxItems"}),", ",e.jsx(t.code,{children:"uniqueItems"})]}),"\n"]}),"\n",e.jsxs(t.p,{children:["If you turn on Structured Outputs by supplying ",e.jsx(t.code,{children:"strict: true"})," and call the API with an unsupported JSON Schema, you will receive an error."]}),"\n",e.jsxs(t.h4,{children:["For ",e.jsx(t.code,{children:"anyOf"}),", the nested schemas must each be a valid JSON Schema per this subset"]}),"\n",e.jsx(t.p,{children:"Here's an example supported anyOf schema:"}),"\n",e.jsx(c,{highlighted:!0,defaultLanguage:"json",code:{json:'\n{\n	"type": "object",\n	"properties": {\n		"item": {\n			"anyOf": [\n				{\n					"type": "object",\n					"description": "The user object to insert into the database",\n					"properties": {\n						"name": {\n							"type": "string",\n							"description": "The name of the user"\n						},\n						"age": {\n							"type": "number",\n							"description": "The age of the user"\n						}\n					},\n					"additionalProperties": false,\n					"required": [\n						"name",\n						"age"\n					]\n				},\n				{\n					"type": "object",\n					"description": "The address object to insert into the database",\n					"properties": {\n						"number": {\n							"type": "string",\n							"description": "The number of the address. Eg. for 123 main st, this would be 123"\n						},\n						"street": {\n							"type": "string",\n							"description": "The street name. Eg. for 123 main st, this would be main st"\n						},\n						"city": {\n							"type": "string",\n							"description": "The city of the address"\n						}\n					},\n					"additionalProperties": false,\n					"required": [\n						"number",\n						"street",\n						"city"\n					]\n				}\n			]\n		}\n	},\n	"additionalProperties": false,\n	"required": [\n		"item"\n	]\n}\n\n\n'.trim()}}),"\n",e.jsx(t.h4,{children:"Definitions are supported"}),"\n",e.jsx(t.p,{children:"You can use definitions to define subschemas which are referenced throughout your schema. The following is a simple example."}),"\n",e.jsx(c,{highlighted:!0,defaultLanguage:"json",code:{json:'\n{\n	"type": "object",\n	"properties": {\n		"steps": {\n			"type": "array",\n			"items": {\n				"$ref": "#/$defs/step"\n			}\n		},\n		"final_answer": {\n			"type": "string"\n		}\n	},\n	"$defs": {\n		"step": {\n			"type": "object",\n			"properties": {\n				"explanation": {\n					"type": "string"\n				},\n				"output": {\n					"type": "string"\n				}\n			},\n			"required": [\n				"explanation",\n				"output"\n			],\n			"additionalProperties": false\n		}\n	},\n	"required": [\n		"steps",\n		"final_answer"\n	],\n	"additionalProperties": false\n}\n\n\n'.trim()}}),"\n",e.jsx(t.h4,{children:"Recursive schemas are supported"}),"\n",e.jsxs(t.p,{children:["Sample recursive schema using ",e.jsx(t.code,{children:"#"})," to indicate root recursion."]}),"\n",e.jsx(c,{highlighted:!0,defaultLanguage:"json",code:{json:'\n{\n        "name": "ui",\n        "description": "Dynamically generated UI",\n        "strict": true,\n        "schema": {\n            "type": "object",\n            "properties": {\n                "type": {\n                    "type": "string",\n                    "description": "The type of the UI component",\n                    "enum": ["div", "button", "header", "section", "field", "form"]\n                },\n                "label": {\n                    "type": "string",\n                    "description": "The label of the UI component, used for buttons or form fields"\n                },\n                "children": {\n                    "type": "array",\n                    "description": "Nested UI components",\n                    "items": {\n                        "$ref": "#"\n                    }\n                },\n                "attributes": {\n                    "type": "array",\n                    "description": "Arbitrary attributes for the UI component, suitable for any element",\n                    "items": {\n                        "type": "object",\n                        "properties": {\n                            "name": {\n                                "type": "string",\n                                "description": "The name of the attribute, for example onClick or className"\n                            },\n                            "value": {\n                                "type": "string",\n                                "description": "The value of the attribute"\n                            }\n                        },\n                      "additionalProperties": false,\n                      "required": ["name", "value"]\n                    }\n                }\n            },\n            "required": ["type", "label", "children", "attributes"],\n            "additionalProperties": false\n        }\n    }\n\n'.trim()}}),"\n",e.jsx(t.p,{children:"Sample recursive schema using explicit recursion:"}),"\n",e.jsx(c,{highlighted:!0,defaultLanguage:"json",code:{json:'\n{\n	"type": "object",\n	"properties": {\n		"linked_list": {\n			"$ref": "#/$defs/linked_list_node"\n		}\n	},\n	"$defs": {\n		"linked_list_node": {\n			"type": "object",\n			"properties": {\n				"value": {\n					"type": "number"\n				},\n				"next": {\n					"anyOf": [\n						{\n							"$ref": "#/$defs/linked_list_node"\n						},\n						{\n							"type": "null"\n						}\n					]\n				}\n			},\n			"additionalProperties": false,\n			"required": [\n				"next",\n				"value"\n			]\n		}\n	},\n	"additionalProperties": false,\n	"required": [\n		"linked_list"\n	]\n}\n\n\n'.trim()}})]})}function Xi(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Et,n)})):Et(n)}function Nt(n){return e.jsx(c,{defaultLanguage:"python",code:{python:'\nwe_did_not_specify_stop_tokens = True\n\ntry:\n    response = client.chat.completions.create(\n        model="gpt-3.5-turbo-0125",\n        messages=[\n            {"role": "system", "content": "You are a helpful assistant designed to output JSON."},\n            {"role": "user", "content": "Who won the world series in 2020? Please respond in the format {winner: ...}"}\n        ],\n        response_format={"type": "json_object"}\n    )\n\n    # Check if the conversation was too long for the context window, resulting in incomplete JSON \n    if response.choices[0].message.finish_reason == "length":\n        # your code should handle this error case\n        pass\n\n    # Check if the OpenAI safety system refused the request and generated a refusal instead\n    if response.choices[0].message[0].get("refusal"):\n        # your code should handle this error case\n        # In this case, the .content field will contain the explanation (if any) that the model generated for why it is refusing\n        print(response.choices[0].message[0]["refusal"])\n\n    # Check if the model\'s output included restricted content, so the generation of JSON was halted and may be partial\n    if response.choices[0].message.finish_reason == "content_filter":\n        # your code should handle this error case\n        pass\n\n    if response.choices[0].message.finish_reason == "stop":\n        # In this case the model has either successfully finished generating the JSON object according to your schema, or the model generated one of the tokens you provided as a "stop token"\n\n        if we_did_not_specify_stop_tokens:\n            # If you didn\'t specify any stop tokens, then the generation is complete and the content key will contain the serialized JSON object\n            # This will parse successfully and should now contain  "{"winner": "Los Angeles Dodgers"}"\n            print(response.choices[0].message.content)\n        else:\n            # Check if the response.choices[0].message.content ends with one of your stop tokens and handle appropriately\n            pass\nexcept Exception as e:\n    # Your code should handle errors here, for example a network error calling the API\n    print(e)\n'.trim(),"node.js":'\nconst we_did_not_specify_stop_tokens = true;\n\n  try {\n    const response = await openai.chat.completions.create({\n      model: "gpt-3.5-turbo-0125",\n      messages: [\n        {\n          role: "system",\n          content: "You are a helpful assistant designed to output JSON.",\n        },\n        { role: "user", content: "Who won the world series in 2020? Please respond in the format {winner: ...}" },\n      ],\n      response_format: { type: "json_object" },\n    });\n\n    // Check if the conversation was too long for the context window, resulting in incomplete JSON \n    if (response.choices[0].message.finish_reason === "length") {\n      // your code should handle this error case\n    }\n\n    // Check if the OpenAI safety system refused the request and generated a refusal instead\n    if (response.choices[0].message[0].refusal) {\n      // your code should handle this error case\n      // In this case, the .content field will contain the explanation (if any) that the model generated for why it is refusing\n      console.log(response.choices[0].message[0].refusal)\n    }\n\n    // Check if the model\'s output included restricted content, so the generation of JSON was halted and may be partial\n    if (response.choices[0].message.finish_reason === "content_filter") {\n      // your code should handle this error case\n    }\n\n    if (response.choices[0].message.finish_reason === "stop") {\n      // In this case the model has either successfully finished generating the JSON object according to your schema, or the model generated one of the tokens you provided as a "stop token"\n\n      if (we_did_not_specify_stop_tokens) {\n        // If you didn\'t specify any stop tokens, then the generation is complete and the content key will contain the serialized JSON object\n        // This will parse successfully and should now contain  {"winner": "Los Angeles Dodgers"}\n        console.log(JSON.parse(response.choices[0].message.content))\n      } else {\n        // Check if the response.choices[0].message.content ends with one of your stop tokens and handle appropriately\n      }\n    }\n  } catch (e) {\n    // Your code should handle errors here, for example a network error calling the API\n    console.error(e)\n  }\n\n'.trim()}})}function Ji(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Nt,n)})):Nt()}function Vt(n){const t=Object.assign({h1:"h1",h2:"h2",p:"p",a:"a",ol:"ol",li:"li",strong:"strong",h3:"h3",ul:"ul",code:"code",table:"table",thead:"thead",tr:"tr",th:"th",tbody:"tbody",td:"td",h4:"h4"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"Structured Outputs"}),"\n",e.jsx(t.h2,{children:"Introduction"}),"\n",e.jsx(t.p,{children:"JSON is one of the most widely used formats in the world for applications to exchange data."}),"\n",e.jsxs(t.p,{children:["Structured Outputs is a feature that ensures the model will always generate responses that adhere to your supplied ",e.jsx(t.a,{href:"https://json-schema.org/overview/what-is-jsonschema",children:"JSON Schema"}),", so you don't need to worry about the model omitting a required key, or hallucinating an invalid enum value."]}),"\n",e.jsx(t.p,{children:"Some benefits of Structed Outputs include:"}),"\n",e.jsxs(t.ol,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Reliable type-safety:"})," No need to validate or retry incorrectly formatted responses"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Explicit refusals:"})," Safety-based model refusals are now programmatically detectable"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Simpler prompting:"})," No need for strongly worded prompts to achieve consistent formatting"]}),"\n"]}),"\n",e.jsxs(t.p,{children:["In addition to supporting JSON Schema in the REST API, the OpenAI SDKs for ",e.jsx(t.a,{href:"https://github.com/openai/openai-python/blob/main/helpers.md#structured-outputs-parsing-helpers",children:"Python"})," and ",e.jsx(t.a,{href:"https://github.com/openai/openai-node/blob/master/helpers.md#structured-outputs-parsing-helpers",children:"JavaScript"})," also make it easy to define object schemas using ",e.jsx(t.a,{href:"https://docs.pydantic.dev/latest/",children:"Pydantic"})," and ",e.jsx(t.a,{href:"https://zod.dev/",children:"Zod"})," respectively. Below, you can see how to extract information from unstructured text that conforms to a schema defined in code."]}),"\n",e.jsx(Di,{}),"\n",e.jsx(t.h3,{children:"Supported models"}),"\n",e.jsxs(t.p,{children:["Structured Outputs are available in our ",e.jsx(t.a,{href:"/docs/models",children:"latest large language models"}),", starting with GPT-4o:"]}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"gpt-4o-mini-2024-07-18"})," and later"]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"gpt-4o-2024-08-06"})," and later"]}),"\n"]}),"\n",e.jsxs(t.p,{children:["Older models like ",e.jsx(t.code,{children:"gpt-4-turbo"})," and earlier may use ",e.jsx(t.a,{href:"/docs/guides/structured-outputs/json-mode",children:"JSON mode"})," instead."]}),"\n",e.jsx(I,{level:2,slug:"function-calling-vs-response-format",children:e.jsxs(t.p,{children:["When to use Structured Outputs via function calling vs via ",e.jsx("span",{class:"monospace",children:"response_format"})]})}),"\n",e.jsx(t.p,{children:"Structured Outputs is available in two forms in the OpenAI API:"}),"\n",e.jsxs(t.ol,{children:["\n",e.jsxs(t.li,{children:["When using ",e.jsx(t.a,{href:"/docs/guides/function-calling",children:"function calling"})]}),"\n",e.jsxs(t.li,{children:["When using a ",e.jsx(t.code,{children:"json_schema"})," response format"]}),"\n"]}),"\n",e.jsx(t.p,{children:"Function calling is useful when you are building an application that bridges the models and functionality of your application."}),"\n",e.jsx(t.p,{children:"For example, you can give the model access to functions that query a database in order to build an AI assistant that can help users with their orders, or functions that can interact with the UI."}),"\n",e.jsxs(t.p,{children:["Conversely, Structured Outputs via ",e.jsx(t.code,{children:"response_format"})," are more suitable when you want to indicate a structured schema for use when the model responds to the user, rather than when the model calls a tool."]}),"\n",e.jsx(t.p,{children:"For example, if you are building a math tutoring application, you might want the assistant to respond to your user using a specific JSON Schema so that you can generate a UI that displays different parts of the model's output in distinct ways."}),"\n",e.jsx(t.p,{children:"Put simply:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"If you are connecting the model to tools, functions, data, etc. in your system, then you should use function calling"}),"\n",e.jsxs(t.li,{children:["If you want to structure the model's output when it responds to the user, then you should use a structured ",e.jsx(t.code,{children:"response_format"})]}),"\n"]}),"\n",e.jsx(b,{children:e.jsxs(t.p,{children:["The remainder of this guide will focus on non-function calling use cases in the Chat Completions API. To learn more about how to use Structured Outputs with function calling, check out the ",e.jsx(A,{to:"/docs/guides/function-calling/function-calling-with-structured-outputs",children:"Function Calling"})," guide."]})}),"\n",e.jsx(t.h3,{children:"Structured Outputs vs JSON mode"}),"\n",e.jsxs(t.p,{children:["Structured Outputs is the evolution of ",e.jsx(t.a,{href:"/docs/guides/structured-outputs/json-mode",children:"JSON mode"}),". While both ensure valid JSON is produced, only Structured Outputs ensure schema adherance. Both Structured Outputs and JSON mode are supported in the Chat Completions API, Assistants API, Fine-tuning API and Batch API."]}),"\n",e.jsx(t.p,{children:"We recommend always using Structured Outputs instead of JSON mode when possible."}),"\n",e.jsxs(t.p,{children:["However, Structured Outputs with ",e.jsx(t.code,{children:'response_format: {type: "json_schema", ...}'})," is only supported with the ",e.jsx(t.code,{children:"gpt-4o-mini"}),", ",e.jsx(t.code,{children:"gpt-4o-mini-2024-07-18"}),", and ",e.jsx(t.code,{children:"gpt-4o-2024-08-06"})," model snapshots and later."]}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{}),e.jsx(t.th,{children:"Structured Outputs"}),e.jsx(t.th,{children:"JSON Mode"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.strong,{children:"Outputs valid JSON"})}),e.jsx(t.td,{children:"Yes"}),e.jsx(t.td,{children:"Yes"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.strong,{children:"Adheres to schema"})}),e.jsxs(t.td,{children:["Yes (see ",e.jsx(t.a,{href:"/docs/guides/structured-outputs/supported-schemas",children:"supported schemas"}),")"]}),e.jsx(t.td,{children:"No"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.strong,{children:"Compatible models"})}),e.jsxs(t.td,{children:[e.jsx(t.code,{children:"gpt-4o-mini"}),", ",e.jsx(t.code,{children:"gpt-4o-2024-08-06"}),", and later"]}),e.jsxs(t.td,{children:[e.jsx(t.code,{children:"gpt-3.5-turbo"}),", ",e.jsx(t.code,{children:"gpt-4-*"})," and ",e.jsx(t.code,{children:"gpt-4o-*"})," models"]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.strong,{children:"Enabling"})}),e.jsx(t.td,{children:e.jsx(t.code,{children:'response_format: { type: "json_schema", json_schema: {"strict": true, "schema": ...} }'})}),e.jsx(t.td,{children:e.jsx(t.code,{children:'response_format: { type: "json_object" }'})})]})]})]}),"\n",e.jsx(t.h2,{children:"Examples"}),"\n",e.jsx(G,{options:[{value:"ex1",label:"Chain of thought",content:e.jsx(Fi,{})},{value:"ex2",label:"Structured data extraction",content:e.jsx(Ri,{})},{value:"ex3",label:"UI generation",content:e.jsx(Gi,{})},{value:"ex4",label:"Moderation",content:e.jsx(Wi,{})}],initialValue:"ex1"}),"\n",e.jsx(I,{level:2,slug:"how-to-use",children:e.jsxs(t.p,{children:["How to use Structured Outputs with ",e.jsx("span",{class:"monospace",children:"response_format"})]})}),"\n",e.jsx(t.p,{children:"You can use Structured Outputs with the new SDK helper to parse the model's output into your desired format, or you can specify the JSON schema directly."}),"\n",e.jsx(b,{children:e.jsxs(t.p,{children:[e.jsx(t.strong,{children:"Note:"})," the first request you make with any schema will have additional latency as our API processes the schema, but subsequent requests with the same schema will not have additional latency."]})}),"\n",e.jsx(G,{options:[{value:"with_parse",label:"SDK objects",content:e.jsx($i,{})},{value:"without_parse",label:"Manual schema",content:e.jsx(Zi,{})}],initialValue:"with_parse"}),"\n",e.jsx(I,{level:3,slug:"refusals",children:e.jsx(t.p,{children:"Refusals with Structured Outputs"})}),"\n",e.jsxs(t.p,{children:["When using Structured Outputs with user-generated input, OpenAI models may occasionally refuse to fulfill the request for safety reasons. Since a refusal does not necessarily follow the schema you have supplied in ",e.jsx(t.code,{children:"response_format"}),", the API response will include a new field called ",e.jsx(t.code,{children:"refusal"})," to indicate that the model refused to fulfill the request."]}),"\n",e.jsxs(t.p,{children:["When the ",e.jsx(t.code,{children:"refusal"})," property appears in your output object, you might present the refusal in your UI, or include conditional logic in code that consumes the response to handle the case of a refused request."]}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\nclass Step(BaseModel):\n    explanation: str\n    output: str\n\nclass MathReasoning(BaseModel):\n    steps: list[Step]\n    final_answer: str\n\ncompletion = client.beta.chat.completions.parse(\n    model="gpt-4o-2024-08-06",\n    messages=[\n        {"role": "system", "content": "You are a helpful math tutor. Guide the user through the solution step by step."},\n        {"role": "user", "content": "how can I solve 8x + 7 = -23"}\n    ],\n    response_format=MathReasoning,\n)\n\nmath_reasoning = completion.choices[0].message\n\n# If the model refuses to respond, you will get a refusal message\nif (math_reasoning.refusal):\n    print(math_reasoning.refusal)\nelse:\n    print(math_reasoning.parsed)\n'.trim(),"node.js":'\n const Step = z.object({\n  explanation: z.string(),\n  output: z.string(),\n});\n\nconst MathReasoning = z.object({\n  steps: z.array(Step),\n  final_answer: z.string(),\n});\n\nconst completion = await openai.beta.chat.completions.parse({\n  model: "gpt-4o-2024-08-06",\n  messages: [\n    { role: "system", content: "You are a helpful math tutor. Guide the user through the solution step by step." },\n    { role: "user", content: "how can I solve 8x + 7 = -23" },\n  ],\n  response_format: zodResponseFormat(MathReasoning, "math_reasoning"),\n});\n\nconst math_reasoning = completion.choices[0].message\n\n// If the model refuses to respond, you will get a refusal message\nif (math_reasoning.refusal) {\n  console.log(math_reasoning.refusal);\n} else {\n  console.log(math_reasoning.parsed);\n}\n'.trim()}}),"\n",e.jsx(t.p,{children:"The API response from a refusal will look something like this:"}),"\n",e.jsx(c,{highlighted:!0,defaultLanguage:"json",code:{json:'\n{\n  "id": "chatcmpl-9nYAG9LPNonX8DAyrkwYfemr3C8HC",\n  "object": "chat.completion",\n  "created": 1721596428,\n  "model": "gpt-4o-2024-08-06",\n  "choices": [\n    {\n	  "index": 0,\n	  "message": {\n            "role": "assistant",\n            // highlight-start\n            "refusal": "I\'m sorry, I cannot assist with that request."\n            // highlight-end\n	  },\n	  "logprobs": null,\n	  "finish_reason": "stop"\n	}\n  ],\n  "usage": {\n      "prompt_tokens": 81,\n      "completion_tokens": 11,\n      "total_tokens": 92\n  },\n  "system_fingerprint": "fp_3407719c7f"\n}\n'.trim()}}),"\n",e.jsx(I,{level:3,slug:"best-practices",children:e.jsx(t.p,{children:"Tips and best practices"})}),"\n",e.jsx(t.h4,{children:"Handling user-generated input"}),"\n",e.jsx(t.p,{children:"If your application is using user-generated input, make sure your prompt includes instructions on how to handle situations where the input cannot result in a valid response."}),"\n",e.jsx(t.p,{children:"The model will always try to adhere to the provided schema, which can result in hallucinations if the input is completely unrelated to the schema."}),"\n",e.jsx(t.p,{children:"You could include language in your prompt to specify that you want to return empty parameters, or a specific sentence, if the model detects that the input is incompatible with the task."}),"\n",e.jsx(t.h4,{children:"Handling mistakes"}),"\n",e.jsxs(t.p,{children:["Structured Outputs can still contain mistakes. If you see mistakes, try adjusting your instructions, providing examples in the system instructions, or splitting tasks into simpler subtasks. Refer to the ",e.jsx(t.a,{href:"/docs/guides/prompt-engineering",children:"prompt engineering guide"})," for more guidance on how to tweak your inputs."]}),"\n",e.jsx(t.h4,{children:"Avoid JSON schema divergence"}),"\n",e.jsx(t.p,{children:"To prevent your JSON Schema and corresponding types in your programming language from diverging, we strongly recommend using the native Pydantic/zod sdk support."}),"\n",e.jsx(t.p,{children:"If you prefer to specify the JSON schema directly, you could add CI rules that flag when either the JSON schema or underlying data objects are edited, or add a CI step that auto-generates the JSON Schema from type definitions (or vice-versa)."}),"\n",e.jsx(t.h2,{children:"Supported schemas"}),"\n",e.jsx(Xi,{}),"\n",e.jsx(t.h2,{children:"JSON mode"}),"\n",e.jsx(b,{children:e.jsx(t.p,{children:"JSON mode is a more basic version of the Structured Outputs feature. While JSON mode ensures that model output is valid JSON, Structured Outputs reliably matches the model's output to the schema you specify.\nWe recommend you use Structured Outputs if it is supported for your use case."})}),"\n",e.jsx(t.p,{children:"When JSON mode is turned on, the model's output is ensured to be valid JSON, except for in some edge cases that you should detect and handle appropriately."}),"\n",e.jsxs(t.p,{children:["To turn on JSON mode with the Chat Completions or Assistants API you can set the ",e.jsx(t.code,{children:"response_format"})," to ",e.jsx(t.code,{children:'{ "type": "json_object" }'}),". If you are using function calling, JSON mode is always turned on."]}),"\n",e.jsx(t.p,{children:"Important notes:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"When using JSON mode, you must always instruct the model to produce JSON via some message in the conversation, for example via your system message. If you don't include an explicit instruction to generate JSON, the model may generate an unending stream of whitespace and the request may run continually until it reaches the token limit. To help ensure you don't forget, the API will throw an error if the string \"JSON\" does not appear somewhere in the context."}),"\n",e.jsx(t.li,{children:"JSON mode will not guarantee the output matches any specific schema, only that it is valid and parses without errors. You should use Structured Outputs to ensure it matches your schema, or if that is not possible, you should use a validation library and potentially retries to ensure that the output matches your desired schema."}),"\n",e.jsx(t.li,{children:"Your application must detect and handle the edge cases that can result in the model output not being a complete JSON object (see below)"}),"\n"]}),"\n",e.jsx(j,{label:"Handling edge cases",autoScroll:!0,showCollapse:!0,children:e.jsx(Ji,{})}),"\n",e.jsx(t.h2,{children:"Resources"}),"\n",e.jsx(t.p,{children:"To learn more about Structured Outputs, we recommend browsing the following resources:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["Check out our ",e.jsx(t.a,{href:"https://cookbook.openai.com/examples/structured_outputs_intro",children:"introductory cookbook"})," on Structured Outputs"]}),"\n",e.jsxs(t.li,{children:["Learn ",e.jsx(t.a,{href:"https://cookbook.openai.com/examples/structured_outputs_multi_agent",children:"how to build multi-agent systems"})," with Structured Outputs"]}),"\n"]})]})}function Ki(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Vt,n)})):Vt(n)}function zt(n){const t=Object.assign({h1:"h1",p:"p",a:"a",ul:"ul",li:"li",code:"code",h2:"h2",h3:"h3",em:"em"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"Text generation models"}),"\n",e.jsxs(t.p,{children:["OpenAI's text generation ",e.jsx(t.a,{href:"/docs/models",children:"models"}),' (often called generative pre-trained transformers or large language models) have been trained to understand natural language, code, and images. The models provide text outputs in response to their inputs. The text inputs to these models are also referred to as "prompts". Designing a prompt is essentially how you “program” a large language model model, usually by providing instructions or some examples of how to successfully complete a task.']}),"\n",e.jsx(t.p,{children:"Using OpenAI's text generation models, you can build applications to:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Draft documents"}),"\n",e.jsx(t.li,{children:"Write computer code"}),"\n",e.jsx(t.li,{children:"Answer questions about a knowledge base"}),"\n",e.jsx(t.li,{children:"Analyze texts"}),"\n",e.jsx(t.li,{children:"Give software a natural language interface"}),"\n",e.jsx(t.li,{children:"Tutor in a range of subjects"}),"\n",e.jsx(t.li,{children:"Translate languages"}),"\n",e.jsx(t.li,{children:"Simulate characters for games"}),"\n"]}),"\n",e.jsx(A,{to:"/docs/examples",children:e.jsx(C,{icon:e.jsx(me,{}),title:"Prompt examples",className:"mt-6 mb-6",children:e.jsx(t.p,{children:"Explore prompt examples for inspiration"})})}),"\n",e.jsx(t.p,{children:"To use one of these models via the OpenAI API, you’ll send a request to the Chat Completions API containing the inputs and your API key, and receive a response containing the model’s output."}),"\n",e.jsxs(t.p,{children:["You can experiment with various models in the ",e.jsx(t.a,{href:"https://platform.openai.com/playground?mode=chat",children:"chat playground"}),". If you’re not sure which model to use then try ",e.jsx(t.code,{children:"gpt-4o"})," if you need high intelligence or ",e.jsx(t.code,{children:"gpt-4o-mini"})," if you need the fastest speed and lowest cost."]}),"\n",e.jsx(t.h2,{children:"Quickstart"}),"\n",e.jsx(t.p,{children:"Chat models take a list of messages as input and return a model-generated message as output. Although the chat format is designed to make multi-turn conversations easy, it's just as useful for single-turn tasks without any conversation."}),"\n",e.jsx(t.p,{children:"An example Chat Completions API call looks like the following:"}),"\n",e.jsx(c,{defaultLanguage:"python",code:{python:'\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n  model="gpt-4o-mini",\n  messages=[\n    {"role": "system", "content": "You are a helpful assistant."},\n    {"role": "user", "content": "What is a LLM?"}\n  ]\n)\n'.trim(),"node.js":'\nimport OpenAI from "openai";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const completion = await openai.chat.completions.create({\n    messages: [\n        {"role": "system", "content": "You are a helpful assistant."},\n        {"role": "user", "content": "What is a LLM?"}\n      ],\n    model: "gpt-4o-mini",\n  });\n\n  console.log(completion.choices[0]);\n}\nmain();\n'.trim(),curl:'\ncurl https://api.openai.com/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -d \'{\n    "model": "gpt-4o-mini",\n    "messages": [\n      {\n        "role": "system",\n        "content": "You are a helpful assistant."\n      },\n      {\n        "role": "user",\n        "content": "What is a LLM?"\n      }\n    ]\n  }\'\n'.trim()}}),"\n",e.jsxs(t.p,{children:["To learn more, you can view the ",e.jsx(t.a,{href:"/docs/guides/chat-completions",children:"Chat Completions guide"}),"."]}),"\n",e.jsx(t.h2,{children:"Prompt engineering"}),"\n",e.jsx(t.p,{children:'An awareness of the best practices for working with OpenAI models can make a significant difference in application performance.\nThe failure modes that each exhibit and the ways of working around or correcting those failure modes are not always intuitive.\nThere is an entire field related to working with language models which has come to be known as "prompt engineering", but as the field has progressed its scope has outgrown merely engineering the prompt into engineering systems that use model queries as components.'}),"\n",e.jsxs(t.p,{children:["To learn more, read our guide on ",e.jsx(t.a,{href:"/docs/guides/prompt-engineering",children:"prompt engineering"})," which covers methods to improve model reasoning, reduce the likelihood of model hallucinations, and more."]}),"\n",e.jsxs(t.p,{children:["You can also find many useful resources including code samples in the ",e.jsx(t.a,{href:"https://cookbook.openai.com",children:"OpenAI Cookbook"}),"."]}),"\n",e.jsx(t.h2,{children:"FAQ"}),"\n",e.jsx(t.h3,{children:"Which model should I use?"}),"\n",e.jsxs(t.p,{children:["We generally recommend that you default to using either ",e.jsx(t.code,{children:"gpt-4o"})," or ",e.jsx(t.code,{children:"gpt-4o-mini"}),"."]}),"\n",e.jsxs(t.p,{children:["If your use case requires high intelligence or reasoning about images as well as text, we recommend you evaluate both ",e.jsx(t.code,{children:"gpt-4o"})," and ",e.jsx(t.code,{children:"gpt-4-turbo"})," (although they have very similar intelligence, note that ",e.jsx(t.code,{children:"gpt-4o"})," is both faster and cheaper)."]}),"\n",e.jsxs(t.p,{children:["If your use case requires the fastest speed and lowest cost, we recommend ",e.jsx(t.code,{children:"gpt-4o-mini"})," since it is optimized for these aspects."]}),"\n",e.jsxs(t.p,{children:["We recommend using ",e.jsx(t.code,{children:"gpt-4o-mini"})," where you would have previously used ",e.jsx(t.code,{children:"gpt-3.5-turbo"})," as it is cheaper with higher intelligence, has a larger context window (up to 128,000 tokens compared to 4,096 tokens for ",e.jsx(t.code,{children:"gpt-3.5-turbo"}),"), and is multimodal."]}),"\n",e.jsxs(t.p,{children:["You can experiment in the ",e.jsx(t.a,{href:"https://platform.openai.com/playground?mode=chat",children:"playground"})," to investigate which models provide the best price performance trade-off for your usage. A common design pattern is to use several distinct query types which are each dispatched to the model appropriate to handle them."]}),"\n",e.jsx(t.h3,{children:"How should I set the temperature parameter?"}),"\n",e.jsxs(t.p,{children:["You can think of temperature like ",e.jsx(t.em,{children:"randomness"}),", with 0 being ",e.jsx(t.em,{children:"least random"})," (or ",e.jsx(t.em,{children:"most deterministic"}),")\nand 2 being ",e.jsx(t.em,{children:"most random"})," (",e.jsx(t.em,{children:"least deterministic"}),"). When using low values for temperature (e.g. 0.2) the\nmodel responses will tend to be more consistent but may feel more robotic. Values higher than 1.0,\nespecially values close to 2.0, can lead to erratic model outputs. If your goal is creative\noutputs, a combination of a slightly higher than normal temperature (e.g. 1.2) combined with a prompt\nspecifically asking the model to be creative may be your best bet, but we encourage experimentation."]}),"\n",e.jsx(t.h3,{children:"Is fine-tuning available for the latest models?"}),"\n",e.jsxs(t.p,{children:["See the ",e.jsx(t.a,{href:"/docs/guides/fine-tuning",children:"fine-tuning guide"})," for the latest information on which models are available for fine-tuning and how to get started."]}),"\n",e.jsx(t.h3,{children:"Do you store the data that is passed into the API?"}),"\n",e.jsxs(t.p,{children:["As of March 1st, 2023, we retain your API data for 30 days but no longer use your data sent via the API to improve our models. Learn more in our ",e.jsx(t.a,{href:"https://openai.com/policies/usage-policies",children:"data usage policy"}),". Some endpoints offer ",e.jsx(t.a,{href:"/docs/models/default-usage-policies-by-endpoint",children:"zero retention"}),"."]}),"\n",e.jsx(t.h3,{children:"How can I make my application more safe?"}),"\n",e.jsxs(t.p,{children:["If you want to add a moderation layer to the outputs of the Chat API, you can follow our ",e.jsx(t.a,{href:"/docs/guides/moderation",children:"moderation guide"})," to prevent content that violates OpenAI’s usage policies from being shown. We also encourage you to read our ",e.jsx(t.a,{href:"/docs/guides/safety-best-practices",children:"safety guide"})," for more information on how to build safer systems."]}),"\n",e.jsx(t.h3,{children:"Should I use ChatGPT or the API?"}),"\n",e.jsxs(t.p,{children:[e.jsx(t.a,{href:"https://chatgpt.com",children:"ChatGPT"})," offers a chat interface for our models and a range of built-in features such as integrated browsing, code execution, plugins, and more. By contrast, using OpenAI’s API provides more flexibility but requires that you write code or send the requests to our models programmatically."]})]})}function Qi(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(zt,n)})):zt(n)}function Dt(n){const t=Object.assign({h1:"h1",p:"p",h2:"h2",a:"a",code:"code",ul:"ul",li:"li",h3:"h3",h4:"h4",pre:"pre",strong:"strong"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"Text to speech"}),"\n",e.jsx(t.p,{children:"Learn how to turn text into lifelike spoken audio"}),"\n",e.jsx(t.h2,{children:"Overview"}),"\n",e.jsxs(t.p,{children:["The Audio API provides a ",e.jsx(t.a,{href:"/docs/api-reference/audio/createSpeech",children:e.jsx(t.code,{children:"speech"})})," endpoint based on our ",e.jsx(t.a,{href:"/docs/models/tts",children:"TTS (text-to-speech) model"}),". It comes with 6 built-in voices and can be used to:"]}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Narrate a written blog post"}),"\n",e.jsx(t.li,{children:"Produce spoken audio in multiple languages"}),"\n",e.jsx(t.li,{children:"Give real time audio output using streaming"}),"\n"]}),"\n",e.jsxs(t.p,{children:["Here is an example of the ",e.jsx(t.code,{children:"alloy"})," voice:"]}),"\n",e.jsx(F,{audioUrl:"https://cdn.openai.com/API/docs/audio/alloy.wav"}),"\n",e.jsx(b,{children:e.jsxs(t.p,{children:["Please note that our ",e.jsx(t.a,{href:"https://openai.com/policies/usage-policies",children:"usage policies"})," require you to provide a clear disclosure to end users that the TTS voice they are hearing is AI-generated and not a human voice."]})}),"\n",e.jsx(t.h2,{children:"Quickstart"}),"\n",e.jsxs(t.p,{children:["The ",e.jsx(t.code,{children:"speech"})," endpoint takes in three key inputs: the ",e.jsx(t.a,{href:"/docs/api-reference/audio/createSpeech#audio-createspeech-model",children:"model"}),", the ",e.jsx(t.a,{href:"/docs/api-reference/audio/createSpeech#audio-createspeech-input",children:"text"})," that should be turned into audio, and the ",e.jsx(t.a,{href:"/docs/api-reference/audio/createSpeech#audio-createspeech-voice",children:"voice"})," to be used for the audio generation. A simple request would look like the following:"]}),"\n",e.jsx(c,{title:"Generate spoken audio from input text",defaultLanguage:"python",code:{python:'\nfrom pathlib import Path\nfrom openai import OpenAI\nclient = OpenAI()\n\nspeech_file_path = Path(__file__).parent / "speech.mp3"\nresponse = client.audio.speech.create(\n  model="tts-1",\n  voice="alloy",\n  input="Today is a wonderful day to build something people love!"\n)\n\nresponse.stream_to_file(speech_file_path)\n'.trim(),curl:'\ncurl https://api.openai.com/v1/audio/speech \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "tts-1",\n    "input": "Today is a wonderful day to build something people love!",\n    "voice": "alloy"\n  }\' \\\n  --output speech.mp3\n'.trim(),node:'\nimport fs from "fs";\nimport path from "path";\nimport OpenAI from "openai";\n\nconst openai = new OpenAI();\n\nconst speechFile = path.resolve("./speech.mp3");\n\nasync function main() {\n  const mp3 = await openai.audio.speech.create({\n    model: "tts-1",\n    voice: "alloy",\n    input: "Today is a wonderful day to build something people love!",\n  });\n  console.log(speechFile);\n  const buffer = Buffer.from(await mp3.arrayBuffer());\n  await fs.promises.writeFile(speechFile, buffer);\n}\nmain();\n'.trim()}}),"\n",e.jsxs(t.p,{children:["By default, the endpoint will output a MP3 file of the spoken audio but it can also be configured to output any of our ",e.jsx(t.a,{href:"/docs/guides/text-to-speech/supported-output-formats",children:"supported formats"}),"."]}),"\n",e.jsx(t.h3,{children:"Audio quality"}),"\n",e.jsxs(t.p,{children:["For real-time applications, the standard ",e.jsx(t.code,{children:"tts-1"})," model provides the lowest latency but at a lower quality than the ",e.jsx(t.code,{children:"tts-1-hd"})," model. Due to the way the audio is generated, ",e.jsx(t.code,{children:"tts-1"})," is likely to generate content that has more static in certain situations than ",e.jsx(t.code,{children:"tts-1-hd"}),". In some cases, the audio may not have noticeable differences depending on your listening device and the individual person."]}),"\n",e.jsx(t.h3,{children:"Voice options"}),"\n",e.jsxs(t.p,{children:["Experiment with different voices (",e.jsx(t.code,{children:"alloy"}),", ",e.jsx(t.code,{children:"echo"}),", ",e.jsx(t.code,{children:"fable"}),", ",e.jsx(t.code,{children:"onyx"}),", ",e.jsx(t.code,{children:"nova"}),", and ",e.jsx(t.code,{children:"shimmer"}),") to find one that matches your desired tone and audience. The current voices are optimized for English."]}),"\n",e.jsx(t.h4,{children:"Alloy"}),"\n",e.jsx(F,{audioUrl:"https://cdn.openai.com/API/docs/audio/alloy.wav"}),"\n",e.jsx(t.h4,{children:"Echo"}),"\n",e.jsx(F,{audioUrl:"https://cdn.openai.com/API/docs/audio/echo.wav"}),"\n",e.jsx(t.h4,{children:"Fable"}),"\n",e.jsx(F,{audioUrl:"https://cdn.openai.com/API/docs/audio/fable.wav"}),"\n",e.jsx(t.h4,{children:"Onyx"}),"\n",e.jsx(F,{audioUrl:"https://cdn.openai.com/API/docs/audio/onyx.wav"}),"\n",e.jsx(t.h4,{children:"Nova"}),"\n",e.jsx(F,{audioUrl:"https://cdn.openai.com/API/docs/audio/nova.wav"}),"\n",e.jsx(t.h4,{children:"Shimmer"}),"\n",e.jsx(F,{audioUrl:"https://cdn.openai.com/API/docs/audio/shimmer.wav"}),"\n",e.jsx(t.h3,{children:"Streaming real time audio"}),"\n",e.jsxs(t.p,{children:["The Speech API provides support for real time audio streaming using ",e.jsx(t.a,{href:"https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Transfer-Encoding",children:"chunk transfer encoding"}),". This means that the audio is able to be played before the full file has been generated and made accessible."]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:'from openai import OpenAI\n\nclient = OpenAI()\n\nresponse = client.audio.speech.create(\n    model="tts-1",\n    voice="alloy",\n    input="Hello world! This is a streaming test.",\n)\n\nresponse.stream_to_file("output.mp3")\n'})}),"\n",e.jsx(t.h2,{children:"Supported output formats"}),"\n",e.jsx(t.p,{children:'The default response format is "mp3", but other formats like "opus", "aac", "flac", and "pcm" are available.'}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"Opus"}),": For internet streaming and communication, low latency."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"AAC"}),": For digital audio compression, preferred by YouTube, Android, iOS."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"FLAC"}),": For lossless audio compression, favored by audio enthusiasts for archiving."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"WAV"}),": Uncompressed WAV audio, suitable for low-latency applications to avoid decoding overhead."]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.strong,{children:"PCM"}),": Similar to WAV but containing the raw samples in 24kHz (16-bit signed, low-endian), without the header."]}),"\n"]}),"\n",e.jsx(t.h2,{children:"Supported languages"}),"\n",e.jsxs(t.p,{children:["The TTS model generally follows the Whisper model in terms of language support. Whisper ",e.jsx(t.a,{href:"https://github.com/openai/whisper#available-models-and-languages",children:"supports the following languages"})," and performs well despite the current voices being optimized for English:"]}),"\n",e.jsx(t.p,{children:"Afrikaans, Arabic, Armenian, Azerbaijani, Belarusian, Bosnian, Bulgarian, Catalan, Chinese, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, Galician, German, Greek, Hebrew, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Kannada, Kazakh, Korean, Latvian, Lithuanian, Macedonian, Malay, Marathi, Maori, Nepali, Norwegian, Persian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Slovenian, Spanish, Swahili, Swedish, Tagalog, Tamil, Thai, Turkish, Ukrainian, Urdu, Vietnamese, and Welsh."}),"\n",e.jsx(t.p,{children:"You can generate spoken audio in these languages by providing the input text in the language of your choice."}),"\n",e.jsx(t.h2,{children:"FAQ"}),"\n",e.jsx(t.h3,{children:"How can I control the emotional range of the generated audio?"}),"\n",e.jsx(t.p,{children:"There is no direct mechanism to control the emotional output of the audio generated. Certain factors may influence the output audio like capitalization or grammar but our internal tests with these have yielded mixed results."}),"\n",e.jsx(t.h3,{children:"Can I create a custom copy of my own voice?"}),"\n",e.jsx(t.p,{children:"No, this is not something we support."}),"\n",e.jsx(t.h3,{children:"Do I own the outputted audio files?"}),"\n",e.jsx(t.p,{children:"Yes, like with all outputs from our API, the person who created them owns the output. You are still required to inform end users that they are hearing audio generated by AI and not a real person talking to them."})]})}function eo(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Dt,n)})):Dt(n)}function Ft(n){const t=Object.assign({h1:"h1",p:"p",h2:"h2",code:"code",a:"a",pre:"pre",ul:"ul",li:"li",strong:"strong",h3:"h3"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"Vision"}),"\n",e.jsx(t.p,{children:"Learn how to use vision capabilities to understand images."}),"\n",e.jsx(t.h2,{children:"Introduction"}),"\n",e.jsx(t.p,{children:"GPT-4o, GPT-4o mini, and GPT-4 Turbo have vision capabilities, meaning the models can take in images and answer questions about them. Historically, language model systems have been limited by taking in a single input modality, text."}),"\n",e.jsx(t.h2,{children:"Quickstart"}),"\n",e.jsxs(t.p,{children:["Images are made available to the model in two main ways: by passing a link to the image or by passing the base64 encoded image directly in the request. Images can be passed in the ",e.jsx(t.code,{children:"user"})," messages."]}),"\n",e.jsx(c,{title:"What's in this image?",defaultLanguage:"python",code:{python:'\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n  model="gpt-4o-mini",\n  messages=[\n    {\n      "role": "user",\n      "content": [\n        {"type": "text", "text": "What’s in this image?"},\n        {\n          "type": "image_url",\n          "image_url": {\n            "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",\n          },\n        },\n      ],\n    }\n  ],\n  max_tokens=300,\n)\n\nprint(response.choices[0])\n'.trim(),curl:'\ncurl https://api.openai.com/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -d \'{\n    "model": "gpt-4o-mini",\n    "messages": [\n      {\n        "role": "user",\n        "content": [\n          {\n            "type": "text",\n            "text": "What’s in this image?"\n          },\n          {\n            "type": "image_url",\n            "image_url": {\n              "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"\n            }\n          }\n        ]\n      }\n    ],\n    "max_tokens": 300\n  }\'\n'.trim(),node:'\nimport OpenAI from "openai";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const response = await openai.chat.completions.create({\n    model: "gpt-4o-mini",\n    messages: [\n      {\n        role: "user",\n        content: [\n          { type: "text", text: "What’s in this image?" },\n          {\n            type: "image_url",\n            image_url: {\n              "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",\n            },\n          },\n        ],\n      },\n    ],\n  });\n  console.log(response.choices[0]);\n}\nmain();\n'.trim()}}),"\n",e.jsx(t.p,{children:"The model is best at answering general questions about what is present in the images. While it does understand the relationship between objects in images, it is not yet optimized to answer detailed questions about the location of certain objects in an image. For example, you can ask it what color a car is or what some ideas for dinner might be based on what is in you fridge, but if you show it an image of a room and ask it where the chair is, it may not answer the question correctly."}),"\n",e.jsxs(t.p,{children:["It is important to keep in mind the ",e.jsx(t.a,{href:"/docs/guides/vision/limitations",children:"limitations of the model"})," as you explore what use-cases visual understanding can be applied to."]}),"\n",e.jsx("a",{href:"https://cookbook.openai.com/examples/gpt_with_vision_for_video_understanding",target:"_blank",children:e.jsx(C,{icon:e.jsx(an,{}),color:"red",title:"Video understanding with vision",className:"mt-6",children:e.jsx(t.p,{children:"Learn how to use use GPT-4 with Vision to understand videos in the OpenAI Cookbook"})})}),"\n",e.jsx(t.h2,{children:"Uploading base 64 encoded images"}),"\n",e.jsx(t.p,{children:"If you have an image or set of images locally, you can pass those to the model in base 64 encoded format, here is an example of this in action:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:'import base64\nimport requests\n\n# OpenAI API Key\napi_key = "YOUR_OPENAI_API_KEY"\n\n# Function to encode the image\ndef encode_image(image_path):\n  with open(image_path, "rb") as image_file:\n    return base64.b64encode(image_file.read()).decode(\'utf-8\')\n\n# Path to your image\nimage_path = "path_to_your_image.jpg"\n\n# Getting the base64 string\nbase64_image = encode_image(image_path)\n\nheaders = {\n  "Content-Type": "application/json",\n  "Authorization": f"Bearer {api_key}"\n}\n\npayload = {\n  "model": "gpt-4o-mini",\n  "messages": [\n    {\n      "role": "user",\n      "content": [\n        {\n          "type": "text",\n          "text": "What’s in this image?"\n        },\n        {\n          "type": "image_url",\n          "image_url": {\n            "url": f"data:image/jpeg;base64,{base64_image}"\n          }\n        }\n      ]\n    }\n  ],\n  "max_tokens": 300\n}\n\nresponse = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=payload)\n\nprint(response.json())\n'})}),"\n",e.jsx(t.h2,{children:"Multiple image inputs"}),"\n",e.jsx(t.p,{children:"The Chat Completions API is capable of taking in and processing multiple image inputs in both base64 encoded format or as an image URL. The model will process each image and use the information from all of them to answer the question."}),"\n",e.jsx(c,{title:"Multiple image inputs",defaultLanguage:"python",code:{python:'\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.create(\n  model="gpt-4o-mini",\n  messages=[\n    {\n      "role": "user",\n      "content": [\n        {\n          "type": "text",\n          "text": "What are in these images? Is there any difference between them?",\n        },\n        {\n          "type": "image_url",\n          "image_url": {\n            "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",\n          },\n        },\n        {\n          "type": "image_url",\n          "image_url": {\n            "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",\n          },\n        },\n      ],\n    }\n  ],\n  max_tokens=300,\n)\nprint(response.choices[0])\n'.trim(),curl:'\ncurl https://api.openai.com/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -d \'{\n    "model": "gpt-4o-mini",\n    "messages": [\n      {\n        "role": "user",\n        "content": [\n          {\n            "type": "text",\n            "text": "What are in these images? Is there any difference between them?"\n          },\n          {\n            "type": "image_url",\n            "image_url": {\n              "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",\n            }\n          },\n          {\n            "type": "image_url",\n            "image_url": {\n              "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",\n            }\n          }\n        ]\n      }\n    ],\n    "max_tokens": 300\n  }\'\n'.trim(),node:'\nimport OpenAI from "openai";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const response = await openai.chat.completions.create({\n    model: "gpt-4o-mini",\n    messages: [\n      {\n        role: "user",\n        content: [\n          { type: "text", text: "What are in these images? Is there any difference between them?" },\n          {\n            type: "image_url",\n            image_url: {\n              "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",\n            },\n          },\n          {\n            type: "image_url",\n            image_url: {\n              "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",\n            },\n          }\n        ],\n      },\n    ],\n  });\n  console.log(response.choices[0]);\n}\nmain();\n'.trim()}}),"\n",e.jsx(t.p,{children:"Here the model is shown two copies of the same image and can answer questions about both or each of the images independently."}),"\n",e.jsx(t.h2,{children:"Low or high fidelity image understanding"}),"\n",e.jsxs(t.p,{children:["By controlling the ",e.jsx(t.code,{children:"detail"})," parameter, which has three options, ",e.jsx(t.code,{children:"low"}),", ",e.jsx(t.code,{children:"high"}),", or ",e.jsx(t.code,{children:"auto"}),", you have control over how the model processes the image and generates its textual understanding. By default, the model will use the ",e.jsx(t.code,{children:"auto"})," setting which will look at the image input size and decide if it should use the ",e.jsx(t.code,{children:"low"})," or ",e.jsx(t.code,{children:"high"})," setting."]}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"low"}),' will enable the "low res" mode. The model will receive a low-res 512px x 512px version of the image, and represent the image with a budget of 85 tokens. This allows the API to return faster responses and consume fewer input tokens for use cases that do not require high detail.']}),"\n",e.jsxs(t.li,{children:[e.jsx(t.code,{children:"high"}),' will enable "high res" mode, which first allows the model to first see the low res image (using 85 tokens) and then creates detailed crops using 170 tokens for each 512px x 512px tile.']}),"\n"]}),"\n",e.jsx(c,{title:"Choosing the detail level",defaultLanguage:"python",code:{python:'\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n  model="gpt-4o-mini",\n  messages=[\n    {\n      "role": "user",\n      "content": [\n        {"type": "text", "text": "What’s in this image?"},\n        {\n          "type": "image_url",\n          "image_url": {\n            "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",\n            "detail": "high"\n          },\n        },\n      ],\n    }\n  ],\n  max_tokens=300,\n)\n\nprint(response.choices[0].message.content)\n'.trim(),curl:'\ncurl https://api.openai.com/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -d \'{\n    "model": "gpt-4o-mini",\n    "messages": [\n      {\n        "role": "user",\n        "content": [\n          {\n            "type": "text",\n            "text": "What’s in this image?"\n          },\n          {\n            "type": "image_url",\n            "image_url": {\n              "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",\n              "detail": "high"\n            }\n          }\n        ]\n      }\n    ],\n    "max_tokens": 300\n  }\'\n'.trim(),node:'\nimport OpenAI from "openai";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const response = await openai.chat.completions.create({\n    model: "gpt-4o-mini",\n    messages: [\n      {\n        role: "user",\n        content: [\n          { type: "text", text: "What’s in this image?" },\n          {\n            type: "image_url",\n            image_url: {\n              "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",\n              "detail": "low"\n            },\n          },\n        ],\n      },\n    ],\n  });\n  console.log(response.choices[0]);\n}\nmain();\n'.trim()}}),"\n",e.jsx(t.h2,{children:"Managing images"}),"\n",e.jsx(t.p,{children:"The Chat Completions API, unlike the Assistants API, is not stateful. That means you have to manage the messages (including images) you pass to the model yourself. If you want to pass the same image to the model multiple times, you will have to pass the image each time you make a request to the API."}),"\n",e.jsx(t.p,{children:"For long running conversations, we suggest passing images via URL's instead of base64. The latency of the model can also be improved by downsizing your images ahead of time to be less than the maximum size they are expected them to be. For low res mode, we expect a 512px x 512px image. For high res mode, the short side of the image should be less than 768px and the long side should be less than 2,000px."}),"\n",e.jsxs(t.p,{children:["After an image has been processed by the model, it is deleted from OpenAI servers and not retained. ",e.jsx(t.a,{href:"https://openai.com/enterprise-privacy",children:"We do not use data uploaded via the OpenAI API to train our models"}),"."]}),"\n",e.jsx(t.h2,{children:"Limitations"}),"\n",e.jsx(t.p,{children:"While GPT-4 with vision is powerful and can be used in many situations, it is important to understand the limitations of the model. Here are some of the limitations we are aware of:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Medical images: The model is not suitable for interpreting specialized medical images like CT scans and shouldn't be used for medical advice."}),"\n",e.jsx(t.li,{children:"Non-English: The model may not perform optimally when handling images with text of non-Latin alphabets, such as Japanese or Korean."}),"\n",e.jsx(t.li,{children:"Small text: Enlarge text within the image to improve readability, but avoid cropping important details."}),"\n",e.jsx(t.li,{children:"Rotation: The model may misinterpret rotated / upside-down text or images."}),"\n",e.jsx(t.li,{children:"Visual elements: The model may struggle to understand graphs or text where colors or styles like solid, dashed, or dotted lines vary."}),"\n",e.jsx(t.li,{children:"Spatial reasoning: The model struggles with tasks requiring precise spatial localization, such as identifying chess positions."}),"\n",e.jsx(t.li,{children:"Accuracy: The model may generate incorrect descriptions or captions in certain scenarios."}),"\n",e.jsx(t.li,{children:"Image shape: The model struggles with panoramic and fisheye images."}),"\n",e.jsx(t.li,{children:"Metadata and resizing: The model doesn't process original file names or metadata, and images are resized before analysis, affecting their original dimensions."}),"\n",e.jsx(t.li,{children:"Counting: May give approximate counts for objects in images."}),"\n",e.jsx(t.li,{children:"CAPTCHAS: For safety reasons, we have implemented a system to block the submission of CAPTCHAs."}),"\n"]}),"\n",e.jsx(t.h2,{children:"Calculating costs"}),"\n",e.jsxs(t.p,{children:["Image inputs are metered and charged in tokens, just as text inputs are. The token cost of a given image is determined by two factors: its size, and the ",e.jsx(t.code,{children:"detail"})," option on each image_url block. All images with ",e.jsx(t.code,{children:"detail: low"})," cost 85 tokens each. ",e.jsx(t.code,{children:"detail: high"})," images are first scaled to fit within a 2048 x 2048 square, maintaining their aspect ratio. Then, they are scaled such that the shortest side of the image is 768px long. Finally, we count how many 512px squares the image consists of. Each of those squares costs ",e.jsx(t.strong,{children:"170 tokens"}),". Another ",e.jsx(t.strong,{children:"85 tokens"})," are always added to the final total."]}),"\n",e.jsx(t.p,{children:"Here are some examples demonstrating the above."}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:["A 1024 x 1024 square image in ",e.jsx(t.code,{children:"detail: high"})," mode costs 765 tokens","\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"1024 is less than 2048, so there is no initial resize."}),"\n",e.jsx(t.li,{children:"The shortest side is 1024, so we scale the image down to 768 x 768."}),"\n",e.jsxs(t.li,{children:["4 512px square tiles are needed to represent the image, so the final token cost is ",e.jsx(t.code,{children:"170 * 4 + 85 = 765"}),"."]}),"\n"]}),"\n"]}),"\n",e.jsxs(t.li,{children:["A 2048 x 4096 image in ",e.jsx(t.code,{children:"detail: high"})," mode costs 1105 tokens","\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"We scale down the image to 1024 x 2048 to fit within the 2048 square."}),"\n",e.jsx(t.li,{children:"The shortest side is 1024, so we further scale down to 768 x 1536."}),"\n",e.jsxs(t.li,{children:["6 512px tiles are needed, so the final token cost is ",e.jsx(t.code,{children:"170 * 6 + 85 = 1105"}),"."]}),"\n"]}),"\n"]}),"\n",e.jsxs(t.li,{children:["A 4096 x 8192 image in ",e.jsx(t.code,{children:"detail: low"})," most costs 85 tokens","\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Regardless of input size, low detail images are a fixed cost."}),"\n"]}),"\n"]}),"\n"]}),"\n",e.jsx(t.h2,{children:"FAQ"}),"\n",e.jsxs(t.h3,{children:["Can I fine-tune the image capabilities in ",e.jsx(t.code,{children:"gpt-4"}),"?"]}),"\n",e.jsxs(t.p,{children:["No, we do not support fine-tuning the image capabilities of ",e.jsx(t.code,{children:"gpt-4"})," at this time."]}),"\n",e.jsxs(t.h3,{children:["Can I use ",e.jsx(t.code,{children:"gpt-4"})," to generate images?"]}),"\n",e.jsxs(t.p,{children:["No, you can use ",e.jsx(t.code,{children:"dall-e-3"})," to generate images and ",e.jsx(t.code,{children:"gpt-4o"}),", ",e.jsx(t.code,{children:"gpt-4o-mini"})," or ",e.jsx(t.code,{children:"gpt-4-turbo"})," to understand images."]}),"\n",e.jsx(t.h3,{children:"What type of files can I upload?"}),"\n",e.jsx(t.p,{children:"We currently support PNG (.png), JPEG (.jpeg and .jpg), WEBP (.webp), and non-animated GIF (.gif)."}),"\n",e.jsx(t.h3,{children:"Is there a limit to the size of the image I can upload?"}),"\n",e.jsx(t.p,{children:"Yes, we restrict image uploads to 20MB per image."}),"\n",e.jsx(t.h3,{children:"Can I delete an image I uploaded?"}),"\n",e.jsx(t.p,{children:"No, we will delete the image for you automatically after it has been processed by the model."}),"\n",e.jsx(t.h3,{children:"Where can I learn more about the considerations of GPT-4 with Vision?"}),"\n",e.jsxs(t.p,{children:["You can find details about our evaluations, preparation, and mitigation work in the ",e.jsx(t.a,{href:"https://openai.com/contributions/gpt-4v",children:"GPT-4 with Vision system card"}),"."]}),"\n",e.jsx(t.p,{children:"We have further implemented a system to block the submission of CAPTCHAs."}),"\n",e.jsx(t.h3,{children:"How do rate limits for GPT-4 with Vision work?"}),"\n",e.jsx(t.p,{children:"We process images at the token level, so each image we process counts towards your tokens per minute (TPM) limit. See the calculating costs section for details on the formula used to determine token count per image."}),"\n",e.jsx(t.h3,{children:"Can GPT-4 with Vision understand image metadata?"}),"\n",e.jsx(t.p,{children:"No, the model does not receive image metadata."}),"\n",e.jsx(t.h3,{children:"What happens if my image is unclear?"}),"\n",e.jsx(t.p,{children:"If an image is ambiguous or unclear, the model will do its best to interpret it. However, the results may be less accurate. A good rule of thumb is that if an average human cannot see the info in an image at the resolutions used in low/high res mode, then the model cannot either."})]})}function Rt(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Ft,n)})):Ft(n)}function Gt(n){const t=Object.assign({h1:"h1",p:"p",h2:"h2",ul:"ul",li:"li",h4:"h4"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"Hackathon support request"}),"\n",e.jsx(t.p,{children:"Thank you for your interest in having OpenAI support your developer hackathon. This page includes the relevant information about what types of events we support, how to request support, and more."}),"\n",e.jsx(t.h2,{children:"Event criteria"}),"\n",e.jsx(t.p,{children:"We consider many event requests but tend to give priority to those that meet the following criteria:"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"The event is developer-focused"}),"\n",e.jsx(t.li,{children:"There is a public website for the event with information about the hackathon and a code of conduct displayed"}),"\n",e.jsx(t.li,{children:"The event begins more than 21 days from the time of requesting support"}),"\n",e.jsx(t.li,{children:"Post-event, we’d kindly ask for feedback via survey"}),"\n"]}),"\n",e.jsx(t.h2,{children:"Branding guidelines"}),"\n",e.jsx(t.p,{children:"Once approved, we ask for the ability to review marketing materials that mention OpenAI or our support. We typically do not allow our logo to be used for marketing materials at hackathons."}),"\n",e.jsx(t.p,{children:'We do allow hackathons to say that "credits are provided by OpenAI" or "supported by OpenAI" if they are selected to receive support.'}),"\n",e.jsx(t.h2,{children:"FAQ"}),"\n",e.jsx(t.h4,{children:"Is there a limit to the number of people I can have at my event?"}),"\n",e.jsx(t.p,{children:"No. But for events with more than 1,000 people, we require a discussion with your event organizers so we can determine how best we can accommodate them."}),"\n",e.jsx(t.h4,{children:"Can I say that OpenAI is sponsoring my hackathon?"}),"\n",e.jsx(t.p,{children:'We reserve the “sponsoring” language for events that we have a sponsorship agreement with. We are okay with you describing our support as follows: "Credits are being provided by OpenAI" or “Supported by OpenAI”.'}),"\n",e.jsx(t.h4,{children:"Does OpenAI do cash sponsorships of hackathons?"}),"\n",e.jsx(t.p,{children:"Not at this time."}),"\n",e.jsx(t.h4,{children:"Can someone from OpenAI speak at my hackathon?"}),"\n",e.jsx(t.p,{children:"Maybe! We love getting out and connecting with the developer community but have limited bandwidth to attend events."}),"\n",e.jsx(t.h4,{children:"Can my hackathon be virtual?"}),"\n",e.jsx(t.p,{children:"Yes, we support in-person, virtual, and hybrid hackathon events!"})]})}function to(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Gt,n)})):Gt(n)}function Wt(n){const t=Object.assign({h1:"h1",h2:"h2",p:"p",a:"a",pre:"pre",code:"code",hr:"hr",ul:"ul",li:"li",strong:"strong",h3:"h3"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"Libraries"}),"\n",e.jsx(t.h2,{children:"Python library"}),"\n",e.jsxs(t.p,{children:["We provide a ",e.jsx(t.a,{href:"https://github.com/openai/openai-python",children:"Python library"}),", which you can install by running:"]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-bash",children:"pip install openai\n"})}),"\n",e.jsx(t.p,{children:"Once installed, you can use the library and your secret key to run the following:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:'from openai import OpenAI\nclient = OpenAI(\n    # Defaults to os.environ.get("OPENAI_API_KEY")\n)\n\nchat_completion = client.chat.completions.create(\n    model="gpt-4o-mini",\n    messages=[{"role": "user", "content": "Hello world"}]\n)\n'})}),"\n",e.jsx(t.p,{children:"The bindings also will install a command-line utility you can use as follows:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-bash",children:'$ openai api chat_completions.create -m gpt-4o-mini -g user "Hello world"\n'})}),"\n",e.jsx(t.hr,{}),"\n",e.jsx(I,{level:2,slug:"node-js-library",children:e.jsx(t.p,{children:"TypeScript / JavaScript library"})}),"\n",e.jsxs(t.p,{children:["We provide a ",e.jsx(t.a,{href:"https://github.com/openai/openai-node",children:"TypeScript / JavaScript library"})," with support for Node.js and various ",e.jsx(t.a,{href:"https://deno.land/x/openai",children:"other runtimes"}),". Install it by running:"]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-bash",children:"npm install --save openai\n# or\nyarn add openai\n"})}),"\n",e.jsx(t.p,{children:"Once installed, you can use the library and your secret key to run the following:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-javascript",children:'import OpenAI from "openai";\n\nconst openai = new OpenAI({\n    apiKey: process.env.OPENAI_API_KEY,\n});\n\nconst chatCompletion = await openai.chat.completions.create({\n    messages: [{ role: "user", content: "Say this is a test" }],\n    model: "gpt-4o-mini",\n});\n'})}),"\n",e.jsx(t.hr,{}),"\n",e.jsxs(I,{slug:"dotnet-library",level:2,children:[".NET library ",e.jsx(H,{children:"Beta"})]}),"\n",e.jsxs(t.p,{children:["We provide a ",e.jsx(t.a,{href:"https://github.com/openai/openai-dotnet",children:".NET library"}),", which you can install by running:"]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{children:"dotnet add package OpenAI --prerelease\n"})}),"\n",e.jsx(t.p,{children:"Once installed, you can use the library and your secret key to run the following:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-csharp",children:'using OpenAI.Chat;\n\nChatClient client = new("gpt-4o-mini", Environment.GetEnvironmentVariable("OPENAI_API_KEY"));\n\nChatCompletion chatCompletion = client.CompleteChat(\n    [\n        new UserChatMessage("Say \'this is a test.\'"),\n    ]);\n'})}),"\n",e.jsx(t.hr,{}),"\n",e.jsx(t.h2,{children:"Azure OpenAI libraries"}),"\n",e.jsx(t.p,{children:"Microsoft's Azure team maintains libraries that are compatible with both the OpenAI API and Azure OpenAI services. Read the library documentation below to learn how you can use them with the OpenAI API."}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:e.jsx(t.a,{href:"https://github.com/Azure/azure-sdk-for-net/tree/main/sdk/openai/Azure.AI.OpenAI",children:"Azure OpenAI client library for .NET"})}),"\n",e.jsx(t.li,{children:e.jsx(t.a,{href:"https://github.com/Azure/azure-sdk-for-js/tree/main/sdk/openai/openai",children:"Azure OpenAI client library for JavaScript"})}),"\n",e.jsx(t.li,{children:e.jsx(t.a,{href:"https://github.com/Azure/azure-sdk-for-java/tree/main/sdk/openai/azure-ai-openai",children:"Azure OpenAI client library for Java"})}),"\n",e.jsx(t.li,{children:e.jsx(t.a,{href:"https://github.com/Azure/azure-sdk-for-go/tree/main/sdk/ai/azopenai",children:"Azure OpenAI client library for Go"})}),"\n"]}),"\n",e.jsx(t.hr,{}),"\n",e.jsx(t.h2,{children:"Community libraries"}),"\n",e.jsxs(t.p,{children:["The libraries below are built and maintained by the broader developer community. If you'd like to add a new library here, please follow the instructions in our ",e.jsx(t.a,{href:"https://help.openai.com/en/articles/6684216-adding-your-api-client-to-the-community-libraries-page",children:"help center article"})," on adding community libraries. You can also ",e.jsx(t.a,{href:"https://github.com/openai/openai-openapi",children:"watch our OpenAPI specification"})," repository on GitHub to get timely updates on when we make changes to our API."]}),"\n",e.jsxs(t.p,{children:["Please note that OpenAI does not verify the correctness or security of these projects. ",e.jsx(t.strong,{children:"Use them at your own risk!"})]}),"\n",e.jsx(t.h3,{children:"C# / .NET"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://github.com/betalgo/openai",children:"Betalgo.OpenAI"})," by ",e.jsx(t.a,{href:"https://github.com/betalgo",children:"Betalgo"})]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://github.com/OkGoDoIt/OpenAI-API-dotnet",children:"OpenAI-API-dotnet"})," by ",e.jsx(t.a,{href:"https://github.com/OkGoDoIt",children:"OkGoDoIt"})]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://github.com/RageAgainstThePixel/OpenAI-DotNet",children:"OpenAI-DotNet"})," by ",e.jsx(t.a,{href:"https://github.com/RageAgainstThePixel",children:"RageAgainstThePixel"})]}),"\n"]}),"\n",e.jsx(t.h3,{children:"C++"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://github.com/D7EAD/liboai",children:"liboai"})," by ",e.jsx(t.a,{href:"https://github.com/D7EAD",children:"D7EAD"})]}),"\n"]}),"\n",e.jsx(t.h3,{children:"Clojure"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://github.com/wkok/openai-clojure",children:"openai-clojure"})," by ",e.jsx(t.a,{href:"https://github.com/wkok",children:"wkok"})]}),"\n"]}),"\n",e.jsx(t.h3,{children:"Crystal"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://github.com/sferik/openai-crystal",children:"openai-crystal"})," by ",e.jsx(t.a,{href:"https://github.com/sferik",children:"sferik"})]}),"\n"]}),"\n",e.jsx(t.h3,{children:"Dart/Flutter"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://github.com/anasfik/openai",children:"openai"})," by ",e.jsx(t.a,{href:"https://github.com/anasfik",children:"anasfik"})]}),"\n"]}),"\n",e.jsx(t.h3,{children:"Delphi"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://github.com/HemulGM/DelphiOpenAI",children:"DelphiOpenAI"})," by ",e.jsx(t.a,{href:"https://github.com/HemulGM",children:"HemulGM"})]}),"\n"]}),"\n",e.jsx(t.h3,{children:"Elixir"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://github.com/mgallo/openai.ex",children:"openai.ex"})," by ",e.jsx(t.a,{href:"https://github.com/mgallo",children:"mgallo"})]}),"\n"]}),"\n",e.jsx(t.h3,{children:"Go"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://github.com/sashabaranov/go-gpt3",children:"go-gpt3"})," by ",e.jsx(t.a,{href:"https://github.com/sashabaranov",children:"sashabaranov"})]}),"\n"]}),"\n",e.jsx(t.h3,{children:"Java"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://github.com/TheoKanning/openai-java",children:"openai-java"})," by ",e.jsx(t.a,{href:"https://github.com/TheoKanning",children:"Theo Kanning"})]}),"\n"]}),"\n",e.jsx(t.h3,{children:"Julia"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://github.com/rory-linehan/OpenAI.jl",children:"OpenAI.jl"})," by ",e.jsx(t.a,{href:"https://github.com/rory-linehan",children:"rory-linehan"})]}),"\n"]}),"\n",e.jsx(t.h3,{children:"Kotlin"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://github.com/Aallam/openai-kotlin",children:"openai-kotlin"})," by ",e.jsx(t.a,{href:"https://github.com/Aallam",children:"Mouaad Aallam"})]}),"\n"]}),"\n",e.jsx(t.h3,{children:"Node.js"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://www.npmjs.com/package/openai-api",children:"openai-api"})," by ",e.jsx(t.a,{href:"https://github.com/Njerschow",children:"Njerschow"})]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://www.npmjs.com/package/openai-api-node",children:"openai-api-node"})," by ",e.jsx(t.a,{href:"https://github.com/erlapso",children:"erlapso"})]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://www.npmjs.com/package/gpt-x",children:"gpt-x"})," by ",e.jsx(t.a,{href:"https://github.com/ceifa",children:"ceifa"})]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://www.npmjs.com/package/gpt3",children:"gpt3"})," by ",e.jsx(t.a,{href:"https://github.com/poteat",children:"poteat"})]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://www.npmjs.com/package/gpts",children:"gpts"})," by ",e.jsx(t.a,{href:"https://github.com/thencc",children:"thencc"})]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://www.npmjs.com/package/@dalenguyen/openai",children:"@dalenguyen/openai"})," by ",e.jsx(t.a,{href:"https://github.com/dalenguyen",children:"dalenguyen"})]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://github.com/tectalichq/public-openai-client-js",children:"tectalic/openai"})," by ",e.jsx(t.a,{href:"https://tectalic.com/",children:"tectalic"})]}),"\n"]}),"\n",e.jsx(t.h3,{children:"PHP"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://packagist.org/packages/orhanerday/open-ai",children:"orhanerday/open-ai"})," by ",e.jsx(t.a,{href:"https://github.com/orhanerday",children:"orhanerday"})]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://github.com/tectalichq/public-openai-client-php",children:"tectalic/openai"})," by ",e.jsx(t.a,{href:"https://tectalic.com/",children:"tectalic"})]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://github.com/openai-php/client",children:"openai-php client"})," by ",e.jsx(t.a,{href:"https://github.com/openai-php",children:"openai-php"})]}),"\n"]}),"\n",e.jsx(t.h3,{children:"Python"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://github.com/OthersideAI/chronology",children:"chronology"})," by ",e.jsx(t.a,{href:"https://www.othersideai.com/",children:"OthersideAI"})]}),"\n"]}),"\n",e.jsx(t.h3,{children:"R"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://github.com/ben-aaron188/rgpt3",children:"rgpt3"})," by ",e.jsx(t.a,{href:"https://github.com/ben-aaron188",children:"ben-aaron188"})]}),"\n"]}),"\n",e.jsx(t.h3,{children:"Ruby"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://github.com/nileshtrivedi/openai/",children:"openai"})," by ",e.jsx(t.a,{href:"https://github.com/nileshtrivedi",children:"nileshtrivedi"})]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://github.com/alexrudall/ruby-openai",children:"ruby-openai"})," by ",e.jsx(t.a,{href:"https://github.com/alexrudall",children:"alexrudall"})]}),"\n"]}),"\n",e.jsx(t.h3,{children:"Rust"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://github.com/64bit/async-openai",children:"async-openai"})," by ",e.jsx(t.a,{href:"https://github.com/64bit",children:"64bit"})]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://github.com/lbkolev/fieri",children:"fieri"})," by ",e.jsx(t.a,{href:"https://github.com/lbkolev",children:"lbkolev"})]}),"\n"]}),"\n",e.jsx(t.h3,{children:"Scala"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://github.com/cequence-io/openai-scala-client",children:"openai-scala-client"})," by ",e.jsx(t.a,{href:"https://github.com/cequence-io",children:"cequence-io"})]}),"\n"]}),"\n",e.jsx(t.h3,{children:"Swift"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://github.com/dylanshine/openai-kit",children:"OpenAIKit"})," by ",e.jsx(t.a,{href:"https://github.com/dylanshine",children:"dylanshine"})]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://github.com/MacPaw/OpenAI/",children:"OpenAI"})," by ",e.jsx(t.a,{href:"https://github.com/MacPaw",children:"MacPaw"})]}),"\n"]}),"\n",e.jsx(t.h3,{children:"Unity"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://github.com/hexthedev/OpenAi-Api-Unity",children:"OpenAi-Api-Unity"})," by ",e.jsx(t.a,{href:"https://github.com/hexthedev",children:"hexthedev"})]}),"\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://github.com/RageAgainstThePixel/com.openai.unity",children:"com.openai.unity"})," by ",e.jsx(t.a,{href:"https://github.com/RageAgainstThePixel",children:"RageAgainstThePixel"})]}),"\n"]}),"\n",e.jsx(t.h3,{children:"Unreal Engine"}),"\n",e.jsxs(t.ul,{children:["\n",e.jsxs(t.li,{children:[e.jsx(t.a,{href:"https://github.com/KellanM/OpenAI-Api-Unreal",children:"OpenAI-Api-Unreal"})," by ",e.jsx(t.a,{href:"https://github.com/KellanM",children:"KellanM"})]}),"\n"]})]})}function no(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Wt,n)})):Wt(n)}const Yt=({className:n="",modelName:t,modelDescription:s,capabilities:i,contextLength:r,priceDetails:o,slug:l,tag:d})=>e.jsx("div",{className:B(n,"model-card"),children:e.jsxs(A,{to:"/docs/models/".concat(l),children:[e.jsxs("div",{className:"model-title-row",children:[e.jsxs("div",{className:"model-title",children:[t,e.jsx("div",{className:"pointer",children:e.jsx(Z,{width:24,height:24})})]}),d&&e.jsx(he,{color:"green",children:d})]}),e.jsx("p",{className:"model-description",children:s}),e.jsxs("div",{children:[e.jsx("div",{className:"detail",children:i}),e.jsx("div",{className:"detail",children:r}),e.jsx("div",{className:"detail",children:o})]})]})}),dn=()=>e.jsxs("div",{children:[e.jsxs("div",{className:"model-cards",children:[e.jsx(Yt,{modelName:"GPT-4o",modelDescription:"Our high-intelligence flagship model for complex, multi‑step tasks",capabilities:"Text and image input, text output",contextLength:"128k context length",priceDetails:"Optimized for intelligence, higher price per token",slug:"gpt-4o",tag:""}),e.jsx(Yt,{modelName:"GPT-4o mini",modelDescription:"Our affordable and intelligent small model for fast, lightweight tasks",capabilities:"Text and image input, text output",contextLength:"128k context length",priceDetails:"Optimized for speed, lower price per token",slug:"gpt-4o-mini",tag:"New"})]}),e.jsx("p",{className:"model-price-note flex justify-center",children:e.jsxs("a",{className:"flex items-center",href:"https://openai.com/api/pricing",target:"_blank",rel:"noreferrer",children:[e.jsx("span",{children:"Model pricing details"}),e.jsx(Z,{width:20,height:20})]})})]});function Bt(n){const t=Object.assign({h1:"h1",h2:"h2",p:"p",a:"a",table:"table",thead:"thead",tr:"tr",th:"th",tbody:"tbody",td:"td",code:"code",strong:"strong",h4:"h4",hr:"hr",h3:"h3"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"Models"}),"\n",e.jsx(t.h2,{children:"Flagship models"}),"\n",e.jsx(dn,{}),"\n",e.jsx(t.h2,{children:"Models overview"}),"\n",e.jsxs(t.p,{children:["The OpenAI API is powered by a diverse set of models with different capabilities and price points. You can also make customizations to our models for your specific use case with ",e.jsx(t.a,{href:"/docs/guides/fine-tuning",children:"fine-tuning"}),"."]}),"\n",e.jsxs("div",{className:"docs-models-toc",children:[e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Model"}),e.jsx(t.th,{children:"Description"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.a,{href:"/docs/models/gpt-4o",children:"GPT-4o"})}),e.jsx(t.td,{children:"Our high-intelligence flagship model for complex, multi-step tasks"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.a,{href:"/docs/models/gpt-4o-mini",children:"GPT-4o mini"})}),e.jsx(t.td,{children:"Our affordable and intelligent small model for fast, lightweight tasks"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.a,{href:"/docs/models/gpt-4-turbo-and-gpt-4",children:"GPT-4 Turbo and GPT-4"})}),e.jsx(t.td,{children:"The previous set of high-intelligence models"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.a,{href:"/docs/models/gpt-3-5-turbo",children:"GPT-3.5 Turbo"})}),e.jsx(t.td,{children:"A fast, inexpensive model for simple tasks"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.a,{href:"/docs/models/dall-e",children:"DALL·E"})}),e.jsx(t.td,{children:"A model that can generate and edit images given a natural language prompt"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.a,{href:"/docs/models/tts",children:"TTS"})}),e.jsx(t.td,{children:"A set of models that can convert text into natural sounding spoken audio"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.a,{href:"/docs/models/whisper",children:"Whisper"})}),e.jsx(t.td,{children:"A model that can convert audio into text"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.a,{href:"/docs/models/embeddings",children:"Embeddings"})}),e.jsx(t.td,{children:"A set of models that can convert text into a numerical form"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.a,{href:"/docs/models/moderation",children:"Moderation"})}),e.jsx(t.td,{children:"A fine-tuned model that can detect whether text may be sensitive or unsafe"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.a,{href:"/docs/models/gpt-base",children:"GPT base"})}),e.jsx(t.td,{children:"A set of models without instruction following that can understand as well as generate natural language or code"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.a,{href:"/docs/deprecations",children:"Deprecated"})}),e.jsx(t.td,{children:"A full list of models that have been deprecated along with the suggested replacement"})]})]})]}),e.jsx(t.p,{children:"For GPT-series models, the context window refers to the maximum number of tokens that can be used in a single request, inclusive of both input and output tokens."}),e.jsxs(t.p,{children:["We have also published open source models including ",e.jsx(t.a,{href:"https://github.com/openai/point-e",children:"Point-E"}),", ",e.jsx(t.a,{href:"https://github.com/openai/whisper",children:"Whisper"}),", ",e.jsx(t.a,{href:"https://github.com/openai/jukebox",children:"Jukebox"}),", and ",e.jsx(t.a,{href:"https://github.com/openai/CLIP",children:"CLIP"}),"."]})]}),"\n",e.jsx(t.h2,{children:"Continuous model upgrades"}),"\n",e.jsxs(t.p,{children:[e.jsx(t.code,{children:"gpt-4o"}),", ",e.jsx(t.code,{children:"gpt-4o-mini"}),", ",e.jsx(t.code,{children:"gpt-4-turbo"}),", ",e.jsx(t.code,{children:"gpt-4"}),", and ",e.jsx(t.code,{children:"gpt-3.5-turbo"})," point to their respective latest model version. You can verify this by looking at the ",e.jsx(t.a,{href:"/docs/api-reference/chat/object",children:"response object"})," after sending a request. The response will include the specific model version used (e.g. ",e.jsx(t.code,{children:"gpt-3.5-turbo-1106"}),"). The ",e.jsx(t.code,{children:"chatgpt-4o-latest"})," model version continuously points to the version of GPT-4o used in ",e.jsx(t.a,{href:"https://chatgpt.com",children:"ChatGPT"}),", and is updated frequently."]}),"\n",e.jsxs(t.p,{children:["With the exception of ",e.jsx(t.code,{children:"chatgpt-4o-latest"}),", we offer pinned model versions that developers can continue using for at least three months after an updated model has been introduced. With the new cadence of model updates, we are also giving developers the ability to contribute evals to help us improve the model for different use cases. If you are interested, check out the ",e.jsx(t.a,{href:"https://github.com/openai/evals",children:"OpenAI Evals"})," repository."]}),"\n",e.jsxs(t.p,{children:["Learn more about model deprecation on our ",e.jsx(t.a,{href:"/docs/deprecations",children:"deprecation page"}),"."]}),"\n",e.jsx(t.h2,{children:"GPT-4o"}),"\n",e.jsxs(t.p,{children:["GPT-4o (“o” for “omni”) is our most advanced model. It is multimodal (accepting text or image inputs and outputting text), and it has the same high intelligence as GPT-4 Turbo but is much more efficient—it generates text 2x faster and is 50% cheaper. Additionally, GPT-4o has the best vision and performance across non-English languages of any of our models. GPT-4o is available in the OpenAI API to paying customers. Learn how to use GPT-4o in our ",e.jsx(t.a,{href:"/docs/guides/text-generation",children:"text generation guide"}),"."]}),"\n",e.jsx("div",{className:"models-table",children:e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Model"}),e.jsx(t.th,{children:"Description"}),e.jsx(t.th,{children:"Context window"}),e.jsx(t.th,{children:"Max output tokens"}),e.jsx(t.th,{children:"Training data"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"gpt-4o"}),e.jsxs(t.td,{children:[e.jsx(t.strong,{children:"GPT-4o"}),": Our high-intelligence flagship model for complex, multi-step tasks. GPT-4o is cheaper and faster than GPT-4 Turbo. ",e.jsxs(t.a,{href:"#4ofootnote",children:["Currently points to ",e.jsx(t.code,{children:"gpt-4o-2024-05-13"})," [1]"]}),"."]}),e.jsx(t.td,{children:"128,000 tokens"}),e.jsx(t.td,{children:"4,096 tokens"}),e.jsx(t.td,{children:"Up to Oct 2023"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"gpt-4o-2024-05-13"}),e.jsxs(t.td,{children:[e.jsx(t.code,{children:"gpt-4o"})," currently points to this version."]}),e.jsx(t.td,{children:"128,000 tokens"}),e.jsx(t.td,{children:"4,096 tokens"}),e.jsx(t.td,{children:"Up to Oct 2023"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"gpt-4o-2024-08-06"}),e.jsxs(t.td,{children:["Latest snapshot that supports ",e.jsx(t.a,{href:"/docs/guides/structured-outputs",children:"Structured Outputs"})]}),e.jsx(t.td,{children:"128,000 tokens"}),e.jsx(t.td,{children:"16,384 tokens"}),e.jsx(t.td,{children:"Up to Oct 2023"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"chatgpt-4o-latest"}),e.jsxs(t.td,{children:["Dynamic model continuously updated to the current version of GPT-4o in ChatGPT. ",e.jsx(t.a,{href:"#4ofootnote",children:"Intended for research and evaluation [2]"}),"."]}),e.jsx(t.td,{children:"128,000 tokens"}),e.jsx(t.td,{children:"16,384 tokens"}),e.jsx(t.td,{children:"Up to Oct 2023"})]})]})]})}),"\n",e.jsx("a",{id:"4ofootnote"}),"\n",e.jsxs(b,{children:[e.jsxs(t.p,{children:["[1] We will give a 3-week notice before updating ",e.jsx(t.code,{children:"gpt-4o"})," to point to the new snapshot ",e.jsx(t.code,{children:"gpt-4o-2024-08-06"}),"."]}),e.jsx(t.p,{children:"[2] We are releasing this model for developers and researchers to explore OpenAI's latest research. For production use, OpenAI recommends using dated GPT models, which are optimized for API usage."})]}),"\n",e.jsx(t.h2,{children:"GPT-4o mini"}),"\n",e.jsxs(t.p,{children:["GPT-4o mini (“o” for “omni”) is our most advanced model in the small models category, and our cheapest model yet.\nIt is multimodal (accepting text or image inputs and outputting text), has higher intelligence than ",e.jsx(t.code,{children:"gpt-3.5-turbo"})," but is just as fast.\nIt is meant to be used for smaller tasks, including vision tasks."]}),"\n",e.jsxs(t.p,{children:["We recommend choosing ",e.jsx(t.code,{children:"gpt-4o-mini"})," where you would have previously used ",e.jsx(t.code,{children:"gpt-3.5-turbo"})," as this model is more capable and cheaper."]}),"\n",e.jsx("div",{className:"models-table",children:e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Model"}),e.jsx(t.th,{children:"Description"}),e.jsx(t.th,{children:"Context window"}),e.jsx(t.th,{children:"Max output tokens"}),e.jsx(t.th,{children:"Training data"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"gpt-4o-mini"}),e.jsxs(t.td,{children:[e.jsx(he,{color:"green",children:"New"})," ",e.jsx(t.strong,{children:"GPT-4o-mini"}),e.jsx("br",{})," Our affordable and intelligent small model for fast, lightweight tasks. GPT-4o mini is cheaper and more capable than GPT-3.5 Turbo. Currently points to ",e.jsx(t.code,{children:"gpt-4o-mini-2024-07-18"}),"."]}),e.jsx(t.td,{children:"128,000 tokens"}),e.jsx(t.td,{children:"16,384 tokens"}),e.jsx(t.td,{children:"Up to Oct 2023"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"gpt-4o-mini-2024-07-18"}),e.jsxs(t.td,{children:[e.jsx(t.code,{children:"gpt-4o-mini"})," currently points to this version."]}),e.jsx(t.td,{children:"128,000 tokens"}),e.jsx(t.td,{children:"16,384 tokens"}),e.jsx(t.td,{children:"Up to Oct 2023"})]})]})]})}),"\n",e.jsx(t.h2,{children:"GPT-4 Turbo and GPT-4"}),"\n",e.jsxs(t.p,{children:["GPT-4 is a large multimodal model (accepting text or image inputs and outputting text) that can solve difficult problems with greater accuracy than any of our previous models, thanks to its broader general knowledge and advanced reasoning capabilities. GPT-4 is available in the OpenAI API to ",e.jsx(t.a,{href:"https://help.openai.com/en/articles/7102672-how-can-i-access-gpt-4",children:"paying customers"}),". Like ",e.jsx(t.code,{children:"gpt-3.5-turbo"}),", GPT-4 is optimized for chat but works well for traditional completions tasks using the ",e.jsx(t.a,{href:"/docs/api-reference/chat",children:"Chat Completions API"}),". Learn how to use GPT-4 in our ",e.jsx(t.a,{href:"/docs/guides/text-generation",children:"text generation guide"}),"."]}),"\n",e.jsx("div",{className:"models-table",children:e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Model"}),e.jsx(t.th,{children:"Description"}),e.jsx(t.th,{children:"Context window"}),e.jsx(t.th,{children:"Max output tokens"}),e.jsx(t.th,{children:"Training data"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"gpt-4-turbo"}),e.jsxs(t.td,{children:["The latest GPT-4 Turbo model with vision capabilities. Vision requests can now use JSON mode and function calling. Currently points to ",e.jsx(t.code,{children:"gpt-4-turbo-2024-04-09"}),"."]}),e.jsx(t.td,{children:"128,000 tokens"}),e.jsx(t.td,{children:"4,096 tokens"}),e.jsx(t.td,{children:"Up to Dec 2023"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"gpt-4-turbo-2024-04-09"}),e.jsxs(t.td,{children:["GPT-4 Turbo with Vision model. Vision requests can now use JSON mode and function calling. ",e.jsx(t.code,{children:"gpt-4-turbo"})," currently points to this version."]}),e.jsx(t.td,{children:"128,000 tokens"}),e.jsx(t.td,{children:"4,096 tokens"}),e.jsx(t.td,{children:"Up to Dec 2023"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"gpt-4-turbo-preview"}),e.jsxs(t.td,{children:["GPT-4 Turbo preview model. Currently points to ",e.jsx(t.code,{children:"gpt-4-0125-preview"}),"."]}),e.jsx(t.td,{children:"128,000 tokens"}),e.jsx(t.td,{children:"4,096 tokens"}),e.jsx(t.td,{children:"Up to Dec 2023"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"gpt-4-0125-preview"}),e.jsxs(t.td,{children:["GPT-4 Turbo preview model intended to reduce cases of “laziness” where the model doesn’t complete a task. ",e.jsx(t.a,{href:"https://openai.com/blog/new-embedding-models-and-api-updates",children:"Learn more"}),"."]}),e.jsx(t.td,{children:"128,000 tokens"}),e.jsx(t.td,{children:"4,096 tokens"}),e.jsx(t.td,{children:"Up to Dec 2023"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"gpt-4-1106-preview"}),e.jsxs(t.td,{children:["GPT-4 Turbo preview model featuring improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. This is a preview model. ",e.jsx(t.a,{href:"https://openai.com/blog/new-models-and-developer-products-announced-at-devday",children:"Learn more"}),"."]}),e.jsx(t.td,{children:"128,000 tokens"}),e.jsx(t.td,{children:"4,096 tokens"}),e.jsx(t.td,{children:"Up to Apr 2023"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"gpt-4"}),e.jsxs(t.td,{children:["Currently points to ",e.jsx(t.code,{children:"gpt-4-0613"}),". See ",e.jsx(t.a,{href:"/docs/models/continuous-model-upgrades",children:"continuous model upgrades"}),"."]}),e.jsx(t.td,{children:"8,192 tokens"}),e.jsx(t.td,{children:"8,192 tokens"}),e.jsx(t.td,{children:"Up to Sep 2021"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"gpt-4-0613"}),e.jsxs(t.td,{children:["Snapshot of ",e.jsx(t.code,{children:"gpt-4"})," from June 13th 2023 with improved function calling support."]}),e.jsx(t.td,{children:"8,192 tokens"}),e.jsx(t.td,{children:"8,192 tokens"}),e.jsx(t.td,{children:"Up to Sep 2021"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"gpt-4-0314"}),e.jsxs(t.td,{children:[e.jsx(he,{color:"orange",children:"Legacy"})," Snapshot of ",e.jsx(t.code,{children:"gpt-4"})," from March 14th 2023."]}),e.jsx(t.td,{children:"8,192 tokens"}),e.jsx(t.td,{children:"8,192 tokens"}),e.jsx(t.td,{children:"Up to Sep 2021"})]})]})]})}),"\n",e.jsx(t.p,{children:"For many basic tasks, the difference between GPT-4 and GPT-3.5 models is not significant. However, in more complex reasoning situations, GPT-4 is much more capable than any of our previous models."}),"\n",e.jsx(t.h4,{children:"Multilingual capabilities"}),"\n",e.jsxs(t.p,{children:["GPT-4 ",e.jsx(t.a,{href:"https://cdn.openai.com/papers/gpt-4.pdf",children:"outperforms both previous large language models"})," and as of 2023, most state-of-the-art systems (which often have benchmark-specific training or hand-engineering). On the MMLU benchmark, an English-language suite of multiple-choice questions covering 57 subjects, GPT-4 not only outperforms existing models by a considerable margin in English, but also demonstrates strong performance in other languages."]}),"\n",e.jsx(t.h2,{children:"GPT-3.5 Turbo"}),"\n",e.jsxs(t.p,{children:["GPT-3.5 Turbo models can understand and generate natural language or code and have been optimized for chat using the ",e.jsx(t.a,{href:"/docs/api-reference/chat",children:"Chat Completions API"})," but work well for non-chat tasks as well."]}),"\n",e.jsx(b,{children:e.jsxs(t.p,{children:["As of July 2024, ",e.jsx(t.code,{children:"gpt-4o-mini"})," should be used in place of ",e.jsx(t.code,{children:"gpt-3.5-turbo"}),", as it is cheaper, more capable, multimodal, and just as fast. ",e.jsx(t.code,{children:"gpt-3.5-turbo"})," is still available for use in the API."]})}),"\n",e.jsx("div",{className:"models-table",children:e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Model"}),e.jsx(t.th,{children:"Description"}),e.jsx(t.th,{children:"Context window"}),e.jsx(t.th,{children:"Max output tokens"}),e.jsx(t.th,{children:"Training data"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"gpt-3.5-turbo-0125"}),e.jsxs(t.td,{children:["The latest GPT-3.5 Turbo model with higher accuracy at responding in requested formats and a fix for a bug which caused a text encoding issue for non-English language function calls. ",e.jsx(t.a,{href:"https://openai.com/blog/new-embedding-models-and-api-updates#:~:text=Other%20new%20models%20and%20lower%20pricing",children:"Learn more"}),"."]}),e.jsx(t.td,{children:"16,385 tokens"}),e.jsx(t.td,{children:"4,096 tokens"}),e.jsx(t.td,{children:"Up to Sep 2021"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"gpt-3.5-turbo"}),e.jsxs(t.td,{children:["Currently points to ",e.jsx(t.code,{children:"gpt-3.5-turbo-0125"}),"."]}),e.jsx(t.td,{children:"16,385 tokens"}),e.jsx(t.td,{children:"4,096 tokens"}),e.jsx(t.td,{children:"Up to Sep 2021"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"gpt-3.5-turbo-1106"}),e.jsxs(t.td,{children:["GPT-3.5 Turbo model with improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. ",e.jsx(t.a,{href:"https://openai.com/blog/new-models-and-developer-products-announced-at-devday",children:"Learn more"}),"."]}),e.jsx(t.td,{children:"16,385 tokens"}),e.jsx(t.td,{children:"4,096 tokens"}),e.jsx(t.td,{children:"Up to Sep 2021"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"gpt-3.5-turbo-instruct"}),e.jsx(t.td,{children:"Similar capabilities as GPT-3 era models. Compatible with legacy Completions endpoint and not Chat Completions."}),e.jsx(t.td,{children:"4,096 tokens"}),e.jsx(t.td,{children:"4,096 tokens"}),e.jsx(t.td,{children:"Up to Sep 2021"})]})]})]})}),"\n",e.jsx(t.h2,{children:"DALL·E"}),"\n",e.jsx(t.p,{children:"DALL·E is a AI system that can create realistic images and art from a description in natural language. DALL·E 3 currently supports the ability, given a prompt, to create a new image with a specific size. DALL·E 2 also support the ability to edit an existing image, or create variations of a user provided image."}),"\n",e.jsxs(t.p,{children:[e.jsx(t.a,{href:"https://openai.com/dall-e-3",children:"DALL·E 3"})," is available through our ",e.jsx(t.a,{href:"/docs/guides/images/introduction",children:"Images API"})," along with ",e.jsx(t.a,{href:"https://openai.com/blog/dall-e-api-now-available-in-public-beta",children:"DALL·E 2"}),". You can try DALL·E 3 through ",e.jsx(t.a,{href:"https://chatgpt.com",children:"ChatGPT Plus"}),"."]}),"\n",e.jsx("div",{className:"models-table",children:e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Model"}),e.jsx(t.th,{children:"Description"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"dall-e-3"})}),e.jsxs(t.td,{children:["The latest DALL·E model released in Nov 2023. ",e.jsx(t.a,{href:"https://openai.com/blog/new-models-and-developer-products-announced-at-devday",children:"Learn more"}),"."]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"dall-e-2"})}),e.jsx(t.td,{children:"The previous DALL·E model released in Nov 2022. The 2nd iteration of DALL·E with more realistic, accurate, and 4x greater resolution images than the original model."})]})]})]})}),"\n",e.jsx(t.h2,{children:"TTS"}),"\n",e.jsxs(t.p,{children:["TTS is an AI model that converts text to natural sounding spoken text. We offer two different model variates, ",e.jsx(t.code,{children:"tts-1"})," is optimized for real time text to speech use cases and ",e.jsx(t.code,{children:"tts-1-hd"})," is optimized for quality. These models can be used with the ",e.jsx(t.a,{href:"/docs/guides/text-to-speech",children:"Speech endpoint in the Audio API"}),"."]}),"\n",e.jsx("div",{className:"models-table",children:e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Model"}),e.jsx(t.th,{children:"Description"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"tts-1"})}),e.jsx(t.td,{children:"The latest text to speech model, optimized for speed."})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"tts-1-hd"})}),e.jsx(t.td,{children:"The latest text to speech model, optimized for quality."})]})]})]})}),"\n",e.jsx(t.h2,{children:"Whisper"}),"\n",e.jsxs(t.p,{children:["Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification. The Whisper v2-large model is currently available through our API with the ",e.jsx(t.code,{children:"whisper-1"})," model name."]}),"\n",e.jsxs(t.p,{children:["Currently, there is no difference between the ",e.jsx(t.a,{href:"https://github.com/openai/whisper",children:"open source version of Whisper"})," and the version available through our API. However, ",e.jsx(t.a,{href:"/docs/guides/speech-to-text",children:"through our API"}),", we offer an optimized inference process which makes running Whisper through our API much faster than doing it through other means. For more technical details on Whisper, you can ",e.jsx(t.a,{href:"https://arxiv.org/abs/2212.04356",children:"read the paper"}),"."]}),"\n",e.jsx(t.h2,{children:"Embeddings"}),"\n",e.jsxs(t.p,{children:["Embeddings are a numerical representation of text that can be used to measure the relatedness between two pieces of text. Embeddings are useful for search, clustering, recommendations, anomaly detection, and classification tasks. You can read more about our latest embedding models in the ",e.jsx(t.a,{href:"https://openai.com/blog/new-embedding-models-and-api-updates",children:"announcement blog post"}),"."]}),"\n",e.jsx("div",{className:"models-table",children:e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Model"}),e.jsx(t.th,{children:"Description"}),e.jsx(t.th,{children:"Output Dimension"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-3-large"})}),e.jsx(t.td,{children:"Most capable embedding model for both english and non-english tasks"}),e.jsx(t.td,{children:"3,072"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-3-small"})}),e.jsx(t.td,{children:"Increased performance over 2nd generation ada embedding model"}),e.jsx(t.td,{children:"1,536"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"text-embedding-ada-002"})}),e.jsx(t.td,{children:"Most capable 2nd generation embedding model, replacing 16 first generation models"}),e.jsx(t.td,{children:"1,536"})]})]})]})}),"\n",e.jsx(t.hr,{}),"\n",e.jsx(t.h2,{children:"Moderation"}),"\n",e.jsxs(t.p,{children:["The Moderation models are designed to check whether content complies with OpenAI's ",e.jsx(t.a,{href:"https://openai.com/policies/usage-policies",children:"usage policies"}),". The models provide classification capabilities that look for content in the following categories: hate, hate/threatening, self-harm, sexual, sexual/minors, violence, and violence/graphic. You can find out more in our ",e.jsx(t.a,{href:"/docs/guides/moderation/overview",children:"moderation guide"}),"."]}),"\n",e.jsx(t.p,{children:"Moderation models take in an arbitrary sized input that is automatically broken up into chunks of 4,096 tokens. In cases where the input is more than 32,768 tokens, truncation is used which in a rare condition may omit a small number of tokens from the moderation check."}),"\n",e.jsx(t.p,{children:"The final results from each request to the moderation endpoint shows the maximum value on a per category basis. For example, if one chunk of 4K tokens had a category score of 0.9901 and the other had a score of 0.1901, the results would show 0.9901 in the API response since it is higher."}),"\n",e.jsx("div",{className:"models-table",children:e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Model"}),e.jsx(t.th,{children:"Description"}),e.jsx(t.th,{children:"Max tokens"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"text-moderation-latest"})}),e.jsxs(t.td,{children:["Currently points to ",e.jsx(t.code,{children:"text-moderation-007"}),"."]}),e.jsx(t.td,{children:"32,768"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"text-moderation-stable"})}),e.jsxs(t.td,{children:["Currently points to ",e.jsx(t.code,{children:"text-moderation-007"}),"."]}),e.jsx(t.td,{children:"32,768"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"text-moderation-007"})}),e.jsx(t.td,{children:"Most capable moderation model across all categories."}),e.jsx(t.td,{children:"32,768"})]})]})]})}),"\n",e.jsx(t.h2,{children:"GPT base"}),"\n",e.jsx(t.p,{children:"GPT base models can understand and generate natural language or code but are not trained with instruction following. These models are made to be replacements for our original GPT-3 base models and use the legacy Completions API. Most customers should use GPT-3.5 or GPT-4."}),"\n",e.jsx("div",{className:"models-table",children:e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Model"}),e.jsx(t.th,{children:"Description"}),e.jsx(t.th,{children:"Max tokens"}),e.jsx(t.th,{children:"Training data"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"babbage-002"})}),e.jsxs(t.td,{children:["Replacement for the GPT-3 ",e.jsx(t.code,{children:"ada"})," and ",e.jsx(t.code,{children:"babbage"})," base models."]}),e.jsx(t.td,{children:"16,384 tokens"}),e.jsx(t.td,{children:"Up to Sep 2021"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"davinci-002"})}),e.jsxs(t.td,{children:["Replacement for the GPT-3 ",e.jsx(t.code,{children:"curie"})," and ",e.jsx(t.code,{children:"davinci"})," base models."]}),e.jsx(t.td,{children:"16,384 tokens"}),e.jsx(t.td,{children:"Up to Sep 2021"})]})]})]})}),"\n",e.jsx(t.h2,{children:"How we use your data"}),"\n",e.jsx(t.p,{children:"Your data is your data."}),"\n",e.jsxs(t.p,{children:["As of March 1, 2023, data sent to the OpenAI API will not be used to train or improve OpenAI models (unless you explicitly ",e.jsx(t.a,{href:"https://docs.google.com/forms/d/e/1FAIpQLSevgtKyiSWIOj6CV6XWBHl1daPZSOcIWzcUYUXQ1xttjBgDpA/viewform",children:"opt in"}),"). One advantage to opting in is that the models may get better at your use case over time."]}),"\n",e.jsx(t.p,{children:"To help identify abuse, API data may be retained for up to 30 days, after which it will be deleted (unless otherwise required by law). For trusted customers with sensitive applications, zero data retention may be available. With zero data retention, request and response bodies are not persisted to any logging mechanism and exist only in memory in order to serve the request."}),"\n",e.jsxs(t.p,{children:["Note that this data policy does not apply to OpenAI's non-API consumer services like ",e.jsx(t.a,{href:"https://chatgpt.com/",children:"ChatGPT"})," or ",e.jsx(t.a,{href:"https://labs.openai.com/",children:"DALL·E Labs"}),"."]}),"\n",e.jsx(t.h3,{children:"Default usage policies by endpoint"}),"\n",e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Endpoint"}),e.jsx(t.th,{children:"Data used for training"}),e.jsx(t.th,{children:"Default retention"}),e.jsx(t.th,{children:"Eligible for zero retention"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsxs(t.td,{children:[e.jsx(t.code,{children:"/v1/chat/completions"}),"*"]}),e.jsx(t.td,{children:"No"}),e.jsx(t.td,{children:"30 days"}),e.jsx(t.td,{children:"Yes, except (a) image inputs or (b) schemas provided for Structured Outputs*"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"/v1/assistants"})}),e.jsx(t.td,{children:"No"}),e.jsx(t.td,{children:"30 days **"}),e.jsx(t.td,{children:"No"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"/v1/threads"})}),e.jsx(t.td,{children:"No"}),e.jsx(t.td,{children:"30 days **"}),e.jsx(t.td,{children:"No"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"/v1/threads/messages"})}),e.jsx(t.td,{children:"No"}),e.jsx(t.td,{children:"30 days **"}),e.jsx(t.td,{children:"No"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"/v1/threads/runs"})}),e.jsx(t.td,{children:"No"}),e.jsx(t.td,{children:"30 days **"}),e.jsx(t.td,{children:"No"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"/v1/vector_stores"})}),e.jsx(t.td,{children:"No"}),e.jsx(t.td,{children:"30 days **"}),e.jsx(t.td,{children:"No"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"/v1/threads/runs/steps"})}),e.jsx(t.td,{children:"No"}),e.jsx(t.td,{children:"30 days **"}),e.jsx(t.td,{children:"No"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"/v1/images/generations"})}),e.jsx(t.td,{children:"No"}),e.jsx(t.td,{children:"30 days"}),e.jsx(t.td,{children:"No"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"/v1/images/edits"})}),e.jsx(t.td,{children:"No"}),e.jsx(t.td,{children:"30 days"}),e.jsx(t.td,{children:"No"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"/v1/images/variations"})}),e.jsx(t.td,{children:"No"}),e.jsx(t.td,{children:"30 days"}),e.jsx(t.td,{children:"No"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"/v1/embeddings"})}),e.jsx(t.td,{children:"No"}),e.jsx(t.td,{children:"30 days"}),e.jsx(t.td,{children:"Yes"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"/v1/audio/transcriptions"})}),e.jsx(t.td,{children:"No"}),e.jsx(t.td,{children:"Zero data retention"}),e.jsx(t.td,{children:"-"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"/v1/audio/translations"})}),e.jsx(t.td,{children:"No"}),e.jsx(t.td,{children:"Zero data retention"}),e.jsx(t.td,{children:"-"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"/v1/audio/speech"})}),e.jsx(t.td,{children:"No"}),e.jsx(t.td,{children:"30 days"}),e.jsx(t.td,{children:"Yes"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"/v1/files"})}),e.jsx(t.td,{children:"No"}),e.jsx(t.td,{children:"Until deleted by customer"}),e.jsx(t.td,{children:"No"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"/v1/fine_tuning/jobs"})}),e.jsx(t.td,{children:"No"}),e.jsx(t.td,{children:"Until deleted by customer"}),e.jsx(t.td,{children:"No"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"/v1/batches"})}),e.jsx(t.td,{children:"No"}),e.jsx(t.td,{children:"Until deleted by customer"}),e.jsx(t.td,{children:"No"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"/v1/moderations"})}),e.jsx(t.td,{children:"No"}),e.jsx(t.td,{children:"Zero data retention"}),e.jsx(t.td,{children:"-"})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:e.jsx(t.code,{children:"/v1/completions"})}),e.jsx(t.td,{children:"No"}),e.jsx(t.td,{children:"30 days"}),e.jsx(t.td,{children:"Yes"})]})]})]}),"\n",e.jsxs(t.p,{children:["* Image inputs via the ",e.jsx(t.code,{children:"gpt-4o"}),", ",e.jsx(t.code,{children:"gpt-4o-mini"}),", ",e.jsx(t.code,{children:"chatgpt-4o-latest"}),", or ",e.jsx(t.code,{children:"gpt-4-turbo"})," models (or previously ",e.jsx(t.code,{children:"gpt-4-vision-preview"}),") are not eligible for zero retention.\nWhen Structured Outputs is enabled, schemas provided (either as the ",e.jsx(t.code,{children:"response_format"})," or in the function definition) are not eligible for zero retention, though the completions themselves are."]}),"\n",e.jsx(t.p,{children:"** Objects related to the Assistants API are deleted from our servers 30 days after you delete them via the API or the dashboard. Objects that are not deleted via the API or dashboard are retained indefinitely."}),"\n",e.jsxs(t.p,{children:["For details, see our ",e.jsx(t.a,{href:"https://openai.com/policies/api-data-usage-policies",children:"API data usage policies"}),". To learn more about zero retention, get in touch with our ",e.jsx(t.a,{href:"https://openai.com/contact-sales",children:"sales team"}),"."]}),"\n",e.jsx(t.h2,{children:"Model endpoint compatibility"}),"\n",e.jsxs("div",{className:"models-table",children:[e.jsxs(t.table,{children:[e.jsx(t.thead,{children:e.jsxs(t.tr,{children:[e.jsx(t.th,{children:"Endpoint"}),e.jsx(t.th,{children:"Latest models"})]})}),e.jsxs(t.tbody,{children:[e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"/v1/assistants"}),e.jsxs(t.td,{children:["All GPT-4o (except ",e.jsx(t.code,{children:"chatgpt-4o-latest"}),"), GPT-4o-mini, GPT-4, and GPT-3.5 Turbo models. The ",e.jsx(t.code,{children:"retrieval"})," tool requires ",e.jsx(t.code,{children:"gpt-4-turbo-preview"})," (and subsequent dated model releases) or ",e.jsx(t.code,{children:"gpt-3.5-turbo-1106"})," (and subsequent versions)."]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"/v1/audio/transcriptions"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"whisper-1"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"/v1/audio/translations"}),e.jsx(t.td,{children:e.jsx(t.code,{children:"whisper-1"})})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"/v1/audio/speech"}),e.jsxs(t.td,{children:[e.jsx(t.code,{children:"tts-1"}),",  ",e.jsx(t.code,{children:"tts-1-hd"})]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"/v1/chat/completions"}),e.jsxs(t.td,{children:["All GPT-4o, GPT-4o-mini, GPT-4, and GPT-3.5 Turbo models and their dated releases. ",e.jsx(t.code,{children:"chatgpt-4o-latest"})," dynamic model. ",e.jsx(t.a,{href:"/docs/guides/fine-tuning",children:"Fine-tuned"})," versions of ",e.jsx(t.code,{children:"gpt-4o"}),",  ",e.jsx(t.code,{children:"gpt-4o-mini"}),",  ",e.jsx(t.code,{children:"gpt-4"}),",  and ",e.jsx(t.code,{children:"gpt-3.5-turbo"}),"."]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"/v1/completions (Legacy)"}),e.jsxs(t.td,{children:[e.jsx(t.code,{children:"gpt-3.5-turbo-instruct"}),",  ",e.jsx(t.code,{children:"babbage-002"}),",  ",e.jsx(t.code,{children:"davinci-002"})]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"/v1/embeddings"}),e.jsxs(t.td,{children:[e.jsx(t.code,{children:"text-embedding-3-small"}),",  ",e.jsx(t.code,{children:"text-embedding-3-large"}),",  ",e.jsx(t.code,{children:"text-embedding-ada-002"})]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"/v1/fine_tuning/jobs"}),e.jsxs(t.td,{children:[e.jsx(t.code,{children:"gpt-4o"}),",  ",e.jsx(t.code,{children:"gpt-4o-mini"}),",  ",e.jsx(t.code,{children:"gpt-4"}),",  ",e.jsx(t.code,{children:"gpt-3.5-turbo"}),",  ",e.jsx(t.code,{children:"babbage-002"}),",  ",e.jsx(t.code,{children:"davinci-002"})]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"/v1/moderations"}),e.jsxs(t.td,{children:[e.jsx(t.code,{children:"text-moderation-stable"}),",  ",e.jsx(t.code,{children:"text-moderation-latest"})]})]}),e.jsxs(t.tr,{children:[e.jsx(t.td,{children:"/v1/images/generations"}),e.jsxs(t.td,{children:[e.jsx(t.code,{children:"dall-e-2"}),",  ",e.jsx(t.code,{children:"dall-e-3"})]})]})]})]}),e.jsxs(t.p,{children:["This list excludes all of our ",e.jsx(t.a,{href:"/docs/deprecations",children:"deprecated models"}),"."]})]})]})}function so(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Bt,n)})):Bt(n)}function Ut(n){const t=Object.assign({h1:"h1",p:"p",ul:"ul",li:"li"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"Supported countries and territories"}),"\n",e.jsx(t.p,{children:"We provide below a list of the countries and territories in which we currently support access to our API services."}),"\n",e.jsx(t.p,{children:"Accessing or offering access to our services outside of the countries and territories listed below may result in your account being blocked or suspended."}),"\n",e.jsxs(t.ul,{children:["\n",e.jsx(t.li,{children:"Albania"}),"\n",e.jsx(t.li,{children:"Algeria"}),"\n",e.jsx(t.li,{children:"Afghanistan"}),"\n",e.jsx(t.li,{children:"Andorra"}),"\n",e.jsx(t.li,{children:"Angola"}),"\n",e.jsx(t.li,{children:"Antigua and Barbuda"}),"\n",e.jsx(t.li,{children:"Argentina"}),"\n",e.jsx(t.li,{children:"Armenia"}),"\n",e.jsx(t.li,{children:"Australia"}),"\n",e.jsx(t.li,{children:"Austria"}),"\n",e.jsx(t.li,{children:"Azerbaijan"}),"\n",e.jsx(t.li,{children:"Bahamas"}),"\n",e.jsx(t.li,{children:"Bahrain"}),"\n",e.jsx(t.li,{children:"Bangladesh"}),"\n",e.jsx(t.li,{children:"Barbados"}),"\n",e.jsx(t.li,{children:"Belgium"}),"\n",e.jsx(t.li,{children:"Belize"}),"\n",e.jsx(t.li,{children:"Benin"}),"\n",e.jsx(t.li,{children:"Bhutan"}),"\n",e.jsx(t.li,{children:"Bolivia"}),"\n",e.jsx(t.li,{children:"Bosnia and Herzegovina"}),"\n",e.jsx(t.li,{children:"Botswana"}),"\n",e.jsx(t.li,{children:"Brazil"}),"\n",e.jsx(t.li,{children:"Brunei"}),"\n",e.jsx(t.li,{children:"Bulgaria"}),"\n",e.jsx(t.li,{children:"Burkina Faso"}),"\n",e.jsx(t.li,{children:"Burundi"}),"\n",e.jsx(t.li,{children:"Cabo Verde"}),"\n",e.jsx(t.li,{children:"Cambodia"}),"\n",e.jsx(t.li,{children:"Cameroon"}),"\n",e.jsx(t.li,{children:"Canada"}),"\n",e.jsx(t.li,{children:"Central African Republic"}),"\n",e.jsx(t.li,{children:"Chad"}),"\n",e.jsx(t.li,{children:"Chile"}),"\n",e.jsx(t.li,{children:"Colombia"}),"\n",e.jsx(t.li,{children:"Comoros"}),"\n",e.jsx(t.li,{children:"Congo (Brazzaville)"}),"\n",e.jsx(t.li,{children:"Congo (DRC)"}),"\n",e.jsx(t.li,{children:"Costa Rica"}),"\n",e.jsx(t.li,{children:"Côte d'Ivoire"}),"\n",e.jsx(t.li,{children:"Croatia"}),"\n",e.jsx(t.li,{children:"Cyprus"}),"\n",e.jsx(t.li,{children:"Czechia (Czech Republic)"}),"\n",e.jsx(t.li,{children:"Denmark"}),"\n",e.jsx(t.li,{children:"Djibouti"}),"\n",e.jsx(t.li,{children:"Dominica"}),"\n",e.jsx(t.li,{children:"Dominican Republic"}),"\n",e.jsx(t.li,{children:"Ecuador"}),"\n",e.jsx(t.li,{children:"Egypt"}),"\n",e.jsx(t.li,{children:"El Salvador"}),"\n",e.jsx(t.li,{children:"Equatorial Guinea"}),"\n",e.jsx(t.li,{children:"Eritrea"}),"\n",e.jsx(t.li,{children:"Estonia"}),"\n",e.jsx(t.li,{children:"Eswatini (Swaziland)"}),"\n",e.jsx(t.li,{children:"Ethiopia"}),"\n",e.jsx(t.li,{children:"Fiji"}),"\n",e.jsx(t.li,{children:"Finland"}),"\n",e.jsx(t.li,{children:"France"}),"\n",e.jsx(t.li,{children:"Gabon"}),"\n",e.jsx(t.li,{children:"Gambia"}),"\n",e.jsx(t.li,{children:"Georgia"}),"\n",e.jsx(t.li,{children:"Germany"}),"\n",e.jsx(t.li,{children:"Ghana"}),"\n",e.jsx(t.li,{children:"Greece"}),"\n",e.jsx(t.li,{children:"Grenada"}),"\n",e.jsx(t.li,{children:"Guatemala"}),"\n",e.jsx(t.li,{children:"Guinea"}),"\n",e.jsx(t.li,{children:"Guinea-Bissau"}),"\n",e.jsx(t.li,{children:"Guyana"}),"\n",e.jsx(t.li,{children:"Haiti"}),"\n",e.jsx(t.li,{children:"Holy See (Vatican City)"}),"\n",e.jsx(t.li,{children:"Honduras"}),"\n",e.jsx(t.li,{children:"Hungary"}),"\n",e.jsx(t.li,{children:"Iceland"}),"\n",e.jsx(t.li,{children:"India"}),"\n",e.jsx(t.li,{children:"Indonesia"}),"\n",e.jsx(t.li,{children:"Iraq"}),"\n",e.jsx(t.li,{children:"Ireland"}),"\n",e.jsx(t.li,{children:"Israel"}),"\n",e.jsx(t.li,{children:"Italy"}),"\n",e.jsx(t.li,{children:"Jamaica"}),"\n",e.jsx(t.li,{children:"Japan"}),"\n",e.jsx(t.li,{children:"Jordan"}),"\n",e.jsx(t.li,{children:"Kazakhstan"}),"\n",e.jsx(t.li,{children:"Kenya"}),"\n",e.jsx(t.li,{children:"Kiribati"}),"\n",e.jsx(t.li,{children:"Kuwait"}),"\n",e.jsx(t.li,{children:"Kyrgyzstan"}),"\n",e.jsx(t.li,{children:"Laos"}),"\n",e.jsx(t.li,{children:"Latvia"}),"\n",e.jsx(t.li,{children:"Lebanon"}),"\n",e.jsx(t.li,{children:"Lesotho"}),"\n",e.jsx(t.li,{children:"Liberia"}),"\n",e.jsx(t.li,{children:"Libya"}),"\n",e.jsx(t.li,{children:"Liechtenstein"}),"\n",e.jsx(t.li,{children:"Lithuania"}),"\n",e.jsx(t.li,{children:"Luxembourg"}),"\n",e.jsx(t.li,{children:"Madagascar"}),"\n",e.jsx(t.li,{children:"Malawi"}),"\n",e.jsx(t.li,{children:"Malaysia"}),"\n",e.jsx(t.li,{children:"Maldives"}),"\n",e.jsx(t.li,{children:"Mali"}),"\n",e.jsx(t.li,{children:"Malta"}),"\n",e.jsx(t.li,{children:"Marshall Islands"}),"\n",e.jsx(t.li,{children:"Mauritania"}),"\n",e.jsx(t.li,{children:"Mauritius"}),"\n",e.jsx(t.li,{children:"Mexico"}),"\n",e.jsx(t.li,{children:"Micronesia"}),"\n",e.jsx(t.li,{children:"Moldova"}),"\n",e.jsx(t.li,{children:"Monaco"}),"\n",e.jsx(t.li,{children:"Mongolia"}),"\n",e.jsx(t.li,{children:"Montenegro"}),"\n",e.jsx(t.li,{children:"Morocco"}),"\n",e.jsx(t.li,{children:"Mozambique"}),"\n",e.jsx(t.li,{children:"Myanmar"}),"\n",e.jsx(t.li,{children:"Namibia"}),"\n",e.jsx(t.li,{children:"Nauru"}),"\n",e.jsx(t.li,{children:"Nepal"}),"\n",e.jsx(t.li,{children:"Netherlands"}),"\n",e.jsx(t.li,{children:"New Zealand"}),"\n",e.jsx(t.li,{children:"Nicaragua"}),"\n",e.jsx(t.li,{children:"Niger"}),"\n",e.jsx(t.li,{children:"Nigeria"}),"\n",e.jsx(t.li,{children:"North Macedonia"}),"\n",e.jsx(t.li,{children:"Norway"}),"\n",e.jsx(t.li,{children:"Oman"}),"\n",e.jsx(t.li,{children:"Pakistan"}),"\n",e.jsx(t.li,{children:"Palau"}),"\n",e.jsx(t.li,{children:"Palestine"}),"\n",e.jsx(t.li,{children:"Panama"}),"\n",e.jsx(t.li,{children:"Papua New Guinea"}),"\n",e.jsx(t.li,{children:"Paraguay"}),"\n",e.jsx(t.li,{children:"Peru"}),"\n",e.jsx(t.li,{children:"Philippines"}),"\n",e.jsx(t.li,{children:"Poland"}),"\n",e.jsx(t.li,{children:"Portugal"}),"\n",e.jsx(t.li,{children:"Qatar"}),"\n",e.jsx(t.li,{children:"Romania"}),"\n",e.jsx(t.li,{children:"Rwanda"}),"\n",e.jsx(t.li,{children:"Saint Kitts and Nevis"}),"\n",e.jsx(t.li,{children:"Saint Lucia"}),"\n",e.jsx(t.li,{children:"Saint Vincent and the Grenadines"}),"\n",e.jsx(t.li,{children:"Samoa"}),"\n",e.jsx(t.li,{children:"San Marino"}),"\n",e.jsx(t.li,{children:"Sao Tome and Principe"}),"\n",e.jsx(t.li,{children:"Saudi Arabia"}),"\n",e.jsx(t.li,{children:"Senegal"}),"\n",e.jsx(t.li,{children:"Serbia"}),"\n",e.jsx(t.li,{children:"Seychelles"}),"\n",e.jsx(t.li,{children:"Sierra Leone"}),"\n",e.jsx(t.li,{children:"Singapore"}),"\n",e.jsx(t.li,{children:"Slovakia"}),"\n",e.jsx(t.li,{children:"Slovenia"}),"\n",e.jsx(t.li,{children:"Solomon Islands"}),"\n",e.jsx(t.li,{children:"Somalia"}),"\n",e.jsx(t.li,{children:"South Africa"}),"\n",e.jsx(t.li,{children:"South Korea"}),"\n",e.jsx(t.li,{children:"South Sudan"}),"\n",e.jsx(t.li,{children:"Spain"}),"\n",e.jsx(t.li,{children:"Sri Lanka"}),"\n",e.jsx(t.li,{children:"Suriname"}),"\n",e.jsx(t.li,{children:"Sweden"}),"\n",e.jsx(t.li,{children:"Switzerland"}),"\n",e.jsx(t.li,{children:"Sudan"}),"\n",e.jsx(t.li,{children:"Taiwan"}),"\n",e.jsx(t.li,{children:"Tajikistan"}),"\n",e.jsx(t.li,{children:"Tanzania"}),"\n",e.jsx(t.li,{children:"Thailand"}),"\n",e.jsx(t.li,{children:"Timor-Leste (East Timor)"}),"\n",e.jsx(t.li,{children:"Togo"}),"\n",e.jsx(t.li,{children:"Tonga"}),"\n",e.jsx(t.li,{children:"Trinidad and Tobago"}),"\n",e.jsx(t.li,{children:"Tunisia"}),"\n",e.jsx(t.li,{children:"Turkey"}),"\n",e.jsx(t.li,{children:"Turkmenistan"}),"\n",e.jsx(t.li,{children:"Tuvalu"}),"\n",e.jsx(t.li,{children:"Uganda"}),"\n",e.jsx(t.li,{children:"Ukraine (with certain exceptions)"}),"\n",e.jsx(t.li,{children:"United Arab Emirates"}),"\n",e.jsx(t.li,{children:"United Kingdom"}),"\n",e.jsx(t.li,{children:"United States of America"}),"\n",e.jsx(t.li,{children:"Uruguay"}),"\n",e.jsx(t.li,{children:"Uzbekistan"}),"\n",e.jsx(t.li,{children:"Vanuatu"}),"\n",e.jsx(t.li,{children:"Vietnam"}),"\n",e.jsx(t.li,{children:"Yemen"}),"\n",e.jsx(t.li,{children:"Zambia"}),"\n",e.jsx(t.li,{children:"Zimbabwe"}),"\n"]})]})}function io(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Ut,n)})):Ut(n)}function $t(n){const t=Object.assign({h1:"h1",p:"p",a:"a",pre:"pre",code:"code",h2:"h2",div:"div",blockquote:"blockquote",hr:"hr"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"How to build an AI that can answer questions about your website"}),"\n",e.jsxs(t.p,{children:["This tutorial walks through a simple example of crawling a website (in this example, the OpenAI website), turning the crawled pages into embeddings using the ",e.jsx(t.a,{href:"/docs/guides/embeddings",children:"Embeddings API"}),", and then creating a basic search functionality that allows a user to ask questions about the embedded information. This is intended to be a starting point for more sophisticated applications that make use of custom knowledge bases."]}),"\n",e.jsx(t.h1,{children:"Getting started"}),"\n",e.jsxs(t.p,{children:["Some basic knowledge of Python and GitHub is helpful for this tutorial. Before diving in, make sure to ",e.jsx(t.a,{href:"/docs/api-reference/introduction",children:"set up an OpenAI API key"})," and walk through the ",e.jsx(t.a,{href:"/docs/quickstart",children:"quickstart tutorial"}),". This will give a good intuition on how to use the API to its full potential."]}),"\n",e.jsxs(t.p,{children:["Python is used as the main programming language along with the OpenAI, Pandas, transformers, NumPy, and other popular packages. If you run into any issues working through this tutorial, please ask a question on the ",e.jsx(t.a,{href:"https://community.openai.com",children:"OpenAI Community Forum"}),"."]}),"\n",e.jsxs(t.p,{children:["To start with the code, clone the ",e.jsx(t.a,{href:"https://github.com/openai/web-crawl-q-and-a-example",children:"full code for this tutorial on GitHub"}),". Alternatively, follow along and copy each section into a Jupyter notebook and run the code step by step, or just read along. A good way to avoid any issues is to set up a new virtual environment and install the required packages by running the following commands:"]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-bash",children:"python -m venv env\n\nsource env/bin/activate\n\npip install -r requirements.txt\n"})}),"\n",e.jsx(t.h2,{children:"Setting up a web crawler"}),"\n",e.jsxs(t.p,{children:["The primary focus of this tutorial is the OpenAI API so if you prefer, you can skip the context on how to create a web crawler and just ",e.jsx(t.a,{href:"https://github.com/openai/web-crawl-q-and-a-example",children:"download the source code"}),". Otherwise, expand the section below to work through the scraping mechanism implementation."]}),"\n",e.jsxs(j,{label:"Learn how to build a web crawler",autoScroll:!0,showCollapse:!0,children:[e.jsxs(t.div,{className:"sandbox-preview",children:[e.jsx(t.div,{className:"sandbox-screenshot",children:e.jsx(V,{png:"https://cdn.openai.com/API/docs/images/tutorials/web-qa/DALL-E-coding-a-web-crawling-system-pixel-art.png",webp:"https://cdn.openai.com/API/docs/images/tutorials/web-qa/DALL-E-coding-a-web-crawling-system-pixel-art.webp",alt:"DALL-E: Coding a web crawling system pixel art",width:"1024",height:"1024"})}),e.jsxs(t.div,{className:"preview-info",children:[e.jsx(t.div,{className:"description",children:"Acquiring data in text form is the first step to use embeddings. This tutorial creates a new set of data by crawling the OpenAI website, a technique that you can also use for your own company or personal website."}),e.jsx(t.div,{className:"actions",children:e.jsx(R,{size:"small",color:te.neutral,href:"https://github.com/openai/web-crawl-q-and-a-example",target:"_blank",children:"View source code"})})]})]}),e.jsxs(t.p,{children:["While this crawler is written from scratch, open source packages like ",e.jsx(t.a,{href:"https://github.com/scrapy/scrapy",children:"Scrapy"})," can also help with these operations."]}),e.jsx(t.p,{children:"This crawler will start from the root URL passed in at the bottom of the code below, visit each page, find additional links, and visit those pages as well (as long as they have the same root domain). To begin, import the required packages, set up the basic URL, and define a HTMLParser class."}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:'import requests\nimport re\nimport urllib.request\nfrom bs4 import BeautifulSoup\nfrom collections import deque\nfrom html.parser import HTMLParser\nfrom urllib.parse import urlparse\nimport os\n\n# Regex pattern to match a URL\nHTTP_URL_PATTERN = r\'^http[s]*://.+\'\n\ndomain = "openai.com" # <- put your domain to be crawled\nfull_url = "https://openai.com/" # <- put your domain to be crawled with https or http\n\n# Create a class to parse the HTML and get the hyperlinks\nclass HyperlinkParser(HTMLParser):\n    def __init__(self):\n        super().__init__()\n        # Create a list to store the hyperlinks\n        self.hyperlinks = []\n\n    # Override the HTMLParser\'s handle_starttag method to get the hyperlinks\n    def handle_starttag(self, tag, attrs):\n        attrs = dict(attrs)\n\n        # If the tag is an anchor tag and it has an href attribute, add the href attribute to the list of hyperlinks\n        if tag == "a" and "href" in attrs:\n            self.hyperlinks.append(attrs["href"])\n'})}),e.jsx(t.p,{children:"The next function takes a URL as an argument, opens the URL, and reads the HTML content. Then, it returns all the hyperlinks found on that page."}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:"# Function to get the hyperlinks from a URL\ndef get_hyperlinks(url):\n\n    # Try to open the URL and read the HTML\n    try:\n        # Open the URL and read the HTML\n        with urllib.request.urlopen(url) as response:\n\n            # If the response is not HTML, return an empty list\n            if not response.info().get('Content-Type').startswith(\"text/html\"):\n                return []\n\n            # Decode the HTML\n            html = response.read().decode('utf-8')\n    except Exception as e:\n        print(e)\n        return []\n\n    # Create the HTML Parser and then Parse the HTML to get hyperlinks\n    parser = HyperlinkParser()\n    parser.feed(html)\n\n    return parser.hyperlinks\n"})}),e.jsxs(t.p,{children:["The goal is to crawl through and index only the content that lives under the OpenAI domain. For this purpose, a function that calls the ",e.jsx(t.code,{children:"get_hyperlinks"})," function but filters out any URLs that are not part of the specified domain is needed."]}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:'# Function to get the hyperlinks from a URL that are within the same domain\ndef get_domain_hyperlinks(local_domain, url):\n    clean_links = []\n    for link in set(get_hyperlinks(url)):\n        clean_link = None\n\n        # If the link is a URL, check if it is within the same domain\n        if re.search(HTTP_URL_PATTERN, link):\n            # Parse the URL and check if the domain is the same\n            url_obj = urlparse(link)\n            if url_obj.netloc == local_domain:\n                clean_link = link\n\n        # If the link is not a URL, check if it is a relative link\n        else:\n            if link.startswith("/"):\n                link = link[1:]\n            elif link.startswith("#") or link.startswith("mailto:"):\n                continue\n            clean_link = "https://" + local_domain + "/" + link\n\n        if clean_link is not None:\n            if clean_link.endswith("/"):\n                clean_link = clean_link[:-1]\n            clean_links.append(clean_link)\n\n    # Return the list of hyperlinks that are within the same domain\n    return list(set(clean_links))\n'})}),e.jsxs(t.p,{children:["The ",e.jsx(t.code,{children:"crawl"})," function is the final step in the web scraping task setup. It keeps track of the visited URLs to avoid repeating the same page, which might be linked across multiple pages on a site. It also extracts the raw text from a page without the HTML tags, and writes the text content into a local .txt file specific to the page."]}),e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:'def crawl(url):\n    # Parse the URL and get the domain\n    local_domain = urlparse(url).netloc\n\n    # Create a queue to store the URLs to crawl\n    queue = deque([url])\n\n    # Create a set to store the URLs that have already been seen (no duplicates)\n    seen = set([url])\n\n    # Create a directory to store the text files\n    if not os.path.exists("text/"):\n            os.mkdir("text/")\n\n    if not os.path.exists("text/"+local_domain+"/"):\n            os.mkdir("text/" + local_domain + "/")\n\n    # Create a directory to store the csv files\n    if not os.path.exists("processed"):\n            os.mkdir("processed")\n\n    # While the queue is not empty, continue crawling\n    while queue:\n\n        # Get the next URL from the queue\n        url = queue.pop()\n        print(url) # for debugging and to see the progress\n\n        # Save text from the url to a <url>.txt file\n        with open(\'text/\'+local_domain+\'/\'+url[8:].replace("/", "_") + ".txt", "w", encoding="UTF-8") as f:\n\n            # Get the text from the URL using BeautifulSoup\n            soup = BeautifulSoup(requests.get(url).text, "html.parser")\n\n            # Get the text but remove the tags\n            text = soup.get_text()\n\n            # If the crawler gets to a page that requires JavaScript, it will stop the crawl\n            if ("You need to enable JavaScript to run this app." in text):\n                print("Unable to parse page " + url + " due to JavaScript being required")\n\n            # Otherwise, write the text to the file in the text directory\n            f.write(text)\n\n        # Get the hyperlinks from the URL and add them to the queue\n        for link in get_domain_hyperlinks(local_domain, url):\n            if link not in seen:\n                queue.append(link)\n                seen.add(link)\n\ncrawl(full_url)\n'})}),e.jsx(t.p,{children:"The last line of the above example runs the crawler which goes through all the accessible links and turns those pages into text files. This will take a few minutes to run depending on the size and complexity of your site."})]}),"\n",e.jsx(t.h2,{children:"Building an embeddings index"}),"\n",e.jsxs(t.div,{className:"sandbox-preview",children:[e.jsx(t.div,{className:"sandbox-screenshot",children:e.jsx(V,{png:"https://cdn.openai.com/API/docs/images/tutorials/web-qa/DALL-E-woman-turning-a-stack-of-papers-into-numbers-pixel-art.png",webp:"https://cdn.openai.com/API/docs/images/tutorials/web-qa/DALL-E-woman-turning-a-stack-of-papers-into-numbers-pixel-art.webp",alt:"DALL-E: Woman turning a stack of papers into numbers pixel art",width:"1024",height:"1024"})}),e.jsxs(t.div,{className:"preview-info",children:[e.jsx(t.div,{className:"description",children:"CSV is a common format for storing embeddings. You can use this format with Python by converting the raw text files (which are in the text directory) into Pandas data frames. Pandas is a popular open source library that helps you work with tabular data (data stored in rows and columns)."}),e.jsx(t.div,{className:"description",children:"Blank empty lines can clutter the text files and make them harder to process. A simple function can remove those lines and tidy up the files."})]})]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:"def remove_newlines(serie):\n    serie = serie.str.replace('\\n', ' ')\n    serie = serie.str.replace('\\\\n', ' ')\n    serie = serie.str.replace('  ', ' ')\n    serie = serie.str.replace('  ', ' ')\n    return serie\n"})}),"\n",e.jsx(t.p,{children:"Converting the text to CSV requires looping through the text files in the text directory created earlier. After opening each file, remove the extra spacing and append the modified text to a list. Then, add the text with the new lines removed to an empty Pandas data frame and write the data frame to a CSV file."}),"\n",e.jsx(b,{children:e.jsx(t.p,{children:"Extra spacing and new lines can clutter the text and complicate the embeddings process. The code used here helps to remove some of them but you may find 3rd party libraries or other methods useful to get rid of more unnecessary characters."})}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:"import pandas as pd\n\n# Create a list to store the text files\ntexts=[]\n\n# Get all the text files in the text directory\nfor file in os.listdir(\"text/\" + domain + \"/\"):\n\n    # Open the file and read the text\n    with open(\"text/\" + domain + \"/\" + file, \"r\", encoding=\"UTF-8\") as f:\n        text = f.read()\n\n        # Omit the first 11 lines and the last 4 lines, then replace -, _, and #update with spaces.\n        texts.append((file[11:-4].replace('-',' ').replace('_', ' ').replace('#update',''), text))\n\n# Create a dataframe from the list of texts\ndf = pd.DataFrame(texts, columns = ['fname', 'text'])\n\n# Set the text column to be the raw text with the newlines removed\ndf['text'] = df.fname + \". \" + remove_newlines(df.text)\ndf.to_csv('processed/scraped.csv')\ndf.head()\n"})}),"\n",e.jsxs(t.p,{children:["Tokenization is the next step after saving the raw text into a CSV file. This process splits the input text into tokens by breaking down the sentences and words. A visual demonstration of this can be seen by ",e.jsx(t.a,{href:"/tokenizer",children:"checking out our Tokenizer"})," in the docs."]}),"\n",e.jsxs(t.blockquote,{children:["\n",e.jsx(t.p,{children:"A helpful rule of thumb is that one token generally corresponds to ~4 characters of text for common English text. This translates to roughly ¾ of a word (so 100 tokens ~= 75 words)."}),"\n"]}),"\n",e.jsx(t.p,{children:"The API has a limit on the maximum number of input tokens for embeddings. To stay below the limit, the text in the CSV file needs to be broken down into multiple rows. The existing length of each row will be recorded first to identify which rows need to be split."}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:"import tiktoken\n\n# Load the cl100k_base tokenizer which is designed to work with the ada-002 model\ntokenizer = tiktoken.get_encoding(\"cl100k_base\")\n\ndf = pd.read_csv('processed/scraped.csv', index_col=0)\ndf.columns = ['title', 'text']\n\n# Tokenize the text and save the number of tokens to a new column\ndf['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\n\n# Visualize the distribution of the number of tokens per row using a histogram\ndf.n_tokens.hist()\n"})}),"\n",e.jsx("div",{className:"sandbox-preview",children:e.jsx("div",{className:"sandbox-screenshot",children:e.jsx("img",{src:"https://cdn.openai.com/API/docs/images/tutorials/web-qa/embeddings-initial-histrogram.png",alt:"Embeddings histogram",width:"553",height:"413"})})}),"\n",e.jsx(t.p,{children:"The newest embeddings model can handle inputs with up to 8191 input tokens so most of the rows would not need any chunking, but this may not be the case for every subpage scraped so the next code chunk will split the longer lines into smaller chunks."}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-Python",children:"max_tokens = 500\n\n# Function to split the text into chunks of a maximum number of tokens\ndef split_into_many(text, max_tokens = max_tokens):\n\n    # Split the text into sentences\n    sentences = text.split('. ')\n\n    # Get the number of tokens for each sentence\n    n_tokens = [len(tokenizer.encode(\" \" + sentence)) for sentence in sentences]\n\n    chunks = []\n    tokens_so_far = 0\n    chunk = []\n\n    # Loop through the sentences and tokens joined together in a tuple\n    for sentence, token in zip(sentences, n_tokens):\n\n        # If the number of tokens so far plus the number of tokens in the current sentence is greater\n        # than the max number of tokens, then add the chunk to the list of chunks and reset\n        # the chunk and tokens so far\n        if tokens_so_far + token > max_tokens:\n            chunks.append(\". \".join(chunk) + \".\")\n            chunk = []\n            tokens_so_far = 0\n\n        # If the number of tokens in the current sentence is greater than the max number of\n        # tokens, go to the next sentence\n        if token > max_tokens:\n            continue\n\n        # Otherwise, add the sentence to the chunk and add the number of tokens to the total\n        chunk.append(sentence)\n        tokens_so_far += token + 1\n\n    return chunks\n\n\nshortened = []\n\n# Loop through the dataframe\nfor row in df.iterrows():\n\n    # If the text is None, go to the next row\n    if row[1]['text'] is None:\n        continue\n\n    # If the number of tokens is greater than the max number of tokens, split the text into chunks\n    if row[1]['n_tokens'] > max_tokens:\n        shortened += split_into_many(row[1]['text'])\n\n    # Otherwise, add the text to the list of shortened texts\n    else:\n        shortened.append( row[1]['text'] )\n"})}),"\n",e.jsx(t.p,{children:"Visualizing the updated histogram again can help to confirm if the rows were successfully split into shortened sections."}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:"df = pd.DataFrame(shortened, columns = ['text'])\ndf['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\ndf.n_tokens.hist()\n"})}),"\n",e.jsx("div",{className:"sandbox-preview",children:e.jsx("div",{className:"sandbox-screenshot",children:e.jsx("img",{src:"https://cdn.openai.com/API/docs/images/tutorials/web-qa/embeddings-tokenized-output.png",alt:"Embeddings tokenized output",width:"552",height:"418"})})}),"\n",e.jsx(t.p,{children:"The content is now broken down into smaller chunks and a simple request can be sent to the OpenAI API specifying the use of the new text-embedding-ada-002 model to create the embeddings:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:"from openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\ndf['embeddings'] = df.text.apply(lambda x: client.embeddings.create(input=x, engine='text-embedding-ada-002')['data'][0]['embedding'])\n\ndf.to_csv('processed/embeddings.csv')\ndf.head()\n"})}),"\n",e.jsx(t.p,{children:"This should take about 3-5 minutes but after you will have your embeddings ready to use!"}),"\n",e.jsx(t.h2,{children:"Building a question answer system with your embeddings"}),"\n",e.jsxs(t.div,{className:"sandbox-preview",children:[e.jsx(t.div,{className:"sandbox-screenshot",children:e.jsx(V,{png:"https://cdn.openai.com/API/docs/images/tutorials/web-qa/DALL-E-friendly-robot-question-and-answer-system-pixel-art.png",webp:"https://cdn.openai.com/API/docs/images/tutorials/web-qa/DALL-E-friendly-robot-question-and-answer-system-pixel-art.webp",alt:"DALL-E: Friendly robot question and answer system pixel art",width:"1024",height:"1024"})}),e.jsx(t.div,{className:"preview-info",children:e.jsx(t.div,{className:"description",children:"The embeddings are ready and the final step of this process is to create a simple question and answer system. This will take a user's question, create an embedding of it, and compare it with the existing embeddings to retrieve the most relevant text from the scraped website. The gpt-3.5-turbo-instruct model will then generate a natural sounding answer based on the retrieved text."})})]}),"\n",e.jsx(t.hr,{}),"\n",e.jsx(t.p,{children:"Turning the embeddings into a NumPy array is the first step, which will provide more flexibility in how to use it given the many functions available that operate on NumPy arrays. It will also flatten the dimension to 1-D, which is the required format for many subsequent operations."}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:"import numpy as np\nfrom openai.embeddings_utils import distances_from_embeddings\n\ndf=pd.read_csv('processed/embeddings.csv', index_col=0)\ndf['embeddings'] = df['embeddings'].apply(eval).apply(np.array)\n\ndf.head()\n"})}),"\n",e.jsxs(t.p,{children:["The question needs to be converted to an embedding with a simple function, now that the data is ready. This is important because the search with embeddings compares the vector of numbers (which was the conversion of the raw text) using cosine distance. The vectors are likely related and might be the answer to the question if they are close in cosine distance. The OpenAI python package has a built in ",e.jsx(t.code,{children:"distances_from_embeddings"})," function which is useful here."]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:"def create_context(\n    question, df, max_len=1800, size=\"ada\"\n):\n    \"\"\"\n    Create a context for a question by finding the most similar context from the dataframe\n    \"\"\"\n\n    # Get the embeddings for the question\n    q_embeddings = client.embeddings.create(input=question, engine='text-embedding-ada-002')['data'][0]['embedding']\n\n    # Get the distances from the embeddings\n    df['distances'] = distances_from_embeddings(q_embeddings, df['embeddings'].values, distance_metric='cosine')\n\n\n    returns = []\n    cur_len = 0\n\n    # Sort by distance and add the text to the context until the context is too long\n    for i, row in df.sort_values('distances', ascending=True).iterrows():\n\n        # Add the length of the text to the current length\n        cur_len += row['n_tokens'] + 4\n\n        # If the context is too long, break\n        if cur_len > max_len:\n            break\n\n        # Else add it to the text that is being returned\n        returns.append(row[\"text\"])\n\n    # Return the context\n    return \"\\n\\n###\\n\\n\".join(returns)\n"})}),"\n",e.jsx(t.p,{children:"The text was broken up into smaller sets of tokens, so looping through in ascending order and continuing to add the text is a critical step to ensure a full answer. The max_len can also be modified to something smaller, if more content than desired is returned."}),"\n",e.jsx(t.p,{children:"The previous step only retrieved chunks of texts that are semantically related to the question, so they might contain the answer, but there's no guarantee of it. The chance of finding an answer can be further increased by returning the top 5 most likely results."}),"\n",e.jsx(t.p,{children:"The answering prompt will then try to extract the relevant facts from the retrieved contexts, in order to formulate a coherent answer. If there is no relevant answer, the prompt will return “I don’t know”."}),"\n",e.jsxs(t.p,{children:["A realistic sounding answer to the question can be created with the completion endpoint using ",e.jsx(t.code,{children:"gpt-3.5-turbo-instruct"}),"."]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:'def answer_question(\n    df,\n    model="gpt-3.5-turbo",\n    question="Am I allowed to publish model outputs to Twitter, without a human review?",\n    max_len=1800,\n    size="ada",\n    debug=False,\n    max_tokens=150,\n    stop_sequence=None\n):\n    """\n    Answer a question based on the most similar context from the dataframe texts\n    """\n    context = create_context(\n        question,\n        df,\n        max_len=max_len,\n        size=size,\n    )\n    # If debug, print the raw model response\n    if debug:\n        print("Context:\\n" + context)\n        print("\\n\\n")\n\n    try:\n        # Create a chat completion using the question and context\n        response = client.chat.completions.create(\n            model="gpt-3.5-turbo",\n            messages=[\n                {"role": "system", "content": "Answer the question based on the context below, and if the question can\'t be answered based on the context, say \\"I don\'t know\\"\\n\\n"},\n                {"role": "user", f"content": "Context: {context}\\n\\n---\\n\\nQuestion: {question}\\nAnswer:"}\n            ],\n            temperature=0,\n            max_tokens=max_tokens,\n            top_p=1,\n            frequency_penalty=0,\n            presence_penalty=0,\n            stop=stop_sequence,\n        )\n        return response.choices[0].message.strip()\n    except Exception as e:\n        print(e)\n        return ""\n'})}),"\n",e.jsx(t.p,{children:"It is done! A working Q/A system that has the knowledge embedded from the OpenAI website is now ready. A few quick tests can be done to see the quality of the output:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:'answer_question(df, question="What day is it?", debug=False)\n\nanswer_question(df, question="What is our newest embeddings model?")\n\nanswer_question(df, question="What is ChatGPT?")\n'})}),"\n",e.jsx(t.p,{children:"The responses will look something like the following:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-response",children:"\"I don't know.\"\n\n'The newest embeddings model is text-embedding-ada-002.'\n\n'ChatGPT is a model trained to interact in a conversational way. It is able to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests.'\n"})}),"\n",e.jsx(t.p,{children:"If the system is not able to answer a question that is expected, it is worth searching through the raw text files to see if the information that is expected to be known actually ended up being embedded or not. The crawling process that was done initially was setup to skip sites outside the original domain that was provided, so it might not have that knowledge if there was a subdomain setup."}),"\n",e.jsxs(t.p,{children:["Currently, the dataframe is being passed in each time to answer a question. For more production workflows, a ",e.jsx(t.a,{href:"/docs/guides/embeddings/how-can-i-retrieve-k-nearest-embedding-vectors-quickly",children:"vector database solution"})," should be used instead of storing the embeddings in a CSV file, but the current approach is a great option for prototyping."]})]})}function oo(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx($t,n)})):$t(n)}function Zt(n){const t=Object.assign({h1:"h1",p:"p",a:"a",hr:"hr"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"Tutorials"}),"\n",e.jsx(t.p,{children:"Get started with the OpenAI API by building real AI apps step by step."}),"\n",e.jsx(t.p,{children:e.jsx(t.a,{href:"/docs/tutorials/web-qa-embeddings",children:e.jsx(C,{icon:e.jsx(Mn,{}),color:"green",title:"Website Q&A with Embeddings",className:"mt-6",children:"Learn how to build an AI that can answer questions about your website."})})}),"\n",e.jsx(t.p,{children:e.jsx(t.a,{href:"/docs/tutorials/meeting-minutes",children:e.jsx(C,{icon:e.jsx(qn,{}),color:"pink",title:"Meeting minutes transcription with Whisper",tagColor:"gray",className:"mt-6",children:"Learn how to create an automated meeting minutes generator with Whisper and GPT-4."})})}),"\n",e.jsx(C,{icon:e.jsx(Hn,{}),color:"purple",title:"Coming soon",tagColor:"gray",className:"mt-6",children:"Learn how to build and deploy an AI chat bot that understands multiple knowledge bases."}),"\n",e.jsx(t.hr,{}),"\n",e.jsxs(t.p,{children:["Looking for more ideas? Check out our ",e.jsx(t.a,{href:"/examples",children:"Examples"})," or the ",e.jsx(t.a,{href:"https://cookbook.openai.com/",children:"OpenAI Cookbook"})," on GitHub."]})]})}function ro(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Zt,n)})):Zt(n)}function Xt(n){const t=Object.assign({h1:"h1",p:"p",h2:"h2",a:"a",pre:"pre",code:"code",div:"div",h3:"h3"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"Creating an automated meeting minutes generator with Whisper and GPT-4"}),"\n",e.jsx(t.p,{children:"In this tutorial, we'll harness the power of OpenAI's Whisper and GPT-4 models to develop an automated meeting minutes generator. The application transcribes audio from a meeting, provides a summary of the discussion, extracts key points and action items, and performs a sentiment analysis."}),"\n",e.jsx(t.h2,{children:"Getting started"}),"\n",e.jsxs(t.p,{children:["This tutorial assumes a basic understanding of Python and an ",e.jsx(t.a,{href:"/account/api-keys",children:"OpenAI API key"}),". You can use the audio file provided with this tutorial or your own."]}),"\n",e.jsxs(t.p,{children:["Additionally, you will need to install the ",e.jsx(t.a,{href:"https://python-docx.readthedocs.io/en/latest/",children:"python-docx"})," and ",e.jsx(t.a,{href:"/docs/libraries/libraries",children:"OpenAI"})," libraries. You can create a new Python environment and install the required packages with the following commands:"]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-bash",children:"python -m venv env\n\nsource env/bin/activate\n\npip install openai\npip install python-docx\n"})}),"\n",e.jsx(t.h2,{children:"Transcribing audio with Whisper"}),"\n",e.jsxs(t.div,{className:"sandbox-preview",children:[e.jsx(t.div,{className:"sandbox-screenshot-small",children:e.jsx(V,{png:"https://cdn.openai.com/API/docs/images/tutorials/meeting-minutes/waveform3.png",webp:"https://cdn.openai.com/API/docs/images/tutorials/meeting-minutes/waveform3.webp",alt:"Audio Waveform created by DALL·E",width:"1024",height:"1024"})}),e.jsxs(t.div,{className:"preview-info",children:[e.jsxs(t.div,{className:"description",children:["The first step in transcribing the audio from a meeting is to pass the audio file of the meeting into our"," ",e.jsx(t.a,{href:"/docs/api-reference/audio",children:"/v1/audio API"}),". Whisper, the model that powers the audio API, is capable of converting spoken language into written text. To start, we will avoid passing a"," ",e.jsx(t.a,{href:"/docs/api-reference/audio/createTranscription#audio/createTranscription-prompt",children:"prompt"})," ","or"," ",e.jsx(t.a,{href:"/docs/api-reference/audio/createTranscription#audio/createTranscription-temperature-4",children:"temperature"})," ","(optional parameters to control the model's output) and stick with the default values."]}),e.jsx(t.div,{className:"actions",children:e.jsx(R,{size:"medium",color:te.neutral,href:"https://cdn.openai.com/API/docs/images/tutorials/meeting-minutes/EarningsCall.wav",target:"_blank",children:"Download sample audio"})})]})]}),"\n",e.jsx("br",{}),"\n",e.jsx(t.p,{children:"Next, we import the required packages and define a function that uses the Whisper model to take in the audio file and\ntranscribe it:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:'from openai import OpenAI\n\nclient = OpenAI(\n    # defaults to os.environ.get("OPENAI_API_KEY")\n    # api_key="My API Key",\n)\nfrom docx import Document\n\ndef transcribe_audio(audio_file_path):\n    with open(audio_file_path, \'rb\') as audio_file:\n        transcription = client.audio.transcriptions.create("whisper-1", audio_file)\n    return transcription[\'text\']\n'})}),"\n",e.jsxs(t.p,{children:["In this function, ",e.jsx(t.code,{children:"audio_file_path"})," is the path to the audio file you want to transcribe. The function opens this file and passes it to the Whisper ASR model (",e.jsx(t.code,{children:"whisper-1"}),") for transcription. The result is returned as raw text. It’s important to note that the ",e.jsx(t.code,{children:"openai.Audio.transcribe"})," function requires the actual audio file to be passed in, not just the path to the file locally or on a remote server. This means that if you are running this code on a server where you might not also be storing your audio files, you will need to have a preprocessing step that first downloads the audio files onto that device."]}),"\n",e.jsx(t.h2,{children:"Summarizing and analyzing the transcript with GPT-4"}),"\n",e.jsxs(t.p,{children:["Having obtained the transcript, we now pass it to GPT-4 via the ",e.jsx(t.a,{href:"/docs/api-reference/chat/create",children:"Chat Completions API"}),". GPT-4 is OpenAI's state-of-the-art large language model which we'll use to generate a summary, extract key points, action items, and perform sentiment analysis."]}),"\n",e.jsx(t.p,{children:"This tutorial uses distinct functions for each task we want GPT-4 to perform. This is not the most efficient way to do this task - you can put these instructions into one function, however, splitting them up can lead to higher quality summarization."}),"\n",e.jsxs(t.p,{children:["To split the tasks up, we define the ",e.jsx(t.code,{children:"meeting_minutes"})," function which will serve as the main function of this application:"]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:"def meeting_minutes(transcription):\n    abstract_summary = abstract_summary_extraction(transcription)\n    key_points = key_points_extraction(transcription)\n    action_items = action_item_extraction(transcription)\n    sentiment = sentiment_analysis(transcription)\n    return {\n        'abstract_summary': abstract_summary,\n        'key_points': key_points,\n        'action_items': action_items,\n        'sentiment': sentiment\n    }\n"})}),"\n",e.jsxs(t.p,{children:["In this function, ",e.jsx(t.code,{children:"transcription"})," is the text we obtained from Whisper. The transcription can be passed to the four other functions, each designed to perform a specific task: ",e.jsx(t.code,{children:"abstract_summary_extraction"})," generates a summary of the meeting, ",e.jsx(t.code,{children:"key_points_extraction"})," extracts the main points, ",e.jsx(t.code,{children:"action_item_extraction"})," identifies the action items, and ",e.jsx(t.code,{children:"sentiment_analysis performs"})," a sentiment analysis. If there are other capabilities you want, you can add those in as well using the same framework shown above."]}),"\n",e.jsx(t.p,{children:"Here is how each of these functions works:"}),"\n",e.jsx(t.h3,{children:"Summary extraction"}),"\n",e.jsxs(t.p,{children:["The ",e.jsx(t.code,{children:"abstract_summary_extraction"})," function takes the transcription and summarizes it into a concise abstract paragraph with the aim to retain the most important points while avoiding unnecessary details or tangential points. The main mechanism to enable this process is the system message as shown below. There are many different possible ways of achieving similar results through the process commonly referred to as prompt engineering. You can read our ",e.jsx(t.a,{href:"/docs/guides/prompt-engineering",children:"prompt engineering guide"})," which gives in depth advice on how to do this most effectively."]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:'def abstract_summary_extraction(transcription):\n    response = client.chat.completions.create(\n        model="gpt-4",\n        temperature=0,\n        messages=[\n            {\n                "role": "system",\n                "content": "You are a highly skilled AI trained in language comprehension and summarization. I would like you to read the following text and summarize it into a concise abstract paragraph. Aim to retain the most important points, providing a coherent and readable summary that could help a person understand the main points of the discussion without needing to read the entire text. Please avoid unnecessary details or tangential points."\n            },\n            {\n                "role": "user",\n                "content": transcription\n            }\n        ]\n    )\n    return completion.choices[0].message.content\n'})}),"\n",e.jsx(t.h3,{children:"Key points extraction"}),"\n",e.jsxs(t.p,{children:["The ",e.jsx(t.code,{children:"key_points_extraction"})," function identifies and lists the main points discussed in the meeting. These points should represent the most important ideas, findings, or topics crucial to the essence of the discussion. Again, the main mechanism for controlling the way these points are identified is the system message. You might want to give some additional context here around the way your project or company runs such as “We are a company that sells race cars to consumers. We do XYZ with the goal of XYZ”. This additional context could dramatically improve the models ability to extract information that is relevant."]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:'\ndef key_points_extraction(transcription):\n    response = client.chat.completions.create(\n        model="gpt-4",\n        temperature=0,\n        messages=[\n            {\n                "role": "system",\n                "content": "You are a proficient AI with a specialty in distilling information into key points. Based on the following text, identify and list the main points that were discussed or brought up. These should be the most important ideas, findings, or topics that are crucial to the essence of the discussion. Your goal is to provide a list that someone could read to quickly understand what was talked about."\n            },\n            {\n                "role": "user",\n                "content": transcription\n            }\n        ]\n    )\n    return completion.choices[0].message.content\n'})}),"\n",e.jsx(t.h3,{children:"Action item extraction"}),"\n",e.jsxs(t.p,{children:["The ",e.jsx(t.code,{children:"action_item_extraction"})," function identifies tasks, assignments, or actions agreed upon or mentioned during the meeting. These could be tasks assigned to specific individuals or general actions the group decided to take. While not covered in this tutorial, the Chat Completions API provides a ",e.jsx(t.a,{href:"/docs/guides/function-calling",children:"function calling capability"})," which would allow you to build in the ability to automatically create tasks in your task management software and assign it to the relevant person."]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:'\ndef action_item_extraction(transcription):\n    response = client.chat.completions.create(\n        model="gpt-4",\n        temperature=0,\n        messages=[\n            {\n                "role": "system",\n                "content": "You are an AI expert in analyzing conversations and extracting action items. Please review the text and identify any tasks, assignments, or actions that were agreed upon or mentioned as needing to be done. These could be tasks assigned to specific individuals, or general actions that the group has decided to take. Please list these action items clearly and concisely."\n            },\n            {\n                "role": "user",\n                "content": transcription\n            }\n        ]\n    )\n    return completion.choices[0].message.content\n'})}),"\n",e.jsx(t.h3,{children:"Sentiment analysis"}),"\n",e.jsxs(t.p,{children:["The ",e.jsx(t.code,{children:"sentiment_analysis"})," function analyzes the overall sentiment of the discussion. It considers the tone, the emotions conveyed by the language used, and the context in which words and phrases are used. For tasks which are less complicated, it may also be worthwhile to try out ",e.jsx(t.code,{children:"gpt-3.5-turbo"})," in addition to ",e.jsx(t.code,{children:"gpt-4"})," to see if you can get a similar level of performance. It might also be useful to experiment with taking the results of the ",e.jsx(t.code,{children:"sentiment_analysis"})," function and passing it to the other functions to see how having the sentiment of the conversation impacts the other attributes."]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:'def sentiment_analysis(transcription):\n    response = client.chat.completions.create(\n        model="gpt-4",\n        temperature=0,\n        messages=[\n            {\n                "role": "system",\n                "content": "As an AI with expertise in language and emotion analysis, your task is to analyze the sentiment of the following text. Please consider the overall tone of the discussion, the emotion conveyed by the language used, and the context in which words and phrases are used. Indicate whether the sentiment is generally positive, negative, or neutral, and provide brief explanations for your analysis where possible."\n            },\n            {\n                "role": "user",\n                "content": transcription\n            }\n        ]\n    )\n    return completion.choices[0].message.content\n'})}),"\n",e.jsx(t.h2,{children:"Exporting meeting minutes"}),"\n",e.jsxs("div",{className:"sandbox-preview",children:[e.jsx("div",{className:"sandbox-screenshot-small",children:e.jsx(V,{png:"https://cdn.openai.com/API/docs/images/tutorials/meeting-minutes/waveform4.png",webp:"https://cdn.openai.com/API/docs/images/tutorials/meeting-minutes/waveform4.webp",alt:"Audio Waveform created by DALL·E",width:"1024",height:"1024"})}),e.jsx("div",{className:"preview-info",children:e.jsx("div",{className:"description",children:e.jsx(t.p,{children:"Once we've generated the meeting minutes, it's beneficial to save them into a\nreadable format that can be easily distributed. One common format for such\nreports is Microsoft Word. The Python docx library is a popular open source\nlibrary for creating Word documents. If you wanted to build an end-to-end\nmeeting minute application, you might consider removing this export step in\nfavor of sending the summary inline as an email followup."})})})]}),"\n",e.jsx("br",{}),"\n",e.jsxs(t.p,{children:["To handle the exporting process, define a function ",e.jsx(t.code,{children:"save_as_docx"})," that converts the raw text to a Word document:"]}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:"def save_as_docx(minutes, filename):\n    doc = Document()\n    for key, value in minutes.items():\n        # Replace underscores with spaces and capitalize each word for the heading\n        heading = ' '.join(word.capitalize() for word in key.split('_'))\n        doc.add_heading(heading, level=1)\n        doc.add_paragraph(value)\n        # Add a line break between sections\n        doc.add_paragraph()\n    doc.save(filename)\n"})}),"\n",e.jsx(t.p,{children:"In this function, minutes is a dictionary containing the abstract summary, key points, action items, and sentiment analysis from the meeting. Filename is the name of the Word document file to be created. The function creates a new Word document, adds headings and content for each part of the minutes, and then saves the document to the current working directory."}),"\n",e.jsx(t.p,{children:"Finally, you can put it all together and generate the meeting minutes from an audio file:"}),"\n",e.jsx(t.pre,{children:e.jsx(t.code,{className:"language-python",children:"audio_file_path = \"Earningscall.wav\"\ntranscription = transcribe_audio(audio_file_path)\nminutes = meeting_minutes(transcription)\nprint(minutes)\n\nsave_as_docx(minutes, 'meeting_minutes.docx')\n"})}),"\n",e.jsxs(t.p,{children:["This code will transcribe the audio file ",e.jsx(t.code,{children:"Earningscall.wav"}),", generates the meeting minutes, prints them, and then saves them into a Word document called ",e.jsx(t.code,{children:"meeting_minutes.docx"}),"."]}),"\n",e.jsxs(t.p,{children:["Now that you have the basic meeting minutes processing setup, consider trying to optimize the performance with ",e.jsx(t.a,{href:"/docs/guides/prompt-engineering",children:"prompt engineering"})," or build an end-to-end system with native ",e.jsx(t.a,{href:"/docs/guides/function-calling",children:"function calling"}),"."]})]})}function ao(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Xt,n)})):Xt(n)}function J(n){var l;const{options:t,initialValue:s}=n,[i,r]=p.useState(()=>{const d="tabs-".concat(n.id);let u=window.localStorage.getItem(d);u=new URLSearchParams(window.location.search).get(n.id)||u;let h;return t.some(x=>{if(x.value===u)return h=u,!0}),h||s}),o=d=>{const u="tabs-".concat(n.id);r(d),window.localStorage.setItem(u,d);const m=new URL(window.location.href);m.searchParams.set(n.id,d),window.history.pushState({},"",m.toString())};return e.jsxs("div",{children:[e.jsx("div",{className:"tabs",children:e.jsx(ln,{children:t.map(d=>e.jsx(R,{onClick:()=>o(d.value),"aria-pressed":i===d.value?"true":"false",variant:i===d.value?void 0:"outlined",size:"small",children:d.label},d.value))})}),(l=t.find(d=>d.value===i))==null?void 0:l.content]})}const lo='\nimport OpenAI from "openai";\nconst openai = new OpenAI();\n\nconst completion = await openai.chat.completions.create({\n    model: "gpt-4o-mini",\n    messages: [\n        { role: "system", content: "You are a helpful assistant." },\n        {\n            role: "user",\n            content: "Write a haiku about recursion in programming.",\n        },\n    ],\n});\n\nconsole.log(completion.choices[0].message);\n'.trim(),co='\nimport OpenAI from "openai";\nconst openai = new OpenAI();\n\nconst image = await openai.images.generate({ prompt: "A cute baby sea otter" });\n\nconsole.log(image.data[0].url);\n'.trim(),ho='\nimport OpenAI from "openai";\nconst openai = new OpenAI();\n\nconst embedding = await openai.embeddings.create({\n    model: "text-embedding-3-large",\n    input: "The quick brown fox jumped over the lazy dog",\n});\n\nconsole.log(embedding);\n'.trim(),uo='\nfrom openai import OpenAI\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n    model="gpt-4o-mini",\n    messages=[\n        {"role": "system", "content": "You are a helpful assistant."},\n        {\n            "role": "user",\n            "content": "Write a haiku about recursion in programming."\n        }\n    ]\n)\n\nprint(completion.choices[0].message)\n'.trim(),po='\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.images.generate(\n    prompt="A cute baby sea otter",\n    n=2,\n    size="1024x1024"\n)\n\nprint(response.data[0].url)\n'.trim(),mo='\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.embeddings.create(\n    model="text-embedding-3-large",\n    input="The food was delicious and the waiter..."\n)\n\nprint(response)\n'.trim(),xo='\ncurl "https://api.openai.com/v1/chat/completions" \\\n    -H "Content-Type: application/json" \\\n    -H "Authorization: Bearer $OPENAI_API_KEY" \\\n    -d \'{\n        "model": "gpt-4o-mini",\n        "messages": [\n            {\n                "role": "system",\n                "content": "You are a helpful assistant."\n            },\n            {\n                "role": "user",\n                "content": "Write a haiku that explains the concept of recursion."\n            }\n        ]\n    }\'\n'.trim(),go='\ncurl "https://api.openai.com/v1/images/generations" \\\n    -H "Content-Type: application/json" \\\n    -H "Authorization: Bearer $OPENAI_API_KEY" \\\n    -d \'{\n        "prompt": "A cute baby sea otter",\n        "n": 2,\n        "size": "1024x1024"\n    }\'\n'.trim(),fo='\ncurl "https://api.openai.com/v1/embeddings" \\\n    -H "Content-Type: application/json" \\\n    -H "Authorization: Bearer $OPENAI_API_KEY" \\\n    -d \'{\n        "input": "The food was delicious and the waiter...",\n        "model": "text-embedding-3-large"\n    }\'\n'.trim();function Jt(n){const t=Object.assign({p:"p",a:"a",code:"code"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsxs(t.p,{children:["To use the OpenAI API in server-side JavaScript environments like Node.js, Deno, or Bun, you can use the official ",e.jsx(t.a,{href:"https://github.com/openai/openai-node",children:"OpenAI SDK for TypeScript and JavaScript"}),". Get started by installing the SDK using ",e.jsx(t.a,{href:"https://www.npmjs.com/",children:"npm"})," or your preferred package manager:"]}),"\n",e.jsx(c,{title:"Install the OpenAI SDK with npm",highlighted:!0,language:"bash",code:"npm install openai"}),"\n",e.jsxs(t.p,{children:["With the OpenAI SDK installed, create a file called ",e.jsx(t.code,{children:"example.mjs"})," and copy one of the following examples into it:"]}),"\n",e.jsx(J,{id:"quickstart-example",initialValue:"completions",options:[{value:"completions",label:"Generate text",content:e.jsx(c,{title:"Create a human-like response to a prompt",highlighted:!0,language:"javascript",code:lo})},{value:"images",label:"Generate an image",content:e.jsx(c,{title:"Generate an image based on a textual prompt",highlighted:!0,language:"javascript",code:co})},{value:"embeddings",label:"Create vector embeddings",content:e.jsx(c,{title:"Create vector embeddings for a string of text",highlighted:!0,language:"javascript",code:ho})}]}),"\n",e.jsxs(t.p,{children:["Execute the code with ",e.jsx(t.code,{children:"node example.mjs"})," (or the equivalent command for Deno or Bun). In a few moments, you should see the output of your API request!"]})]})}function jo(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Jt,n)})):Jt(n)}function Kt(n){const t=Object.assign({p:"p",a:"a",code:"code"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsxs(t.p,{children:["To use the OpenAI API in Python, you can use the official ",e.jsx(t.a,{href:"https://github.com/openai/openai-python",children:"OpenAI SDK for Python"}),". Get started by installing the SDK using ",e.jsx(t.a,{href:"https://pypi.org/project/pip/",children:"pip"}),":"]}),"\n",e.jsx(c,{title:"Install the OpenAI SDK with pip",highlighted:!0,language:"bash",code:"pip install openai"}),"\n",e.jsxs(t.p,{children:["With the OpenAI SDK installed, create a file called ",e.jsx(t.code,{children:"example.py"})," and copy one of the following examples into it:"]}),"\n",e.jsx(J,{id:"quickstart-example",initialValue:"completions",options:[{value:"completions",label:"Generate text",content:e.jsx(c,{title:"Create a human-like response to a prompt",highlighted:!0,language:"python",code:uo})},{value:"images",label:"Generate an image",content:e.jsx(c,{title:"Generate an image based on a textual prompt",highlighted:!0,language:"python",code:po})},{value:"embeddings",label:"Create vector embeddings",content:e.jsx(c,{title:"Create vector embeddings for a string of text",highlighted:!0,language:"python",code:mo})}]}),"\n",e.jsxs(t.p,{children:["Execute the code with ",e.jsx(t.code,{children:"python example.py"}),". In a few moments, you should see the output of your API request!"]})]})}function yo(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Kt,n)})):Kt(n)}function Qt(n){const t=Object.assign({p:"p",a:"a",code:"code"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsxs(t.p,{children:["On Unix-based systems, you can test out the ",e.jsx(t.a,{href:"/docs/api-reference",children:"OpenAI REST API"})," using ",e.jsx(t.a,{href:"https://curl.se/",children:"curl"}),". The following commands assume that you have exported the ",e.jsx(t.code,{children:"OPENAI_API_KEY"})," system environment variable as shown above."]}),"\n",e.jsx(J,{id:"quickstart-example",initialValue:"completions",options:[{value:"completions",label:"Generate text",content:e.jsx(c,{title:"Create a human-like response to a prompt",highlighted:!0,language:"bash",code:xo})},{value:"images",label:"Generate an image",content:e.jsx(c,{title:"Generate an image based on a textual prompt",highlighted:!0,language:"bash",code:go})},{value:"embeddings",label:"Create vector embeddings",content:e.jsx(c,{title:"Create vector embeddings for a string of text",highlighted:!0,language:"bash",code:fo})}]}),"\n",e.jsx(t.p,{children:"Execute the curl commands above in your terminal. In a few moments, you should see the output of your API request!"})]})}function bo(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(Qt,n)})):Qt(n)}function en(n){const t=Object.assign({h1:"h1",p:"p",a:"a",h2:"h2",code:"code"},a(),n.components);return e.jsxs(e.Fragment,{children:[e.jsx(t.h1,{children:"Developer quickstart"}),"\n",e.jsxs(t.p,{children:["The OpenAI API provides a simple interface to state-of-the-art AI ",e.jsx(t.a,{href:"/docs/models",children:"models"})," for natural language processing, image generation, semantic search, and speech recognition. Follow this guide to learn how to generate human-like responses to ",e.jsx(t.a,{href:"/docs/guides/chat-completions",children:"natural language prompts"}),", ",e.jsx(t.a,{href:"/docs/guides/embeddings",children:"create vector embeddings"})," for semantic search, and ",e.jsx(t.a,{href:"/docs/guides/images",children:"generate images"})," from textual descriptions."]}),"\n",e.jsx(t.h2,{children:"Create and export an API key"}),"\n",e.jsxs(t.p,{children:[e.jsx(t.a,{href:"/api-keys",children:"Create an API key in the dashboard here"}),", which you’ll use to securely ",e.jsx(t.a,{href:"/docs/api-reference/authentication",children:"access the API"}),". Store the key in a safe location, like a ",e.jsxs(t.a,{href:"https://www.freecodecamp.org/news/how-do-zsh-configuration-files-work/",children:[e.jsx(t.code,{children:".zshrc"})," file"]})," or another text file on your computer. Once you’ve generated an API key, export it as an ",e.jsx(t.a,{href:"https://en.wikipedia.org/wiki/Environment_variable",children:"environment variable"})," in your terminal."]}),"\n",e.jsx(J,{id:"desktop-os",initialValue:"macOS",options:[{value:"macOS",label:"macOS / Linux",content:e.jsx(c,{title:"Export an envrionment variable on *nix systems",code:'export OPENAI_API_KEY="your_api_key_here"',highlighted:!0,language:"bash"})},{value:"windows",label:"Windows",content:e.jsx(c,{title:"Export an envrionment variable in PowerShell",code:'setx OPENAI_API_KEY "your_api_key_here"',highlighted:!0,language:"bash"})}]}),"\n",e.jsx(t.h2,{children:"Make your first API request"}),"\n",e.jsxs(t.p,{children:["With your OpenAI API key exported as an environment variable, you're ready to make your first API request. You can either use the ",e.jsx(t.a,{href:"/docs/api-reference",children:"REST API"})," directly with the HTTP client of your choice, or use one of our ",e.jsx(t.a,{href:"/docs/libraries",children:"official SDKs"})," as shown below."]}),"\n",e.jsx(J,{id:"language-preference",initialValue:"javascript",options:[{value:"javascript",label:"JavaScript",content:e.jsx(jo,{})},{value:"python",label:"Python",content:e.jsx(yo,{})},{value:"curl",label:"curl",content:e.jsx(bo,{})}]}),"\n",e.jsx(t.h2,{children:"Next steps"}),"\n",e.jsx(t.p,{children:"Now that you've made your first OpenAI API request, you can explore the following resources:"}),"\n",e.jsx(A,{to:"/docs/guides/chat-completions",children:e.jsx(C,{icon:e.jsx(me,{}),title:"Chat Completions",className:"mt-6",children:e.jsx(t.p,{children:"Learn more about generating text responses to natural language prompts"})})}),"\n",e.jsx(A,{to:"/docs/guides/images",children:e.jsx(C,{icon:e.jsx(En,{}),title:"Image Generation",className:"mt-6",children:e.jsx(t.p,{children:"Generate images using our DALL·E model"})})}),"\n",e.jsx(A,{to:"/docs/guides/embeddings",children:e.jsx(C,{icon:e.jsx(Nn,{}),title:"Embeddings",className:"mt-6",children:e.jsx(t.p,{children:"Create vector representations of text, used for similarity search"})})}),"\n",e.jsx(A,{to:"/docs/guides/text-to-speech",children:e.jsx(C,{icon:e.jsx(Vn,{}),title:"Text-to-speech",className:"mt-6",children:e.jsx(t.p,{children:"Generate human-like voice recordings with our text-to-speech model"})})}),"\n",e.jsx(A,{to:"/docs/guides/speech-to-text",children:e.jsx(C,{icon:e.jsx(zn,{}),title:"Speech-to-text",className:"mt-6",children:e.jsx(t.p,{children:"Create transcriptions of voice recordings with our Whisper model"})})}),"\n",e.jsx(A,{to:"/docs/guides/moderation",children:e.jsx(C,{icon:e.jsx(Dn,{}),title:"Moderation",className:"mt-6",children:e.jsx(t.p,{children:"Analyze and filter user-created content with our moderation model"})})}),"\n",e.jsx(A,{to:"/docs/guides/fine-tuning",children:e.jsx(C,{icon:e.jsx(Fn,{}),title:"Fine-tuning",className:"mt-6",children:e.jsx(t.p,{children:"Fine-tune our models with your own data"})})}),"\n",e.jsx(A,{to:"/docs/guides/batch",children:e.jsx(C,{icon:e.jsx(Rn,{}),title:"Batch",className:"mt-6",children:e.jsx(t.p,{children:"Batch requests for async jobs"})})}),"\n",e.jsx(A,{to:"/docs/api-reference",children:e.jsx(C,{icon:e.jsx(on,{}),title:"Full API Reference",className:"mt-6",children:e.jsx(t.p,{children:"View the full REST API reference for OpenAI"})})})]})}function wo(n={}){const{wrapper:t}=Object.assign({},a(),n.components);return t?e.jsx(t,Object.assign({},n,{children:e.jsx(en,n)})):en(n)}const vo=(n,t)=>p.createElement("svg",{width:"1em",height:"1em",viewBox:"0 0 214 130",fill:"none",xmlns:"http://www.w3.org/2000/svg",ref:t,...n},p.createElement("path",{d:"M0.333313 7.92207C0.333313 4.00541 3.5084 0.830322 7.42506 0.830322H297.3V129.368H0.333313V7.92207Z",fill:"url(#paint0_radial_293_16838)"}),p.createElement("path",{d:"M48.3157 30.9919C48.3157 26.5737 51.8974 22.9919 56.3157 22.9919H199.767C204.185 22.9919 207.767 26.5737 207.767 30.9919V48.4725C207.767 49.577 206.872 50.4725 205.767 50.4725H56.3157C51.8974 50.4725 48.3157 46.8907 48.3157 42.4725V30.9919Z",fill:"#ACACBE"}),p.createElement("path",{d:"M61.6195 32.9037H62.4695L63.9895 38.6537L65.5195 32.9037H66.4695L64.4995 40.0837H63.5595L62.0295 34.4937L60.5095 40.0837H59.5695L57.5895 32.9037H58.5795L60.1095 38.6537L61.6195 32.9037ZM67.3592 40.0837V32.9037H68.2592V35.5637C68.5592 35.1337 69.0792 34.7537 69.8192 34.7537C70.9292 34.7537 71.5592 35.5237 71.5592 36.6337V40.0837H70.6592V36.8737C70.6592 36.1437 70.3492 35.5937 69.5692 35.5937C68.9192 35.5937 68.2592 36.0937 68.2592 36.9637V40.0837H67.3592ZM74.3137 40.1737C73.3637 40.1737 72.5937 39.6137 72.5937 38.6737C72.5937 37.6337 73.4037 37.2037 74.5737 36.9637L75.8437 36.7037V36.4837C75.8437 35.8837 75.5137 35.5437 74.8137 35.5437C74.1537 35.5437 73.7637 35.8537 73.6037 36.4337L72.7537 36.2137C73.0037 35.3637 73.7737 34.7537 74.8437 34.7537C76.0137 34.7537 76.7337 35.3237 76.7337 36.4437V38.9937C76.7337 39.3337 76.9437 39.4437 77.2937 39.3637V40.0837C76.4937 40.1837 76.0337 39.9937 75.9237 39.4937C75.6037 39.9037 75.0237 40.1737 74.3137 40.1737ZM75.8437 38.3037V37.4037L74.8237 37.6237C74.0237 37.7837 73.4837 38.0037 73.4837 38.6337C73.4837 39.1437 73.8537 39.4437 74.4237 39.4437C75.1937 39.4437 75.8437 38.9737 75.8437 38.3037ZM78.4074 38.8337V35.6237H77.5974V34.8537H78.4074V33.3237H79.2974V34.8537H80.4874V35.6237H79.2974V38.6937C79.2974 39.2437 79.5674 39.3437 80.0274 39.3437C80.2374 39.3437 80.3874 39.3337 80.5974 39.2837V40.0537C80.3674 40.1037 80.1074 40.1337 79.8074 40.1337C78.9174 40.1337 78.4074 39.8337 78.4074 38.8337ZM81.3842 34.0637V32.9037H82.5242V34.1337C82.5242 34.8937 82.0242 35.3837 81.2242 35.4037V34.8837C81.6642 34.8437 81.9042 34.6137 81.9042 34.1737V34.0637H81.3842ZM82.5116 38.9737L83.2116 38.4637C83.4916 39.0737 84.1316 39.4437 84.8116 39.4437C85.4216 39.4437 85.9616 39.1837 85.9616 38.6137C85.9616 38.0437 85.4616 37.9637 84.6016 37.7837C83.7416 37.5937 82.8416 37.4037 82.8416 36.3337C82.8416 35.4037 83.6516 34.7537 84.7616 34.7537C85.7016 34.7537 86.4716 35.2137 86.7816 35.8337L86.1416 36.3437C85.8716 35.8237 85.3816 35.4937 84.7216 35.4937C84.1216 35.4937 83.7216 35.7937 83.7216 36.2637C83.7216 36.7537 84.1816 36.8437 84.9116 37.0037C85.8116 37.2137 86.8416 37.3937 86.8416 38.5337C86.8416 39.5337 85.9316 40.1937 84.8116 40.1937C83.8616 40.1937 82.9116 39.7837 82.5116 38.9737ZM90.2042 38.8337V35.6237H89.3942V34.8537H90.2042V33.3237H91.0942V34.8537H92.2842V35.6237H91.0942V38.6937C91.0942 39.2437 91.3642 39.3437 91.8242 39.3437C92.0342 39.3437 92.1842 39.3337 92.3942 39.2837V40.0537C92.1642 40.1037 91.9042 40.1337 91.6042 40.1337C90.7142 40.1337 90.2042 39.8337 90.2042 38.8337ZM93.4627 40.0837V32.9037H94.3627V35.5637C94.6627 35.1337 95.1827 34.7537 95.9227 34.7537C97.0327 34.7537 97.6627 35.5237 97.6627 36.6337V40.0837H96.7627V36.8737C96.7627 36.1437 96.4527 35.5937 95.6727 35.5937C95.0227 35.5937 94.3627 36.0937 94.3627 36.9637V40.0837H93.4627ZM101.197 40.1937C99.7472 40.1937 98.7472 39.0837 98.7472 37.4737C98.7472 35.9137 99.7372 34.7537 101.157 34.7537C102.617 34.7537 103.387 35.8637 103.387 37.3137V37.6137H99.6072C99.6572 38.7137 100.287 39.4137 101.197 39.4137C101.897 39.4137 102.397 39.0337 102.557 38.4137L103.347 38.6937C103.027 39.6237 102.237 40.1937 101.197 40.1937ZM101.147 35.5237C100.387 35.5237 99.8172 36.0337 99.6572 36.9237H102.477C102.457 36.1837 102.057 35.5237 101.147 35.5237ZM109.107 34.8537H109.867L110.947 38.7537L112.037 34.8537H112.937L111.347 40.0837H110.557L109.467 36.2037L108.377 40.0837H107.587L105.997 34.8537H106.927L108.027 38.7437L109.107 34.8537ZM115.807 40.1937C114.357 40.1937 113.357 39.0837 113.357 37.4737C113.357 35.9137 114.347 34.7537 115.767 34.7537C117.227 34.7537 117.997 35.8637 117.997 37.3137V37.6137H114.217C114.267 38.7137 114.897 39.4137 115.807 39.4137C116.507 39.4137 117.007 39.0337 117.167 38.4137L117.957 38.6937C117.637 39.6237 116.847 40.1937 115.807 40.1937ZM115.757 35.5237C114.997 35.5237 114.427 36.0337 114.267 36.9237H117.087C117.067 36.1837 116.667 35.5237 115.757 35.5237ZM120.486 40.1737C119.536 40.1737 118.766 39.6137 118.766 38.6737C118.766 37.6337 119.576 37.2037 120.746 36.9637L122.016 36.7037V36.4837C122.016 35.8837 121.686 35.5437 120.986 35.5437C120.326 35.5437 119.936 35.8537 119.776 36.4337L118.926 36.2137C119.176 35.3637 119.946 34.7537 121.016 34.7537C122.186 34.7537 122.906 35.3237 122.906 36.4437V38.9937C122.906 39.3337 123.116 39.4437 123.466 39.3637V40.0837C122.666 40.1837 122.206 39.9937 122.096 39.4937C121.776 39.9037 121.196 40.1737 120.486 40.1737ZM122.016 38.3037V37.4037L120.996 37.6237C120.196 37.7837 119.656 38.0037 119.656 38.6337C119.656 39.1437 120.026 39.4437 120.596 39.4437C121.366 39.4437 122.016 38.9737 122.016 38.3037ZM124.579 38.8337V35.6237H123.769V34.8537H124.579V33.3237H125.469V34.8537H126.659V35.6237H125.469V38.6937C125.469 39.2437 125.739 39.3437 126.199 39.3437C126.409 39.3437 126.559 39.3337 126.769 39.2837V40.0537C126.539 40.1037 126.279 40.1337 125.979 40.1337C125.089 40.1337 124.579 39.8337 124.579 38.8337ZM127.838 40.0837V32.9037H128.738V35.5637C129.038 35.1337 129.558 34.7537 130.298 34.7537C131.408 34.7537 132.038 35.5237 132.038 36.6337V40.0837H131.138V36.8737C131.138 36.1437 130.828 35.5937 130.048 35.5937C129.398 35.5937 128.738 36.0937 128.738 36.9637V40.0837H127.838ZM135.572 40.1937C134.122 40.1937 133.122 39.0837 133.122 37.4737C133.122 35.9137 134.112 34.7537 135.532 34.7537C136.992 34.7537 137.762 35.8637 137.762 37.3137V37.6137H133.982C134.032 38.7137 134.662 39.4137 135.572 39.4137C136.272 39.4137 136.772 39.0337 136.932 38.4137L137.722 38.6937C137.402 39.6237 136.612 40.1937 135.572 40.1937ZM135.522 35.5237C134.762 35.5237 134.192 36.0337 134.032 36.9237H136.852C136.832 36.1837 136.432 35.5237 135.522 35.5237ZM141.593 34.8337V35.7437C141.443 35.7137 141.323 35.7037 141.153 35.7037C140.393 35.7037 139.783 36.3037 139.783 37.2237V40.0837H138.883V34.8537H139.783V35.7437C140.013 35.2137 140.523 34.8137 141.233 34.8137C141.373 34.8137 141.503 34.8237 141.593 34.8337ZM144.653 33.9437V32.9037H145.633V33.9437H144.653ZM145.593 34.8537V40.0837H144.693V34.8537H145.593ZM147.095 40.0837V34.8537H147.995V35.5637C148.295 35.1337 148.815 34.7537 149.555 34.7537C150.665 34.7537 151.295 35.5237 151.295 36.6337V40.0837H150.395V36.8737C150.395 36.1437 150.085 35.5937 149.305 35.5937C148.655 35.5937 147.995 36.0937 147.995 36.9637V40.0837H147.095ZM154.889 40.0837V32.9037H157.539C159.059 32.9037 159.869 33.5737 159.869 34.7737C159.869 35.6937 159.309 36.1537 158.759 36.3437C159.569 36.5637 160.109 37.1937 160.109 38.1037C160.109 39.3537 159.199 40.0837 157.749 40.0837H154.889ZM155.819 35.9837H157.459C158.419 35.9837 158.949 35.6137 158.949 34.8537C158.949 34.0837 158.419 33.7137 157.459 33.7137H155.819V35.9837ZM155.819 36.8037V39.2737H157.709C158.669 39.2737 159.189 38.7737 159.189 38.0337C159.189 37.3037 158.669 36.8037 157.709 36.8037H155.819ZM163.355 40.1937C161.905 40.1937 160.905 39.0837 160.905 37.4737C160.905 35.9137 161.895 34.7537 163.315 34.7537C164.775 34.7537 165.545 35.8637 165.545 37.3137V37.6137H161.765C161.815 38.7137 162.445 39.4137 163.355 39.4137C164.055 39.4137 164.555 39.0337 164.715 38.4137L165.505 38.6937C165.185 39.6237 164.395 40.1937 163.355 40.1937ZM163.305 35.5237C162.545 35.5237 161.975 36.0337 161.815 36.9237H164.635C164.615 36.1837 164.215 35.5237 163.305 35.5237ZM169.376 34.8337V35.7437C169.226 35.7137 169.106 35.7037 168.936 35.7037C168.176 35.7037 167.566 36.3037 167.566 37.2237V40.0837H166.666V34.8537H167.566V35.7437C167.796 35.2137 168.306 34.8137 169.016 34.8137C169.156 34.8137 169.286 34.8237 169.376 34.8337ZM171.267 32.9037V37.2937L173.587 34.8537H174.707L172.667 36.9137L174.877 40.0837H173.827L172.027 37.5637L171.267 38.3337V40.0837H170.367V32.9037H171.267ZM177.584 40.1937C176.134 40.1937 175.134 39.0837 175.134 37.4737C175.134 35.9137 176.124 34.7537 177.544 34.7537C179.004 34.7537 179.774 35.8637 179.774 37.3137V37.6137H175.994C176.044 38.7137 176.674 39.4137 177.584 39.4137C178.284 39.4137 178.784 39.0337 178.944 38.4137L179.734 38.6937C179.414 39.6237 178.624 40.1937 177.584 40.1937ZM177.534 35.5237C176.774 35.5237 176.204 36.0337 176.044 36.9237H178.864C178.844 36.1837 178.444 35.5237 177.534 35.5237ZM181.794 32.9037V40.0837H180.894V32.9037H181.794ZM185.387 40.1937C183.937 40.1937 182.937 39.0837 182.937 37.4737C182.937 35.9137 183.927 34.7537 185.347 34.7537C186.807 34.7537 187.577 35.8637 187.577 37.3137V37.6137H183.797C183.847 38.7137 184.477 39.4137 185.387 39.4137C186.087 39.4137 186.587 39.0337 186.747 38.4137L187.537 38.6937C187.217 39.6237 186.427 40.1937 185.387 40.1937ZM185.337 35.5237C184.577 35.5237 184.007 36.0337 183.847 36.9237H186.667C186.647 36.1837 186.247 35.5237 185.337 35.5237ZM190.559 40.6537C190.259 41.4437 189.849 41.9437 188.939 41.9437C188.719 41.9437 188.609 41.9337 188.449 41.9037V41.1237C188.619 41.1637 188.729 41.1737 188.879 41.1737C189.269 41.1737 189.479 41.0337 189.689 40.5137L189.939 39.8937L187.999 34.8537H188.949L190.419 38.8537L191.869 34.8537H192.809L190.559 40.6537ZM195.772 33.6137C194.852 33.6137 194.292 34.2237 194.152 35.1337L193.242 34.9837C193.412 33.7637 194.352 32.7937 195.812 32.7937C197.192 32.7937 198.132 33.6637 198.132 34.9037C198.132 36.0937 197.212 36.8637 196.192 36.9937V37.9537H195.322V36.3537C196.432 36.3537 197.182 35.8537 197.182 34.9537C197.182 34.1437 196.602 33.6137 195.772 33.6137ZM195.192 40.0837V38.9037H196.332V40.0837H195.192Z",fill:"white"}),p.createElement("path",{d:"M11.4422 69C11.4422 64.5817 15.0239 61 19.4422 61H148.442C152.86 61 156.442 64.5817 156.442 69V110C156.442 114.418 152.86 118 148.442 118H13.4422C12.3376 118 11.4422 117.105 11.4422 116V69Z",fill:"#ECECF1"}),p.createElement("path",{d:"M24.2322 78V70.82H26.8822C28.4022 70.82 29.2122 71.49 29.2122 72.69C29.2122 73.61 28.6522 74.07 28.1022 74.26C28.9122 74.48 29.4522 75.11 29.4522 76.02C29.4522 77.27 28.5422 78 27.0922 78H24.2322ZM25.1622 73.9H26.8022C27.7622 73.9 28.2922 73.53 28.2922 72.77C28.2922 72 27.7622 71.63 26.8022 71.63H25.1622V73.9ZM25.1622 74.72V77.19H27.0522C28.0122 77.19 28.5322 76.69 28.5322 75.95C28.5322 75.22 28.0122 74.72 27.0522 74.72H25.1622ZM32.6982 78.11C31.2482 78.11 30.2482 77 30.2482 75.39C30.2482 73.83 31.2382 72.67 32.6582 72.67C34.1182 72.67 34.8882 73.78 34.8882 75.23V75.53H31.1082C31.1582 76.63 31.7882 77.33 32.6982 77.33C33.3982 77.33 33.8982 76.95 34.0582 76.33L34.8482 76.61C34.5282 77.54 33.7382 78.11 32.6982 78.11ZM32.6482 73.44C31.8882 73.44 31.3182 73.95 31.1582 74.84H33.9782C33.9582 74.1 33.5582 73.44 32.6482 73.44ZM38.7186 72.75V73.66C38.5686 73.63 38.4486 73.62 38.2786 73.62C37.5186 73.62 36.9086 74.22 36.9086 75.14V78H36.0086V72.77H36.9086V73.66C37.1386 73.13 37.6486 72.73 38.3586 72.73C38.4986 72.73 38.6286 72.74 38.7186 72.75ZM40.6098 70.82V75.21L42.9298 72.77H44.0498L42.0098 74.83L44.2198 78H43.1698L41.3698 75.48L40.6098 76.25V78H39.7098V70.82H40.6098ZM46.9267 78.11C45.4767 78.11 44.4767 77 44.4767 75.39C44.4767 73.83 45.4667 72.67 46.8867 72.67C48.3467 72.67 49.1167 73.78 49.1167 75.23V75.53H45.3367C45.3867 76.63 46.0167 77.33 46.9267 77.33C47.6267 77.33 48.1267 76.95 48.2867 76.33L49.0767 76.61C48.7567 77.54 47.9667 78.11 46.9267 78.11ZM46.8767 73.44C46.1167 73.44 45.5467 73.95 45.3867 74.84H48.2067C48.1867 74.1 47.7867 73.44 46.8767 73.44ZM51.1371 70.82V78H50.2371V70.82H51.1371ZM54.7295 78.11C53.2795 78.11 52.2795 77 52.2795 75.39C52.2795 73.83 53.2695 72.67 54.6895 72.67C56.1495 72.67 56.9195 73.78 56.9195 75.23V75.53H53.1395C53.1895 76.63 53.8195 77.33 54.7295 77.33C55.4295 77.33 55.9295 76.95 56.0895 76.33L56.8795 76.61C56.5595 77.54 55.7695 78.11 54.7295 78.11ZM54.6795 73.44C53.9195 73.44 53.3495 73.95 53.1895 74.84H56.0095C55.9895 74.1 55.5895 73.44 54.6795 73.44ZM59.9017 78.57C59.6017 79.36 59.1917 79.86 58.2817 79.86C58.0617 79.86 57.9517 79.85 57.7917 79.82V79.04C57.9617 79.08 58.0717 79.09 58.2217 79.09C58.6117 79.09 58.8217 78.95 59.0317 78.43L59.2817 77.81L57.3417 72.77H58.2917L59.7617 76.77L61.2117 72.77H62.1517L59.9017 78.57ZM61.8924 78V76.82H63.0324V78.06C63.0324 78.84 62.5324 79.31 61.7324 79.33V78.81C62.1824 78.79 62.4124 78.56 62.4124 78.08V78H61.8924ZM67.0183 74.41C67.0183 76.11 67.9583 77.26 69.2883 77.26C70.2183 77.26 70.8883 76.72 71.1483 75.81L72.0583 76.11C71.6383 77.35 70.6583 78.11 69.2883 78.11C67.4283 78.11 66.0583 76.6 66.0583 74.41C66.0583 72.22 67.4283 70.71 69.2883 70.71C70.6583 70.71 71.6383 71.47 72.0583 72.71L71.1483 73.01C70.8883 72.1 70.2183 71.56 69.2883 71.56C67.9583 71.56 67.0183 72.71 67.0183 74.41ZM75.0148 70.82H76.0748L78.8448 78H77.8748L77.1448 76.11H73.9248L73.2048 78H72.2448L75.0148 70.82ZM75.5248 71.9L74.2448 75.27H76.8248L75.5248 71.9Z",fill:"#353740"}),p.createElement("path",{d:"M26.4322 85.71C27.7422 85.71 28.7222 86.48 28.7222 87.63C28.7222 88.35 28.3122 88.91 27.5922 89.19C28.4522 89.45 29.0922 90.11 29.0922 91.02C29.0922 92.28 27.9722 93.1 26.4322 93.1C24.9022 93.1 23.7822 92.28 23.7822 91.02C23.7822 90.11 24.4222 89.45 25.2722 89.19C24.5622 88.91 24.1522 88.35 24.1522 87.63C24.1522 86.48 25.1322 85.71 26.4322 85.71ZM26.4322 88.81C27.2822 88.81 27.8522 88.35 27.8522 87.64C27.8522 86.94 27.2822 86.47 26.4322 86.47C25.5922 86.47 25.0122 86.94 25.0122 87.64C25.0122 88.35 25.5922 88.81 26.4322 88.81ZM26.4322 89.6C25.4222 89.6 24.7222 90.16 24.7222 90.96C24.7222 91.78 25.4222 92.34 26.4322 92.34C27.4422 92.34 28.1622 91.78 28.1622 90.96C28.1622 90.16 27.4422 89.6 26.4322 89.6ZM29.7206 89.41C29.7206 87.22 30.7506 85.71 32.4606 85.71C34.1606 85.71 35.1906 87.22 35.1906 89.41C35.1906 91.6 34.1606 93.11 32.4606 93.11C30.7506 93.11 29.7206 91.6 29.7206 89.41ZM30.6806 89.41C30.6806 91.09 31.2906 92.27 32.4606 92.27C33.6206 92.27 34.2406 91.09 34.2406 89.41C34.2406 87.73 33.6206 86.55 32.4606 86.55C31.2906 86.55 30.6806 87.73 30.6806 89.41ZM37.1511 85.71C37.9411 85.71 38.5611 86.33 38.5611 87.12C38.5611 87.91 37.9411 88.55 37.1511 88.55C36.3711 88.55 35.7411 87.91 35.7411 87.12C35.7411 86.33 36.3711 85.71 37.1511 85.71ZM37.1511 87.9C37.5911 87.9 37.9211 87.55 37.9211 87.12C37.9211 86.71 37.5911 86.35 37.1511 86.35C36.7211 86.35 36.3811 86.71 36.3811 87.12C36.3811 87.55 36.7211 87.9 37.1511 87.9ZM40.4752 93H39.5252V85.82H44.1652V86.66H40.4752V88.97H43.5952V89.81H40.4752V93Z",fill:"#353740"}),p.createElement("path",{d:"M23.4822 106.52L24.2322 105.92C24.6822 106.78 25.4922 107.28 26.4222 107.28C27.3522 107.28 28.0722 106.82 28.0722 106C28.0722 105.13 27.2422 104.98 26.1522 104.72C25.0222 104.45 23.9022 104.14 23.9022 102.79C23.9022 101.53 24.9922 100.71 26.3522 100.71C27.6022 100.71 28.5022 101.31 28.9322 102.12L28.2022 102.7C27.8322 102.01 27.2622 101.54 26.3322 101.54C25.4622 101.54 24.8322 102.02 24.8322 102.71C24.8322 103.47 25.5022 103.63 26.5122 103.87C27.7022 104.15 29.0122 104.4 29.0122 105.92C29.0122 107.26 27.8222 108.11 26.4022 108.11C25.1622 108.11 23.9722 107.49 23.4822 106.52ZM34.2113 102.77V108H33.3113V107.28C33.0113 107.72 32.5013 108.11 31.7613 108.11C30.6413 108.11 30.0113 107.34 30.0113 106.23V102.77H30.9113V106.04C30.9113 106.78 31.2313 107.28 32.0013 107.28C32.6513 107.28 33.3113 106.79 33.3113 105.92V102.77H34.2113ZM35.7059 108V102.77H36.6059V103.48C36.9059 103.05 37.4259 102.67 38.1659 102.67C39.2759 102.67 39.9059 103.44 39.9059 104.55V108H39.0059V104.79C39.0059 104.06 38.6959 103.51 37.9159 103.51C37.2659 103.51 36.6059 104.01 36.6059 104.88V108H35.7059ZM41.3504 108V102.77H42.2504V103.48C42.5504 103.05 43.0704 102.67 43.8104 102.67C44.9204 102.67 45.5504 103.44 45.5504 104.55V108H44.6504V104.79C44.6504 104.06 44.3404 103.51 43.5604 103.51C42.9104 103.51 42.2504 104.01 42.2504 104.88V108H41.3504ZM48.9056 108.57C48.6056 109.36 48.1956 109.86 47.2856 109.86C47.0656 109.86 46.9556 109.85 46.7956 109.82V109.04C46.9656 109.08 47.0756 109.09 47.2256 109.09C47.6156 109.09 47.8256 108.95 48.0356 108.43L48.2856 107.81L46.3456 102.77H47.2956L48.7656 106.77L50.2156 102.77H51.1556L48.9056 108.57ZM56.6334 102.77H57.3934L58.4734 106.67L59.5634 102.77H60.4634L58.8734 108H58.0834L56.9934 104.12L55.9034 108H55.1134L53.5234 102.77H54.4534L55.5534 106.66L56.6334 102.77ZM61.3006 101.86V100.82H62.2806V101.86H61.3006ZM62.2406 102.77V108H61.3406V102.77H62.2406ZM63.883 106.75V103.54H63.073V102.77H63.883V101.24H64.773V102.77H65.963V103.54H64.773V106.61C64.773 107.16 65.043 107.26 65.503 107.26C65.713 107.26 65.863 107.25 66.073 107.2V107.97C65.843 108.02 65.583 108.05 65.283 108.05C64.393 108.05 63.883 107.75 63.883 106.75ZM67.1414 108V100.82H68.0414V103.48C68.3414 103.05 68.8614 102.67 69.6014 102.67C70.7114 102.67 71.3414 103.44 71.3414 104.55V108H70.4414V104.79C70.4414 104.06 70.1314 103.51 69.3514 103.51C68.7014 103.51 68.0414 104.01 68.0414 104.88V108H67.1414ZM76.2053 108.09C75.2553 108.09 74.4853 107.53 74.4853 106.59C74.4853 105.55 75.2953 105.12 76.4653 104.88L77.7353 104.62V104.4C77.7353 103.8 77.4053 103.46 76.7053 103.46C76.0453 103.46 75.6553 103.77 75.4953 104.35L74.6453 104.13C74.8953 103.28 75.6653 102.67 76.7353 102.67C77.9053 102.67 78.6253 103.24 78.6253 104.36V106.91C78.6253 107.25 78.8353 107.36 79.1853 107.28V108C78.3853 108.1 77.9253 107.91 77.8153 107.41C77.4953 107.82 76.9153 108.09 76.2053 108.09ZM77.7353 106.22V105.32L76.7153 105.54C75.9153 105.7 75.3753 105.92 75.3753 106.55C75.3753 107.06 75.7453 107.36 76.3153 107.36C77.0853 107.36 77.7353 106.89 77.7353 106.22ZM82.8084 105.39C82.8084 106.58 83.4484 107.32 84.3784 107.32C85.0984 107.32 85.5284 106.86 85.6784 106.21L86.4784 106.58C86.2284 107.45 85.4684 108.11 84.3784 108.11C82.9184 108.11 81.9084 107 81.9084 105.39C81.9084 103.77 82.9184 102.67 84.3784 102.67C85.4684 102.67 86.2084 103.3 86.4584 104.17L85.6784 104.56C85.5284 103.92 85.0984 103.45 84.3784 103.45C83.4484 103.45 82.8084 104.19 82.8084 105.39ZM87.4637 108V100.82H88.3637V103.48C88.6637 103.05 89.1837 102.67 89.9237 102.67C91.0337 102.67 91.6637 103.44 91.6637 104.55V108H90.7637V104.79C90.7637 104.06 90.4537 103.51 89.6737 103.51C89.0237 103.51 88.3637 104.01 88.3637 104.88V108H87.4637ZM94.4182 108.09C93.4682 108.09 92.6982 107.53 92.6982 106.59C92.6982 105.55 93.5082 105.12 94.6782 104.88L95.9482 104.62V104.4C95.9482 103.8 95.6182 103.46 94.9182 103.46C94.2582 103.46 93.8682 103.77 93.7082 104.35L92.8582 104.13C93.1082 103.28 93.8782 102.67 94.9482 102.67C96.1182 102.67 96.8382 103.24 96.8382 104.36V106.91C96.8382 107.25 97.0482 107.36 97.3982 107.28V108C96.5982 108.1 96.1382 107.91 96.0282 107.41C95.7082 107.82 95.1282 108.09 94.4182 108.09ZM95.9482 106.22V105.32L94.9282 105.54C94.1282 105.7 93.5882 105.92 93.5882 106.55C93.5882 107.06 93.9582 107.36 94.5282 107.36C95.2982 107.36 95.9482 106.89 95.9482 106.22ZM98.3719 108V102.77H99.2719V103.48C99.5719 103.05 100.092 102.67 100.832 102.67C101.942 102.67 102.572 103.44 102.572 104.55V108H101.672V104.79C101.672 104.06 101.362 103.51 100.582 103.51C99.9319 103.51 99.2719 104.01 99.2719 104.88V108H98.3719ZM104.556 105.39C104.556 106.58 105.196 107.32 106.126 107.32C106.846 107.32 107.276 106.86 107.426 106.21L108.226 106.58C107.976 107.45 107.216 108.11 106.126 108.11C104.666 108.11 103.656 107 103.656 105.39C103.656 103.77 104.666 102.67 106.126 102.67C107.216 102.67 107.956 103.3 108.206 104.17L107.426 104.56C107.276 103.92 106.846 103.45 106.126 103.45C105.196 103.45 104.556 104.19 104.556 105.39ZM111.302 108.11C109.852 108.11 108.852 107 108.852 105.39C108.852 103.83 109.842 102.67 111.262 102.67C112.722 102.67 113.492 103.78 113.492 105.23V105.53H109.712C109.762 106.63 110.392 107.33 111.302 107.33C112.002 107.33 112.502 106.95 112.662 106.33L113.452 106.61C113.132 107.54 112.342 108.11 111.302 108.11ZM111.252 103.44C110.492 103.44 109.922 103.95 109.762 104.84H112.582C112.562 104.1 112.162 103.44 111.252 103.44ZM121.221 105.39C121.221 107.01 120.221 108.11 118.791 108.11C117.361 108.11 116.361 107.01 116.361 105.39C116.361 103.77 117.361 102.67 118.791 102.67C120.221 102.67 121.221 103.77 121.221 105.39ZM117.271 105.39C117.271 106.58 117.871 107.34 118.791 107.34C119.711 107.34 120.311 106.58 120.311 105.39C120.311 104.19 119.711 103.43 118.791 103.43C117.871 103.43 117.271 104.19 117.271 105.39ZM124.646 100.73V101.53C124.496 101.51 124.406 101.51 124.256 101.51C123.726 101.51 123.436 101.74 123.436 102.41V102.77H124.596V103.54H123.436V108H122.536V103.54H121.746V102.77H122.536V102.35C122.536 101.33 123.096 100.71 124.166 100.71C124.346 100.71 124.446 100.72 124.646 100.73ZM130.252 102.75V103.66C130.102 103.63 129.982 103.62 129.812 103.62C129.052 103.62 128.442 104.22 128.442 105.14V108H127.542V102.77H128.442V103.66C128.672 103.13 129.182 102.73 129.892 102.73C130.032 102.73 130.162 102.74 130.252 102.75ZM132.446 108.09C131.496 108.09 130.726 107.53 130.726 106.59C130.726 105.55 131.536 105.12 132.706 104.88L133.976 104.62V104.4C133.976 103.8 133.646 103.46 132.946 103.46C132.286 103.46 131.896 103.77 131.736 104.35L130.886 104.13C131.136 103.28 131.906 102.67 132.976 102.67C134.146 102.67 134.866 103.24 134.866 104.36V106.91C134.866 107.25 135.076 107.36 135.426 107.28V108C134.626 108.1 134.166 107.91 134.056 107.41C133.736 107.82 133.156 108.09 132.446 108.09ZM133.976 106.22V105.32L132.956 105.54C132.156 105.7 131.616 105.92 131.616 106.55C131.616 107.06 131.986 107.36 132.556 107.36C133.326 107.36 133.976 106.89 133.976 106.22ZM136.359 101.86V100.82H137.339V101.86H136.359ZM137.299 102.77V108H136.399V102.77H137.299ZM138.802 108V102.77H139.702V103.48C140.002 103.05 140.522 102.67 141.262 102.67C142.372 102.67 143.002 103.44 143.002 104.55V108H142.102V104.79C142.102 104.06 141.792 103.51 141.012 103.51C140.362 103.51 139.702 104.01 139.702 104.88V108H138.802Z",fill:"#353740"}),p.createElement("circle",{opacity:.5,cx:10.9709,cy:10.5815,r:2.65941,fill:"#6E6E80"}),p.createElement("circle",{opacity:.5,cx:19.8356,cy:10.5815,r:2.65941,fill:"#6E6E80"}),p.createElement("circle",{opacity:.5,cx:28.7003,cy:10.5815,r:2.65941,fill:"#6E6E80"}),p.createElement("defs",null,p.createElement("radialGradient",{id:"paint0_radial_293_16838",cx:0,cy:0,r:1,gradientUnits:"userSpaceOnUse",gradientTransform:"translate(6.13089 0.83036) rotate(33.8753) scale(693.64 1179.61)"},p.createElement("stop",{stopColor:"white"}),p.createElement("stop",{offset:1,stopColor:"#E9E9E9"})))),ko=p.forwardRef(vo),_o=(n,t)=>p.createElement("svg",{width:"1em",height:"1em",viewBox:"0 0 214 130",fill:"none",xmlns:"http://www.w3.org/2000/svg",ref:t,...n},p.createElement("path",{d:"M0.833313 7.92207C0.833313 4.00541 4.0084 0.830322 7.92506 0.830322H297.8V129.368H0.833313V7.92207Z",fill:"url(#paint0_radial_293_16940)"}),p.createElement("path",{d:"M19.3567 39.121C18.6637 39.121 18.1467 38.824 17.8057 38.395V39H16.4857V31.102H17.8057V33.841C18.1467 33.412 18.6637 33.115 19.3567 33.115C20.8857 33.115 21.7767 34.479 21.7767 36.118C21.7767 37.757 20.8857 39.121 19.3567 39.121ZM17.7727 35.953V36.294C17.7727 37.361 18.3447 37.966 19.0817 37.966C19.9617 37.966 20.4347 37.218 20.4347 36.118C20.4347 35.018 19.9617 34.27 19.0817 34.27C18.3447 34.27 17.7727 34.864 17.7727 35.953ZM24.8084 39.099C23.6644 39.099 22.7954 38.428 22.7954 37.394C22.7954 36.294 23.6424 35.799 24.7974 35.579L26.5024 35.249V35.15C26.5024 34.6 26.1724 34.248 25.4354 34.248C24.7864 34.248 24.3794 34.556 24.2144 35.117L22.9824 34.82C23.2794 33.863 24.1594 33.115 25.4904 33.115C26.9204 33.115 27.7894 33.775 27.7894 35.106V37.592C27.7894 37.922 27.9434 38.021 28.3174 37.977V39C27.3274 39.11 26.8104 38.923 26.6014 38.45C26.2164 38.868 25.5894 39.099 24.8084 39.099ZM26.5024 36.965V36.25L25.1384 36.514C24.5664 36.624 24.1154 36.822 24.1154 37.339C24.1154 37.79 24.4674 38.043 24.9954 38.043C25.7324 38.043 26.5024 37.669 26.5024 36.965ZM30.8101 37.02V34.347H29.2481V33.225H30.8101V31.531H32.1081V33.225H34.7591V34.347H32.1081V36.899C32.1081 37.647 32.5151 37.889 33.3951 37.889C33.8791 37.889 34.3301 37.812 34.7371 37.702V38.879C34.3521 38.989 33.7361 39.077 33.1531 39.077C31.9211 39.077 30.8101 38.571 30.8101 37.02ZM37.3948 36.118C37.3948 37.295 38.0878 37.977 39.0998 37.977C39.8808 37.977 40.3868 37.515 40.5738 36.877L41.6628 37.449C41.3438 38.384 40.3978 39.121 39.0888 39.121C37.3838 39.121 36.1078 37.9 36.1078 36.118C36.1078 34.336 37.3838 33.115 39.0888 33.115C40.3868 33.115 41.3108 33.819 41.6408 34.743L40.5738 35.348C40.3868 34.721 39.8808 34.248 39.0998 34.248C38.0878 34.248 37.3948 34.941 37.3948 36.118ZM42.9785 39V31.102H44.2985V33.841C44.6285 33.456 45.1455 33.115 45.8935 33.115C47.1035 33.115 47.8295 33.951 47.8295 35.194V39H46.5095V35.579C46.5095 34.864 46.2235 34.347 45.4975 34.347C44.9035 34.347 44.2985 34.787 44.2985 35.612V39H42.9785ZM50.0252 40.716V39.605H53.9192V40.716H50.0252ZM59.0959 32.455V31.102H60.4599V32.455H59.0959ZM59.1179 38.791V34.347H56.3789V33.225H60.4379V38.912C60.4379 40.463 59.0299 41.079 57.7979 41.079C57.2149 41.079 56.6099 40.991 56.2469 40.848V39.671C56.6539 39.781 57.1929 39.891 57.6769 39.891C58.5679 39.891 59.1179 39.539 59.1179 38.791ZM67.9466 36.118C67.9466 37.911 66.7916 39.121 65.1636 39.121C63.5356 39.121 62.3806 37.911 62.3806 36.118C62.3806 34.325 63.5356 33.115 65.1636 33.115C66.7916 33.115 67.9466 34.325 67.9466 36.118ZM63.7116 36.118C63.7116 37.295 64.2726 38.01 65.1636 38.01C66.0546 38.01 66.6156 37.295 66.6156 36.118C66.6156 34.941 66.0546 34.226 65.1636 34.226C64.2726 34.226 63.7116 34.941 63.7116 36.118ZM71.2643 32.455V31.102H72.6283V32.455H71.2643ZM69.3283 39V37.878H71.2863V34.347H69.3283V33.225H72.6063V37.878H74.5643V39H69.3283ZM78.377 34.479L80.17 31.102H81.644L79.059 35.744V39H77.651V35.744L75.055 31.102H76.584L78.377 34.479ZM88.0857 33.258L86.7767 33.709C86.5567 32.785 85.9847 32.224 85.2037 32.224C84.1477 32.224 83.3557 33.346 83.3557 35.051C83.3557 36.756 84.1477 37.878 85.2037 37.878C85.9847 37.878 86.5567 37.317 86.7767 36.393L88.0857 36.844C87.6677 38.241 86.6447 39.121 85.2147 39.121C83.3447 39.121 82.0027 37.449 82.0027 35.051C82.0027 32.653 83.3447 30.981 85.2147 30.981C86.6447 30.981 87.6677 31.861 88.0857 33.258ZM89.9514 35.227V39H88.7524V33.225H89.9514V33.742C90.1274 33.412 90.4794 33.115 90.9634 33.115C91.4254 33.115 91.7444 33.379 91.9644 33.808C92.1294 33.489 92.5474 33.115 93.0974 33.115C93.9004 33.115 94.3734 33.786 94.3734 34.776V39H93.1744V35.205C93.1744 34.699 93.1084 34.347 92.7014 34.347C92.3384 34.347 92.1624 34.644 92.1624 35.227V39H90.9634V35.205C90.9634 34.699 90.9084 34.347 90.5014 34.347C90.1274 34.347 89.9514 34.644 89.9514 35.227ZM98.8021 39H97.4711L95.1391 33.225H96.5251L98.1531 37.449L99.7701 33.225H101.134L98.8021 39ZM104.738 39.121C102.945 39.121 101.79 37.449 101.79 35.051C101.79 32.653 102.945 30.981 104.738 30.981C106.531 30.981 107.686 32.653 107.686 35.051C107.686 36.459 107.29 37.625 106.597 38.34C106.905 38.813 107.466 39.165 108.148 39.33L107.334 40.331C106.619 40.122 105.882 39.583 105.552 39C105.299 39.077 105.024 39.121 104.738 39.121ZM104.738 32.224C103.704 32.224 103.154 33.335 103.154 35.051C103.154 36.767 103.704 37.878 104.738 37.878C105.772 37.878 106.322 36.767 106.322 35.051C106.322 33.335 105.772 32.224 104.738 32.224ZM113.732 33.225V39H112.412V38.384C112.082 38.769 111.587 39.121 110.839 39.121C109.629 39.121 108.881 38.296 108.881 37.053V33.225H110.201V36.767C110.201 37.471 110.509 37.922 111.224 37.922C111.818 37.922 112.412 37.482 112.412 36.657V33.225H113.732ZM120.492 37.878V39H115.344V38.01L118.886 34.347H115.388V33.225H120.459V34.226L117.005 37.878H120.492ZM124.888 39.121C124.195 39.121 123.678 38.824 123.337 38.395V40.98H122.017V33.225H123.337V33.841C123.678 33.412 124.195 33.115 124.888 33.115C126.417 33.115 127.308 34.479 127.308 36.118C127.308 37.757 126.417 39.121 124.888 39.121ZM123.304 35.953V36.294C123.304 37.361 123.876 37.966 124.613 37.966C125.493 37.966 125.966 37.218 125.966 36.118C125.966 35.018 125.493 34.27 124.613 34.27C123.876 34.27 123.304 34.864 123.304 35.953ZM129.889 31.102V34.446L132.705 31.102H134.366L131.451 34.446L134.432 39H132.804L130.505 35.535L129.889 36.239V39H128.481V31.102H129.889Z",fill:"#5D6D78"}),p.createElement("path",{d:"M21.1103 54.9999C21.1103 54.7698 20.9238 54.5833 20.6937 54.5833C20.4635 54.5833 20.277 54.7698 20.277 54.9999V56.6666C20.277 56.8967 20.4635 57.0833 20.6937 57.0833C20.9238 57.0833 21.1103 56.8967 21.1103 56.6666V54.9999Z",fill:"#6E6E80"}),p.createElement("path",{d:"M20.6937 53.9583C20.9813 53.9583 21.2145 53.7251 21.2145 53.4374C21.2145 53.1498 20.9813 52.9166 20.6937 52.9166C20.406 52.9166 20.1728 53.1498 20.1728 53.4374C20.1728 53.7251 20.406 53.9583 20.6937 53.9583Z",fill:"#6E6E80"}),p.createElement("path",{fillRule:"evenodd",clipRule:"evenodd",d:"M20.6937 50.8333C18.3925 50.8333 16.527 52.6987 16.527 54.9999C16.527 57.3011 18.3925 59.1666 20.6937 59.1666C22.9948 59.1666 24.8603 57.3011 24.8603 54.9999C24.8603 52.6987 22.9948 50.8333 20.6937 50.8333ZM17.3603 54.9999C17.3603 53.159 18.8527 51.6666 20.6937 51.6666C22.5346 51.6666 24.027 53.159 24.027 54.9999C24.027 56.8409 22.5346 58.3333 20.6937 58.3333C18.8527 58.3333 17.3603 56.8409 17.3603 54.9999Z",fill:"#6E6E80"}),p.createElement("path",{fillRule:"evenodd",clipRule:"evenodd",d:"M17.3603 71.9999C17.3603 70.159 18.8527 68.6666 20.6937 68.6666C22.5346 68.6666 24.027 70.159 24.027 71.9999C24.027 73.8409 22.5346 75.3333 20.6937 75.3333C18.8527 75.3333 17.3603 73.8409 17.3603 71.9999ZM20.6937 67.8333C18.3925 67.8333 16.527 69.6987 16.527 71.9999C16.527 74.3011 18.3925 76.1666 20.6937 76.1666C22.9948 76.1666 24.8603 74.3011 24.8603 71.9999C24.8603 69.6987 22.9948 67.8333 20.6937 67.8333ZM21.1103 69.9166C21.1103 69.6865 20.9238 69.4999 20.6937 69.4999C20.4635 69.4999 20.277 69.6865 20.277 69.9166V71.8273L19.3574 72.747C19.1946 72.9097 19.1946 73.1735 19.3574 73.3362C19.5201 73.4989 19.7839 73.4989 19.9466 73.3362L20.9883 72.2945C21.0664 72.2164 21.1103 72.1104 21.1103 71.9999V69.9166Z",fill:"#6E6E80"}),p.createElement("path",{fillRule:"evenodd",clipRule:"evenodd",d:"M21.0875 84.4878C21.3141 84.5278 21.4655 84.7439 21.4255 84.9705L21.272 85.841C21.2321 86.0676 21.016 86.2189 20.7893 86.179C20.5627 86.139 20.4114 85.9229 20.4514 85.6963L20.6048 84.8258C20.6448 84.5992 20.8609 84.4479 21.0875 84.4878ZM18.1309 85.1375C18.3194 85.0056 18.5792 85.0514 18.7112 85.2399L19.1891 85.9225C19.3211 86.111 19.2753 86.3708 19.0868 86.5028C18.8983 86.6348 18.6385 86.5889 18.5065 86.4004L18.0285 85.7178C17.8965 85.5293 17.9424 85.2695 18.1309 85.1375ZM19.6823 87.3575C19.8127 87.2651 19.9844 87.2553 20.1244 87.3325L24.3924 89.6849C24.5183 89.7544 24.5996 89.8839 24.6073 90.0275C24.615 90.1712 24.548 90.3086 24.4302 90.3912L23.7012 90.9016L24.1755 91.591C24.3052 91.7794 24.2587 92.0372 24.0713 92.1685L22.7041 93.1257C22.6136 93.1891 22.5016 93.214 22.3928 93.1948C22.284 93.1756 22.1872 93.1139 22.1238 93.0234L21.6458 92.3408L20.9651 92.8174C20.848 92.8995 20.697 92.9157 20.5651 92.8604C20.4332 92.8052 20.3389 92.6862 20.3152 92.5452L19.5124 87.7664C19.4859 87.6088 19.5519 87.4499 19.6823 87.3575ZM20.477 88.4784L21.0277 91.7563L21.5092 91.4192C21.5997 91.3558 21.7117 91.331 21.8205 91.3501C21.9294 91.3693 22.0261 91.431 22.0895 91.5215L22.5675 92.2041L23.2548 91.7229L22.7805 91.0335C22.6508 90.845 22.6973 90.5872 22.8847 90.456L23.4046 90.092L20.477 88.4784ZM16.5092 87.6937C16.5492 87.4671 16.7653 87.3158 16.9919 87.3557L17.8622 87.5092C18.0888 87.5492 18.2402 87.7653 18.2002 87.9919C18.1602 88.2185 17.9441 88.3698 17.7175 88.3299L16.8472 88.1764C16.6205 88.1365 16.4692 87.9203 16.5092 87.6937Z",fill:"#6E6E80"}),p.createElement("path",{fillRule:"evenodd",clipRule:"evenodd",d:"M17.777 103.5C18.0071 103.5 18.1937 103.687 18.1937 103.917V105.583C18.1937 105.813 18.3802 106 18.6103 106H22.6044L21.649 105.045C21.4863 104.882 21.4863 104.618 21.649 104.455C21.8118 104.293 22.0756 104.293 22.2383 104.455L23.905 106.122C24.0677 106.285 24.0677 106.549 23.905 106.711L22.2383 108.378C22.0756 108.541 21.8118 108.541 21.649 108.378C21.4863 108.215 21.4863 107.951 21.649 107.789L22.6044 106.833H18.6103C17.92 106.833 17.3603 106.274 17.3603 105.583V103.917C17.3603 103.687 17.5469 103.5 17.777 103.5Z",fill:"#6E6E80"}),p.createElement("path",{d:"M33.7297 57.668L34.4047 57.128C34.8097 57.902 35.5387 58.352 36.3757 58.352C37.2127 58.352 37.8607 57.938 37.8607 57.2C37.8607 56.417 37.1137 56.282 36.1327 56.048C35.1157 55.805 34.1077 55.526 34.1077 54.311C34.1077 53.177 35.0887 52.439 36.3127 52.439C37.4377 52.439 38.2477 52.979 38.6347 53.708L37.9777 54.23C37.6447 53.609 37.1317 53.186 36.2947 53.186C35.5117 53.186 34.9447 53.618 34.9447 54.239C34.9447 54.923 35.5477 55.067 36.4567 55.283C37.5277 55.535 38.7067 55.76 38.7067 57.128C38.7067 58.334 37.6357 59.099 36.3577 59.099C35.2417 59.099 34.1707 58.541 33.7297 57.668ZM39.8296 57.875V54.986H39.1006V54.293H39.8296V52.916H40.6306V54.293H41.7016V54.986H40.6306V57.749C40.6306 58.244 40.8736 58.334 41.2876 58.334C41.4766 58.334 41.6116 58.325 41.8006 58.28V58.973C41.5936 59.018 41.3596 59.045 41.0896 59.045C40.2886 59.045 39.8296 58.775 39.8296 57.875ZM43.9412 59.081C43.0862 59.081 42.3932 58.577 42.3932 57.731C42.3932 56.795 43.1222 56.408 44.1752 56.192L45.3182 55.958V55.76C45.3182 55.22 45.0212 54.914 44.3912 54.914C43.7972 54.914 43.4462 55.193 43.3022 55.715L42.5372 55.517C42.7622 54.752 43.4552 54.203 44.4182 54.203C45.4712 54.203 46.1192 54.716 46.1192 55.724V58.019C46.1192 58.325 46.3082 58.424 46.6232 58.352V59C45.9032 59.09 45.4892 58.919 45.3902 58.469C45.1022 58.838 44.5802 59.081 43.9412 59.081ZM45.3182 57.398V56.588L44.4002 56.786C43.6802 56.93 43.1942 57.128 43.1942 57.695C43.1942 58.154 43.5272 58.424 44.0402 58.424C44.7332 58.424 45.3182 58.001 45.3182 57.398ZM47.6255 57.875V54.986H46.8965V54.293H47.6255V52.916H48.4265V54.293H49.4975V54.986H48.4265V57.749C48.4265 58.244 48.6695 58.334 49.0835 58.334C49.2725 58.334 49.4075 58.325 49.5965 58.28V58.973C49.3895 59.018 49.1555 59.045 48.8855 59.045C48.0845 59.045 47.6255 58.775 47.6255 57.875ZM54.2492 54.293V59H53.4392V58.352C53.1692 58.748 52.7102 59.099 52.0442 59.099C51.0362 59.099 50.4692 58.406 50.4692 57.407V54.293H51.2792V57.236C51.2792 57.902 51.5672 58.352 52.2602 58.352C52.8452 58.352 53.4392 57.911 53.4392 57.128V54.293H54.2492ZM55.0723 58.001L55.7023 57.542C55.9543 58.091 56.5303 58.424 57.1423 58.424C57.6913 58.424 58.1773 58.19 58.1773 57.677C58.1773 57.164 57.7273 57.092 56.9533 56.93C56.1793 56.759 55.3693 56.588 55.3693 55.625C55.3693 54.788 56.0983 54.203 57.0973 54.203C57.9433 54.203 58.6363 54.617 58.9153 55.175L58.3393 55.634C58.0963 55.166 57.6553 54.869 57.0613 54.869C56.5213 54.869 56.1613 55.139 56.1613 55.562C56.1613 56.003 56.5753 56.084 57.2323 56.228C58.0423 56.417 58.9693 56.579 58.9693 57.605C58.9693 58.505 58.1503 59.099 57.1423 59.099C56.2873 59.099 55.4323 58.73 55.0723 58.001Z",fill:"#6E6E80"}),p.createElement("path",{d:"M34.8907 72.769C34.8907 74.299 35.7367 75.334 36.9337 75.334C37.7707 75.334 38.3737 74.848 38.6077 74.029L39.4267 74.299C39.0487 75.415 38.1667 76.099 36.9337 76.099C35.2597 76.099 34.0267 74.74 34.0267 72.769C34.0267 70.798 35.2597 69.439 36.9337 69.439C38.1667 69.439 39.0487 70.123 39.4267 71.239L38.6077 71.509C38.3737 70.69 37.7707 70.204 36.9337 70.204C35.7367 70.204 34.8907 71.239 34.8907 72.769ZM44.3513 73.651C44.3513 75.109 43.4513 76.099 42.1643 76.099C40.8773 76.099 39.9773 75.109 39.9773 73.651C39.9773 72.193 40.8773 71.203 42.1643 71.203C43.4513 71.203 44.3513 72.193 44.3513 73.651ZM40.7963 73.651C40.7963 74.722 41.3363 75.406 42.1643 75.406C42.9923 75.406 43.5323 74.722 43.5323 73.651C43.5323 72.571 42.9923 71.887 42.1643 71.887C41.3363 71.887 40.7963 72.571 40.7963 73.651ZM46.1914 73.183V76H45.3814V71.293H46.1914V71.932C46.4434 71.545 46.8934 71.203 47.5504 71.203C48.1804 71.203 48.6214 71.518 48.8554 72.004C49.1164 71.617 49.6204 71.203 50.3584 71.203C51.3304 71.203 51.9154 71.896 51.9154 72.895V76H51.1054V73.111C51.1054 72.454 50.8354 71.959 50.1694 71.959C49.6294 71.959 49.0534 72.418 49.0534 73.183V76H48.2434V73.111C48.2434 72.454 47.9734 71.959 47.3074 71.959C46.7674 71.959 46.1914 72.418 46.1914 73.183ZM55.4892 76.099C54.8322 76.099 54.3102 75.784 54.0312 75.316V77.62H53.2212V71.293H54.0312V71.977C54.3102 71.518 54.8322 71.203 55.4892 71.203C56.7942 71.203 57.5412 72.31 57.5412 73.651C57.5412 74.983 56.7942 76.099 55.4892 76.099ZM54.0132 73.489V73.822C54.0132 74.902 54.6792 75.388 55.3452 75.388C56.2272 75.388 56.7222 74.659 56.7222 73.651C56.7222 72.634 56.2272 71.914 55.3452 71.914C54.6792 71.914 54.0132 72.391 54.0132 73.489ZM59.375 69.538V76H58.565V69.538H59.375ZM62.6081 76.099C61.3031 76.099 60.4031 75.1 60.4031 73.651C60.4031 72.247 61.2941 71.203 62.5721 71.203C63.8861 71.203 64.5791 72.202 64.5791 73.507V73.777H61.1771C61.2221 74.767 61.7891 75.397 62.6081 75.397C63.2381 75.397 63.6881 75.055 63.8321 74.497L64.5431 74.749C64.2551 75.586 63.5441 76.099 62.6081 76.099ZM62.5631 71.896C61.8791 71.896 61.3661 72.355 61.2221 73.156H63.7601C63.7421 72.49 63.3821 71.896 62.5631 71.896ZM65.7134 74.875V71.986H64.9844V71.293H65.7134V69.916H66.5144V71.293H67.5854V71.986H66.5144V74.749C66.5144 75.244 66.7574 75.334 67.1714 75.334C67.3604 75.334 67.4954 75.325 67.6844 75.28V75.973C67.4774 76.018 67.2434 76.045 66.9734 76.045C66.1724 76.045 65.7134 75.775 65.7134 74.875ZM68.61 70.474V69.538H69.492V70.474H68.61ZM69.456 71.293V76H68.646V71.293H69.456ZM74.8581 73.651C74.8581 75.109 73.9581 76.099 72.6711 76.099C71.3841 76.099 70.4841 75.109 70.4841 73.651C70.4841 72.193 71.3841 71.203 72.6711 71.203C73.9581 71.203 74.8581 72.193 74.8581 73.651ZM71.3031 73.651C71.3031 74.722 71.8431 75.406 72.6711 75.406C73.4991 75.406 74.0391 74.722 74.0391 73.651C74.0391 72.571 73.4991 71.887 72.6711 71.887C71.8431 71.887 71.3031 72.571 71.3031 73.651ZM75.8882 76V71.293H76.6982V71.932C76.9682 71.545 77.4362 71.203 78.1022 71.203C79.1012 71.203 79.6682 71.896 79.6682 72.895V76H78.8582V73.111C78.8582 72.454 78.5792 71.959 77.8772 71.959C77.2922 71.959 76.6982 72.409 76.6982 73.192V76H75.8882ZM82.9927 74.875V71.986H82.2637V71.293H82.9927V69.916H83.7937V71.293H84.8647V71.986H83.7937V74.749C83.7937 75.244 84.0367 75.334 84.4507 75.334C84.6397 75.334 84.7747 75.325 84.9637 75.28V75.973C84.7567 76.018 84.5227 76.045 84.2527 76.045C83.4517 76.045 82.9927 75.775 82.9927 74.875ZM85.8893 70.474V69.538H86.7713V70.474H85.8893ZM86.7353 71.293V76H85.9253V71.293H86.7353ZM88.8974 73.183V76H88.0874V71.293H88.8974V71.932C89.1494 71.545 89.5994 71.203 90.2564 71.203C90.8864 71.203 91.3274 71.518 91.5614 72.004C91.8224 71.617 92.3264 71.203 93.0644 71.203C94.0364 71.203 94.6214 71.896 94.6214 72.895V76H93.8114V73.111C93.8114 72.454 93.5414 71.959 92.8754 71.959C92.3354 71.959 91.7594 72.418 91.7594 73.183V76H90.9494V73.111C90.9494 72.454 90.6794 71.959 90.0134 71.959C89.4734 71.959 88.8974 72.418 88.8974 73.183ZM97.8083 76.099C96.5033 76.099 95.6033 75.1 95.6033 73.651C95.6033 72.247 96.4943 71.203 97.7723 71.203C99.0863 71.203 99.7793 72.202 99.7793 73.507V73.777H96.3773C96.4223 74.767 96.9893 75.397 97.8083 75.397C98.4383 75.397 98.8883 75.055 99.0323 74.497L99.7433 74.749C99.4553 75.586 98.7443 76.099 97.8083 76.099ZM97.7633 71.896C97.0793 71.896 96.5663 72.355 96.4223 73.156H98.9603C98.9423 72.49 98.5823 71.896 97.7633 71.896Z",fill:"#6E6E80"}),p.createElement("path",{d:"M34.4047 86.538H38.6257V87.294H35.2597V89.283H38.1127V90.039H35.2597V92.244H38.6257V93H34.4047V86.538ZM39.6157 93V88.293H40.4257V88.932C40.6957 88.545 41.1637 88.203 41.8297 88.203C42.8287 88.203 43.3957 88.896 43.3957 89.895V93H42.5857V90.111C42.5857 89.454 42.3067 88.959 41.6047 88.959C41.0197 88.959 40.4257 89.409 40.4257 90.192V93H39.6157ZM46.4238 93.099C45.1188 93.099 44.3718 91.983 44.3718 90.651C44.3718 89.31 45.1188 88.203 46.4238 88.203C47.0808 88.203 47.6028 88.518 47.8818 88.977V86.538H48.6918V93H47.8818V92.316C47.6028 92.784 47.0808 93.099 46.4238 93.099ZM47.8998 90.489C47.8998 89.391 47.2338 88.914 46.5678 88.914C45.6858 88.914 45.1908 89.634 45.1908 90.651C45.1908 91.659 45.6858 92.388 46.5678 92.388C47.2338 92.388 47.8998 91.902 47.8998 90.822V90.489ZM52.2988 93.099C51.6418 93.099 51.1198 92.784 50.8408 92.316V94.62H50.0308V88.293H50.8408V88.977C51.1198 88.518 51.6418 88.203 52.2988 88.203C53.6038 88.203 54.3508 89.31 54.3508 90.651C54.3508 91.983 53.6038 93.099 52.2988 93.099ZM50.8228 90.489V90.822C50.8228 91.902 51.4888 92.388 52.1548 92.388C53.0368 92.388 53.5318 91.659 53.5318 90.651C53.5318 89.634 53.0368 88.914 52.1548 88.914C51.4888 88.914 50.8228 89.391 50.8228 90.489ZM59.4245 90.651C59.4245 92.109 58.5245 93.099 57.2375 93.099C55.9505 93.099 55.0505 92.109 55.0505 90.651C55.0505 89.193 55.9505 88.203 57.2375 8