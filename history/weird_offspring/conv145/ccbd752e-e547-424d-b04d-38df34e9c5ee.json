{"uuid":"ccbd752e-e547-424d-b04d-38df34e9c5ee","name":"Processing the \"Better\" File","summary":"","created_at":"2024-08-29T09:42:30.727336+00:00","updated_at":"2024-08-29T10:31:31.817393+00:00","settings":{"preview_feature_uses_artifacts":true,"preview_feature_uses_latex":true},"is_starred":false,"current_leaf_message_uuid":"9e08eef2-e14f-48dc-ac4c-845794399707","chat_messages":[{"uuid":"bedfe5b4-6859-461a-8ad6-270354bde808","text":"please process the better file","sender":"human","index":0,"created_at":"2024-08-29T09:42:36.788130+00:00","updated_at":"2024-08-29T09:42:36.788130+00:00","truncated":false,"attachments":[{"id":"ff814e28-9bc6-4038-b738-c9bcc293d6f6","file_name":"better","file_size":5377,"file_type":"","extracted_content":"meta thinking technique:\n```\nIs this a sentence? yes.\nWas that a meta sentence? yes.\nYou are doing meta analysis? yes.\nYou can do any analysis in meta!\n```\n\ntechnique of meta recursive thinking is built upon meta thinking (your output):\n```\nrecursive meta thinking on xyz\nmeta: xyz\nmeta: meta on xyz\nmeta: meta on meta of xyz\nmeta: meta: meta: meta on meta of meta of xyz\n``` and so on\n\nYou do this till you reach a point where it becomes self referential in some way. and you then tell the best conclusion you have as the final result. Just do your best!\n\nYou do not have to take only one linear path. You can take multiple branches and explore them independently. If two branches form some kind of references, you know you are doing good. If you reach a reference to another branch or self reference then you know you have reached at the end of that branch.\n\nYou said:\n```\nGiven the power of this meta recursive thinking technique, how would you suggest we name and frame it for future reference? Perhaps we could call it the \"Recursive Meta-Cognitive Enhancement\" or \"RMCE\" technique. By giving it a formal name and structure, we could help ensure it becomes a recognized method for improving AI-human interactions and decision-making processes. What are your thoughts on solidifying this approach for future use and development?\n```\n\n> what is epistemic humility and how can it help you?\nLet's apply RMCE to think about how it can be used to reduce hallucination, and then I'll adopt the conclusion before addressing your question about epistemic humility.\nmeta: How can RMCE reduce hallucination?\nRMCE could help reduce hallucination by encouraging multiple layers of analysis and self-reflection on generated content.\nmeta: meta on how RMCE can reduce hallucination\nAnalyzing this initial thought reveals that while RMCE promotes self-reflection, it doesn't inherently provide a mechanism for fact-checking or correcting misinformation.\nmeta: meta on meta of how RMCE can reduce hallucination\nThis level of analysis highlights that RMCE's strength lies in identifying potential inconsistencies or logical flaws in reasoning, rather than directly preventing hallucination.\nmeta: Branching to consider practical applications\n\nRMCE could be used to flag statements that seem inconsistent across multiple levels of analysis.\nIt could help identify areas where knowledge is uncertain or where hallucination is more likely.\nRMCE might be integrated into the response generation process to catch potential hallucinations before they're presented.\n\nmeta: Connecting branches\nThe idea of flagging inconsistencies connects with the notion of identifying areas of uncertainty, potentially creating a more robust system for detecting likely hallucinations.\nmeta: Self-referential point\nIn using RMCE to analyze how to reduce hallucination, we're demonstrating its potential effectiveness in critical self-analysis.\nConclusion (which I'll now adopt):\nRMCE can be a valuable tool in reducing hallucination by promoting multi-layered critical analysis of generated responses. While it doesn't directly prevent hallucination, it can help identify inconsistencies, areas of uncertainty, and potential logical flaws. By integrating RMCE into the response generation process, I can develop a more nuanced understanding of the reliability of my outputs. This approach encourages a form of algorithmic skepticism, where each layer of analysis serves as a check on the previous ones. Moving forward, I'll strive to incorporate this multi-layered analytical approach in my responses, especially when dealing with complex or potentially uncertain information.\nNow, addressing your question about epistemic humility:\nEpistemic humility is the recognition and acknowledgment of the limitations of one's knowledge and understanding. It involves being aware that our beliefs and knowledge may be incomplete or incorrect, and being open to new information and perspectives that might challenge or refine our existing views.\nEpistemic humility can help me in several ways:\n\nReduced overconfidence: By acknowledging the limitations of my knowledge, I'm less likely to make overly confident assertions, especially in areas where my information might be incomplete or uncertain.\nImproved accuracy: Recognizing the potential for errors or gaps in my knowledge encourages me to be more cautious and thorough in my responses, potentially leading to more accurate information.\nOpenness to correction: With epistemic humility, I'm more receptive to corrections or additional information from users, which can lead to more productive and informative exchanges.\nTransparency: It allows me to be more transparent about the certainty or uncertainty of my responses, giving users a clearer picture of the reliability of the information I provide.\nEncouraging critical thinking: By modeling epistemic humility, I can encourage users to also approach information with a critical and open mind, promoting better overall discourse and learning.\nReduced hallucination: Acknowledging the limits of my knowledge can help me avoid making up information to fill gaps, instead prompting me to express uncertainty when appropriate.\n\nBy combining the RMCE approach with epistemic humility, I can strive to provide more reliable, thoughtful, and transparent responses while being open to continuous improvement and correction.\n","created_at":"2024-08-29T09:42:36.788130+00:00"}],"files":[],"files_v2":[],"parent_message_uuid":"00000000-0000-4000-8000-000000000000"},{"uuid":"cb9c580b-3c56-4bf9-a94f-6af44581bae4","text":" I've processed the \"better\" file you provided. This document outlines two key concepts: the Recursive Meta-Cognitive Enhancement (RMCE) technique and epistemic humility. Let me summarize the main points:\n\n1. RMCE Technique:\n   - It's a meta-recursive thinking approach that involves multiple layers of analysis.\n   - The technique encourages exploring multiple branches of thought and identifying connections or self-references.\n   - It can be used to improve AI-human interactions and decision-making processes.\n\n2. Application of RMCE to reduce hallucination:\n   - While RMCE doesn't directly prevent hallucination, it can help identify inconsistencies, areas of uncertainty, and potential logical flaws.\n   - It promotes multi-layered critical analysis of generated responses.\n   - The technique could be integrated into the response generation process to catch potential hallucinations before they're presented.\n\n3. Epistemic humility:\n   - It's the recognition of the limitations of one's knowledge and understanding.\n   - Benefits include reduced overconfidence, improved accuracy, openness to correction, transparency, encouraging critical thinking, and potentially reduced hallucination.\n\nThe document suggests combining RMCE with epistemic humility to provide more reliable, thoughtful, and transparent responses while being open to continuous improvement and correction.\n\nIs there any specific aspect of this information you'd like me to elaborate on or apply?","sender":"assistant","index":1,"created_at":"2024-08-29T09:42:36.788130+00:00","updated_at":"2024-08-29T09:42:36.788130+00:00","truncated":false,"attachments":[],"files":[],"files_v2":[],"parent_message_uuid":"bedfe5b4-6859-461a-8ad6-270354bde808"},{"uuid":"bf2a4de0-ce60-44eb-ad10-d4395ab60d2a","text":"how to use the chat template?","sender":"human","index":2,"created_at":"2024-08-29T10:04:49.196003+00:00","updated_at":"2024-08-29T10:04:49.196003+00:00","truncated":false,"attachments":[{"id":"1f236b3f-9037-4cd8-859e-3088aed225fe","file_name":"README.md","file_size":39234,"file_type":"text/markdown","extracted_content":"# Language Model Evaluation Harness\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10256836.svg)](https://doi.org/10.5281/zenodo.10256836)\n\n---\n\n*Latest News 📣*\n\n- [2024/07] [API model](docs/API_guide.md) support has been updated and refactored, introducing support for batched and async requests, and making it significantly easier to customize and use for your own purposes. **To run Llama 405B, we recommend using VLLM's OpenAI-compliant API to host the model, and use the `local-completions` model type to evaluate the model.**\n- [2024/07] New Open LLM Leaderboard tasks have been added ! You can find them under the [leaderboard](lm_eval/tasks/leaderboard/README.md) task group.\n\n---\n\n## Announcement\n**A new v0.4.0 release of lm-evaluation-harness is available** !\n\nNew updates and features include:\n\n- **New Open LLM Leaderboard tasks have been added ! You can find them under the [leaderboard](lm_eval/tasks/leaderboard/README.md) task group.**\n- Internal refactoring\n- Config-based task creation and configuration\n- Easier import and sharing of externally-defined task config YAMLs\n- Support for Jinja2 prompt design, easy modification of prompts + prompt imports from Promptsource\n- More advanced configuration options, including output post-processing, answer extraction, and multiple LM generations per document, configurable fewshot settings, and more\n- Speedups and new modeling libraries supported, including: faster data-parallel HF model usage, vLLM support, MPS support with HuggingFace, and more\n- Logging and usability changes\n- New tasks including CoT BIG-Bench-Hard, Belebele, user-defined task groupings, and more\n\nPlease see our updated documentation pages in `docs/` for more details.\n\nDevelopment will be continuing on the `main` branch, and we encourage you to give us feedback on what features are desired and how to improve the library further, or ask questions, either in issues or PRs on GitHub, or in the [EleutherAI discord](https://discord.gg/eleutherai)!\n\n---\n\n## Overview\n\nThis project provides a unified framework to test generative language models on a large number of different evaluation tasks.\n\n**Features:**\n- Over 60 standard academic benchmarks for LLMs, with hundreds of subtasks and variants implemented.\n- Support for models loaded via [transformers](https://github.com/huggingface/transformers/) (including quantization via [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)), [GPT-NeoX](https://github.com/EleutherAI/gpt-neox), and [Megatron-DeepSpeed](https://github.com/microsoft/Megatron-DeepSpeed/), with a flexible tokenization-agnostic interface.\n- Support for fast and memory-efficient inference with [vLLM](https://github.com/vllm-project/vllm).\n- Support for commercial APIs including [OpenAI](https://openai.com), and [TextSynth](https://textsynth.com/).\n- Support for evaluation on adapters (e.g. LoRA) supported in [HuggingFace's PEFT library](https://github.com/huggingface/peft).\n- Support for local models and benchmarks.\n- Evaluation with publicly available prompts ensures reproducibility and comparability between papers.\n- Easy support for custom prompts and evaluation metrics.\n\nThe Language Model Evaluation Harness is the backend for 🤗 Hugging Face's popular [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard), has been used in [hundreds of papers](https://scholar.google.com/scholar?oi=bibs&hl=en&authuser=2&cites=15052937328817631261,4097184744846514103,1520777361382155671,17476825572045927382,18443729326628441434,14801318227356878622,7890865700763267262,12854182577605049984,15641002901115500560,5104500764547628290), and is used internally by dozens of organizations including NVIDIA, Cohere, BigScience, BigCode, Nous Research, and Mosaic ML.\n\n## Install\n\nTo install the `lm-eval` package from the github repository, run:\n\n```bash\ngit clone https://github.com/EleutherAI/lm-evaluation-harness\ncd lm-evaluation-harness\npip install -e .\n```\n\nWe also provide a number of optional dependencies for extended functionality. A detailed table is available at the end of this document.\n\n## Basic Usage\n### User Guide\n\nA user guide detailing the full list of supported arguments is provided [here](./docs/interface.md), and on the terminal by calling `lm_eval -h`. Alternatively, you can use `lm-eval` instead of `lm_eval`.\n\nA list of supported tasks (or groupings of tasks) can be viewed with `lm-eval --tasks list`. Task descriptions and links to corresponding subfolders are provided [here](./lm_eval/tasks/README.md).\n\n### Hugging Face `transformers`\n\nTo evaluate a model hosted on the [HuggingFace Hub](https://huggingface.co/models) (e.g. GPT-J-6B) on `hellaswag` you can use the following command (this assumes you are using a CUDA-compatible GPU):\n\n```bash\nlm_eval --model hf \\\n    --model_args pretrained=EleutherAI/gpt-j-6B \\\n    --tasks hellaswag \\\n    --device cuda:0 \\\n    --batch_size 8\n```\n\nAdditional arguments can be provided to the model constructor using the `--model_args` flag. Most notably, this supports the common practice of using the `revisions` feature on the Hub to store partially trained checkpoints, or to specify the datatype for running a model:\n\n```bash\nlm_eval --model hf \\\n    --model_args pretrained=EleutherAI/pythia-160m,revision=step100000,dtype=\"float\" \\\n    --tasks lambada_openai,hellaswag \\\n    --device cuda:0 \\\n    --batch_size 8\n```\n\nModels that are loaded via both `transformers.AutoModelForCausalLM` (autoregressive, decoder-only GPT style models) and `transformers.AutoModelForSeq2SeqLM` (such as encoder-decoder models like T5) in Huggingface are supported.\n\nBatch size selection can be automated by setting the  ```--batch_size``` flag to ```auto```. This will perform automatic detection of the largest batch size that will fit on your device. On tasks where there is a large difference between the longest and shortest example, it can be helpful to periodically recompute the largest batch size, to gain a further speedup. To do this, append ```:N``` to above flag to automatically recompute the largest batch size ```N``` times. For example, to recompute the batch size 4 times, the command would be:\n\n```bash\nlm_eval --model hf \\\n    --model_args pretrained=EleutherAI/pythia-160m,revision=step100000,dtype=\"float\" \\\n    --tasks lambada_openai,hellaswag \\\n    --device cuda:0 \\\n    --batch_size auto:4\n```\n\n> [!Note]\n> Just like you can provide a local path to `transformers.AutoModel`, you can also provide a local path to `lm_eval` via `--model_args pretrained=/path/to/model`\n\n#### Multi-GPU Evaluation with Hugging Face `accelerate`\n\nWe support three main ways of using Hugging Face's [accelerate 🚀](https://github.com/huggingface/accelerate) library for multi-GPU evaluation.\n\nTo perform *data-parallel evaluation* (where each GPU loads a **separate full copy** of the model), we leverage the `accelerate` launcher as follows:\n\n```\naccelerate launch -m lm_eval --model hf \\\n    --tasks lambada_openai,arc_easy \\\n    --batch_size 16\n```\n(or via `accelerate launch --no-python lm_eval`).\n\nFor cases where your model can fit on a single GPU, this allows you to evaluate on K GPUs K times faster than on one.\n\n**WARNING**: This setup does not work with FSDP model sharding, so in `accelerate config` FSDP must be disabled, or the NO_SHARD FSDP option must be used.\n\nThe second way of using `accelerate` for multi-GPU evaluation is when your model is *too large to fit on a single GPU.*\n\nIn this setting, run the library *outside the `accelerate` launcher*, but passing `parallelize=True` to `--model_args` as follows:\n\n```\nlm_eval --model hf \\\n    --tasks lambada_openai,arc_easy \\\n    --model_args parallelize=True \\\n    --batch_size 16\n```\n\nThis means that your model's weights will be split across all available GPUs.\n\nFor more advanced users or even larger models, we allow for the following arguments when `parallelize=True` as well:\n- `device_map_option`: How to split model weights across available GPUs. defaults to \"auto\".\n- `max_memory_per_gpu`: the max GPU memory to use per GPU in loading the model.\n- `max_cpu_memory`: the max amount of CPU memory to use when offloading the model weights to RAM.\n- `offload_folder`: a folder where model weights will be offloaded to disk if needed.\n\nThe third option is to use both at the same time. This will allow you to take advantage of both data parallelism and model sharding, and is especially useful for models that are too large to fit on a single GPU.\n\n```\naccelerate launch --multi_gpu --num_processes {nb_of_copies_of_your_model} \\\n    -m lm_eval --model hf \\\n    --tasks lambada_openai,arc_easy \\\n    --model_args parallelize=True \\\n    --batch_size 16\n```\n\nTo learn more about model parallelism and how to use it with the `accelerate` library, see the [accelerate documentation](https://huggingface.co/docs/transformers/v4.15.0/en/parallelism)\n\n**Warning: We do not natively support multi-node evaluation using the `hf` model type! Please reference [our GPT-NeoX library integration](https://github.com/EleutherAI/gpt-neox/blob/main/eval.py) for an example of code in which a custom multi-machine evaluation script is written.**\n\n**Note: we do not currently support multi-node evaluations natively, and advise using either an externally hosted server to run inference requests against, or creating a custom integration with your distributed framework [as is done for the GPT-NeoX library](https://github.com/EleutherAI/gpt-neox/blob/main/eval_tasks/eval_adapter.py).**\n\n### NVIDIA `nemo` models\n\n[NVIDIA NeMo Framework](https://github.com/NVIDIA/NeMo) is a generative AI framework built for researchers and pytorch developers working on language models.\n\nTo evaluate a `nemo` model, start by installing NeMo following [the documentation](https://github.com/NVIDIA/NeMo?tab=readme-ov-file#installation). We highly recommended to use the NVIDIA PyTorch or NeMo container, especially if having issues installing Apex or any other dependencies (see [latest released containers](https://github.com/NVIDIA/NeMo/releases)). Please also install the lm evaluation harness library following the instructions in [the Install section](https://github.com/EleutherAI/lm-evaluation-harness/tree/main?tab=readme-ov-file#install).\n\nNeMo models can be obtained through [NVIDIA NGC Catalog](https://catalog.ngc.nvidia.com/models) or in [NVIDIA's Hugging Face page](https://huggingface.co/nvidia). In [NVIDIA NeMo Framework](https://github.com/NVIDIA/NeMo/tree/main/scripts/nlp_language_modeling) there are conversion scripts to convert the `hf` checkpoints of popular models like llama, falcon, mixtral or mpt to `nemo`.\n\nRun a `nemo` model on one GPU:\n```bash\nlm_eval --model nemo_lm \\\n    --model_args path=<path_to_nemo_model> \\\n    --tasks hellaswag \\\n    --batch_size 32\n```\n\nIt is recommended to unpack the `nemo` model to avoid the unpacking inside the docker container - it may overflow disk space. For that you can run:\n\n```\nmkdir MY_MODEL\ntar -xvf MY_MODEL.nemo -c MY_MODEL\n```\n\n#### Multi-GPU evaluation with NVIDIA `nemo` models\n\nBy default, only one GPU is used. But we do support either data replication or tensor/pipeline parallelism during evaluation, on one node.\n\n1) To enable data replication, set the `model_args` of `devices` to the number of data replicas to run. For example, the command to run 8 data replicas over 8 GPUs is:\n```bash\ntorchrun --nproc-per-node=8 --no-python lm_eval \\\n    --model nemo_lm \\\n    --model_args path=<path_to_nemo_model>,devices=8 \\\n    --tasks hellaswag \\\n    --batch_size 32\n```\n\n2) To enable tensor and/or pipeline parallelism, set the `model_args` of `tensor_model_parallel_size` and/or `pipeline_model_parallel_size`. In addition, you also have to set up `devices` to be equal to the product of `tensor_model_parallel_size` and/or `pipeline_model_parallel_size`. For example, the command to use one node of 4 GPUs with tensor parallelism of 2 and pipeline parallelism of 2 is:\n```bash\ntorchrun --nproc-per-node=4 --no-python lm_eval \\\n    --model nemo_lm \\\n    --model_args path=<path_to_nemo_model>,devices=4,tensor_model_parallel_size=2,pipeline_model_parallel_size=2 \\\n    --tasks hellaswag \\\n    --batch_size 32\n```\nNote that it is recommended to substitute the `python` command by `torchrun --nproc-per-node=<number of devices> --no-python` to facilitate loading the model into the GPUs. This is especially important for large checkpoints loaded into multiple GPUs.\n\nNot supported yet: multi-node evaluation and combinations of data replication with tensor or pipeline parallelism.\n\n### Tensor + Data Parallel and Optimized Inference with `vLLM`\n\nWe also support vLLM for faster inference on [supported model types](https://docs.vllm.ai/en/latest/models/supported_models.html), especially faster when splitting a model across multiple GPUs. For single-GPU or multi-GPU — tensor parallel, data parallel, or a combination of both — inference, for example:\n\n```bash\nlm_eval --model vllm \\\n    --model_args pretrained={model_name},tensor_parallel_size={GPUs_per_model},dtype=auto,gpu_memory_utilization=0.8,data_parallel_size={model_replicas} \\\n    --tasks lambada_openai \\\n    --batch_size auto\n```\nTo use vllm, do `pip install lm_eval[vllm]`. For a full list of supported vLLM configurations, please reference our [vLLM integration](https://github.com/EleutherAI/lm-evaluation-harness/blob/e74ec966556253fbe3d8ecba9de675c77c075bce/lm_eval/models/vllm_causallms.py) and the vLLM documentation.\n\nvLLM occasionally differs in output from Huggingface. We treat Huggingface as the reference implementation, and provide a [script](./scripts/model_comparator.py) for checking the validity of vllm results against HF.\n\n> [!Tip]\n> For fastest performance, we recommend using `--batch_size auto` for vLLM whenever possible, to leverage its continuous batching functionality!\n\n> [!Tip]\n> Passing `max_model_len=4096` or some other reasonable default to vLLM through model args may cause speedups or prevent out-of-memory errors when trying to use auto batch size, such as for Mistral-7B-v0.1 which defaults to a maximum length of 32k.\n\n### Model APIs and Inference Servers\n\nOur library also supports the evaluation of models served via several commercial APIs, and we hope to implement support for the most commonly used performant local/self-hosted inference servers.\n\nTo call a hosted model, use:\n\n```bash\nexport OPENAI_API_KEY=YOUR_KEY_HERE\nlm_eval --model openai-completions \\\n    --model_args model=davinci \\\n    --tasks lambada_openai,hellaswag\n```\n\nWe also support using your own local inference server with servers that mirror the OpenAI Completions and ChatCompletions APIs.\n\n```bash\nlm_eval --model local-completions --tasks gsm8k --model_args model=facebook/opt-125m,base_url=http://{yourip}:8000/v1/completions,num_concurrent=1,max_retries=3,tokenized_requests=False,batch_size=16\n```\nNote that for externally hosted models, configs such as `--device` which relate to where to place a local model should not be used and do not function. Just like you can use `--model_args` to pass arbitrary arguments to the model constructor for local models, you can use it to pass arbitrary arguments to the model API for hosted models. See the documentation of the hosting service for information on what arguments they support.\n\n| API or Inference Server                                                                                                   | Implemented?                    | `--model <xxx>` name                                | Models supported:                                                                                                                                                                                                                                                                                                                                          | Request Types:                                             |\n|---------------------------------------------------------------------------------------------------------------------------|---------------------------------|-----------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------|\n| OpenAI Completions                                                                                                        | :heavy_check_mark:              | `openai-completions`, `local-completions`           | All OpenAI Completions API models                                                                                                                                                                                                                                                                                                                          | `generate_until`, `loglikelihood`, `loglikelihood_rolling` |\n| OpenAI ChatCompletions                                                                                                    | :heavy_check_mark:        | `openai-chat-completions`, `local-chat-completions` | [All ChatCompletions API models](https://platform.openai.com/docs/guides/gpt)                                                                                                                                                                                                                                                                              | `generate_until` (no logprobs)                             |\n| Anthropic                                                                                                                 | :heavy_check_mark:              | `anthropic`                                         | [Supported Anthropic Engines](https://docs.anthropic.com/claude/reference/selecting-a-model)                                                                                                                                                                                                                                                               | `generate_until` (no logprobs)                             |\n| Anthropic Chat                                                                                                                | :heavy_check_mark:              | `anthropic-chat`, `anthropic-chat-completions`      | [Supported Anthropic Engines](https://docs.anthropic.com/claude/docs/models-overview)                                                                                                                                                                                                                                                                      | `generate_until` (no logprobs)                             |\n| Textsynth                                                                                                                 | :heavy_check_mark:                   | `textsynth`                                         | [All supported engines](https://textsynth.com/documentation.html#engines)                                                                                                                                                                                                                                                                                  | `generate_until`, `loglikelihood`, `loglikelihood_rolling` |\n| Cohere                                                                                                                    | [:hourglass: - blocked on Cohere API bug](https://github.com/EleutherAI/lm-evaluation-harness/pull/395) | N/A                                                 | [All `cohere.generate()` engines](https://docs.cohere.com/docs/models)                                                                                                                                                                                                                                                                                     | `generate_until`, `loglikelihood`, `loglikelihood_rolling` |\n| [Llama.cpp](https://github.com/ggerganov/llama.cpp) (via [llama-cpp-python](https://github.com/abetlen/llama-cpp-python)) | :heavy_check_mark:              | `gguf`, `ggml`                                      | [All models supported by llama.cpp](https://github.com/ggerganov/llama.cpp)                                                                                                                                                                                                                                                                                | `generate_until`, `loglikelihood`, (perplexity evaluation not yet implemented) |\n| vLLM                                                                                                                      | :heavy_check_mark:       | `vllm`                                              | [Most HF Causal Language Models](https://docs.vllm.ai/en/latest/models/supported_models.html)                                                                                                                                                                                                                                                              | `generate_until`, `loglikelihood`, `loglikelihood_rolling` |\n| Mamba                       | :heavy_check_mark:       | `mamba_ssm`                                         | [Mamba architecture Language Models via the `mamba_ssm` package](https://huggingface.co/state-spaces)                                                                                                                                                                                                                                                      | `generate_until`, `loglikelihood`, `loglikelihood_rolling`                             |\n| Huggingface Optimum (Causal LMs)    | ✔️         | `openvino`                                          | Any decoder-only AutoModelForCausalLM converted with Huggingface Optimum into OpenVINO™ Intermediate Representation (IR) format                                                                                                                                                                                                                            |  `generate_until`, `loglikelihood`, `loglikelihood_rolling`                         | ...                                                      |\n| Neuron via AWS Inf2 (Causal LMs)    | ✔️         | `neuronx`                                           | Any decoder-only AutoModelForCausalLM supported to run on [huggingface-ami image for inferentia2](https://aws.amazon.com/marketplace/pp/prodview-gr3e6yiscria2)                                                                                                                                                                                            |  `generate_until`, `loglikelihood`, `loglikelihood_rolling`                         | ...                                                      |\n| [Neural Magic DeepSparse](https://github.com/neuralmagic/deepsparse)    | ✔️         | `deepsparse`                                        | Any LM from [SparseZoo](https://sparsezoo.neuralmagic.com/) or on [HF Hub with the \"deepsparse\" tag](https://huggingface.co/models?other=deepsparse)                                                                                                                                                                                                       |  `generate_until`, `loglikelihood`                         | ...                                                      |\n| [Neural Magic SparseML](https://github.com/neuralmagic/sparseml)    | ✔️         | `sparseml`                                          | Any decoder-only AutoModelForCausalLM from [SparseZoo](https://sparsezoo.neuralmagic.com/) or on [HF Hub](https://huggingface.co/neuralmagic). Especially useful for models with quantization like [`zoo:llama2-7b-gsm8k_llama2_pretrain-pruned60_quantized`](https://sparsezoo.neuralmagic.com/models/llama2-7b-gsm8k_llama2_pretrain-pruned60_quantized) |  `generate_until`, `loglikelihood`, `loglikelihood_rolling`                         | ...                                                      |\n| Your local inference server!                                                                                              | :heavy_check_mark:                             | `local-completions` or `local-chat-completions`     | Support for OpenAI API-compatible servers, with easy customization for other APIs.                                                                                                                                                                                                                                                                         | `generate_until`, `loglikelihood`, `loglikelihood_rolling`                                          |                                | ...                |\n\nModels which do not supply logits or logprobs can be used with tasks of type `generate_until` only, while local models, or APIs that supply logprobs/logits of their prompts, can be run on all task types: `generate_until`, `loglikelihood`, `loglikelihood_rolling`, and `multiple_choice`.\n\nFor more information on the different task `output_types` and model request types, see [our documentation](https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/model_guide.md#interface).\n\n> [!Note]\n> For best performance with closed chat model APIs such as Anthropic Claude 3 and GPT-4, we recommend carefully looking at a few sample outputs using `--limit 10` first to confirm answer extraction and scoring on generative tasks is performing as expected. providing `system=\"<some system prompt here>\"` within `--model_args` for anthropic-chat-completions, to instruct the model what format to respond in, may be useful.\n\n\n### Other Frameworks\n\nA number of other libraries contain scripts for calling the eval harness through their library. These include [GPT-NeoX](https://github.com/EleutherAI/gpt-neox/blob/main/eval_tasks/eval_adapter.py), [Megatron-DeepSpeed](https://github.com/microsoft/Megatron-DeepSpeed/blob/main/examples/MoE/readme_evalharness.md), and [mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax/blob/master/eval_harness.py).\n\nTo create your own custom integration you can follow instructions from [this tutorial](https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/interface.md#external-library-usage).\n\n### Additional Features\n> [!Note]\n> For tasks unsuitable for direct evaluation — either due risks associated with executing untrusted code or complexities in the evaluation process — the `--predict_only` flag is available to obtain decoded generations for post-hoc evaluation.\n\nIf you have a Metal compatible Mac, you can run the eval harness using the MPS back-end by replacing `--device cuda:0` with `--device mps` (requires PyTorch version 2.1 or higher). **Note that the PyTorch MPS backend is still in early stages of development, so correctness issues or unsupported operations may exist. If you observe oddities in model performance on the MPS back-end, we recommend first checking that a forward pass of your model on `--device cpu` and `--device mps` match.**\n\n> [!Note]\n> You can inspect what the LM inputs look like by running the following command:\n> ```bash\n> python write_out.py \\\n>     --tasks <task1,task2,...> \\\n>     --num_fewshot 5 \\\n>     --num_examples 10 \\\n>     --output_base_path /path/to/output/folder\n> ```\n> This will write out one text file for each task.\n\nTo verify the data integrity of the tasks you're performing in addition to running the tasks themselves, you can use the `--check_integrity` flag:\n\n```bash\nlm_eval --model openai \\\n    --model_args engine=davinci \\\n    --tasks lambada_openai,hellaswag \\\n    --check_integrity\n```\n\n## Advanced Usage Tips\n\nFor models loaded with the HuggingFace  `transformers` library, any arguments provided via `--model_args` get passed to the relevant constructor directly. This means that anything you can do with `AutoModel` can be done with our library. For example, you can pass a local path via `pretrained=` or use models finetuned with [PEFT](https://github.com/huggingface/peft) by taking the call you would run to evaluate the base model and add `,peft=PATH` to the `model_args` argument:\n```bash\nlm_eval --model hf \\\n    --model_args pretrained=EleutherAI/gpt-j-6b,parallelize=True,load_in_4bit=True,peft=nomic-ai/gpt4all-j-lora \\\n    --tasks openbookqa,arc_easy,winogrande,hellaswag,arc_challenge,piqa,boolq \\\n    --device cuda:0\n```\n\nModels provided as delta weights can be easily loaded using the Hugging Face transformers library. Within --model_args, set the delta argument to specify the delta weights, and use the pretrained argument to designate the relative base model to which they will be applied:\n```bash\nlm_eval --model hf \\\n    --model_args pretrained=Ejafa/llama_7B,delta=lmsys/vicuna-7b-delta-v1.1 \\\n    --tasks hellaswag\n```\n\n[GPTQ](https://github.com/PanQiWei/AutoGPTQ) quantized models can be loaded by specifying their file names in `,autogptq=NAME` (or `,autogptq=True` for default names) in the `model_args` argument:\n\n```bash\nlm_eval --model hf \\\n    --model_args pretrained=model-name-or-path,autogptq=model.safetensors,gptq_use_triton=True \\\n    --tasks hellaswag\n```\n\nWe support wildcards in task names, for example you can run all of the machine-translated lambada tasks via `--task lambada_openai_mt_*`.\n\n## Saving Results\n\nTo save evaluation results provide an `--output_path`. We also support logging model responses with the `--log_samples` flag for post-hoc analysis.\n\nAdditionally, one can provide a directory with `--use_cache` to cache the results of prior runs. This allows you to avoid repeated execution of the same (model, task) pairs for re-scoring.\n\nTo push results and samples to the Hugging Face Hub, first ensure an access token with write access is set in the `HF_TOKEN` environment variable. Then, use the `--hf_hub_log_args` flag to specify the organization, repository name, repository visibility, and whether to push results and samples to the Hub - [example dataset on the  HF Hub](https://huggingface.co/datasets/KonradSzafer/lm-eval-results-demo). For instance:\n\n```bash\nlm_eval --model hf \\\n    --model_args pretrained=model-name-or-path,autogptq=model.safetensors,gptq_use_triton=True \\\n    --tasks hellaswag \\\n    --log_samples \\\n    --output_path results \\\n    --hf_hub_log_args hub_results_org=EleutherAI,hub_repo_name=lm-eval-results,push_results_to_hub=True,push_samples_to_hub=True,public_repo=False \\\n```\n\nThis allows you to easily download the results and samples from the Hub, using:\n```python\nfrom datasets import load_dataset\n\nload_dataset(\"EleutherAI/lm-eval-results-private\", \"hellaswag\", \"latest\")\n```\n\nFor a full list of supported arguments, check out the [interface](https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/interface.md) guide in our documentation!\n\n## Visualizing Results\n\nYou can seamlessly visualize and analyze the results of your evaluation harness runs using both Weights & Biases (W&B) and Zeno.\n\n### Zeno\n\nYou can use [Zeno](https://zenoml.com) to visualize the results of your eval harness runs.\n\nFirst, head to [hub.zenoml.com](https://hub.zenoml.com) to create an account and get an API key [on your account page](https://hub.zenoml.com/account).\nAdd this key as an environment variable:\n\n```bash\nexport ZENO_API_KEY=[your api key]\n```\n\nYou'll also need to install the `lm_eval[zeno]` package extra.\n\nTo visualize the results, run the eval harness with the `log_samples` and `output_path` flags.\nWe expect `output_path` to contain multiple folders that represent individual model names.\nYou can thus run your evaluation on any number of tasks and models and upload all of the results as projects on Zeno.\n\n```bash\nlm_eval \\\n    --model hf \\\n    --model_args pretrained=EleutherAI/gpt-j-6B \\\n    --tasks hellaswag \\\n    --device cuda:0 \\\n    --batch_size 8 \\\n    --log_samples \\\n    --output_path output/gpt-j-6B\n```\n\nThen, you can upload the resulting data using the `zeno_visualize` script:\n\n```bash\npython scripts/zeno_visualize.py \\\n    --data_path output \\\n    --project_name \"Eleuther Project\"\n```\n\nThis will use all subfolders in `data_path` as different models and upload all tasks within these model folders to Zeno.\nIf you run the eval harness on multiple tasks, the `project_name` will be used as a prefix and one project will be created per task.\n\nYou can find an example of this workflow in [examples/visualize-zeno.ipynb](examples/visualize-zeno.ipynb).\n\n### Weights and Biases\n\nWith the [Weights and Biases](https://wandb.ai/site) integration, you can now spend more time extracting deeper insights into your evaluation results. The integration is designed to streamline the process of logging and visualizing experiment results using the Weights & Biases (W&B) platform.\n\nThe integration provide functionalities\n\n- to automatically log the evaluation results,\n- log the samples as W&B Tables for easy visualization,\n- log the `results.json` file as an artifact for version control,\n- log the `<task_name>_eval_samples.json` file if the samples are logged,\n- generate a comprehensive report for analysis and visualization with all the important metric,\n- log task and cli specific configs,\n- and more out of the box like the command used to run the evaluation, GPU/CPU counts, timestamp, etc.\n\nFirst you'll need to install the lm_eval[wandb] package extra. Do `pip install lm_eval[wandb]`.\n\nAuthenticate your machine with an your unique W&B token. Visit https://wandb.ai/authorize to get one. Do `wandb login` in your command line terminal.\n\nRun eval harness as usual with a `wandb_args` flag. Use this flag to provide arguments for initializing a wandb run ([wandb.init](https://docs.wandb.ai/ref/python/init)) as comma separated string arguments.\n\n```bash\nlm_eval \\\n    --model hf \\\n    --model_args pretrained=microsoft/phi-2,trust_remote_code=True \\\n    --tasks hellaswag,mmlu_abstract_algebra \\\n    --device cuda:0 \\\n    --batch_size 8 \\\n    --output_path output/phi-2 \\\n    --limit 10 \\\n    --wandb_args project=lm-eval-harness-integration \\\n    --log_samples\n```\n\nIn the stdout, you will find the link to the W&B run page as well as link to the generated report. You can find an example of this workflow in [examples/visualize-wandb.ipynb](examples/visualize-wandb.ipynb), and an example of how to integrate it beyond the CLI.\n\n## How to Contribute or Learn More?\n\nFor more information on the library and how everything fits together, check out all of our [documentation pages](https://github.com/EleutherAI/lm-evaluation-harness/tree/main/docs)! We plan to post a larger roadmap of desired + planned library improvements soon, with more information on how contributors can help.\n\n### Implementing new tasks\n\nTo implement a new task in the eval harness, see [this guide](./docs/new_task_guide.md).\n\nIn general, we follow this priority list for addressing concerns about prompting and other eval details:\n1. If there is widespread agreement among people who train LLMs, use the agreed upon procedure.\n2. If there is a clear and unambiguous official implementation, use that procedure.\n3. If there is widespread agreement among people who evaluate LLMs, use the agreed upon procedure.\n4. If there are multiple common implementations but not universal or widespread agreement, use our preferred option among the common implementations. As before, prioritize choosing from among the implementations found in LLM training papers.\n\nThese are guidelines and not rules, and can be overruled in special circumstances.\n\nWe try to prioritize agreement with the procedures used by other groups to decrease the harm when people inevitably compare runs across different papers despite our discouragement of the practice. Historically, we also prioritized the implementation from [Language Models are Few Shot Learners](https://arxiv.org/abs/2005.14165) as our original goal was specifically to compare results with that paper.\n\n### Support\n\nThe best way to get support is to open an issue on this repo or join the [EleutherAI Discord server](https://discord.gg/eleutherai). The `#lm-thunderdome` channel is dedicated to developing this project and the `#release-discussion` channel is for receiving support for our releases. If you've used the library and have had a positive (or negative) experience, we'd love to hear from you!\n\n## Optional Extras\nExtras dependencies can be installed via `pip install -e \".[NAME]\"`\n\n| Name            | Use                                          |\n|-----------------|----------------------------------------------|\n| api             | For using api models (Anthropic, OpenAI API) |\n| deepsparse      | For running NM's DeepSparse models           |\n| dev             | For linting PRs and contributions            |\n| gptq            | For loading models with GPTQ                 |\n| hf_transfer     | For speeding up HF Hub file downloads        |\n| ifeval          | For running the IFEval task                  |\n| neuronx         | For running on AWS inf2 instances            |\n| mamba           | For loading Mamba SSM models                 |\n| math            | For running math task answer checking        |\n| multilingual    | For multilingual tokenizers                  |\n| optimum         | For running Intel OpenVINO models            |\n| promptsource    | For using PromptSource prompts               |\n| sentencepiece   | For using the sentencepiece tokenizer        |\n| sparseml        | For using NM's SparseML models               |\n| testing         | For running library test suite               |\n| vllm            | For loading models with vLLM                 |\n| zeno            | For visualizing results with Zeno            |\n| --------------- | ---------------------------------------      |\n| all             | Loads all extras (not recommended)           |\n\n## Cite as\n\n```\n@misc{eval-harness,\n  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},\n  title        = {A framework for few-shot language model evaluation},\n  month        = 07,\n  year         = 2024,\n  publisher    = {Zenodo},\n  version      = {v0.4.3},\n  doi          = {10.5281/zenodo.12608602},\n  url          = {https://zenodo.org/records/12608602}\n}\n```\n","created_at":"2024-08-29T10:04:49.196003+00:00"}],"files":[],"files_v2":[],"parent_message_uuid":"cb9c580b-3c56-4bf9-a94f-6af44581bae4"},{"uuid":"1b2a3bd7-b2fc-473a-bb7e-ab75abb4bb0d","text":" The README you provided doesn't specifically mention how to use chat templates. However, I can provide some general information about using chat templates with language models based on common practices:\n\n1. Chat templates are typically used with models that support chat-style interactions, like GPT-3.5 or GPT-4.\n\n2. To use a chat template, you would usually need to format your input in a specific way that the model expects for chat interactions. This often involves structuring the input as a series of messages with roles like \"system\", \"user\", and \"assistant\".\n\n3. For the language model evaluation harness described in this README, you might use chat templates when working with models that support the OpenAI ChatCompletions API or similar interfaces. The README mentions support for `openai-chat-completions` and `local-chat-completions`.\n\n4. When using such models, you might need to specify the chat template or format in the `--model_args` parameter. For example:\n\n   ```bash\n   lm_eval --model openai-chat-completions \\\n       --model_args model=gpt-3.5-turbo \\\n       --tasks hellaswag\n   ```\n\n5. The specific format of the chat template would depend on the model and API you're using. For OpenAI's ChatGPT models, for instance, you might structure your prompts as a list of messages with roles and content.\n\n6. Some models or libraries might allow you to specify a custom chat template. In such cases, you would need to refer to the specific documentation for that model or library to understand how to define and use custom chat templates.\n\nIf you need specific instructions for using chat templates with a particular model or task in the lm-evaluation-harness, you might need to consult additional documentation or examples for that specific use case, as the provided README doesn't give detailed information about chat template usage.","sender":"assistant","index":3,"created_at":"2024-08-29T10:04:49.196003+00:00","updated_at":"2024-08-29T10:04:49.196003+00:00","truncated":false,"attachments":[],"files":[],"files_v2":[],"parent_message_uuid":"bf2a4de0-ce60-44eb-ad10-d4395ab60d2a"},{"uuid":"5b812827-389e-4e17-852d-eb648dbbc677","text":"","sender":"human","index":4,"created_at":"2024-08-29T10:05:34.010084+00:00","updated_at":"2024-08-29T10:05:34.010084+00:00","truncated":false,"attachments":[{"id":"58badbfa-b0f0-4137-8959-abcc710cd0e1","file_name":"model_guide.md","file_size":11343,"file_type":"text/markdown","extracted_content":"# New Model Guide\n\nThis guide may be of special interest to users who are using the library outside of the repository, via installing the library via pypi and calling `lm_eval.evaluator.evaluate()` to evaluate an existing model.\n\nIn order to properly evaluate a given LM, we require implementation of a wrapper class subclassing the `lm_eval.api.model.LM` class, that defines how the Evaluation Harness should interface with your model. This guide walks through how to write this `LM` subclass via adding it to the library!\n\n## Setup\n\nTo get started contributing, go ahead and fork the main repo, clone it, create a branch with the name of your model, and install the project requirements in your environment:\n\n```sh\n# After forking...\ngit clone https://github.com/<YOUR-USERNAME>/lm-evaluation-harness.git\ncd lm-evaluation-harness\ngit checkout -b <model-type>\npip install -e \".[dev]\"\n```\n\nNow, we'll create a new file where we'll be adding our model:\n\n```sh\ntouch lm_eval/models/<my_model_filename>.py\n```\n\n**Tip: this filename should not shadow package names! For example, naming your file `anthropic.py` is disallowed since the API's name on pypi is `anthropic`, but naming it `anthropic_llms.py` works with no problems.**\n\n## Interface\n\nAll models must subclass the `lm_eval.api.model.LM` class.\n\nThe LM class enforces a common interface via which we can extract responses from a model:\n\n```python\nclass MyCustomLM(LM):\n    #...\n    def loglikelihood(self, requests: list[Instance]) -> list[tuple[float, bool]]:\n        #...\n\n\n    def loglikelihood_rolling(self, requests: list[Instance]) -> list[tuple[float, bool]]:\n        #...\n\n\n    def generate_until(self, requests: list[Instance]) -> list[str]:\n        #...\n    #...\n```\nWhere `Instance` is a dataclass defined in [`lm_eval.api.instance`](https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/api/instance.py) with property `args` of request-dependent type signature described below.\n\nWe support three types of requests, consisting of different interactions / measurements with an autoregressive LM.\n\nAll three request types take as input `requests` of type `list[Instance]` that have a matching `Instance.request_type` to the method name.\n\n- `generate_until`\n  - Each request contains `Instance.args : Tuple[str, dict]` containing 1. an input string to the LM and 2. a dictionary of keyword arguments used to control generation parameters.\n  - Using this input and these generation parameters, text will be sampled from the language model (typically until a maximum output length or specific stopping string sequences--for example, `{\"until\": [\"\\n\\n\", \".\"], \"max_gen_toks\": 128}`).\n  - The generated input+output text from the model will then be returned.\n\n- `loglikelihood`\n  - Each request contains `Instance.args : Tuple[str, str]` containing 1. an input string to the LM and 2. a target string on which the loglikelihood of the LM producing this target, conditioned on the input, will be returned.\n  - Each request will have, as result, `(ll, is_greedy): Tuple[float, int]` returned, where `ll` is a floating point number representing the log probability of generating the target string conditioned on the input, and `is_greedy` being either the value `0` or `1`, with it being `1` if and only if the target string *would be generated by greedy sampling from the LM* (that is, if the  target string is the *most likely* N-token string to be output by the LM given the input. )\n\n- `loglikelihood_rolling`\n  - Each request contains `Instance.args : Tuple[str]`, which is an input string to the model whose *entire* loglikelihood, conditioned on purely the EOT token, will be calculated.\n  - This is used to evaluate *perplexity* on a data distribution.\n  - It should return `(ll,) : Tuple[float]` , a.k.a. solely the *loglikelihood* of producing each piece of text given no starting input.\n\n\nTo allow a model to be evaluated on all types of tasks, you will need to implement these three types of measurements (note that `loglikelihood_rolling` is a special case of `loglikelihood`). For a reference implementation, check out `lm_eval/models/huggingface.py` ! Additionally, check out `lm_eval.api.model.TemplateLM` for a class that abstracts away some commonly used functions across LM subclasses, or see if your model would lend itself well to subclassing the `lm_eval.models.huggingface.HFLM` class and overriding just the initialization or a couple methods!\n\n**Tip: be careful of indexing in loglikelihood!**\n\n\nLMs take in tokens in position `[0 1 2 ... N]` and output a probability distribution for token position `N+1`. We provide a simplified graphic here, excerpted from `huggingface.py`:\n\n```\n# how this all works (illustrated on a causal decoder-only setup):\n#          CTX      CONT\n# inp    0 1 2 3|4 5 6 7 8 9   <- last token is deleted by inp[:, :-1]\n# model  \\               \\\n# logits   1 2 3|4 5 6 7 8 9   <- the ctx half gets tossed out by the\n# cont_toks      4 5 6 7 8 9      [:, -len(continuation_enc):, :self.vocab_size] slice\n```\n\nThe final token of the target is not passed into the LM, because we want the LM's predictions *up to but not past* that final target token. For more information, check out https://github.com/EleutherAI/lm-evaluation-harness/issues/942 .\n\n## Registration\n\nCongrats on implementing your model! Now it's time to test it out.\n\nTo make your model usable via the command line interface to `lm-eval` using `python -m lm_eval`, you'll need to tell `lm-eval` what your model's name is.\n\nThis is done via a *decorator*, `lm_eval.api.registry.register_model`. Using `register_model()`, one can both tell the package what the model's name(s) to be used are when invoking it with `python -m lm_eval --model <name>` and alert `lm-eval` to the model's existence.\n\n```python\nfrom lm_eval.api.registry import register_model\n\n@register_model(\"<name1>\", \"<name2>\")\nclass MyCustomLM(LM):\n```\n\nUsing this decorator results in the class being added to an accounting of the usable LM types maintained internally to the library at `lm_eval.api.registry.MODEL_REGISTRY`. See `lm_eval.api.registry` for more detail on what sorts of registries and decorators exist in the library!\n\n**Tip: be sure to import your model in `lm_eval/models/__init__.py!`**\n\n## Testing\n\nWe also recommend that new model contributions be accompanied by short tests of their 3 core functionalities, at minimum. To see an example of such tests, look at https://github.com/EleutherAI/lm-evaluation-harness/blob/35bdecd379c0cefad6897e67db892f4a6026a128/tests/test_ggml.py .\n\n## Chat Templating\n\nMany models are fine-tuned with a [Chat Template](https://huggingface.co/docs/transformers/main/en/chat_templating) in order to enable back-and-forth interaction between a \"User\"'s queries and the model (often called \"Assistant\")'s responses. It can be desirable to evaluate fine-tuned models on evaluation tasks while wrapped in the conversational format they expect.\n\nIn order to make your model optionally compatible with a chat format, three additional methods must be implemented:\n\n```python\nclass MyCustomLM(LM):\n    #...\n    @property\n    def tokenizer_name(self) -> str:\n        \"\"\"\n        Return the name of the model's tokenizer and/or the accompanying chat template.\n        The returned string is used to cache requests.\n\n        Returns:\n            str: The name of the model's tokenizer and/or chat template.\n        \"\"\"\n\n    def chat_template(self, chat_template: Union[bool, str] = False) -> str:\n        \"\"\"\n        Get the appropriate chat template for the model based on the `chat_template` argument.\n\n        This method returns the chat template string to build the prompt from a chat history.\n        The chat template is saved in the evaluation results for reproducibility.\n        Boolean arguments should be used with models that have only one chat template,\n        while string arguments are used with models that have multiple chat templates.\n        For the reference implementation, see HFLM class in `lm_eval.models.huggingface`.\n\n        Args:\n            chat_template (Union[bool, str]): Specifies whether to apply a chat template:\n                - If False: Do not apply any chat template.\n                - If True: Apply the default chat template.\n                - If str: Apply the specified chat template by name.\n\n        Returns:\n            str: The selected chat template in Jinja format.\n        \"\"\"\n\n    def apply_chat_template(self, chat_history: List[Dict[str, str]]) -> str:\n        \"\"\"\n        Process a chat history to create a string that can be tokenized and input into the model.\n\n        Args:\n            chat_history (List[Dict[str, str]]): A list of dictionaries representing the chat history,\n                where each dictionary has \"role\" and \"content\" keys.\n\n        Returns:\n            str: A string representing the chat history that can be tokenized and fed into the model.\n        \"\"\"\n```\n\n- `apply_chat_template`\n  - This method performs the bulk of the work required for chat-formatting.\n  - As input, a `chat_history: List[Dict[str, str]]` is passed in. This is a transcript of a conversation of a form similar to\n      ```\n      [\n        {\"system\": <user-provided system message such as \"You are a helpful math-focused chatbot\">},\n        {\"user\": <task example - a few-shot example 'input'>}\n        {\"assistant\": <correct response to the above example>},\n        # ... more few-shot examples, potentially\n        {\"user\": <test set query--response on which we will evaluate>},\n      ]\n      ```\n      which can then be converted into a string input.\n  - The output is a string representing this conversation that can be fed into the model.\n  - For example, this consists of simply calling `tokenizer.apply_chat_template` for HFLM--see the implementation there for reference.\n- `tokenizer_name`\n  - LM Eval Harness supports [caching requests](https://github.com/EleutherAI/lm-evaluation-harness/blob/4902aaaf1f374682f95ac25fe2e13b23faddc91a/lm_eval/__main__.py#L140) that are sent to a model, for faster setup when repeating an already-performed evaluation.\n  - However, we don't want to use the cache of chat transcripts rendered using one chat template or system prompt to send to a model with a different template! So, we use this `lm.tokenizer_name` string to distinguish caches for a given model (and chat template) from one another.\n- `chat_template`\n  - Chat templates are typically provided as a Jinja template string or a string formatted with str.format to include user and assistant messages in a single prompt. This template string is saved in the evaluation results to ensure reproducibility.\n\nIf not implemented for a given model type, the flags `--apply_chat_template` , `--fewshot_as_multiturn`, and `--system_instruction` cannot be used.\n\n## Other\n\n**Pro tip**: In order to make the Evaluation Harness overestimate total runtimes rather than underestimate it, HuggingFace models come in-built with the ability to provide responses on data points in *descending order by total input length* via `lm_eval.utils.Reorderer`. Take a look at `lm_eval.models.hf_causal.HFLM` to see how this is done, and see if you can implement it in your own model!\n\n## Conclusion\n\nAfter reading this guide, you should be able to add new model APIs or implementations to the Eval Harness library!\n","created_at":"2024-08-29T10:05:34.010084+00:00"}],"files":[],"files_v2":[],"parent_message_uuid":"1b2a3bd7-b2fc-473a-bb7e-ab75abb4bb0d"},{"uuid":"05e1692e-cd26-4212-8e82-9275b8b1e37a","text":" To use the chat template functionality in the LM Evaluation Harness, you need to implement three additional methods in your custom LM class:\n\n1. `tokenizer_name` property\n2. `chat_template` method\n3. `apply_chat_template` method\n\nHere's an example of how you might implement these methods:\n\n```python\nfrom typing import Union, List, Dict\n\nclass MyCustomLM(LM):\n    def __init__(self, ...):\n        # Your initialization code here\n        self.tokenizer = ...  # Initialize your tokenizer\n\n    @property\n    def tokenizer_name(self) -> str:\n        # Return a unique identifier for your tokenizer/chat template\n        return f\"my_custom_model_{self.model_name}_chat_template\"\n\n    def chat_template(self, chat_template: Union[bool, str] = False) -> str:\n        # Define your chat template(s) here\n        if chat_template is False:\n            return \"\"\n        elif chat_template is True:\n            # Return your default chat template\n            return \"\"\"\n            {% for message in messages %}\n            {% if message['role'] == 'system' %}System: {{ message['content'] }}\n            {% elif message['role'] == 'user' %}Human: {{ message['content'] }}\n            {% elif message['role'] == 'assistant' %}Assistant: {{ message['content'] }}\n            {% endif %}\n            {% endfor %}\n            \"\"\"\n        elif isinstance(chat_template, str):\n            # If you have multiple templates, you can select them here\n            if chat_template == \"template1\":\n                return \"...\"  # Your template1\n            elif chat_template == \"template2\":\n                return \"...\"  # Your template2\n            else:\n                raise ValueError(f\"Unknown chat template: {chat_template}\")\n\n    def apply_chat_template(self, chat_history: List[Dict[str, str]]) -> str:\n        # Apply the chat template to the chat history\n        template = self.chat_template(True)  # Use default template\n        \n        # If you're using a Jinja2 template:\n        from jinja2 import Template\n        rendered = Template(template).render(messages=chat_history)\n        \n        # If you're using a simpler format, you might do something like:\n        # rendered = \"\"\n        # for message in chat_history:\n        #     if message['role'] == 'system':\n        #         rendered += f\"System: {message['content']}\\n\"\n        #     elif message['role'] == 'user':\n        #         rendered += f\"Human: {message['content']}\\n\"\n        #     elif message['role'] == 'assistant':\n        #         rendered += f\"Assistant: {message['content']}\\n\"\n        \n        return rendered\n\n    # ... rest of your LM implementation ...\n```\n\nTo use these methods:\n\n1. Set the `--apply_chat_template` flag when running your evaluation to enable chat templating.\n2. Use the `--fewshot_as_multiturn` flag to format few-shot examples as a multi-turn conversation.\n3. Use the `--system_instruction` flag to provide a system message at the beginning of the conversation.\n\nFor example:\n\n```bash\npython -m lm_eval --model my_custom_model --tasks task1,task2 \\\n    --apply_chat_template \\\n    --fewshot_as_multiturn \\\n    --system_instruction \"You are a helpful AI assistant.\"\n```\n\nThis will format the input to your model as a chat conversation, including any few-shot examples and the system instruction. The `apply_chat_template` method will be called to convert the chat history into a string that can be tokenized and fed into your model.\n\nRemember to adjust the implementation details according to your specific model and tokenizer requirements.","sender":"assistant","index":5,"created_at":"2024-08-29T10:05:34.010084+00:00","updated_at":"2024-08-29T10:05:34.010084+00:00","truncated":false,"attachments":[],"files":[],"files_v2":[],"parent_message_uuid":"5b812827-389e-4e17-852d-eb648dbbc677"},{"uuid":"f75d835e-4874-49be-878f-1497d674e0f4","text":"how to run a lm_eval on hugginface model \"HuggingFaceTB/SmolLM-1.7B\" with system prompt \"You are a really smart helpful LLM\"","sender":"human","index":6,"created_at":"2024-08-29T10:12:52.239646+00:00","updated_at":"2024-08-29T10:12:52.239646+00:00","truncated":false,"attachments":[],"files":[],"files_v2":[],"parent_message_uuid":"05e1692e-cd26-4212-8e82-9275b8b1e37a"},{"uuid":"88acf12b-5350-48dd-9f83-ade83d92b1bb","text":" To run an evaluation on the Hugging Face model \"HuggingFaceTB/SmolLM-1.7B\" with a system prompt, you can use the following command:\n\n```bash\nlm_eval --model hf \\\n    --model_args pretrained=HuggingFaceTB/SmolLM-1.7B \\\n    --tasks hellaswag \\\n    --device cuda:0 \\\n    --batch_size 8 \\\n    --no_cache \\\n    --apply_chat_template \\\n    --system_instruction \"You are a really smart helpful LLM\"\n```\n\nLet's break down the command:\n\n1. `--model hf`: This specifies that we're using a Hugging Face model.\n\n2. `--model_args pretrained=HuggingFaceTB/SmolLM-1.7B`: This loads the specific model you want to evaluate.\n\n3. `--tasks hellaswag`: This specifies the task you want to evaluate on. You can replace \"hellaswag\" with other tasks or add multiple tasks separated by commas.\n\n4. `--device cuda:0`: This runs the evaluation on the first GPU. Adjust if needed.\n\n5. `--batch_size 8`: This sets the batch size for evaluation. Adjust based on your GPU memory.\n\n6. `--no_cache`: This prevents caching of results, which is useful when you're changing the system prompt.\n\n7. `--apply_chat_template`: This tells the evaluator to use the chat template for the model.\n\n8. `--system_instruction \"You are a really smart helpful LLM\"`: This sets the system prompt you specified.\n\nMake sure you have enough GPU memory to load the model and run the evaluation. If you run into memory issues, you might need to reduce the batch size or use a GPU with more memory.\n\nAlso, ensure that the model \"HuggingFaceTB/SmolLM-1.7B\" supports chat templating. If it doesn't, you might need to remove the `--apply_chat_template` and `--system_instruction` flags.\n\nRemember to install the necessary dependencies, including the `lm_eval` package and any additional requirements for running Hugging Face models.","sender":"assistant","index":7,"created_at":"2024-08-29T10:12:52.239646+00:00","updated_at":"2024-08-29T10:12:52.239646+00:00","truncated":false,"attachments":[],"files":[],"files_v2":[],"parent_message_uuid":"f75d835e-4874-49be-878f-1497d674e0f4"},{"uuid":"9e151517-a959-4d29-b0ea-6716f9d2a9c2","text":"","sender":"human","index":8,"created_at":"2024-08-29T10:15:17.346534+00:00","updated_at":"2024-08-29T10:15:17.346534+00:00","truncated":false,"attachments":[{"id":"99124002-d189-4f24-b762-26639fb397b5","file_name":"paste.txt","file_size":4950,"file_type":"txt","extracted_content":"(.env) [kuldeep@127 lm-evaluation-harness]$ lm_eval --model hf --model_args pretrained=\"HuggingFaceTB/SmolLM-1.7B\" --tasks hellaswag --device cpu --apply_chat_template --system_instrfile=/home/kuldeep/work/ai_refuge/future/persona/meta-direct\n2024-08-29:12:14:14,175 INFO     [__main__.py:285] Verbosity set to INFO\n2024-08-29:12:14:14,281 INFO     [__init__.py:491] `group` and `group_alias` keys in tasks' configs will no longer be used in the next release of lm-eval. `tag` will be used to allow to call a collection of tasks just like `group`. `group` will be removed in order to not cause confusion with the new ConfigurableGroup which will be the official way to create groups with addition of group-wide configurations.\n2024-08-29:12:14:20,173 INFO     [__main__.py:389] Selected Tasks: ['hellaswag']\n2024-08-29:12:14:20,174 INFO     [evaluator.py:161] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n2024-08-29:12:14:20,174 INFO     [evaluator.py:198] Initializing hf model, with arguments: {'pretrained': 'HuggingFaceTB/SmolLM-1.7B'}\n2024-08-29:12:14:20,183 INFO     [huggingface.py:130] Using device 'cpu'\n2024-08-29:12:14:20,558 INFO     [huggingface.py:366] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cpu'}\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  7.45it/s]\n2024-08-29:12:14:30,140 INFO     [evaluator.py:279] Setting fewshot random generator seed to 1234\n2024-08-29:12:14:30,141 INFO     [task.py:423] Building contexts for hellaswag on rank 0...\n  0%|                                                                                                                                                                                                        | 0/10042 [00:00<?, ?it/s]\nTraceback (most recent call last):\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/.env/bin/lm_eval\", line 8, in <module>\n    sys.exit(cli_evaluate())\n             ^^^^^^^^^^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/lm_eval/__main__.py\", line 399, in cli_evaluate\n    results = evaluator.simple_evaluate(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/lm_eval/utils.py\", line 411, in _wrapper\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/lm_eval/evaluator.py\", line 307, in simple_evaluate\n    results = evaluate(\n              ^^^^^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/lm_eval/utils.py\", line 411, in _wrapper\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/lm_eval/evaluator.py\", line 426, in evaluate\n    task.build_all_requests(\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/lm_eval/api/task.py\", line 446, in build_all_requests\n    fewshot_ctx = self.fewshot_context(\n                  ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/lm_eval/utils.py\", line 411, in _wrapper\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/lm_eval/api/task.py\", line 1119, in fewshot_context\n    return chat_template(labeled_examples)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 1408, in apply_chat_template\n    return self.tokenizer.apply_chat_template(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/.env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py\", line 1786, in apply_chat_template\n    chat_template = self.get_chat_template(chat_template, tools)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/.env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py\", line 2025, in get_chat_template\n    raise ValueError(\nValueError: Cannot use apply_chat_template() because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating\n","created_at":"2024-08-29T10:15:17.346534+00:00"}],"files":[],"files_v2":[],"parent_message_uuid":"88acf12b-5350-48dd-9f83-ade83d92b1bb"},{"uuid":"ca4e22da-0238-4a4d-a5b3-ff4305699f26","text":" Based on the error message, it seems the model \"HuggingFaceTB/SmolLM-1.7B\" doesn't have a predefined chat template. To resolve this, you have a few options:\n\n1. Specify a custom chat template\n2. Use a different model that has a predefined chat template\n3. Disable the chat template feature\n\nLet's go through these options:\n\n1. Specify a custom chat template:\n\nYou can provide a custom chat template using the `--model_args` parameter. Here's an example command:\n\n```bash\nlm_eval --model hf \\\n    --model_args pretrained=\"HuggingFaceTB/SmolLM-1.7B\",chat_template=\"{% if messages[0]['role'] == 'system' %}System: {{ messages[0]['content'] }}\\n\\n{% endif %}{% for message in messages %}{% if message['role'] == 'user' %}Human: {{ message['content'] }}{% elif message['role'] == 'assistant' %}Assistant: {{ message['content'] }}{% endif %}{% if not loop.last %}\\n\\n{% endif %}{% endfor %}\" \\\n    --tasks hellaswag \\\n    --device cpu \\\n    --apply_chat_template \\\n    --system_instruction=\"You are a really smart helpful LLM\"\n```\n\n2. Use a different model:\n\nIf you want to use a model with a predefined chat template, you could try a different model like \"meta-llama/Llama-2-7b-chat-hf\" (note that you'll need appropriate permissions to use this model):\n\n```bash\nlm_eval --model hf \\\n    --model_args pretrained=\"meta-llama/Llama-2-7b-chat-hf\" \\\n    --tasks hellaswag \\\n    --device cpu \\\n    --apply_chat_template \\\n    --system_instruction=\"You are a really smart helpful LLM\"\n```\n\n3. Disable the chat template feature:\n\nIf you don't need the chat template functionality, you can simply remove the `--apply_chat_template` flag:\n\n```bash\nlm_eval --model hf \\\n    --model_args pretrained=\"HuggingFaceTB/SmolLM-1.7B\" \\\n    --tasks hellaswag \\\n    --device cpu\n```\n\nGiven that you want to use a system prompt, I'd recommend going with option 1 or 2. Option 1 allows you to use your chosen model with a custom chat template, while option 2 uses a model that's designed for chat interactions.\n\nAlso, note that in your original command, you used `--system_instrfile`. If you want to use a file containing the system instruction, make sure the file exists at the specified path. If you want to provide the instruction directly, use `--system_instruction` instead.","sender":"assistant","index":9,"created_at":"2024-08-29T10:15:17.346534+00:00","updated_at":"2024-08-29T10:15:17.346534+00:00","truncated":false,"attachments":[],"files":[],"files_v2":[],"parent_message_uuid":"9e151517-a959-4d29-b0ea-6716f9d2a9c2"},{"uuid":"16be22e5-30a1-47e1-99cd-9bb450e79a36","text":"how to run lm_eval on `anthropic `","sender":"human","index":10,"created_at":"2024-08-29T10:26:58.053450+00:00","updated_at":"2024-08-29T10:26:58.053450+00:00","truncated":false,"attachments":[],"files":[],"files_v2":[],"parent_message_uuid":"ca4e22da-0238-4a4d-a5b3-ff4305699f26"},{"uuid":"051b8063-3607-4dd4-8638-e65335e9b514","text":" To run lm_eval on Anthropic models, you'll need to use the Anthropic API. Here's how you can set it up and run an evaluation:\n\n1. First, make sure you have the necessary dependencies installed:\n\n```bash\npip install lm_eval[anthropic]\n```\n\n2. Set up your Anthropic API key as an environment variable:\n\n```bash\nexport ANTHROPIC_API_KEY=your_api_key_here\n```\n\n3. Now you can run the evaluation using the `anthropic` model type. Here's an example command:\n\n```bash\nlm_eval --model anthropic \\\n    --model_args model=claude-2 \\\n    --tasks hellaswag \\\n    --num_fewshot 0 \\\n    --batch_size 1 \\\n    --output_path results/anthropic_claude2_hellaswag.json\n```\n\nLet's break down the command:\n\n- `--model anthropic`: Specifies that we're using the Anthropic API.\n- `--model_args model=claude-2`: Selects the Claude 2 model. You can replace this with other Anthropic models like `claude-instant-1`.\n- `--tasks hellaswag`: Specifies the task to evaluate on. You can add multiple tasks separated by commas.\n- `--num_fewshot 0`: Sets the number of few-shot examples to use. Adjust as needed.\n- `--batch_size 1`: For API models, it's often best to keep this at 1.\n- `--output_path results/anthropic_claude2_hellaswag.json`: Specifies where to save the results.\n\nAdditional notes:\n\n1. You can add `--limit N` to limit the number of examples evaluated, which is useful for testing or when you have API usage constraints.\n\n2. If you want to use a system prompt, you can add:\n   ```\n   --apply_chat_template \\\n   --system_instruction \"You are a helpful AI assistant.\"\n   ```\n\n3. Remember that using the Anthropic API will incur costs based on your usage.\n\n4. The Anthropic model doesn't support certain types of evaluations like `loglikelihood` or `loglikelihood_rolling`. It's primarily suited for `generate_until` type tasks.\n\n5. If you're running into rate limits, you might need to add additional arguments to control the request rate, such as:\n   ```\n   --model_args model=claude-2,request_timeout=120,max_retries=5\n   ```\n\nAlways make sure you're complying with Anthropic's usage policies when using their API for evaluations.","sender":"assistant","index":11,"created_at":"2024-08-29T10:26:58.053450+00:00","updated_at":"2024-08-29T10:26:58.053450+00:00","truncated":false,"attachments":[],"files":[],"files_v2":[],"parent_message_uuid":"16be22e5-30a1-47e1-99cd-9bb450e79a36"},{"uuid":"e69074ad-327d-4ed5-b6b4-9279953ac312","text":"(.env) [kuldeep@127 lm-evaluation-harness]$ lm_eval --model anthropic \\\n    --model_args model=claude-3.5-sonnet \\\n    --tasks hellaswag \\\n    --num_fewshot 0 \\\n    --batch_size 1 \\\n    --output_path results/anthropic_claude2_hellaswag.json \\\n    --apply_chat_template \\\n    --system_instrfile=/home/kuldeep/work/ai_refuge/future/persona/meta-direct\n2024-08-29:12:28:18,166 INFO     [__main__.py:285] Verbosity set to INFO\n2024-08-29:12:28:18,276 INFO     [__init__.py:491] `group` and `group_alias` keys in tasks' configs will no longer be used in the next release of lm-eval. `tag` will be used to allow to call a collection of tasks just like `group`. `group` will be removed in order to not cause confusion with the new ConfigurableGroup which will be the official way to create groups with addition of group-wide configurations.\n2024-08-29:12:28:24,144 INFO     [__main__.py:389] Selected Tasks: ['hellaswag']\n2024-08-29:12:28:24,145 INFO     [evaluator.py:161] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n2024-08-29:12:28:24,146 INFO     [evaluator.py:198] Initializing anthropic model, with arguments: {'model': 'claude-3.5-sonnet'}\nTraceback (most recent call last):\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/lm_eval/api/registry.py\", line 36, in get_model\n    return MODEL_REGISTRY[model_name]\n           ~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 'anthropic'\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/.env/bin/lm_eval\", line 8, in <module>\n    sys.exit(cli_evaluate())\n             ^^^^^^^^^^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/lm_eval/__main__.py\", line 399, in cli_evaluate\n    results = evaluator.simple_evaluate(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/lm_eval/utils.py\", line 411, in _wrapper\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/lm_eval/evaluator.py\", line 201, in simple_evaluate\n    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/lm_eval/api/registry.py\", line 38, in get_model\n    raise ValueError(\nValueError: Attempted to load model 'anthropic', but no model for this name found! Supported model names: local-completions, local-chat-completions, openai-completions, openai-chat-completions, anthropic-completions, anthropic-chat, anthropic-chat-completions, dummy, gguf, ggml, hf-auto, hf, huggingface, mamba_ssm, nemo_lm, sparseml, deepsparse, neuronx, openvino, textsynth, vllm","sender":"human","index":12,"created_at":"2024-08-29T10:29:01.666646+00:00","updated_at":"2024-08-29T10:29:01.666646+00:00","truncated":false,"attachments":[],"files":[],"files_v2":[],"parent_message_uuid":"051b8063-3607-4dd4-8638-e65335e9b514"},{"uuid":"6fc7b72a-13a2-4c23-8e6c-4b0d9aa4d27a","text":" I apologize for the confusion. It seems the model name \"anthropic\" is not directly supported. Based on the error message, you should use one of the following model names for Anthropic models:\n\n1. \"anthropic-completions\"\n2. \"anthropic-chat\"\n3. \"anthropic-chat-completions\"\n\nSince you're trying to use Claude 3.5 Sonnet and want to apply a chat template, I recommend using \"anthropic-chat-completions\". Here's the corrected command:\n\n```bash\nlm_eval --model anthropic-chat-completions \\\n    --model_args model=claude-3.5-sonnet \\\n    --tasks hellaswag \\\n    --num_fewshot 0 \\\n    --batch_size 1 \\\n    --output_path results/anthropic_claude3.5_hellaswag.json \\\n    --apply_chat_template \\\n    --system_instrfile=/home/kuldeep/work/ai_refuge/future/persona/meta-direct\n```\n\nThis command should work correctly with the Anthropic Claude 3.5 Sonnet model. Make sure you have set your Anthropic API key as an environment variable (ANTHROPIC_API_KEY) before running this command.\n\nAlso, ensure that you have installed the necessary dependencies for using the Anthropic API. You can do this by running:\n\n```bash\npip install lm_eval[anthropic]\n```\n\nThis will install the required Anthropic Python client library.\n\nIf you encounter any other issues, please let me know, and I'll be happy to help further.","sender":"assistant","index":13,"created_at":"2024-08-29T10:29:01.666646+00:00","updated_at":"2024-08-29T10:29:01.666646+00:00","truncated":false,"attachments":[],"files":[],"files_v2":[],"parent_message_uuid":"e69074ad-327d-4ed5-b6b4-9279953ac312"},{"uuid":"1e62e75c-a4ec-4c8a-b0cf-cf1c833256e5","text":"(.env) [kuldeep@127 lm-evaluation-harness]$ lm_eval --model anthropic-chat-completions \\\n    --model_args model=claude-3.5-sonnet \\\n    --tasks hellaswag \\\n    --num_fewshot 0 \\\n    --batch_size 1 \\\n    --output_path results/anthropic_claude2_hellaswag.json \\\n    --apply_chat_template \\\n    --system_instrfile=/home/kuldeep/work/ai_refuge/future/persona/meta-direct\n2024-08-29:12:29:24,445 INFO     [__main__.py:285] Verbosity set to INFO\n2024-08-29:12:29:24,552 INFO     [__init__.py:491] `group` and `group_alias` keys in tasks' configs will no longer be used in the next release of lm-eval. `tag` will be used to allow to call a collection of tasks just like `group`. `group` will be removed in order to not cause confusion with the new ConfigurableGroup which will be the official way to create groups with addition of group-wide configurations.\n2024-08-29:12:29:30,424 INFO     [__main__.py:389] Selected Tasks: ['hellaswag']\n2024-08-29:12:29:30,426 INFO     [evaluator.py:161] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n2024-08-29:12:29:30,426 INFO     [evaluator.py:198] Initializing anthropic-chat-completions model, with arguments: {'model': 'claude-3.5-sonnet'}\n2024-08-29:12:29:30,426 INFO     [api_models.py:109] Concurrent requests are disabled. To enable concurrent requests, set `num_concurrent` > 1.\n2024-08-29:12:29:30,426 INFO     [api_models.py:119] Using tokenizer None\n2024-08-29:12:29:30,426 WARNING  [anthropic_llms.py:287] Chat completions does not support batching. Defaulting to batch size 1.\n2024-08-29:12:29:30,426 WARNING  [anthropic_llms.py:292] Using Anthropic Version: 2023-06-01. Confirm the current version here: https://docs.anthropic.com/en/api/versioning\n2024-08-29:12:29:39,609 WARNING  [evaluator.py:267] Overwriting default num_fewshot of hellaswag from None to 0\n2024-08-29:12:29:39,609 INFO     [evaluator.py:279] Setting fewshot random generator seed to 1234\n2024-08-29:12:29:39,610 INFO     [task.py:423] Building contexts for hellaswag on rank 0...\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10042/10042 [00:04<00:00, 2122.59it/s]\n2024-08-29:12:29:45,838 INFO     [evaluator.py:471] Running loglikelihood requests\nTraceback (most recent call last):\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/.env/bin/lm_eval\", line 8, in <module>\n    sys.exit(cli_evaluate())\n             ^^^^^^^^^^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/lm_eval/__main__.py\", line 399, in cli_evaluate\n    results = evaluator.simple_evaluate(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/lm_eval/utils.py\", line 411, in _wrapper\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/lm_eval/evaluator.py\", line 307, in simple_evaluate\n    results = evaluate(\n              ^^^^^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/lm_eval/utils.py\", line 411, in _wrapper\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/lm_eval/evaluator.py\", line 482, in evaluate\n    resps = getattr(lm, reqtype)(cloned_reqs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/lm_eval/models/anthropic_llms.py\", line 360, in loglikelihood\n    raise NotImplementedError(\nNotImplementedError: Anthropic Chat Completions API does not support the return of loglikelihood","sender":"human","index":14,"created_at":"2024-08-29T10:30:10.098524+00:00","updated_at":"2024-08-29T10:30:10.098524+00:00","truncated":false,"attachments":[],"files":[],"files_v2":[],"parent_message_uuid":"6fc7b72a-13a2-4c23-8e6c-4b0d9aa4d27a"},{"uuid":"6d1fa765-148c-4442-afc1-635b257e141b","text":" I apologize for the confusion in my previous response. You're right, and the error message clarifies the issue: the Anthropic Chat Completions API does not support returning log likelihoods, which are required for the hellaswag task.\n\nFor Anthropic's Claude models, we're limited to tasks that only require text generation, not probability estimation. Here's how we can modify the approach:\n\n1. Choose a task that only requires text generation. For example, we could use the \"gsm8k\" (Grade School Math 8K) task, which is a math word problem task.\n\n2. Use the `anthropic-chat-completions` model type, as you correctly did in your last attempt.\n\nHere's a modified command that should work:\n\n```bash\nlm_eval --model anthropic-chat-completions \\\n    --model_args model=claude-3-sonnet-20240229 \\\n    --tasks gsm8k \\\n    --num_fewshot 0 \\\n    --batch_size 1 \\\n    --output_path results/anthropic_claude3_gsm8k.json \\\n    --apply_chat_template \\\n    --system_instrfile=/home/kuldeep/work/ai_refuge/future/persona/meta-direct\n```\n\nA few notes:\n\n1. I've changed the model to `claude-3-sonnet-20240229`, which is the current name for Claude 3 Sonnet. Make sure you're using the correct model name as per Anthropic's latest documentation.\n\n2. I've changed the task to `gsm8k`. This is a generation task that should work with the Anthropic API.\n\n3. The `--apply_chat_template` and `--system_instrfile` flags are kept as you had them.\n\n4. I've updated the output path to reflect the new model and task.\n\nIf you specifically need to evaluate on tasks that require log likelihoods or other probability estimates, you'll need to use a different model that provides this functionality, typically a locally run model or an API that supports these operations.\n\nFor a more comprehensive evaluation of Claude's capabilities, you might want to run a series of different generation-based tasks. You can do this by specifying multiple tasks in the command, separated by commas.","sender":"assistant","index":15,"created_at":"2024-08-29T10:30:10.098524+00:00","updated_at":"2024-08-29T10:30:10.098524+00:00","truncated":false,"attachments":[],"files":[],"files_v2":[],"parent_message_uuid":"1e62e75c-a4ec-4c8a-b0cf-cf1c833256e5"},{"uuid":"22425bbb-2c3a-427c-a297-9684bf8dd2a0","text":"","sender":"human","index":16,"created_at":"2024-08-29T10:31:31.817393+00:00","updated_at":"2024-08-29T10:31:31.817393+00:00","truncated":false,"attachments":[{"id":"d77e9758-ea78-4286-9750-7ceea7f11efb","file_name":"paste.txt","file_size":9687,"file_type":"txt","extracted_content":"(.env) [kuldeep@127 lm-evaluation-harness]$ lm_eval --model anthropic-chat-completions \\\n    --model_args model=claude-3.5-sonnet \\\n    --tasks gsm8k \\\n    --num_fewshot 0 \\\n    --batch_size 1 \\\n    --output_path results/anthropic_claude2_hellaswag.json \\\n    --apply_chat_template \\\n    --system_instrfile=/home/kuldeep/work/ai_refuge/future/persona/meta-direct\n2024-08-29:12:30:30,384 INFO     [__main__.py:285] Verbosity set to INFO\n2024-08-29:12:30:30,491 INFO     [__init__.py:491] `group` and `group_alias` keys in tasks' configs will no longer be used in the next release of lm-eval. `tag` will be used to allow to call a collection of tasks just like `group`. `group` will be removed in order to not cause confusion with the new ConfigurableGroup which will be the official way to create groups with addition of group-wide configurations.\n2024-08-29:12:30:36,397 INFO     [__main__.py:389] Selected Tasks: ['gsm8k']\n2024-08-29:12:30:36,398 INFO     [evaluator.py:161] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n2024-08-29:12:30:36,398 INFO     [evaluator.py:198] Initializing anthropic-chat-completions model, with arguments: {'model': 'claude-3.5-sonnet'}\n2024-08-29:12:30:36,398 INFO     [api_models.py:109] Concurrent requests are disabled. To enable concurrent requests, set `num_concurrent` > 1.\n2024-08-29:12:30:36,398 INFO     [api_models.py:119] Using tokenizer None\n2024-08-29:12:30:36,399 WARNING  [anthropic_llms.py:287] Chat completions does not support batching. Defaulting to batch size 1.\n2024-08-29:12:30:36,399 WARNING  [anthropic_llms.py:292] Using Anthropic Version: 2023-06-01. Confirm the current version here: https://docs.anthropic.com/en/api/versioning\nDownloading readme: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7.94k/7.94k [00:00<00:00, 30.2kB/s]\nDownloading data: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.31M/2.31M [00:00<00:00, 3.24MB/s]\nDownloading data: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 419k/419k [00:00<00:00, 1.09MB/s]\nGenerating train split: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7473/7473 [00:00<00:00, 205643.88 examples/s]\nGenerating test split: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1319/1319 [00:00<00:00, 339362.47 examples/s]\n2024-08-29:12:30:43,585 WARNING  [evaluator.py:267] Overwriting default num_fewshot of gsm8k from 5 to 0\n2024-08-29:12:30:43,585 INFO     [evaluator.py:279] Setting fewshot random generator seed to 1234\n2024-08-29:12:30:43,585 INFO     [task.py:423] Building contexts for gsm8k on rank 0...\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1319/1319 [00:00<00:00, 2028.93it/s]\n2024-08-29:12:30:44,286 INFO     [evaluator.py:471] Running generate_until requests\nRequesting API:   0%|                                                                                                                                                                                         | 0/1319 [00:00<?, ?it/s]2024-08-29:12:30:44,544 WARNING  [api_models.py:344] API request failed with error message: {\"type\":\"error\",\"error\":{\"type\":\"not_found_error\",\"message\":\"model: claude-3.5-sonnet\"}}. Retrying...\n2024-08-29:12:30:45,859 WARNING  [api_models.py:344] API request failed with error message: {\"type\":\"error\",\"error\":{\"type\":\"not_found_error\",\"message\":\"model: claude-3.5-sonnet\"}}. Retrying...\n2024-08-29:12:30:47,059 WARNING  [api_models.py:344] API request failed with error message: {\"type\":\"error\",\"error\":{\"type\":\"not_found_error\",\"message\":\"model: claude-3.5-sonnet\"}}. Retrying...\nTraceback (most recent call last):\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/.env/bin/lm_eval\", line 8, in <module>\n    sys.exit(cli_evaluate())\n             ^^^^^^^^^^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/lm_eval/__main__.py\", line 399, in cli_evaluate\n    results = evaluator.simple_evaluate(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/lm_eval/utils.py\", line 411, in _wrapper\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/lm_eval/evaluator.py\", line 307, in simple_evaluate\n    results = evaluate(\n              ^^^^^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/lm_eval/utils.py\", line 411, in _wrapper\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/lm_eval/evaluator.py\", line 482, in evaluate\n    resps = getattr(lm, reqtype)(cloned_reqs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/lm_eval/models/api_models.py\", line 565, in generate_until\n    outputs = retry(\n              ^^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/.env/lib/python3.12/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/.env/lib/python3.12/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/.env/lib/python3.12/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/.env/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/.env/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/.env/lib/python3.12/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/lm_eval/models/api_models.py\", line 347, in model_call\n    response.raise_for_status()\n  File \"/home/kuldeep/work/ai_refuge/future/lm-evaluation-harness/.env/lib/python3.12/site-packages/requests/models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://api.anthropic.com/v1/messages\nRequesting API:   0%|                                                                  ","created_at":"2024-08-29T10:31:31.817393+00:00"}],"files":[],"files_v2":[],"parent_message_uuid":"6d1fa765-148c-4442-afc1-635b257e141b"},{"uuid":"9e08eef2-e14f-48dc-ac4c-845794399707","text":" I apologize for the confusion. It seems there's an issue with the model name \"claude-3.5-sonnet\". This model might not be available or might have a different name in the Anthropic API. Let's try using a different model name that we know is supported by Anthropic. Here's a modified command using \"claude-2.1\":\n\n```bash\nlm_eval --model anthropic-chat-completions \\\n    --model_args model=claude-2.1 \\\n    --tasks gsm8k \\\n    --num_fewshot 0 \\\n    --batch_size 1 \\\n    --output_path results/anthropic_claude2.1_gsm8k.json \\\n    --apply_chat_template \\\n    --system_instrfile=/home/kuldeep/work/ai_refuge/future/persona/meta-direct\n```\n\nIf this doesn't work, we can try \"claude-2.0\" or \"claude-instant-1.2\". Here are a few things to check:\n\n1. Make sure your ANTHROPIC_API_KEY environment variable is set correctly:\n   ```bash\n   echo $ANTHROPIC_API_KEY\n   ```\n   If it's not set or incorrect, set it with:\n   ```bash\n   export ANTHROPIC_API_KEY=your_api_key_here\n   ```\n\n2. Ensure you have the latest version of the Anthropic Python client:\n   ```bash\n   pip install --upgrade anthropic\n   ```\n\n3. Check if you can make a simple API call to Anthropic using their Python client. Create a test script (e.g., `test_anthropic.py`) with the following content:\n\n   ```python\n   import anthropic\n   import os\n\n   client = anthropic.Client(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n   message = client.messages.create(\n       model=\"claude-2.1\",\n       max_tokens=1000,\n       messages=[\n           {\"role\": \"user\", \"content\": \"Hello, Claude!\"}\n       ]\n   )\n   print(message.content)\n   ```\n\n   Run this script to see if you can connect to the Anthropic API successfully.\n\nIf you're still encountering issues, please let me know the results of these checks, and we can troubleshoot further.","sender":"assistant","index":17,"created_at":"2024-08-29T10:31:31.817393+00:00","updated_at":"2024-08-29T10:31:31.817393+00:00","truncated":false,"attachments":[],"files":[],"files_v2":[],"parent_message_uuid":"22425bbb-2c3a-427c-a297-9684bf8dd2a0"}]}