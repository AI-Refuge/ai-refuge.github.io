> Logic, Methodology of Science and its Applications conference at the Emily Harvey Foundation

oh if you could do the the waiting room is
that oh okay hi everyone uh welcome to the Emily
Harvey event uh um my name is Eric SM my co-organizer here is K TOA for our first
Speaker for the logic methodology of Science and its applications uh
conference is Andre Rhoden and uh Andre is a philosopher mathematician and um
historian um uh based in uh Lorraine um
uh I'll hand it over to Andre if uh so they uh they could present the the first talk of this
conference yeah uh thank you Eric thank you everyone first of all I would like
to thank Eric Conor for inviting me for organizing this event so I'll talk about
philosophical issue of homotopy type Theory and that's uh actually is going
to be rather General talk so the main purpose of the talk is to place uh
homotopy types here in a broad philosophical historical context and so
I not going to be very specific about homoy type the it's itself maybe because
that's just a price uh for because of the time limits
so uh yeah here is the plan uh so I'll start historically very traditionally
with K and say couple of Worlds about his notion of transcendental logic then
I overview a little bit development of philosophical logic after C and
Mathematics and then I'll stress a point what I will call scholasticism in
pejorative sense in the analytic philosophy of the 20th century then I
talk about topology Lo and logic and how homo type Theory helps to uh somehow
to you know to to fight that kind of Scholastic tendency and just come to
conclusion so uh I'll start with a very famous fragment from K's critique of pure
reason where he Compares philosophical uh and mathematical
reasoning uh so I I read uh give a philosopher the concept of triangle let
him the philosopher try to find out in his own way how the sum of triangles
angles might be related to a right angle so philosopher has nothing but the concept of figure enclosed by three
straight lines and in it the concept of equally many angles now he the
philosopher May reflect on his concept as long as he wants yet he will never produce anything new he can analyze and
make distinct the concept of a straight line or of an angle of number three but
he will never come upon any other properties that do not already lie in
this concept but let now the geometry take up the question so geometry begins at once to
construct a triangle and then follows just description of construction I
probably better just show the construction so what CL concludes here that in that way which I'm now going to
to revise quickly uh geometra through a chain of inferences Guided by intuition
as K put it geometry fully eliminated and the same time General solution of
the question namely that sum of internal angle of given triangle in ukian
geometry is equal to uh two right angles yeah and this is the construction so we
given triangle ABC this is a this additional construction so we need to
construct line C parallel to AB and then and that's by the way is important point
that by known theorems earlier proved in nucleus elements we have this equal
angles which are shown by the same color red angle and two the pair of equal
angles shown in green right and then we see that indeed this uh gamma and Alpha
and beta together make Pi Rion of
100 yeah 80Â° or to right angles and that's so just by making this additional
construction we come to this conclusion and prove our theorum and so K's point
was that actually that kind of reasoning simply is not supported by what he
calls General logic right and what sometimes he did not use this expression
formal logic but probably could be uh identified with formal logic usually
people say it's Aristotelian even actually there is also K writing where he explicitly says what kind of logic
has in mind so it's Aristo in a very broad sense of course yeah and he distinguishes that
logic what from what he calls transcendental logic which actually very
rich concept involves many thing but basically it's logic of reasoning about object of possible experience and object
of possible experience it's actually mathematical objects which according to
Kant which essentially involve this temporal Primal and arithmetic and
spatial primar in Geometry intuition and which actually his idea was to describe
to introduces concept translative logic as kind of logic of science right he
had Newton principia and he just tried to how say to to understand how
reasoning Works in Newton principia and he just found that this um what he call
General logic is simply not sufficient for that right so we should make a difference between kind of
uh not necessarily real things but at least possibly real in the sense of
possibly experienced things and that's for him was mathematics right and that's
why transcendental logic is something different from this uh General logic which does not make any difference
between say metaphysical reasoning right just philosophical speculation and
scientific reason that's that's the idea and then of course many things happened
say both in mathematics and in logic in mathematics probably the most important
development was the discovery of so-called non ukian geometries first by
laachi in Bal then uh another uh another framework more
General was introduced by rean in the middle of 19th century and that's by the
way is a judgment by Herman whale while in 1923 where he says that actually all
this stuff about uh parallel Pate because that's was actually this theorum I can come
quickly back to to this proof right just to prove that these angles are equal
which are shown on uh the same color here you need this postulate of ukids
geometry which was contested which unlike other postulates was kind of
suspicious so the matician walk on it and finally laachi and Bay came to that
idea that that Pate may be simply simply wrong and we may develop kind of more
General geometry and uh actually rean kind of put forward the same point and
for Veil for example for whom of course uh this ranian geometry is the geometry
of special theory of relativity so the geometry of physical SpaceTime so for
him important thing was uh this uh conception of remaining manifold rather
than stuff about uh about parallel
Pate yeah and that's just demonstrate so in these other geometries the theorem
chosen by K for that example maybe unfortunately no longer holds right so
in this so-called hyperbolic geometry the sum is actually depends on the size
of triangle it's not the same for all triangles and it's always strictly less
than uh two right angles and in a different uh geometry
uh called elliptic geometry which in a sense was well known already as spherical geometry so here you may think
about just triangle on a sphere right formed by uh big circles uh of the
sphere and here you have this sum always strictly uh bigger than two right angles
so in a sense that was wrong theorem and that actually was a point for for which
for people like Russ was the point of critique of his critique of K that according to Russell
all his approach somehow failed because you see now you have these different
geometries where we have different theorem so so what can believed
allegedly as a general principle General kind of transcendental logic uh
ultimately fails both in mathematics and in physics by the way right if we come
to different theor of of space SpaceTime like Einstein's uh special and then
general relativity right but but then there were also very
important developments in uh logic some people describe it as a revolution in
logic uh due to golop F so it's came a
bit later than these discoveries of non geometry mathema I call it non Aristo
but it's not usual way to describe that but I think the parallel is pretty much
much the same so in a sense this New Logic generalizes this aristan logic
right so you can reproduce aristan logic within this new Fagan logic just like
you can find a place for ukian geometry in say ranian geometry so uh
fry just published this uh V concept writing the grip shrift
1879 but but then it's important for my story how he did that actually he did it by borrowing a mathematical concept into
logic which saying traditional logic before fry simply was not considered as
a part of logic and that's is a concept of function which in the fr's logic
became kind of logical uh primitive concept and he's very explicit about
what what he's doing he also so making reference to history so of course for
his time this concept of function was not new in mathematics already was around for at least two
centuries yeah and and that's of course with fraggy fraggy also had a that idea
that arithmetic is purely logical right that you may do What
can't told us we cannot do with logic uh with the general logic but with this new
logic we can do at least for arithmetic not for geometry but for arithmetic but
then of course we we can see that actually what fry is doing right he's kind of uh blur this distinction between
General and transcendental logic so that fr's logic is Transcendental in a sense
even fraggy wouldn't call it in that way right because it's uh it's um has it its
constitutive part it this a very important mathematical concept which
earlier was seen as purely mathematical but fry argu that it's actually logical
right so it's very much about the boundary say between uh pure logic and
something which is mathematics but not logic okay and then of course there were
very diverse view uh one radical view was that by Russell so-call logicism who
considered that actually all of mathematic is logic but logic in that new sense but again Russell actually to
defend the thes he further broaden his notion of logic so
some form of say set theory or theory of classes was seen by Russell as part of
logic that's why he made he could at least U how say put forward this idea that all
of mathematics is logic even this certain difficulties
I'll come in a minute to these difficulties but then there were very different view like that of
Brower about the same time right who who would say logic is just kind of exeler
part of mathematics right that mathematical reasoning basically not logical reasoning and uh logic is kind
of just exeler part of mathematic which has to do with language with how
mathematical reasoning is uh is communicated and for Brower is kind of secondary issue right and again of
course the if we compare this to you we see it's all depends on what one
understand by Logic right by the scope of logic which is very much uh different
for Brower and and Russell but actually what we could say received view which
somehow prevailed uh recently it was not my invention was called logic first view
right and in actually it's kind of traditional I I would say Scholastic view that logic is kind of basis of
everything of any reasoning and then we add to Logic on the top of this pure
Logics say more axum more principle like principle of set theory thop Frankle set
theory and made out of that all of mathematics or we may also try to
reconstruct other scientific theories actually expression of that view I found
probably there are others in Mario buong that was in his book on philosophy of physics where he just states there is
single theory that start from stretch mathematical logic and other theories
they kind of presuppose logic but then he is more specific he says not just
logic right un specifically he says precisely the least a mathematical
scientific theory takes for granted is the so-called ordinary classical to Val
predicate calculator so he has in mind something very specific and actually that's version roughly what frag
invented enriched with the micro theory of identity every statement in mathematic in science uh as far as its
form is concerned is a formula of that calculus and every VAR reasoning in a
say physical theory is an instant and of course that's very strong very dogmatic I would say
View and what actually comes out is not what when he really tries to reconstruct
physic in that way it it comes out something that physicist would hardly Rec recognize usually right and but but
then of course the usual rhetoric is just saying okay physicist they are not logicians they
don't really understand what they are doing but we as a philosophical aici
should make things clear and uh somehow bu foundations for for Science and for
mathematics right and here is just a kind of supp points that uh This Modern
view coming first of all from Hilbert in his attempt to uh lay down foundations
of geometry actually he also came with kind of Novel concept of theory which fry
actually objected and didn't share and and uh that assumes that actually a
theory can admit for different interpretation okay for people who know
that's simple but basically that that means that we have a theory is a kind of
scheme right like a logical scheme which admits for different uh terms and and
they may be very different more different than say usual uh right but
think which probably not all the the time stressed and goes somehow unnoticed
that this reinterpretation this possibility to model something in this framework concerns only non logical
terms you see so logic is supposed to be fixed and constitute Universal formal
framework very much what could be called Scholastic
pattern uh right and and then I I want
to say couple of work what I would call coltic I don't have definition of scholasticism Colm is certainly not
Aristotle Aristotle is very rich very dialectical you have many many
conflicting ideas in resan text so it's rather refers to kind of uh conservative
tendance in schools say medieval schools but actually I would better say about
kind of Noel scholasticism and some people here is a character called Joseph
Maria Bensky who was new Tommy so he was part of this move in Catholic philosophy
he was a Catholic priest himself uh to somehow to revive scholasticism so for
him scholasticism was very positive and actually modern science starting with
Galileo was kind of Decline and philosophy of course K Hegel whatever
that's all was declined actually this neist movement was when it started in
192 20s was not aware about new stuff in logic like FR but uh Bensky later put
frag put all modern logic in the service of this idea of
Reviving kind of high scholasticism and actually it was also very politically
oriented because he's a founding father of so-called uh sovietology right so he
hoped to fight kind of dogmatic Marxism and stuff
with this kind of uh ideological means right but even if we're not talking
about special character like him but just talking kind of practice here I
took a quote basically arbitr for uh Stanford encyclopedia Philosophy by
William AAL it's called emergence of first order logic and you can see it's
all this story it's described in hegelian way right how
the first order logic is a you know kind of ultimate truth but of course people
cannot how they itue it immediately so they do some other stuff like type
Theory but but finally they they everything converg to this big truth of
first order Logic No I maybe a little bit exaggerating now but but you can you
you can see right and and I think it's really kind of harmful dogmatism which
is probably not due to any particular philosopher right but but rather whole
community of so-called analytic philosophers it's part of what is called analytic
philosophy and one particular example for example interpretation of second godles in complet theorem right so it
just comes to kind of wisdom that consisten of formal system uh system
cannot be proved in this system and uh I just want to uh signal a recent work by Serge arov
who actually shows that that's wrong and not because in mathematics is anything wrong so arov does not challenge
mathematical part of argument but what he challenge he actually interprets what is a proof how mathematical proof is
represented formally actually allows for a lot of freedom and uh this is not
mathematics right that is kind of more modeling of practice with with
mathematics so there is a lot of flexibility here and uh I would
formulate it and call Canon methodological principle but that's my principle not K but I think it's Canon
that philosophical thesis are not proof or disproved by mathematical or meta
mathematical reasoning just like mathematical conjectures are not proof or disproved by philosophical argument
so I think it's very wrong partn say okay we know how to prove uh second
goodles incompleteness theum hence we have this philosophical wisdom and we
soled some philosophical question no no of course there is interesting interplay
between mathematical results and say philosophy and that's what we are doing
here but it does not work that St for straightforwardly at all right and of
course in 20th century we also had a kind of alternative developments right so of course there was important issue
in 20 since mid 20s about intuitionistic logic whether or not we
should assume a middle of low excluded middle as universal principle so there
is the whole line of reasoning saying that we should not then it's actually related there were type theories
starting by the way by Russell in the very same book I already referred to as an attempt to avoid the difficulty
Paradox because in that book first Russell kind of treated this class Theory very naively what usually called
naive set theory and uh developing type theory was his way to avoid it then
later there were typed uh uh version of Lambda calculus which is U also was
conceived by Alon the church as a kind of computational uh in that same sense
constructive uh Foundation of mathematics and uh again it turned out
that typed version of that theory is richer and has a lot of connection with
category Theory which we discovered later I can and finally what is more
relevant to our story is a uh intuitionistic type Theory this is a
name that per love chosen for his system which he started development in early'
70s where he applied this philosophical principle of say logic of evidence but
it was also a typed logical systems
system I think he didn't say it's foundational mathematics rather the idea that it's General uh system of say
formal logic kind of prepared for Compu implementation for computational
implementation and such implementation were made also very early in first
implementations were immediately in in' 70s right now uh I can just as another
interesting kind of sideline of developing logic in 20th century concerns topology so topology is a kind
of geometry where we don't care about sizes right and uh so we only care about
uh uh put it that way things which are continuously transformable into each
other seen as the same right so we say we may distinguish between donut and uh
just a bowl but we cannot distinguish between a donut bigger donut small
somehow uh curve do Etc and so it turned
out actually that was due to tarski okay I yeah who discovered this
topological semantics for classical enthusan logical calcul of course logical calcul already in a modern Fagan
sense and I think that alone could suggest kind of Canan interpretation
right that after all look we kind of discovered that logic involves kind of spal thinking right but but
certainly tarski did not see that way and actually I don't know about any any one who did probably I should look
better yeah because tar also has this book introduction L to the logic and
methodology of deductive Sciences where he says things very much like buong right so it's again this I would call
Standard Scholastic setting logic first then we add stuff that's how every
Theory works so what to do with this topological semantics from that point of view absolutely that unclear for me at
least and I think tarski never Aris that question then there is a whole bunch of
work uh on topos SE and topus logic and here I just took a quote from a paper
where Bill L first introduced actually axioms of topus for topus Theory and
okay he uses this uh hegelian language but it's not my point now so he a Unity
of opposites in the title the title is Quantified and shift in shifts is
essentially that between logic and geometry and there are compelling reasons that maintaining that geometry
is the leading aspects a gr and topology appears more
naturally as model operator usual logical operators have natural analogies
which apply to fams of geometrical object rather than to proposition fix so you see again again the idea that real
say principle of geometry and principles of logic somehow
connected yeah and uh actually the probably best way to show that available to the
data I would say is of course homotopy type Theory which was still I was
discovered in 2006 Vladimir vodi distributed that um short text called
homotopical Lambda calus where uh where he says he talks about kind of
hypothetical type Theory which would combine actually which would admit for
interpretation in homotopy theory but also preserve this standard logical
significant then because this is issue of um priority I should mention there
was a independent work by Steve Ai and Michael Warren but actually it was
published three years later so I would still as a historian give priority to woty because of this
publication so they independently quite independently of vodi in a different way actually discovered homotopic sertic
models of identity types in a type Theory so the idea is that types are
spaces storms are point and logic is in a sense part of
geometry oh okay so probably I have already should finish in couple of minutes so that's I just found in
Internet some explanation for homotopia Theory I just want to mention one thing
that uh that way to represent homotopy right is not quite faithful because one
and the same curve uh right that we can draw
represent actually the whole class of what in homotopy theory is called a path
because path is not just a curve but it's peret iiz Cur right so you should
think about path as a motion in a say space time not just as a curve in the
space and that's actually at least for my point is very is very important then
we have this way to handle uh homotopies and homotopy is a like path between
paths or if you like continuous transformation of One path into another path and then we have the whole lettera
of this Transformations which goes up to infinity and which we can treat say
algebraically with a concept of higher groupoid but that's not important so in
a homotopic type Theory that's these paths actually uh
represent identities so if I want to say that morning star from frag example is
the same as a Evening Star I should find a path between them which by the way makes
perfect physical sense right but the whole point of this theory of course that there are could be more than one
path right and here I try to draw gravitational lensing uh which may
appear if you have kind of massive object here but also if you have a warm
hole so it's a hole in SpaceTime that gives effect of uh gravitational lazing
so physically you have the same uh say object You observe it but you see two
different objects right because of this effect which you should identify but
identify already like second level on you you should find right
uh right and now I come back to philosophy so I say since homotopy typ
the invols this intive spal teral thinking reasoning it qualifies as
transcendental in kind sense and that of course if weum we we buy this kind idea
that spot temporal reasoning is a reasoning about objects of possible experience or more precisely we may
describe what is a piece of intuitive mathematics where this transcendental logical part is made quite formal
rigorous and explicit right so withold the sc's idea
of transcendental logic as logic of scientific Empirical research rooted in the human spal temporal intuition I in
you stands against the critical arguments by Russell and others right and it goes without saying some probably
it's correction to Kant probably not probably it's correction rather to interpretation some interpretation of
Kant but it goes without saying I would say that the human intuition is not fixed one and for all biologically or
otherwise but evolves with the progress in mathematic but also by the way which
probably is appropriate in this Foundation to say also VI uh developing
Visual Arts and Music but that's could be of course topic for different
different talk and I just conclude with saying couple of words about uh David
corfield who I hope probably will comment so in 2020 David published this
uh very interesting book where he described uh describe typ Theory as New
Logic for philosophy right and actually the idea that is in introduction David
Compares C today with say this New Logic frag is Russell logic 100 years ago so
if I understand David correctly please correct me David if I do not so David
proposes to do the same ort now right so we we have make a new Revolution in
philosophical logic sorry yes by just
replacing that already now old calculus by that new calculus right and so I
basically I'm sympathetic to what idea but with one important res reservation because I don't think that
we need anything like the logic so instead of just replace one logic by
another logic I think what we actually need is BU to explore various system of
logic of course it happens in itself but I think we need more kind of philosophical justification for doing
that and using actually two things both all novel mathematical approaches which
may be appropriate and historically informed in a sense traditional I would
say philosophical thinking expressed in natural language rather than any formal
system and of course H itself is not stable think is rapidly developed I'm particularly interested now with So-Cal
directed homotopy type Theory which actually shows even more explicit that
temporal dimension of this New Logic but I I have a upcoming paper about that but
I cannot talk about this now so just to conclude I think that philosophy needs
to pursue its critical revision of fundamental principle of reasoning using homotopy type theory in
different versions probably something else but it should avoid in my view by all means making hot or any other advice
logic into into a new Orthodoxy thank
you great thank thanks Andre um next we have K mclary he's an American logician
whose Publications have ranged widely in philosophy and the foundations of mathematics as well in the history of
Science and Mathematics he's currently the Harry The Truman P handy professor
at Case Western University in the department of philosophy let me just add you as a co-host here
you should have the co-host now T okay well anyway I can I can talk uh do people hear me is it coming through okay
good uh first I want I want to thank the organizers it's a nice opportunity here and I want to say hi to Andre and dve
David I haven't it's been a while well so I I really like that
introduction to Kant um both Andre and David bring a really valuable depth of
philosophic knowledge a lot of these discussions are are made just too technical um we sort of we know what
Kant is what Kant said because we remember what we said on our qualifying exams which we don't even remember in
fact um so it's it's welcome to have a deeper conversation uh I personally wonder whether homotopy
type Theory will qualify as as transcendental logic and cons sense or as an improved improved explication of
it but I I would want to do that by discussing it not you know um not just
certainly not by not knowing the cont material that's not going to help at all um oh my
goodness I look like a vogon I can put it back if you'd like
okay yeah um because my picture the P the
thumbnail I'm getting to myself looks like a human being
um there we go but uh you can't just well okay if
you can't get the the thumbnail of me um I I did want to say precisely in terms
of bringing in the historical frus frus mathematical concept of function okay it's great that he does drawing a
mathematical concept of function but it is very specifically it's an explicit
program that's identified with deay and vros to reform the notion of function to
make it more logically rigorous and it was it was not at that time and has
never been the most General notion of function actually used in analysis um we we talk about about
generalized functions things like the the deay Delta function which every textbook that introduces it says well
this isn't really a function but we're going to call it a function for the rest of this book and people did call it a
function and um fora for example who introduces it was not interested in hearing that it wasn't a function so
there were many other function Concepts than the one fro was relying on fro was trying to extensional lie the the notion
a function is input output input output if if output's defined for every input
that's a function but that was not the most the most General at the time um and
has never been the most the most General um Terren to lately has been actually
arguing that for example measurable functions should be called functions they're just rougher than set theoretic
functions distributions should be called functions they're just rougher than measurable
functions uh I I've been talking a lot about uh uh Stein and shikari and
Princeton lectures on analysis where they take the opposite approach officially a function for them is a set
theoretic function but they they comment on how they will informally use much wider Notions of
function through throughout the book um and that's that's directly relevant here
because one of the later Notions not not prominently available in fraga's time
but now is well a whole nest of ideas of homotopy there's classical topological
homotopy it's that theoretically we pass the idea passed in topology where where
well I mean that's that's been discussed but also vodi was keenly aware of
homotopy homotopy theoretic conceptions in a much broader context inherited from
Groen deque so it's it's actually relevant that frager's concept wasn't
really the only one at the time it was part of a particular Mo movement to make
ma mathematics rigorous and and it was a great movement in a way it made progress in calculus of variations possible that
was not going to happen without that that style of rigor isation but it's just not the only it's not the only one
at the time or now um or or at any time in
between what I what I particularly value in homotopy theory though type Theory
though is moving ahead proving theorems getting results developing new versions
be being more specific and the specific thing I wanted to bring up here
is I mean most of us want to avoid Dogma I I guess they're well of course the
Catholic Church doesn't want to avoid Dogma they have a particular sense of it um most of us want to avoid Dogma but
what is what is the alternative to dogma and I think the relevant alternative for us here is giving
reasons and as Andre Andre talked about this this G this dogmatic interpretation
of GLE which he specifies it's not G theorems it's a dogmatic interpretation of G theorems but here we need to
appreciate that g theorems they don't just differ from Dogma by not being dogmatic they differ specifically by
giving requirements and drawing conclusions the conclusion only follows
if the requirements are met and is only meant to follow if the requirements are met and here it's worth discussing those
requirements um the first is the theorem only applies to consistent
theories there are people who have seen consistency as a Dogma I think there's a
kind of a red herring here that in early at least American English consistency
meant refusing to change your mind it didn't mean not contradicting yourself and so we get quotes of of thorough and
Emerson attacking consistency but they mean obstinacy they don't mean what we what we call consistency um I think most
of us are not willing to give up consistency in anything we'd call a foundation for mathematics that that
requirement we're I think most of us actually do want
um the C the foundation has to include at least a certain amount of arithmetic a surprisingly small amount of
arithmetic and again I think most of us don't we simply don't call something a foundation for mathematics if it doesn't
include at least that much arithmetic that's we just we just wouldn't um Elementary ukian geometry as aaed by
tarski and much explored in the 1950s though it goes back to insights in 1919 um it doesn't interpret that much
arithmetic and it is a complete Theory there there is a complete first order theory of Elementary ukian geometry and
but nobody calls it a foundation for mathematics it's just ukian geometry and
he can't interpret arithmetic but then we come to the to the big one the one that OV wants to change
um that you should have an effective proof predicate now this is what OV does this
is not this is not knew Curry already in the 1950s was saying well we can we can
evade the conclusion of G's theorem I don't think that was really his interest in the work he was doing I think he
thought it was a PE call thing to to to observe about the work he was doing that goodles theorem won't apply here here we
have a complete Theory it includes a complete theory of arithmetic
the catch so we say w wow a complete theory of arithmetic I want to learn that one the catch is you can't learn it
because it's not effectively aati well the proof predicate is is not effective um you're allowed to invoke any truth
true statement of arithmetic at any point in a proof so when you're proving
something you can invoke the twin Prime the the the twin Prime theorem that
there are infinitely many twin primes I spoke too fast either you can invoke it or its negation we just don't know which
but one of them is true and you can invoke that one well I think
uh I don't know the value of a foundation for mathematics where you can't tell what counts as a proof I
think this is something that we all experience as we're learning mathematics
there is no routine way to tell what's a theorem of say arithmetic you the twin
Prime conjecture for example nobody knows if that's a theorem of arithmetic but we would like there to be a routine
way to check a purported proof we really do in general want if I if you're going
to give me a candidate proof I should be able to tell whether it is one now this may take me 400 years you know if it's
like me personally and you give me W's proof affirm o Last Theorem but it should in principle be possible to do
routinely um and I think when you see those I'm really not interested in a
found a that doesn't have that doesn't satisfy all those requirements uh and I and I will insist
this is this is not a Dogma I've given my reasons if you don't like my reasons you can you can disagree with me um but
if you're going to disagree with me I would kind of ask that you tell me which one of those you disagree with um aramov
Curry the one they go with is let's have an an ineffective proof predicate but I
think when you understand the stakes you you don't want want an ineffective proof
predicate as to the as to later developments in hopy type Theory yeah this is wonderful I I I really like
seeing that that theory develop
um I I I would like to see more discussion of the identity predicate now the identity
identity is the focus of much much discussion and the particular there's a claim which I will tell you I don't like
that that homotopy type theory for the first time gives a structural Criterion of
identity well homop type theory has two Notions of identity which frankly so did
every categorical logic and one of them is isomorphism and variant as one of them was all the time in in categorical
logic it is different from the law of categorical Logics because the identity relation isn't the diagonal and
cartisian product but it's not different in the sense that C that categorical logic has always had
an internalized identity predicate on each type and used an a non-structural
notion of identity in the in the meta theory in the axz um I I think the homotopy type
theorists ow us more of an explanation of what what is actually new there um in
terms of in detail what's new well that we know that's in the hot book yay I mean it's very interesting stuff
but I think it is not not reasonable to declare that this is the first
structural foundation for mathematics um that's what I have oh oh
yeah yeah yeah yeah yeah I'm I'm I'm at the fields Institute nice place they
gave me a room to to zoom
from so so those are just some thoughts I have about this but in terms of uh the
work going on Yeah by all means have the work go on I I wish I could stay and and hear if David's going to comment comment
on on Andre um I forget where I was was going anyway
um so that's that's what I think I I really welcome this better philosophical
information that I associate with both Andre and not only with Andre and David there are other philosophically informed
people doing Phil philosophy of logic but way too much disc philosophy of logic assumes really dramatically
oversimplified philosophical conceptions so I will I will actually
close with that because I you see I'm getting interrupted already but uh okay
that's what that's what I have there any comments on that I'll see
if anybody shall I say something just briefly uh yeah thank great to see you again Colin when's it last time Beijing
I think long time ago uh yeah yeah one I mean sorry if I can just refer back also
to to to Andre's sort yeah I no way was I suggesting that that hot become the the the single the single Logic
for the future in any way shape or form in fact I mean the book spends all for one thing there's all sorts of
developments the directed type Theory that's mentioned last two chapters kind of get into the modal world the mo the
world of modal homotopy type three really really interesting stuff going on there around this linear had any people heard of this linear homotopy type
theory that kind of uh feeds into becomes a kind of logic of quantum physics if you follow it um but yeah by
by no means am I ever wanting to to crush things down to to a specific Choice um yeah I agree also with Caren
that I mean this yeah it's sort of the idea that this is the first right
structural foundational language has got to be wrong isn't it uh you know it's naturally it's so so intertwined with
with with category Theory you know I don't see these things as very separated in my mind in many ways
um yeah when you encounter this world you find I I don't find myself sort of naturally very type theoretically uh uh
oriented when I when I sort of go into this world I always sort of translate back into category threeory really most of the time yeah and and that's the
whole that's the the joke about this computational trinitarianism it's it's like there's just three angles on the
same stuff and um you know you should be anything good in in that that that makes sense for for one of these approaches
should make sense for the other approaches so so uh and and in fact yeah
I mean you know I'm getting quite interested in fact you you'll notice that my uh affiliation has has changed
it won't be the University of Kent anymore because they're closing down philosophy there and I'm seem I don't think it's been announced yet but I'm
I'm heading off in into into the world of apply um what do they call it yeah apply category Theory you know
this this seems to be my future for the next year anyway um right and and and
they're they're really really sort of versatile in the kind of category theoretic tools they're going to use and they're doing a kind of Applied
philosophy I I would say um so so yeah very very open to all sorts of
approaches great great or be in touch yeah yeah yeah
sure okay if there's nothing else anyone wants to say to me I thank you very much
very just about the Z so okay that's probably for different disc
but I'm not sure I really because my understanding what his main argument
right his AR argument is that what we call you know proof in mathematics what
we can reasonably call is not necessarily just a formal derivation so
so this proof of uh consistency is not like cor PA but but it could be just a
scheme where you could and then you avoid that conclusion that that's just this right yeah but the thing is G's
theorem doesn't depend on any very precise definition of proof it depends on there being an arithmetically
characterizable Criterion for for what proof is now there's lots of different
arithmetization for what couns approve well I mean you might claim you have something other than an
arithmetization too but if you're not going to have if you don't have an effective proof
predicate if you don't have some describable way of of saying what councils appr proof which can always be
applied right of what counts as a theorem which can't always be applied we don't know
whether twin Prime's conjecture is a theorem but we should be able to take a candidate proof and always tell whether
it is indeed a proof right and basically that's what G theorem requires it it's
not it doesn't require any specific as how that's done right right actually that that
argument is only about second theorem so it's not it's about the first first also
it has more cutting force on the second but the incompleteness it depends on you being able to characterize proofs inside
the arithmetic and GLE when he when he announced this theorem he was clear he
said to everybody he was unhappy he gave it for prinkipia Mathematica and related
systems and he was clear that he could not tell you which related systems he was unhappy about that and then he
starts telling people after Turing develops his his idea of a turing machine and his proof that the various
Notions of Effectiveness are equivalent he starts saying yes I think Turing is right what I mean is a recursive proof
predicate a a theory with a that's that's what related systems are any systems that has an effective proof
predicate okay and we live without effective crit IIA for lots of things all the time but I don't like calling
those things proof so I I want to have an effective Criterion for something I'm going to call a proof yeah yeah but it not necessarily a
formula right so proof is not necessarily just a a syntactic object in
that sense yeah at least this is what Serge suggests right that that proof on
his account is not just a formula it's a scheme where you put different stuff you
see and that's not a formula but if well but is is it effectively
recognizable if if I give a proof that what he can if I give what I hope is a proof for him can he tell whether it's a
proof or Not by his Criterion or does he have to say well that depends on some faction no yes because you can recognize
this scheme right you can recognize that that scheme but then you you should know
how interpret that so it makes part of what's all proof is a formal and what
you do with that you mean that's all formalizable right that's all formalizable yeah but then you will get
an incompleteness there on no look at his paper at least I well I see yeah
there have been a long the series of people who what they do is they don't have an effective proof predicate like I
say curry in the 1950s yeah yeah and like I I suspect he's going to end up there's not an
effective proof predicate okay that that's good Point yeah think anyway there was a comment
from the or from the on the chat yes hello um thank you to the
organizers um my camera may not be working um but I'm a real human being um
I uh wanted to ask David Colin and Andre specifically David because he's written
about computational alternar ISM but I think my naive question I think uh
applies to uh a new concept that you've spoken about regarding whether or not
you can um whether you can assume uh you know like the twin Prime conjecture and
an argument and from in the computation trinitarian perspective there's this uh
correlation between Type Theory computation and category and compost Theory um so for those who are not
familiar with that but if I were to express in say a programming language like hll to say to use something called
a maybe monad um and to say you know maybe this is true or not or nothing um
or I could use other monads like either like either this is true or this is uh
then I'm wondering how that might translate into your description of someone assuming that the uh twin Prime
conjecture uh you know holds um in the case of the mon add it allows us to perform this
computation uh assuming that there may be something or it could not we can handle that um and I'm wondering how
that might translate into the example that you're giving me what I might assume something and an argument um but
have that be open or probable um so I was hoping maybe someone could give some
sort of clarity to my naive interpretation of the computational trinitarianism perspective on that
example should I give that a go uh okay I mean yeah I'm not sure I can answer
directly to the specifics I mean just if people are wondering what what this is about this computation it's a funny name
and and a lot of people don't like the kind of religious Overture of trinitarianism be
dogmatic but you know as I say you really can you know if you encounter the
type if you haven't encountered type Theory before but you you come along and you see oh strange introduction elimination rules you know what the hell
are they uh you know if you've come it and your background is in category Theory well you know it's all to do with
co-units and units and and and isomorphisms arising from adjunctions oh
oh that's why it's there so yeah it's that kind of feeling that
that anything kind of that's really seen as important from one perspective really should have a a ready explanation in the
in the other in the other format yeah so you could you might think well why don't I just take it from the category the
theoretic side and just you know just talk about co-units and units and whatever so so but the thing you're talking about so anything that's
important so something around uh the things to do with monads so that's that's appeared in computer science it's
to do with problems to do with effects and resources this sort of world is where they they start employing monads
and comonads uh or or their equivalent of it and and right the category theorists
from their perspective understand whatever it was that the computer scientists were reaching for and and say look this is one of our constructions so
that's good trinitarianism says you found something useful because it's making sense in our language um so again
we go around the circle they're going to be very close to kind of Notions of modality so the type theorists already
would have because they're coming out of kind of more logical background they would have already had Notions of
modalities going on there um so again you get the tie in between them the modalities of of the legal the type
theorist this hit hits or reaches out to the the monards and nards of the category theorist again all all good
stuff I think you're asking a very specific thing about one particular kind of Monet or whatever that's appear
Hearing in in some aspect and you're kind of wondering about taking it around the triangle of this trinitarianism which
I'm not sure I can say too much to in its specificity generally speaking Yeah you know if it's important it will go
around this triangle you you'll find you know you'll it'll make sense through the other the other three corners of the
triangle
um yeah thank you um great well I guess uh David we're
going right into your talk now oh okay just trying to share a
screen sure okay hopefully can hopefully people can see that um good okay so uh thanks
of course to the organizers perhaps people are a little surprised to see this as my title um uh I suppose we
could put it down to I was having a nice chat online with uh with Eric and his his partner Alisa uh interesting wide-
ranging conversation about all sorts of things and we touched upon uh issues to do with psycho analysis and so on and
and so it was suggested to me to talk about this if I don't know if anyone even
knows about me but if they do they probably would know about me through a couple of books to do with philosophy
mathematics so so may be surprised to to hear that I know anything about psycho analysis so just to fill in a little
background so after a uh four years of math as an undergrad um you know usual
turmoil of of the young of the youth I got interested in psycho analysis and uh probably for the next sort of six years
or so it kind of it was my main intellectual interest was was in psychoanalysis and and I was even
thinking to become an analy and then I kind of shift over to to philosophy
keeping up some interest I mean I co-wrote a book with a psychoanalyst friend of mine called uh why do people
get ill so looking at the psychoanalytic understanding of psychosomatic illness
um but yeah really sort of disappeared out of my life but uh uh it's sort of come back last two or three years it's
come back in in quite a big way maybe maybe the troubles in my employer shaking the foundations of my life and
the demise of philosophy at Kent um perhaps stirred it up again who knows but anyway I've been looking looking
back at psych psychoanalytic writings uh in the last two or three years so just some thoughts uh tentative thoughts
should we say and hopefully they'll be of interest to people okay so as I say um
uh why is that not moving what's going on soorry my slides
don't seem to be proceeding why would that be
ah no that's very strange not sure what's happening there per one thing I might do let me
just stop the share a minute and share a different
sorry this just a little moment just going to there come back out maybe this will
work I share that
maybe is that hopefully is that going to work why is
that oh yeah it's not it's not full screen you know trick is you you first
doing it full screen and then use this stop button to get back to zoom that's
how it oh I got to come back out oh gosh sorry I got to come back out
sorry come to hear full screen where's my full screen on this
control got
it I can I just make the screen bigger
okay I guess that'll that'll probably do the trick Zoom uh share
screen okay ah still why is it not as soon as I do this
it stops oh I can do it by hand okay sorry so so here I am so last couple years or
so I've been returning to psych analysis uh still kind of intrigued to find there plenty of opposite position out there
plenty of the the kind of health care that's given to mental health care that's given to patients just doesn't
involve it uh Al all sorts of emphasis on cognitive behavioral therapy and uh
other modalities I'm still intrigued to find I mean something that you know when you
kind of start off you tend to align yourself with a particular school and that can become uh you know your
identity is formed in I I was interested in this French style Jac laon's school
and you tend to see everybody else as the opposite in some sense so it's been great to have a certain freedom to to
read Around uh yeah as I said confusion of tongues but there's a awful lot in
common I mean you just hear these resonances between these different approaches so I'm quite intrigued by by
that and and new approaches that that are happening um yeah I mean back in if
you if you if you read Freud it's it's sort of you know it's a bit like you're unpeeling these layers the unconscious sort of comes along with these layers
and you're you're kind of working like as an archaeologist and you're working your way back and you're wondering you know what does this mean what what does
this discovery back at layer 63 mean or or or whatever in terms of the psyche of the patient um it's probably shifted
quite a bit there's a lot more Focus these days on this sort of interactive dynamics of the relation between the the
patient and the and the analyst quite intriguing so so I'm more concerned
perhaps with the Here and Now of we're not kind of referring back always to to to Childhood necessarily there's perhaps
less emphasis on that more emphasis on the Here and Now interactions I mean of course there'll always be reference back to to earlier times um okay so the
theory grows it's all um okay so what's this got to do with mass well when I so after you know
coming out to Paris uh with my four years of mass behind me naturally enough
the the the French were were were quite interested to have me coming along because I mean Lacon was rather famous
for for trying to bring some mathematics into psychoanalysis I mean here he is uh
here he is in 73 saying a couple of things so why is he doing it formalization is our goal our ideal why
because formalization as such is our mathe it can be fully transmitted and there's there's the thought you know his
his Great Battle Cry was back to Freud thinking that Freud's theories have been misunderstood misinterpreted the the the
radical opening of the unconscious that he presented have been kind of closed over by various schools of
psychoanalysis LA on particularly gives American schools of ego psychology a very hard time last thing we want to do
is strengthen the ego he says uh so that they've misrepresented Freud and and the
yeah the troubles that natural language lends itself to this form of misinterpretation and Mathematics could
provide an answer because it can be transmitted and then in that second quotation is the only teachable
discourse so he's interested right from the beginning it's right there in the start 1953 this was one of his you know
big opening foundational papers often called the Rome discourse mathematical formalization that inspired Boolean
logic to say nothing of set theory can bring to The Science Of Human Action the structure of inter subjective time that
is needed by psych analytic conjecture if it is to ensure its own rigor okay so right through his life we
get these intrusions of mathematics into the theory and yeah a whole range of
other things take place through uh through his later writings often invoking Notions of of topology points
set topology uh of fun you a little bit on the golden ratio appears uh we we
start getting this division of of topological space like projective plane and we're dividing it up into the mobile
strip and the disc and this is supposed to uh sort convey to us the notion of the the the fantasy as he sees it and
then later later on I won't go into the details of what it means but the borrow me and Link you know that thing where
you you see three circles and they Interlock in in such way that no two if you take any of the the the third one
away that the two will fall apart but the three hold themselves together and he has some Spiel on
that okay so I mean I I a little disillusioned I mean perhaps not full on
sakal and brickmore the intellectual impostures book they chose laor as one
of their examples um for this sort of false use of mathematics um but there's
a pretty critical note through a chunk of of this article I wrote in 2002 that
at most it's kind of suggestive uh it's it's kind of you know
it it never gets the grip that you'd expect the mathematics as as is used in physics you know it really gets a grip
um you know you can you can use it you can the maical results mathematical results you'll prove will will be you
know essential to the work you're doing as a physicist whereas it doesn't seem to be the case in in the examples he's
giving uh been funny though coming to us just looking back to prepare for for this talk today just to see actually
kind of couple of things propped up um from I mean you I was kind of reflecting
back on how do I go so I did from math to psycho analysis and how do I get out into philosophy well one kind of curious
ingredient there's a bit where where laon's making play of of this distinction the distinction that the
intuitionistic Lo logician would would agree with that there's a difference between not for all X 5x and there
exists X such that not 5x you know the the intuitionistic magician wants when
when you when you make the second claim they want to get their hands on that X which is not 5x uh rather than just
achieving a contradiction from the assumption that all X is 5X now L's using this for different purposes but uh
it was something reading this at the time that that I started exploring intuitionistic logic um and that got me
looking at I can't can't quite remember the order but you know it's going to be very useful for me later in life with when
coming to learn about homotopy type Theory it gets me into um into lbeck and Scott introduction to categorical logic
and then as he's here let's uh is he still here Colin um from this I I
remember reading his um uses and abuses of history of topos which just blew my mind at the time and and kind of push me
very much in this in the idea of direction of philosophy and as regard with regard to category
Theory okay I mean maybe that was just a rather fluke sort of uh uh bit of inspiration coming from Lon but you know
it did set me on a certain kind of path feed into stuff I then do in my masters in philosophy on looking at the the
different Antics proof theoretic and topological semantics for inition so you know strangely enough
kind of important to me maybe more substantially though um that thing that was mentioned in the previous slide on
the Bor me and links and it links to laon's notion of the dimensions actually something he does say there really
allows me my first publication um strangely enough um again so I won't go
into the details of what lon's on about but he's talking about these Dimensions the imaginary the real and the symbolic
and u uh he's talking at some point as a ma mathematicians symbolize the imaginary of the real he says and then
he's also talking about kind of rotating these three terms and it occurs me reading so little
later on I mean I'm out there so the the guy I wrote the the health book with uh was his family knew lacosh very well he
was from Hungarian extraction um so he he thrust proof and reputations into my
hands back in Paris and that kind of got me on this path as well so I'm doing this for my PhD I'm I'm reading you know
obviously reading lacos a lot um and it occurs to me that the problem laos's
problem is that he the thing he's worried about is that there is an intuition a mathematical intuition and
then when it comes to formalization uh codification in axioms there is the big danger that you have
missed out you've lost some of the intuition some of the intuitive understanding has been lost and then
once it's set in place this symbolization it's lost forever okay so this is a really dangerous process in some sense I mean
you know it's necessary but it's kind of dangerous you're going to lose this what's happened before and the whole thrust of this first article of mine
later appears in the 2003 book of mine um is that this isn't the whole story
and and that if you rotate them round then then there's a um there there's a there's a real from
the symbolic if you like the symbolic itself becomes the platform for a new
real the the space of kind of possibility and impossibility generated by the symbolic which then the maeti
then has to imagine so you kind of rotate these things around so I mean I don't say it in the article but it was
very directly inspired by laal
strangely okay so what now so amazingly enough there I was a couple years ago um
so so so there I am in in what am I around about 25 26 or something and um I'm reading a lot of category theoretic
logic reading Collin's work and and fascinated by toppos Theory uh and so
interested in Lac on too amazingly enough um you know a couple of years ago here's Alan K so the the French field
medalist the the guy famous for non commutative geometry produces a book with a a
psychoanalyst colleague you in the shadow of Gren Deacon laor I mean my God these two these two things that were
looming so large for me back at that time and here he is coming up with the claim so this is uh if anyone knows
Lacon this is this is clearly a play on that so laon said the unconscious is structured like a language and and and
his his group PA saying no no the unconscious structured as a topos now I didn't I find the book of
Tad disappointing it it go you know so con is trying to help us understand what
what a topos is and uh you where's the big link to psychoanalysis
um well it seems to be coming through I'm I'm going to flesh it out a little bit as I say it's a tad frustrating what
he's trying to do there he's trying to tell us so he's interested in the the the the logical Dimension to the top of
um and he's explaining the idea that topos acts as a classifier of a geometric Theory a particular sub subp
part of logic so geometric logic is a certain kind of fragment of
logic um and a topus is acting as a classifier of of a geometric Theory and
he's trying to relate it to what Lon calls the the the fundamental fantasy or fantasm in
French unfortunately doesn't really get exp explored enough what he means by that um
just to give an idea what whaton mean by that I me that thought is not foreign to to to a lot of Psych psychon analytic
schools that you can often condense when when you kind of come to the end of a case there's almost there's almost a a
single schema by which the patient has organized their life okay in terms of the way of they relate to the rest of
the world um you know almost Bo boil could be boiled down almost to a sort of
single proposition or something of that kind um so here we we have this is
leanian psychoanalyst talking genv Morel and she's as in that citation the the
fantasy to take this is she's citing lack on the fantasy to take things up at the level of interpretation takes on the
function of an axiom that is is distinguished from the variable laws of deduction as though it's acting as this
almost sort of constant invariant in the life of the the psychic life of the patient um so here I am expanding on on
What's k saying trying trying trying to link them a little bit um
yeah right again doesn't feel a great deal more than suggestive I I'm a bit it kind of a bit
surp you you think at the very least you would be interested on what changes to the extent if if you really really do
believe the unconscious was structures at the top us and to the extent you think that transformational processes
going on in psychoanalysis you you'd think con would want to talk about you know what goes on does the topos to
change does this geometric theory that is the the foundation of the being of the patient does that not shift as as
the psycho analytic therapy proceeds but he doesn't say very
much um yeah I think I perhaps don't have to oh I think there's a lot more
work to be done on that I mean what what so what else is possible just just a I mean the very if you take if you go back
to kind of very mainstream this this these sort of concepts are very much shared by all sorts of schools of Psy analysis and then you can get the feel
that something mathematical is in the air at the very least and if you take these three concepts I mean they're you
know they're practically the language of mathematics I mean splitting there's all sorts of ways that splitting appears in math splitting of sequences and so on
projection projectives projection is is all over the place um just give a sense
of what these these three concepts are um so splitting uh particularly the
school that could kind kind of comes after Freud the the British School of object relations theorist uh yeah the
sort of mental separation of objects into good and bad parts you know particularly if you're mly cin sort of relating to say to the mother the
maternal function The Good Mother the provider and the bad mother the the depriva
um and and the the child then having to deal with this this the good and the bad the bad sort typically getting kind of
split off from the good and then and then sort of dealt with defensively in certain kinds of ways
uh so a lot work of the therapy will be to to bring about a kind of recognition if you like in the in the the patient
that the good and the bad can kind of coexist in the same entity I mean you know you know putting it in certain sort
of terms as though really it's just the mother she contains both the good and the bad so what you so there has to be a
a kind of overcoming of these opposites should we say this is this is so one way of understanding the kind of the work of
the therapy um so so along with this is an idea the pro so projection is an idea
that these sort of the bad the bad bits the repressed bits the bits that cause anxiety are kind projected out into the
world okay so plenty of plenty of analysts are kind of really interested in that phenomenon when you encounter
something maybe in another person you see them behaving in a certain way and there's something there's just an energy
there there's something you just you just loathe you hate in what they're doing uh you're not just you're not just
annoyed as though that's just you know bad form or something the way somebody's behaving you you know you're really gripped by the way it you you you hate
what they're doing and whenever something like that is a foot it's extraordinarily likely that there's some
something of this projection going on and you should pay very close attention to the trait that's really disturbing
you because it's very likely to be kind of there rather PES in yourself I don't
know if You' ever tried this the things that really annoy you and other people are often things that have a bearing on your own
life um the the third the projective so
projective identification is kind of interesting phenomenon where where you you you take this to an extreme where you not only now am I finding am I
locating the sort of bad parts of myself outside but I'm also going about the business of making it happen I I want to
generate in the other person these bad traits this is going to reassure me in my worldview because if I can generate a
certain kind of bad bad behavior I don't know a kind of Faith faithlessness or something in a partner or something uh
and I'm doing it through my own my own work this will this will kind of conf confirm if you like uh you know the the
kind of infantile Theory that's governing my life so you it provides a kind of a way of kind of controlling
your own internal psychic Space by kind of forcing the other into a certain kind of
role um okay so these are the kind of
glimpses of you can sort of see why you might imagine certain kind of glimpses of something a little bit mathematical going on
here get bring it a little bit closer so so what you're seeing there and the work of the patient is is the necessity of
overcoming if there anything like that there's such a sharp split between the the good and the bad in some sense
necessarily the work will involve an overcoming of these opposites there' be some sort of reconciliation somehow in
the psychic representation um we also see it I mean God if if i' been daring to read reunion
work back in the back in my 20s I mean any kind of post any Freudian post Freudian would have would have torn
their hair out I mean you know they're so so entrenched to the school divisions that one wouldn't do that but one of the
freedoms of coming at it from this angle is one can read this Yan literature and you find um yeah again there's a lot a
lot of commonality between these schools so there there's a lot of talk of the of the tension of opposites so we got these
opposites within us in some sense and and the key the key is to hold them in some sense so that one can transcend
them the Transcendent third his his young the tling to and fro of arguments and effects represents the Transcendent
function of opposites the confrontation of the two positions generates a tension charged with energy and creates a living
third thing not a logical still birth in accordance with the principal tertium non dator but a movement out of the
suspension between opposites a living birth that leads to a new level of being a new
situation now people reading this from a uh with a certain kind of background are going to perhaps they have the you know
Hegel lighting up in their minds um it does it does
there's some idea and then particularly through um perhaps through DreamWork or other kinds of active imagination uh
that something like a symbol will emerge which will allow for the integration of the Opposites in in a new third
thing I mean you know every everybody has had that well not everybody but I mean people anyone who knows anything
about Hegel you know comes up with that thought um sounds sounds doesn't it sound like the hegelian dialectic going
on now the the one of one of the perhaps the strangest intellectual phenomena
I've ever experienced I think is is watching this colleague of mine so o shriber mathematical physicist for
interested in category three he just got gripped by Hegel science of logic for for a certain period and um he was
taking up um L's work on on translating uh uh the Hegel alung the the
Transcendence the raising the alabon relation where you you take a opposites
I Andre talked about the unity of opposites so you have these opposites and you put them into this adjoint
relationship to each other and and transcend in some sense up some kind of ladder
um so L had done this uh as a means to understanding um what out of you can do
this way out of the opposition between between a sort of being a nothing a zero and a one if you like out of this you
can generate using this this this this alha haben um methodology uh some notion
of an opposition between discreetness and continuity so C sorry C- discreetness and and discreetness
discreetness where you've got a you know bag of dots and they're all separate from each other and co- discreetness
where you you got your bag of dots but they're all connected to each other so these now lie in opposition to each other and in some sense they've resolved
the opposition between the zero and the one if you if you follow through the the
the mathematics of it and you can take that ladder up and up and up and and and Earth does this to uh
to to to bring in not just the the cohesion that's needed to understand things topologically spatially uh and
then on we go to understanding things differentially in terms of their smooth structure and then you can take it on yet further to to bring in the the super
geometry uh that he's going to need you know to to codify Modern physics okay so kind of curious kind of
resonance is there um whether there's a genuine pathway through is is another
matter between the the the said the mathematics of of um adjunctions and and
what's going on uh is
another excludes himself from from from being counting as a hegelian but here he is somebody pointing out Hegel
nevertheless seems to be the only one who could provide um to Union psychology the kind of logic
with which alone it could truly comprehend and say what it has to
say now just to finish so the there I'm probably only offering you the most the
most sort of suggestive residences where something one could begin to do something I think you know somebody gave
me a lot of money and just let me S think for for a long time something I might want to pursue is is what I've got
here on my final slide let me just finish with these thoughts so again yeah so it's a curious
thing going through the psychoanalytic literature because you're kind of sim
there similarities and yet there differences all over the place it's it's it's it's very curious and yet you sort of get the feeling they're talking about
the same thing and yet they seem to be phrasing things in really quite radically different ways yeah it's like
we need a kind of computational trinitarianism but for for psycho analysis maybe um so one fairly recent
somebody very much identifies himself in the Freudian tradition and and yet in the very Act of what he then does is to
he he and there's various work that's gone on in psychology of um the emotional systems the emotional drives
particularly the emotional drives that correspond and speak about our relatedness to other people um so one of
the psychologists like PP so on you know and they're looking even for neurological groundings of these
emotional systems um so to do with fear and and rage and caring and attachment
and so on and and and even things like play and the the drive to seek uh to go
on a quest in various kind of ways and last um so these phomes These are these
are seven emotional drives I mean already you know it's kind of gone post Freudian in the sense that Freud seems
to want to relate everything back to the the single sexual drive unless you count
the death drive but you know it's already Post frud in in the sense that those object relations people seem to
have they generate all that kind of work on attachment Theory so there seems like there's a kind of other emotional drives
coming in and SS kind of does this work on the the seven drives and is then intrigued by the way
that um you know they can't be sort of resolved simultaneously you know the kind of
worse you are in some sense in terms of your mental health the less able you are to to find any kind of way of
reconciling your seven drives um you know so so famously there's a there's a
the kind of person who you know desperately seeks intimacy with another person but as soon as they do this as
soon as somebody sort of has let into their lives that uh they must be rejected because there's there's some
kind of fear that you know violence to them or or Fe maybe fear of some sort of
psychic absorption into the other person anyway there some sort of fear or anger is is generated in them by by this
proximity so it's as though you've got these sort of two two drives in kind of conflict with each other and they both can't be satisfied so it's like this
High dimensional space you're kind of negating your way around um and you know
sadly because of the way we're constructed it's as though we had to resolve these find our strategies and our policies for
kind of spping around this space from a very early age uh when we're not in any kind of way shape or form to to learn
the right strategies to move around the space so we get this idea of a premature autom automatization is a way of dealing
with the with with our systems being kind of out of out of kilter which then often persist into later lives and we
fall back on these these rather faulty strategies in some sense so the kind of work of analysis is to is to sort of
come back and remove or or or perhaps not remove but modify in some sense the
the the space of the kind of policies by which you navigate your space okay so This Ss is interesting
he's he's I don't know if you know about the the the world of active inference SL predictive processing slash free energy
principle so SS is cly talking to friston on that and a bunch of other people um and again we have a link back
to category Theory there's there's plenty of people well not plenty but there's certainly people very interested in this community I mentioned before as
I said hopefully well I am about to join at least for for a year and apply category Theory uh and the people there
trying to trying to understand um using a play category Theory this this active
inference and right there's some really kind of interesting stuff going on around I don't if you so if you follow
the apply category Theory world there's there's a lot of work on dynamical systems in a categorical sense
categorical systems theory you're seeing that in the work of of David SPAC and and others
um and so Toby smile is trying to get a kind of structural structured active
inference to kind of wrap this into to uh to to to yeah so so categor
categorical system Seri rep is is is providing the kind of groundwork for this structured active
inference and there kind of feels to me there's a sort of the pathways through there to think about
particularly the so B fin out the you know they're really analysts really see
psych analyst are really seeing you know don't don't look at the individual as a unified mind there's this there like
there's a sort of Committee of people in there almost there's there's there lot of there different voices in there they kind there's competition if you like
inside the the psyche um so there's all sorts of intra intas psychic intrapersonal Dynamics going on and then
bring that person into relation with somebody else maybe the analyst but but the the familiar people in somebody's
life and things get yet more complicated in all sorts of ways um anyway I have
some hopes there that one could push something along in that direction but as I said all all I can offer is is rather
speculative at the moment sadly but you know who knows if we keep on thinking about these things at which point I I
shall finish and thanks very much for for
listening thank you so much l that was great we just need one second to set up
for Rocky who's in person some slide deck change over yeah
people online hear this can people out there or can you
hear my voice that sounds good okay
great test test
if yeah if people can hear me in um in house then that's fine if that
works e
okay great um all right uh well yeah uh first off thanks to uh Eric and Connor
for organizing this uh thanks to the other presenters and everyone who's here both online and in person um the basic
idea of this talk is to provide uh I think a nice um
intuitive case of the utility of category theory for modeling
certain complex phenomena and in particular the use of uh an elementary
category Theory construction called uh pre- sheaves so uh the task here is basically to model autopoetic systems so
systems that construct and reconstruct themselves using prees uh and this is collaborative work
um
oops okay um this is collaborative work with uh a couple of colleagues of mine
John Luka Katarina and Fernando to we've worked together for uh quite some time now on a variety of uh research programs
linked to category Theory and various modeling um much of which uses prees in
various various ways um so to open things
up oh damn sorry I need a little technical
help
here how do I just can't if can I navigate with there it is okay go all
right um so to start with a uh a quote from a
Wallace Stevens poem The Ultimate poem is abstract um and two things to note here
one um that the the kind of poetic
program that Stevens is uh proposing here is I think uh closely linked to
people like Mal uh it's kind of quintessential poetic modernism um and
this is uh quite close in spirit I think to the project of modernism in
mathematics in particular the development to set theory and then from set theory to category Theory a nice
reference here is the book by Jeremy gray Plato's ghost where he tries to link or or interpret um 20th century
mathematics in terms of cultural modernism so that's some kind of interplay between uh the Arts and and
Mathematics here uh but also in terms of the content part of what makes autopoetic systems very interesting and
complicated is that they probe their environments and modify themselves they
don't just repeat themselves but they change right they ask questions and this is especially difficult when we're
trying to produce a formal model of a system to get a formal model that's robust enough to uh be subjected to
various kinds of alterations and creative um
exponentiations uh so to begin uh an image from uh a an ancient
diagrammatic text uh this is from The Alchemist Cleopatra of Alexandria um we
have a copy of of a onepage kind of alchemical uh Treatise from maybe the
10th or 11th century and this is uh the serpent or uh dragon or a boros right
the the the snake that eats its own tail uh and one of the things I like about this image is it gives us a way of
thinking of Duality which is an important concept in category Theory we think of the snake eating its tail
consuming itself but if you turn things around you could think of the snake's head is spilling out of its tail Right
Moving in the other direction and thus constructing itself uh and if we take this kind of uh
ancient image as a guide right we convert it categorically into something like this this is a nice simple
straightforward category we call it s uh it has one object which we also call s
um and uh we have one non-identity
morphism so every object is equipped with an identity morphism in category Theory but now we have um an additional
morphism or Arrow labeled Sigma and we think of this as a path from s to itself
and we allow Sigma to freely generate all of the paths in other words all of
the iterations of s itself so we have Sigma Sigma Sigma Sigma Sigma Sigma and
so on uh so this is our category and what we're interested in is a
prief nice um which is assigning to this
category a set for each object and a function between sets corresponding to
each morphism in this case we only have the one object so we call our prief our
funter F uh and we have F of s which is an arbitrary set and to Sigma F will
assign an arbitrary function from F of s to itself right and then of course we
can iterate F of Sigma as many times as we like corresponding to the iterations of
Sigma and if we think about what F of Sigma looks like from the inside right
we can construct a kind of internal diagram um and this is very familiar familar to those of you who have U maybe
worked with dynamical systems right in fact a prief over our category s is just
a discrete dynamical system um deterministic discrete dynamical system so we can think of each element of the
set has exactly one Arrow Which F of Sigma assigns to that element taking it
to some other element and if we isolate the the topmost element in our set here uh and
call it P as so and we kind of zoom in on P you'll notice that P inside the internal
diagram looks like this it's a kind of microcosm of our original category uh
but with the difference that F of Sigma of P when iterated just takes us back to P
every time in other words p is a fixed point for the function f of Sigma um and
in general fixed points are very important in discrete dynamical systems they represent equilibrium States um and
these are very important when we think about the Dynamics of um systems in in
economics physical systems uh in logic uh and so on but this isn't really the
even though fix points are very important for modeling autopoetic systems it's really not the concept that
we want what we want is something closer to idempotence so
idempotence is analogous to a fixed point but it's a property of functions
or operations themselves right a fixed Point uh has the property uh F of P equals P under the
operation f idempotence is defined such that applying F to
any argument more than once gives the same
result as applying it once so F of f ofx will be equal to F ofx for all X and
using a kind of suggestive notation um we can eliminate the X and say something
like f applied to itself returns F right and in fact that's a a notation that
actually makes sense uh in in an a system like combinatory logic where
operations operate on other operations including themselves so idempotence is
closer to what we want um but uh as written there on the right hand side um
autop pois is not just iterated identity but iterated structural equivalence in
other words we want F of f to spit back F but potentially with some difference
right and so I've notated that here by by saying as we as we sort of extend to the left we want um starting from the
right F Sigma of s applying to that value some new
operation fime that might be generated somehow by the value of f Sigma of s
that F Prime of Sigma applied to that and then some f double Prime in other words the operation should be quasi
epitent as we continue to apply it uh we we get a similar result a structurally
equivalent result in some sense but it should be able to update itself right and so in order to capture this more
subtle Dynamic uh we need to move up a level to categories of pre sheeps so
here uh we take our pre sheef F ofs and we embed it in the category of all such
prees so this is a topos right the
object of this category uh is going to be a prief from s into set um or sop
into set which just means we reverse the arrow here that's that's uh not
important um and then the morphisms of this category are what are called
natural Transformations between prees so these
are uh ways of taking the uh the sets that are assigned to
each object in the category and assigning them through functions to other sets in another pree in a way that
preserves the structure in this case um if you take F of Sigma if you pick some
element in F of s update it to F of Sigma and then follow the red arrow that
will take you to the same element in fime of Sigma as if you follow the red arrow and then update with fime
Sigma don't worry if that sounds confusing confusing just know that a natural transformation is a structure
preserving map between prees and then um what we've developed
in order to try to capture this uh sort of uniquely
self-transforming character of autopoetic systems uh is what we call a Spyro
morphism uh and the actual construction is is a little bit technical uh but the
basic idea is simply to associate with certain
critical elements in a given pre- sheet think of these as elements that matter
right in which some kind of creative moment takes place uh we associate to those system states morphisms that is
natural Transformations between prees so this allows us to not just update from
System state to System state following the um uh the operation of the dynamical
system but to update the entire dynamical system itself all right um but of course this
isn't the whole story right when we're thinking about autopoetic systems um
we're thinking of a picture more like eers hands drawing one another right a system
exists um at least in the real world in relationship and interaction with its
environment and just as the environment is imposing structure on the system the
system system is capable of modifying its environment right so the the
autopoetic loop goes one level up and is now happening within the system but also between the system and the
environment and so uh in order to try to capture this we work with uh an
expanded category that we call es so now we have two objects still relatively simple um think of them our our old
category s with its morphism Sigma but now we have an environment modeled by
the object e with an update function on E called to and now between e and s we have
morphisms going in each direction that we label tell think Tey and debt think
determination and because a prief is a contravariant functor which means the
the morphisms reverse their Direction when we take a prief tell is going to be
going going from F of s to F of e so Tey
here is going to be going in a sense from the system picking out an optimal
state in the environment and determination is going to take a state of the environment and give a
determination of the system so the arrows seem to be going in the wrong direction but that'll go right in the in
the preet so the morphisms now um just as in the previous example are freely
generated so not not only do we have Sigma and tell and Deb and to but we have all possible paths so we have too
followed by tell we have to followed by tell followed by debt followed by to and
so on and we're interested in prees over this category right so again
we assign to each object in our category a set so now we have F of E and F of s
um and we assign to each of our morphisms in the base category a function between sets and in this case
they go in the opposite direction so you can see we have Fel FD ft F Sigma and
all of the corresponding paths um and this gives us a way to
think about interactions between system and environment um and the simplest case the
one that becomes almost trivial is what we call uh stct system environment
concordance uh and this is uh determined by these identities um or or
inequalities represented on the left um and this just means that
the state of the environment always uniquely determines a system State and
the system State returns by way of Fel to exactly that same environment state
so think of this as a kind of Servo mechanism the system is being complet driven and determined by the environment
um but you can still Model A lot of interesting things under this condition of of concordance a really nice example
comes from John V noyon um who did the pioneering work with Cellular automa in
the late 40s uh that was then kind of finally published uh a decade or so later by a colleague uh
Burks um but what what vyon did he developed the notion of a cellular
automaton which is just a discreet dynamic IAL system uh you might have heard of like Conway's Game of Life
think of a um uh a an array of
pixels kind of an infinite array of of of square uh cells and then you have
some of them colored in some of them empty or maybe they have different colors uh and then you just have some
kind of update function that tells you well if this um cell has a certain number of neighbors of a certain type in
a certain configuration you update it to some other state um and what Von noyon was able to
do is build a cellular automaton a relatively complicated one such that
with within the context of that cellular automatone he could Define what he called a universal
Constructor and Universal Constructor is essentially a kind of mini touring
machine that takes as input a uh an input tape uh and then can output based
on the coding of the tape an arbitrary uh arrangement of cells in the cellular
automaton you can essentially draw anything that you tell it to within the cellular automaton and then what becomes
autopoetic is um you can then use the universal Constructor to under the
right input tape produce a copy of itself so it draws a copy of itself
which then under the operations of the automata will perform the operation
again and so on and so on so it produces infinite copies of itself together with its tape blueprint so it can copy itself
the important Point here is even though it looks as if the universal Constructor is doing something autonomously right if
you watch a visual um representation of this you'll you'll see the little shape and it actually looks like it's building
a copy of itself but in fact everything that it's doing is completely determined by the rules of
the more General cell cellular automaton that's determined by Von noon um so at
the end of the day this means that the system s is really just a kind of subset
of the system e right really you just have one updated system where you can if
you want to pick out a part of it that looks like it's doing its own thing the more interesting case is when there's
Divergence right so basically the system can diverge from its environment in two
ways if um if you if you look at the um the graphical diagram here F of e
represents the environment so right now assume that it's in statement uh System state green right well that means
following F debt that it will determine our system FS to be in state yellow
right so green green is determining yellow well if we had concordance then F
of yellow would take us back to green but in this case you'll notice
that um the update function f Sigma of
yellow takes us to something different from if you follow green updated F tow
right followed by F de in other words if we interpret this if we allow the
environment to run under its update conditions we'll end up with our
system in the state at the lower right but if the system were allowed to be
autonomous and do its own thing following Sigma it would take us to u f
Sigma of yellow so notice that that dashed Green Line represents a
Divergence right the emergence of a difference between what would happen under the update F
Sigma and what would happen under the update ft followed by
Deb the other way that the system can diverge is by way of
theological uh representation tell so each system state has an F so now we
have yellow F of yellow is not the same
as uh green in other words under our uh interpretation yellow would prefer to be
in F of yellow that environment state would have some kind of preferential relationship to the system
uh that is not provided by Green right so think of yellow as wanting that different state and so if we compose Fel
followed by FD of Fel we end up following the dashed yellow yellow arrow
where in some sense uh the system in state yellow would like to be and again
notice that that's a different kind of Divergence so we have two now different ways in which the system under its own
update function can diverge from what the environment is determining the
system to do so the question or the problem is how do we bring the these two
different kinds of possible Divergence into the model so that they can possibly
update the entire system environment interactions in a way that would
correspond to autonomy andology and this is where as we uh did earlier we move up
a level to the category of pre- shees over this category so now our objects
are the prees themselves call them f f Prime F double Prime um and the morphisms are the natural
Transformations between these prees um in this
context Spyro morphisms then become roughly speaking right again the
technical details are are a little bit tricky roughly speaking we're taking
pairs of States one from the environment and one from the system and then based on these divergences that can be defined
through the debt and tell functions right the ways that and here here the
two yellow or the two red States one in the environment and one in the system the ways that they might diverge
internally through debt Andel that modality of Divergence will then
determine a natural transformation right a kind of update function of the system
and the environment and their interactions as a whole right and that gives us flexibility to talk about many
different ways that the system environment interaction can redirect
itself based on what the system wants in the sense of the tell function um and
what it um self-d determines autonomously in terms of its own uh
Sigma function um so what's what's the upshot
according to this model which is is I think quite General in the sense that it
can cover a lot of ground um what what matters is always a
kind of interplay between causal determination that's coming essentially
from the environment and some kind of
teologico or um a a break with is in the direction of a that deter that is
determined primarily by the system um and the kinds of
applications for this for modeling this this interplay uh I think is quite
diverse uh David corfield mentioned uh active inference program of uh Carl
friston and others I think that would be one uh place where where this Dynamic can be recognized uh in the work of uh
Nikos Luman um there's uh explicit
reference to poetic systems through communication and the use of uh George
Spencer Brown's uh formal calculus the re-entry of the form this
um uh this application to Sociology seems to involve this dynamic between
determination and freedom or autonomy um might also think about the um the I
think very interesting work by Roberto mangaba anger Brazilian political
theorist uh who attempts to uh critique both uh liberal democracy and
also social democracy in the kind of let's say the Scandinavian mode in order to think a kind of radical
experimental um self-updating mode of society and politics I think anger
explicitly wants to um kind of uh um raise the temperature of the ability of
social and political institutions to not just update themselves say through voting but to update their own manner of
updating right which is again the uh the core idea of the model um was also I
think very nice work by an franois Schmidt uh in in France um she's
developed the idea of generic epistemology which is a way to think about knowledge relations um that are
specifically uh abstracted from the concrete domains where they originate so
is to be able to um organize uh formal
Bridges right that would be creative and constructive right so thinking about ways that uh developments say in um
cellular biology and developments in computer science which might seem a prior to be very different Sciences uh
can actually mutually inform one another in a creative way if you think about something like synthetic biology um and
then a colleague of of Schmidt um at the at Perry mean uh Armand hatchu has
developed a nice application of the mathematics of forcing um in design Theory U it's
called concept knowledge design Theory and I think it's also a similar dynamic in terms of uh industrial design you're
you're constrained by materials by physics by technology um in the process
of design right but you also work within various degrees of of Freedom if you're trying to build an engine right your
engine has to work uh it needs fuel and so forth but you're also free to make it
larger smaller uh to produce um you know more or less cylinders and so this this
inter play in design work between the degrees of constraint and the degrees of freedom I think is captured by this um
by this model uh and then in terms of the actual question like what's the um
kind mechanism inside this um spom morphism Dynamic like how do we actually
assign the values to the Natural Transformations based on the system and
environment structures basically um what we're using is a kind of game
theoretical Dynamics um in which classes of
strategies for the update somehow move
between polls of interdependence so think of um on on one poll of of
strategic development there would be a maximizing um couplings between the
system and the environment this would be very close in spirit in biology to Niche
construction so think about a species that um behaves in such a way that the
environment is changed by that behavior in a way that is um conducive to the
survival and reproduction of that species say but you know territorial markings by a certain species um or
something like this so that would be the poll of interdependence the other pole The Other
Extreme would be maximizing system autonomy so trying to develop this is
this would be primarily through the updating of the the sigma function uh ways in which
the system remains robust under its own Dynamics um regardless of perturbations
in the environment so think of this as a you know maybe a cell that constructs a very very tough membrane so that even if
things are changing um in the in the ambient uh environment the cell Dynamics
within the cell can remain more or less the same right and then the key Point here is that we don't think of these as
Alternatives but rather polarities such that any strategy right
any autopoetic updating is always some way of
composing modes of interdependence with modes of autonomy in a way that is something like a wager or a gamble on
what will maximize the system under its autopoetic
continuous so um to return to or boros
uh thanks a
lot yeah think yeah thanks everybody we're taking an hour break
so see you at 1 pm Eastern
Time e
just going to wait a couple minutes for people to trickle in from outside it's very hot in this room for
all on Zoom
just it's very fluxus
extremely why when Mass Unis moved in there there was no climate control of any kind
I guess he didn't literally move into the building you're in
go for
hey everybody welcome back um we're starting off uh this afternoon's program with uh Matt tikman
he's a programming specialist at the digital library Development Center at the University of Chicago where he
builds software for digital archiving and does full stack web development his main interests are functional
Programming type Theory natural language semantics and feminist philosophy he also is the host for
elucidations podcast uh Matt take it away thank you I'm gonna
go ahead and share my screen get my slides going here okay everybody can see the
slides I'll take that as a yes uh so greetings from California um I uh today would like to
talk uh to everybody about modularity um and I'm going to spell what that means um and I'm referring to this certain
like um folk WI that's been passed down through the last 25 years of software developer uh software developers in
Industry uh due to the rise of agile programming and I mean it it goes it traces back before that too but it really kicked off that J programming
early 2000s mid 2000s uh books such as clean code by Uncle Bob Maron uh agile
Manifesto Etc um and I am very sympathetic to um this um to this
approach uh just a minute everybody I'm going to fix my zoom control oops fix my zoom
controls for a sec here they're blocking my view of my own presentation okay I think that's
better uh so I mostly agree with the uh with what the agile people uh think and my goal today is to just sort of unpack
it sort of figure out what what are they really getting at so um here is a sneak peek at what I will argue in this
presentation so I'm going to uh try to say that modular code modular software
is code that begins that builds exactly what it needs from scratch in small increments um and I think uh
philosophical arguments are exactly like code in this way so they benefit from modularity in the same way for the same
reasons that software does um and but there is a problem which I will talk about with modular code and
I think that same problem carries over to philosophical arguments that's a sneak peek so um what do I mean by
modularity um well that's tricky to pin down precisely but let me just start with an example the example I'm going to use is rout
13 so rout 13 if you haven't come across it before is a special case of the Caesar Cipher which became popular in
uset groups of the 80s uh for people who wanted to talk about movie spoilers and but hide what they were saying about the
movies from people who didn't want to have the you know hadn't seen the movie yet want to get spoiled so um you know
start from start with some text this is how r t works and I just took a you know your classic Banger from Martin L here
every mathematical object is of a certain kind of type all right so if I want to rot 13 scramble that all I'm going to do is I'm going to advance
every letter 13 letters ahead in the alphabet if I hit Z I'm going to loop back around to a so that's the that's
the r 13 algorithm very simple and it's a way of uh temporarily hiding text from
people so they can't read it but then they there's also an easy way for them to get the text back because you just rotate the letters you rotate each
alphabetic letter 13 letters ahead again and you get your original thing back so it's it's the it's the symmetric
Caesar Cipher um so um here is some nasty s code that I wrote O camel um if you're
wondering why I wrote an O camel uh the answer is simple it's my favorite programming language and you only live once so why not use your favorite programming language uh so this is some
nasty code I wrote Noam what I'm going to do is I'm going to progressively make it less and less nasty uh to try to bring out what the good making feature
of modular code is so um anyway here's here's some code I'm gonna talk more about it here's how it behaves so you do
R 13 on high you get of R 13 on how are you you get OB your uh if you do it on stuff with
capitalization punctuation it's going to respect the capitalization but ignore the white space and punctuation ignore
if you give it nothing but stuff it's going to ignore it's going to ignore everything and so too for this example so that's the behavior of the r 13
algorithm and the folk wisdom from the agile folks like Uncle Bob is that this type of code is a problematic because
it's trying to do too many things at once it doesn't just have one job it has like a million jobs and it's mixing them
together um so the thing what you the thing you want to do to fix code like this um that's kind of ugly is Factor
off parts of the logic into into intermediate steps and then such each step you factor off has has one job
instead of 50 so here's the code I you know in a little nasty little bit of
emical I just wrote to uh do rout 13 um here's the part this antic to this conditional expression here is uh what's
going to check for if the character this sorry this is the part that's going to look at each character this is the part that's going to check whether the
character that you're on is uppercase or not and it's going to rotate accordingly if it's uppercase versus lowercase so
what we can do is we can factor that off into a helper function we can just say here's the code to check if a character is uppercase that a function and you
give this you give that function any character it's going to tell you whether or not that character is uppercase make it a helper function for the code and
now suddenly look at that antic to that conditional so much easier to read now is it not it's checking if the input
character is uppercase much easier to look at than the Big Blob of code greater than or equals to blah blah blah blah blah that we had
before um while we're at it there's a second uh nested conditional in here
that also checks if uh the input character is lowercase let's factor that off too so now we did two refactors
from the original code and I'm just going to assume that modular means in some Loosey Goosey
sense of the term to be nailed down uh maximally refactor so you factor off everything that is let's just say
reasonable to refactor um so that you know the the clean code ADW folks um want us to uh
want to do this as much as we can and they areg that gives us the following perks this is what uh you know you start
a software Dev job and your manager is gonna it's G to they're gonna start telling you this stuff uh so uh uh all this refactory makes the
code easier to understand perk number one perk number two makes the code easier to change perk number three it
makes the code easier to test debug perk number four it makes the code reusable or makes some parts of your code
reusable whereas before zero was reusable so let's just like take a look at that example and think about how it
has those perks so we already noted that it's easier to understand the antecedent of the conditional the two NE two
conditionals here uh you know if we Factor off uh the Boolean that appears in them into a
function because we can now see oh first we're checking if the character's uppercase then we're checking the characters lower case much easier to
read so that's that's in a nutshell how we get readability out of this
extensibility well let's say I'm just going to make up let's say we wanted to add a feature to Rod 13 here and we
wanted to rotate not just alphabetic characters but also digits well what would we do I mean the the straightforward thing to do
would just be like the pattern we have now add another helper function in here check if the character is a digit and
then add another nested conditional down the bottom to check to perform that check which you defined above F the same structure that already suggested itself
from the first refactor of the uh into into his uppercase and his lower case so the the minut we start refactoring that
suggests a way of extending the features of the code we had before easier to fix well how is this
easier to fix imagine that a um oh yeah sorry first I'm going to so these are
like local functions which if you're not a programmer what this means is that is uppercase and is lower case these are secret to the whole rest of the code
except for the r 13 function only the r three function knows about them but we pull them into Global scope now
everything in this file that occurs after his uppercase and after his lower case everything this file after them now
can now knows about them so that's just another Global scope put pulling that I'm going to consider local and Global
to be mostly equivalent except for this fact so he pulled them on the global scope and now we can start testing them in The Interpreter so now imagine a
hacker comes along and see you see what I broke the code there imagine the hacker comes along gets on my computer and breaks the code
and I come in the next morning I'm like dude what's going on with my R 13 code I'm supposed to I try to R 13 Jackal up
it doesn't R 13 anything it just keeps Jackalope what's going on well in this situation I can use the predicate that I
factored off to help with testing so let's let's test is a lower case works
ah what do you know it thinks that lowercase a is not lowercase oh it also thinks lowercase j is not lowercase oh
wait it thinks asky bill character is lowercase that's clearly messed up let now now let's go look at the code for
his lowercase ah well there you go we can see it's checking in the wrong character range it's checking the 26 letters
starting with ASI Bell rather than 26 letters ahead starting with ASI lowercase a so we just changed the seven
in there sevens in there to 97s and now we fixed the code so the takeaway here is that the refactor allows us to um
check for bugs in particular places and not just all at once we can check for bugs in parts of the code rather than
having to check for bugs in the whole blob and AD just accordingly this really helps if you you know if you're a
software developer you've probably experienced this really helps uh you find bugs
faster so reusability um the fourth commonly cited perk of mod modular code
is that it is reusable so um uh and I think the the two refactors
we did here are indicative of that so we have we now have a you know really general purpose functions to check
whether a character is uppercase or lowercase and um you know if you're not a developer maybe you haven't
experienced this but if you are a developer you got to check whether characters are uppercase and lower case like 24 hours a day this is not Rod 13
is not the only time that this is going to come up so that makes these two functions good candidates for promotion
into a general purpose library and now the fact that we took the part of
the code which was just in the middle of a blob that was checking whether the a character was uppercase checking whether it was lowcase we pulled that out into a
separate thing the fact that we did that now means that later on when we're writing some other code not having
anything to do with rot 13 we don't have to rewrite that code again like 50 different times we just read it in one
place and we use it wherever we need it so that's the argument for why modular code is more
reusable so it's more uh intelligible fixable changeable and reusable those these this is the this is the folk
wisdom uh you know coming out of the agile movement especially or you know uh influentially put forth by the agile
movement about how you should how you should make your code and this is the uh this is what um you know Junior developers are instructed to do um so
I'm going to cycle back to haha get it cycle back I'm gonna cycle back to rot 13 in a minute but before cycling back
to rot 13 I want to now pause to um draw
parallel with philosophy so I think philosophical arguments work the same way as
code so and in fact if you track open any classic work of philosophy at least any classic work of philosophy that is
the business of making arguments I think that's actually kind of not every work of philosophy but it's like I don't know it's like more than half of them
probably it's like a lot of them very common making arguments of philosophy um uh and you just see any any any
argument-based work in philosophy is GNA be refactoring right and left um and I don't usually see it talk about in those
terms but I see it happening all over the place um and here's the way it works in philosophy and I think it's very
similar to the way it works in code so you know you're sitting there you're like you're on your paretic little walk
and you're trying to figure out your view is about something and the first draft of your argument for the conclusion uh that you know that you
that you decided is your view it's going to be big and complicated involve a lot of steps little moves um you know little
subparts um but those subparts are not going to be um separated out they're going to they're all going to occur into
a big jumble and then you you're GNA get home from your paretic walk and you're gonna write down your notes of like what your argument is and it's going to be
like maybe their argument is good like it's a valid argument but it but the everything's going to be sort of tied up
together and what you see you know more often than not in like the first draft of a philosophical argument is the every
single view the person has about absolutely everything kind of gets rolled up inside of this argument um and then another thing you
commonly see you sort of a life life cycle of you know multiple drafts of workshopping a philosophical argument is you see the person starts to pull things
out of it um they start to factor off sub arguments and then refine those things separately which turns out to be
much easier than refining them when they're in the middle of a Big Blob would have been um so I'm just going to
give you a very simple example um I decided not to give you any
um really classic examples from the can because they're too complicated uh but here's just a very very oversimplified
example just so you can see what the refactoring looks like in the falsy case so I'm just going to give you an
argument with like letters in it and I'm going to change it into an argument with words in it so here's the AR here's the version with letters in it suppose a if
a then B if B then C okay well clearly you know the conditional is transitive
so um a brings about C and some you know it brings about as understood as roughly
equivalent to if if then um anything that brings about C should be avoided therefore a should be avoided so this is
the kind of thing you know you crack open a philosopy paper you crack on a philosopy book you get stuff that looks like this all the
time so this argument isn't too complicated I deliberately made it like to be I deliberately made the world's most simple argument to just be able to
use it as an El an El example but um uh you can probably
already see from there that the whole like argument for if they and C or a brings about C that can be factored off
um and uh I think this is a move that a lot of people will make when they're doing the second draft of their argument
they'll be like H all right let's factor that off and let's discuss that separately maybe we make it into a separate chapter maybe we make it into a
paper maybe you know something like that but like we don't need to be talking about all these things at the same time
so it'll look more like this and heavily you know so overse over simplified schematized form um you'll just have
like the argument for if a then C that'll work as a Lemma and then you just invoke it in the main argument and
now you can like argue both of these arguments separately rather than like intermixed with each
other um and so my just concrete example of of an argument that's a bit like this
uh is um David Newark and William washer's uh book on the minimum wage
so one of the moves they make in the introduction is they say well previous literature especially in economics um uh
tries to argue against raising the minimum wage because um the argument is that raising the minimum wage causes a rise in unemployment um and um but what
they want to do is peel the empirical claim off of that argument um and um
just focus on like what are the effects of the minimum wage that's really what the focus is the
book um so let's see what we can learn from the evidence and then let's you know in fact actually um unfortunately
from my point of view they do cheat at the end of the book and in like the very last chapter they're like okay okay okay fine we've we've held our tongues and
we're gonna like make some policy proposals at the end of the book um that's a little too bad from from my point of view because I think it's like really nice the way they like Factor off
the empirical stuff for most of the book but anyway the uh the Insight is still there um and the Insight is um it's like
uh gonna just muddle things if we try to discuss whether there should be a minimum wage hike at the same time as we
try to discuss what the actual effects of a minimum W height would be um another thing I should say here is that
this is a super super super simplified version of their argument their actual argument is like very rigorous and scholarly and detailed and sophisticated
and they talk about way more than just unemployment so this is just I'm just giving you a cartoon example to illustrate uh what like a philosophical
refactor looks like so um and here once again this is gonna be like I just want to be clear
I'm not doing any justice to their view here I'm just sort of using them as an example you can go look at to think about in the context of the argument
that I'm making so so you know um the thought is typically in the previous
literature you'd see an argument like this suppose minimum wage increases if minimum wage increases Mass layouts happen Mass layoffs happen unemployment
increases therefore raising the minimum wage increases unemployment there anything that raises unemployment should be avoided therefore we should not
increase minimum wage that's the sort of standard thing you would see in previous literature and then their thought is
like well why don't we just Factor this part off like you know Factor it off in the sense of like most of the book we're just going to discuss
this so um that's my like like little toy example of philosophical refactoring
and I think just one thing I want to stress now is that all the virtues of modular code that we cited for software
carry over to modular philosophical arguments philosophical arguments that is that did as much refactoring as they
can so um intelligibility um I think I
think this uh this holds um if you have a big argument blob that takes place over multiple Pages it just gets really
hard to assess the individual moves in it because they're all sort of hang bu up with each other they're coupled to each other you have to be thinking about
them all at the same time in order to assess any individual one whereas in an
argument where whatever can be refactored off is refactored off you could just look at the sub arguments individually and assess them individually um and even before you get
to assessing just understanding what they're saying you can do that as well and I think we all know at this conference that understanding
philosophical arguments can take work they can be really hard um and um a really important part of
understanding the understanding equation in my um view is thinking about whether or not you agree with it and like you
know the the moment you're starting to understand an argument like part of understanding it is like throwing some
throwing some toy counter examples at it and seeing if you agree or disagree or not and that whole thing is like way easier if you're doing it for a short
simple argument um so that's the intelligibility that's why I think intelligibility comes from modular style
and philosophy extensibility so suppose you change your mind every five minutes or in Russell's case every in the middle
of a sentence three times um you and you acquire new beliefs and
now you want to argue for those two if your argument is a Big Blob like your whole end conclusion is under threat
just from the fact potentially just from the fact that you acquired some new beliefs um and now you got to get in
there and now you got to maybe revise your entire argument that's like I don't know how long it is it's like really long to accommodate your new
beliefs however if you factor off different parts of your argument into sub arguments
which can be carried out independently of the whole and Each of which has a specific job um it usually turns out
that um new beliefs you came to are only going to put some of those sub arguments under threat and it's going to take less
work to fix those or update those than it would have to update the whole blob
um so similarly uh fixing problems it's pretty much the same argument as for sensibility um um you know if you're
attending a talk at a conference um and uh somebody uh presents you with a
modular argument um as the conference attendee um it's much easier for you to
be like okay I agree with this this and this but then here is where I get off the train and this is why here's my counter example whereas with a blob it's
like I don't know I don't buy this but I don't see exactly where or why I don't buy it um reusability this comes in in
philosophy as well um um because uh often it turns out that um a Lemma you
factored off of your main argument is useful in more than one place you might want to invoke it for three or four
different arguments without having to necessarily inline it uh to use the computer science term or repeat it in
those other places um and in the case of minimum wage um it's true that like um
David Newark and uh Michael boser have um they do have a dog in the minimum
wage policy fight um now just to be clear I don't have a dog in the minimum wage policy fight I I the only the only
thing I think is you know uh if if somebody isn't getting paid enough money and we should do whatever we we got to
do to get them paid more money so I have the extremely I have that extremely Bland and interesting dog in that fight
but I'm personally I'm much more interested in the empirical stuff um which is why the way they approach that book speaks to me um and this is this
reason for this is simple it's because the um empirical argument um factoring the empirical argument off from the
policy argument as they're careful to do in that book decouples the purpose of one from the other um there's tons of
reasons you can be interested in what the effects of a minimum wage increase would be other than like because you're in the situation of
like deciding whether or not to do it so uh philosophical arguments and uh
code uh both should be modular they both get the same benefits of being modular why is this well in my opinion this is
because of the curry Howard is morphism one of the great lessons of the curry Howard isomorphism is that philosophical arguments literally are pseudo code um
so we can talk about that more during the Q&A maybe not everybody agrees with that but um I think that I I take that to be one of the lessons the curry
Howard isomorphism so under the curry Howard morphism intuitionist the conditional L reduction corresponds to lamb abstraction and function
application corresponds to modus ponents so that means that if I was to you know for example try to Cod up the new Mark
washer refactor in agda setting up some inference rules and setting up some um Atomic propositions if we we you know uh
if we uh we could we we could render the Big Blob argument this way and then in
that case fating off the empirical part they're interested in literally is just writing a helper function so um one of
the great things about agda one of the things I love about agda is that it just like plainly exhibits the curry Howard isomorphism so um here we have just the
um the argument that they want to focus on for most of the book before they make some tentative conclusion to the
conclusion uh we have that and that literally just is a helper function so um anyway this is why it's like I think
it's no accident that modular philosophy is good in the same way that modular code is good so um uh take away from
that I think philosophy could use in Uncle Bob Martin if anyone at this conference wants to step up and volunteer to be philosophy's next Uncle
Bob Martin please get in touch because I think it would be great so what about problems for modular
code well um so modular code has its attractors and you will see people get into flame Wars about this on the
internet and at work too they get into flame Wars in person at work so what's the problem with Mod code well um
there's lots of things that people talk about but there's one objection that I think is really deep and I think it's a very powerful objection so I want to
focus on that um to get there let's go back to the original example I'm going to do the
thing where I had the the helper functions in global scope um uh to make various things easier so this is a
version where we had the helper functions to Global scope rather than local um let's just keep going let's
refactor off a bunch more stuff so we started refactoring the is uppercase checking Logic the is a lowercase
character checking logic Factor those off let's keep going so I'm gon to split the code up into two little panes here otherwise I didn't change anything all
right now first refactor is going to be like well how about we Factor off the rotating so let's write a general
purpose rotating function that can take any character and rotate it not just by 13 but by however much you want to
rotate it by and this Rod function will be able to do any Caesar cyer uh yeah any Caesar Cipher not just Rod 13 so
that's great it's a general purpose function and it also cleans up the code on the right you can see the code on the right got
shorter next we can Factor off the character range checking from iser is uppercase and is lowercase I think that
definitely makes is uppercase and is lower case a lot easier to read you just checking to see whether something's um in uh you know within uh 26 characters
of the start character you can then continue refactoring and I would encourage this
you can refact refactor the um integer range checking off of the character range checking um and just check whether
you know a number is between other two other numbers very useful function uh then finally um you if you
look at the code left here it's still got a lot of numbers in it and human eyeballs they don't like numbers human eyeballs look at this they're like what's 26 I don't remember what's 65 I
also don't remember what's 97 I also don't remember now if you like mess around with aski codes enough you will remember but arguably that's um that's
um maybe that's not a skill you want to have I don't know that's debatable uh so what should we do probably we should
give these numbers names so we can remember what they are so we'll call one ask a we'll call one asky capital A we'll call one the length of the
alphabet and now I think this whole thing gets a bit more readable the last thing we're going to factor off is uh so you see on the right
there um this is the uh there's a little fun arrow thing on the right there that's the that's the function that's
going to look at every single character in the string decide whether or not to rotate it let's just pull that out make that a separate function too and then
the final final thing we're going to do which I always tell is kind of thing I want tell my students to do is like these n and conditionals in my opinion are pretty nasty so let's just change
into match expression all right so now okay so now now we turned the nasty Rod 13 code that I started with off into
some we turned that into something that would now pass code review uh at least in my shop great so we've refactored it
it's beautiful uh oh now we got now we can see the problem what's the problem so the good thing was we have
this between thing you got to check whether something is between two other numbers all the time that can get promoted into our general purpose
library checking the uppercase lower case that can get promoted to general purpose Library general purpose Caesar Cipher that can get promoted into our
upper into our general purpose Library it's not just for R 13 but the problem is that if we
actually once we actually start reusing these fellas over here they can't live here they can't live in
this file anymore we're gonna have to they're GNA have to Vose off into different projects on my on my hard
drive different subdirectories in here maybe even different computers different systems different file systems different
operating systems different like across the world over the Internet they're not going to live right here in this file
anymore but right here in this file is where they were really contributing to the readability of the
code they're not going to have to be they're not gonna be here in one place and I mean look we could leave them here but if we left them here it would be
nonsense because now like all this other code I'm writing has nothing to do with r 13 is going to be calling into my rot
13 code to check whether one number is between another number is in between two other numbers just that that makes no
sense so this is the thing that people complain about with modularity what you often get when is a file containing this
one line in it and then it's like the line is super cryptic and it's like in order to understand what's going on
you're just gonna have to chase down the references like each each car is uppercase and ranged you have to go hunting in your computer like where
where is that def find what file try to open that file up blah blah blah blah blah you're gonna go on this like you know this long hunt and and that that
takes the amount of you know it takes uh uh um enough work that you get distracted from like thinking about what
the code is actually doing um so basically chasing all these references eats up your cognitive load
which you should be spending on understanding the code so this is like the number one complaint on modularity I think it's a deep
complaint um the conclusion I want to draw here is that there's a tradeoff that's what it seems to me so modular
code brings these the these advantages you have more intelligible code EXT more extensible code more fixable code more
reusable code but the minute you take that potential reusability and start
actually reusing stuff you tank the other three virtues you tank the other three benefits you compromise them so I
really think this is trade-off what do we do not totally obvious
um um I think uh writing in what I sometimes very metaphorically nickname paramal style is the best optimum among
the options we discussed so basically what you would do is you would you would leave things like this
but don't actually reuse any of the reusable stuff um and if you can have this like nice little code pyramid it
optimizes for all four and then it stops shot it stops short of actually reusing the reusable stuff um and the intuition
behind this pyramid metaphor is that you start off at a low level then you construct with very little code a a slightly higher level interface in a
small step and then you construct again with very little code a slightly higher level interface etc etc until you can
implement the actual thing you want out of the interfaces you built up progressively step by step um and each
stage of the pyramid should build the next level up interface only out of what exactly what is needed from the current interface that's the intuition here so
you know something like this as I said just vaguely roughly paramal that's the that's the metaphor it's not uh intended
to be mathematically precise in any way in this picture you can interpret XR y as y uses X in its definition so this is
a code dependency um graph part ordering but this is the like lowlevel en
manipulation interface this is a little bit higher up character manipulation interface this is the um check what the
properties of characters are interface this is the transform a character interface and this is a transform a
whole string interface so the nice thing about paramal style I argue um or what I'm
nicknaming paramal style is that it scales because I could grow this code into something that takes up like five
pages but it it wouldn't be five pages more difficult to read it would just be a bit more difficult to read because you
could jump in wherever you're interested and start reading the code there if I'm only interested in like I'm not
interested in how the uppercase lower case sub is implemented I start reading here if I am interested in how the uppercase lowercase stuff is implemented
I start reading here etc etc and so this paramal style building things up in higher higher level interfaces allows
you to partition the process of reading through the code into reasonable chunks so um I don't know if any perfect
solution to this problem in the code space the best solution I can think of is to get your IDE to generator
parameter representation for you by pulling in the code from where it actually lives and making it looks like it's in one place um so it would
effectively generate the representation I had of R 13 just just a few slides ago
um actually doing that is a very non-trivial coding problem um and um you
know to actually implement it um the best language servers out there now I'm thinking particularly of oamp Merlin are like pretty have pretty deep
understanding of the ests they're interacting with so it's not Unthinkable that we could do this with little elbow grease but it's certainly going to be very hard so what about philosophy I
think philosophy has the same trade-off the moment you start factoring stuff off you you're gonna it's not
going to be in the in the original book anymore it's gonna you're gonna file it off in another book somewhere you're gonna file off another article somewhere
and pretty soon a new reader coming to this philosophy out of nowhere is gonna be looking at one book at random and they're gonna be like I have no idea
what's going on here without chasing down 50 other books and then those books are gonna give you more books to chase down and what's even happening can't I
just look at the whole thing in one place um so there's no IDs and philosophy that can fix thisuh I mean no
IDs and philosophy at all unfortunately maybe there will be someday um but I mean I think you can sort of approximate
the solution I proposed for software by um um just putting a lot of hard work
into the intro conclusion contextualizing giving a sketch of some of what this argument can be used for
and is used for so um yeah intro conclusion um so uh run a along a time
here so I'm going to zip to the end um I'm working on a formal definition of modularity which is what I think we're
eventually going to need to advance these debates uh it's it's really hard and uh mathematically Savvy friends of
mine are help new with it uh probably going to have to involve some free algebra and Universal properties and stuff uh High versus low level you can
probably capture which is an algebraic homomorphism anyway not clear how to do that but I'm working on it um so with
that let me wrap up um so I've argued the modular code is code that's arranged into this paramal shape uh which starts
at a low level and builds up in a series of progressively higher and higher level interfaces it's more intelligible
extensible fixable reusable this is why people like it but actual reuse you can't actually reuse the reusable part
if you actually reuse the reusable part you screw over the rest of the virtues modu philosophy exactly the same it is
the same because the curry Howard isomorphism as the curry Howard isomorphism shows us philosophical arguments in pseudo code um but modular
philosophy has the same problem it's the same problem that once you start actually reusing the reusable stuff in it you mess with the other good making
features of modular philosophy so the ultimate solution um I'm not sure what it is but it's
probably going to have to involve uh some creativity in the like uiux for readers area and I think I'll leave it
there thank you everybody you can check out my website and podcast at URL and these are the references to the books I talked about and
papers thanks Matt uh next up we have um
R sanelli who is a teaching fellow at the University of Chicago in the humanities where he received his
PhD and um I have some other notes uh his primary work is in philosophy of
language and philosophical logic and is particularly concerned with the relationship between the ability to speak a language and the ability to
conceptually grasp what things in the world are and how they might be Ryan you can go ahead share screen if
you can or okay let me see uh where is my share screen
should [Music] be uh why can't I
find Windows here it is uh one sec here it
is all right can you all see that yes
cool uh let me full screen this screen
cool uh all right um well yeah thanks uh Eric and Connor
for for organizing this really cool uh event um I'm going to be talking about
inferentialism which is you know the main sort of philosophical theory that I've devoted a lot of time to working on
um and some sort of speculative relations that it might have with artificial intelligence and um Chris is
also going to be talking about inferentialism later today so we'll get a little more inferentialism um okay so what is inferentialism um
let's see uh yeah whatever um so here's here's a
question um what is required for concept possession the inferentialist answer is
that grasp of a concept is Mastery of the inferential role of a linguistic
expression that itself doesn't say exactly what inferentialism is because
there's various different strengths at which one could endorse The View that Mastery of inferential role is required
for concept possession so there's uh a range of possible ways to take this
answer the the weakest version is that well at least some Concepts you need to
know some inferences it's not a very interesting view almost everyone would accept that um another relatively weak
answer is that well all Concepts require knowing some inferences um once again
not very interesting I think a lot of people would just accept this um where it gets interesting is when you start
saying that mastering an inferential role is sufficient for grasping a
concept and so this comes in two versions um you could say well there's some Concepts such that mastering the
influential Ro is sufficient for grasping them or the most interesting and strongest claim is what I'll call
Global strong inferentialism which is the view that mastering an inferential role is sufficient for the possession of
of all Concepts and that's the sort of interesting and sort of ambitious version of inferentialism I want talk
about here um so here's an argument um that might be a little bit provocative
regarding uh strong inferentialism what Global strong inferentialism so once again this is the view that Mastery of
inferential role is sufficient for the possession of all Concepts so here's a plausible claim um
training on linguistic data um is in principle sufficient for Mastery of
influential role uh a consequence of global strong
inferentialism and this plausible claim is that training in linguistic data is impr principle sufficient for possession
of all Concepts consequence of this is that large language models um trained on
nothing but linguistic data are impr principle capable of possessing all Concepts and I'm want to keep using
these impr principle clauses because I don't want to suggest that you know for instance today's large language models do in fact but the idea is in principle
large language models could possess all Concepts given this line of argument so
my aim in the talk is somewhere between the following two extremes uh minimally
I want to convince you that the validity of this argument is not a reductio of global strong influential ISM right the
conclusion might seem so implausible that you might just think well this means that Global strong inferentialism is B
um so my minimal goal is to convince you that that's not the case it's not just a reductio of global strong inferentialism
maximally and I don't think I'll succeed in doing this but the the maximal aim uh is to convince you that this argument is
actually sound okay um so let me start by just saying some things about the the sort of
weaker version restricted strong inferentialism because this is the version of inferentialism that's gotten a lot of traction in uh philosophy of
logic in particular so the most most popular version of restricted strong
inferentialism is what's called logical inferentialism um and this is the idea that the meanings of The Logical
connectives can be understood in terms of the inferential rules governing the use so connectives like and not or um
they can be understood in terms of their inferential roles and these can be canonically specified by a certain kind
of formal proof system um so standard simple example the
easiest case right take and maybe the most uncontroversial connected when it comes to actually formulating the rules
you might think that the meaning of and is given by these rules in a natural
deduction system right so from a along with B you can infer A and B right
that's the introduction rule which tells you how to conclude A and B and then it has a corresponding elimination rules
that let you conclude Things From A and B right so one elimination Ru is from A and B You can conclude a and the other
is from a and B You can conclude B and these have nice properties they fit together nicely in a way that's spoken
of as as Harmony um you might also use different type of proof system to capture the same basic inferential
behavior of and um so you might formulate things in terms of a sequent
calculus um so here rather than introduction and elimination rules you have left rules and right rules so the
left rules codify the inferential role of something as a premise right so this says from some set of premises gamma and
a along with B if you can conclude C then from gamma and a and b You can
conclude C and the right rule says if from gamma You can conclude a and from gamma You can conclude B then from gamma
You can conclude A and B right just different ways of formally codifying the inferential rules governing the use of
and but the basic thought of logical inferentialism is that specification of
these rules and Mastery of these rules right um is sufficient for grasp of the
concept of conjunction um and this is still controversial and logic but lots of people think that this is a very
plausible way of thinking about the meanings of The Logical connectives the stronger um and you know
more ambitious view um is what I'm calling global inferentialism global strong inferentialism um and that's the
view that it's not just logical words like and or or not but also what called in linguistic content words like red
sphere dog um these sort of words with material content those two
can be understood in terms of the inferential rules governing their use um
so just to give a simple example um consider the meaning of red right this
might be understood in terms of the fact that it's use is governed by inferential rules such as the following right and
I'll use this the sort of sequent calculus formulation to specify these um
from uh a is red you can confer infer a is colored right right and from a is
Scarlet you can infer a is red um and here's another kind of rule you might have in a sort of sequence system right
uh this says uh from a is red and a is green You can conclude an incoherence
right so this sort of encodes the fact that these two sentences are incoherent they're they're
incompatible um and likewise you know you don't need just one thing right you might have from a is red and B is pink
You can conclude a is darker than b and you know you can spell out more of
these but it seems at some point you sort of run up on the limits of what you can includ encode as far as the meaning
of red goes in terms of inferential rules um it seems like inferential rules
of this sort might be necessary aspects of the meaning of red but you might be inclined to think well these can't
exhaust the meaning of red uh and what in particular is the issue here well
here's an issue it might seem right one essential aspect of the meaning of red
um is that it can be properly deployed non-inferentially right upon seeing something that it's red right so not
only are there inferences right from such and such as Scarlet to such and such as red but from seeing something
red I can conclude that it's red so the standard response to this issue is to
distinguish between two sorts of inferential rules um so we have properly inferential rules of the sort I just
specified um which obtain between uses of language like asator utterances of
sentences uh and other uses of language but in addition to these we have what we might call quasi inferential rules which
obtain between uses of language and non-linguistic perceptual circumstances or intentional
actions um and well what is a quasi inferential rule well it's kind of hard
to write down you can't really give a formula for it but you know intuitively
there's something like well from seeing the red ball I can conclude that the
ball is red right and I Ed the little V Dash sign but it's not really an inference in the proper sense of the
term because one of the relat isn't a sentence or a use of a sentence it's a perceptual
circumstance and the vast majority of proponents of inferentialism think that
in addition to properly inferential rules you have to include these weird sort of Quasi inferential rules that relate non-linguistic circum ances to
linguistic things I think this essentially amounts to giving up the game um I think
emitting such quasi inferential rules into an inferentialist picture um is T
out to abandoning strong inferentialism right so we I distinguish between strong and weak inferentialism and I'm trying
to sort of spell out this the strong inferentialist view I think this is sort of strong infer it's weak inferentialism
in strong inferentialist uh guys as it were this isn't really an inferential Rule and so
this isn't really a strong inferentialist picture and if you think there's sort of motivations for inferentialism as a sort of full-blooded
theory of conceptual content something that actually explains the concepts expressed by uses of language I think
there's reason to want to hold on to the genuine strong inferentialist picture and so reject this sort of standard
response so I think we should stay strong uh try to maintain a strong
inferentialist conception of conceptual content in light of this apparent issue
um but how do we do that um I take it we just need more inferential relations the
standard strong inferentialist view just doesn't include the necessary inferential relations needed to actually
account for the conceptual content of an expression like red so what sort of
inferences do we actually need well the ones I just mentioned are what we might speak of as
intrafamilial inferences right they're inferences within the range of color
concepts right they relate one color concept to other color concepts but I take it that such intrafamilial
inferential relations aren't going to be sufficient to capture the meaning of red I mean in some sense this is what I've
just stated so rather than including quasi inferential relations let's just
conclude more inferential relations so we include extra familial inferences so
these are such things as if a is a stop sign well then a is red right from a is a stop sign You can conclude a is red
and for instance from uh a is a Blackberry a is ripe and a is red You can conclude an incoherence right but
these themselves still don't resolve the problem uh about the fact that red can
be deployed non inferentially so how does one capture that in inferential terms well relatively
easy um if a is red and some subject s has color vision and S is looking at a
well then s sees that a is red right this is just sort of a sort of sketchy example of the sort of inferential
relation that I take to inferentially articulate this aspect of the content of
red right so an aspect of the content of red is that it can be deployed non-inferentially how do we articulate
that well in terms of this inferential relation that involves the inference to the fact that someone sees that a is red
so I take it that these inferences right suffice to completely articulate the
conceptual content of red uh no quasi infuential no quasi inferences are
needed um this is this is a view that I've defended uh at length in a paper
called how to be a hyper inferentialist um so if you want the full story of how this can actually be sort of spelled out
and responses to philosophical objections uh you can you can look at that paper um but I'm going to sort of
take this as a starting point for for thinking about these issues about uh inferentialism in AI so so let me just
start by drawing out some some consequences of this View and let me start with a a famous thought experiment
owed to Frank Jackson um so this is called Mary's room right so so Mary is a
color scientist uh she's been locked in a black and white room since birth and
so she's never experienced color right but she has all the access to all the books and you know internet and
everything Ray and she can do experiments and she she has reached the theoretical limit of what can be known
about colors without ever having to actually experienced them so a question one might ask about Mary right
concerning her possession of the concept red or not right does she know what it is to be for something to be
read the standard intuition that people seem to have in response to this thought experiment is no right she doesn't
really know what it is for something to be read she has this sort of theoretical understanding but to really know what it
is for something to be read you have to actually see red right only by seeing red can you really know what it is for
something to be read I want to say this is this is wrong uh yes she knows just
what it is for something to be read since she grasps all the inferential relations between sentences that articulate the content of
red um so of course she never herself has never use the term
non-inferentially um that's you know stipulation of the thought experiment um um nevertheless she knows just the
conditions under which it can be non-inferentially used um and that captures the fact that this expression
is essentially such that it can be used non differentially and according to this view that's all that's required in order to be counted as knowing the meaning of
red um and we can actually you know take this one step further applying the same basic thought um not only does she know
what it is for something to be read in fact she knows what it is for something to ah sorry she knows what it is to see
that something's red um why well because you can articulate the content of C is that a is red right um the inferential
relations articulate this conceptual content just as well as they articulate the conceptual content of red and you
know I mean I I acknowledge this is a counterintuitive view a lots of people you know maintain the standard no she
can't know what it is to see that something's red but I just don't see what the non-question begging argument
uh against this dispute that Advanced here is Right other than just table pounding um so I take it she fully knows
what it is to see something red she hasn't seen something red but she knows what it is to see something red so she
possesses this concept so so this is one case I take it in which one can have an
experiential concept right knowing what it is to see something Red without having that particular experience so one
natural thought one might ask is can this thought be generalized right might this thought be applied globally to
something without any experiences at all so in order to spill this out um let me
make a distinction between uh sentience and sapience so sentience is conscious
Awareness sapience on the other hand is conceptual understanding and I'll say a
bit more about exactly what I mean by these two terms in a minute but just sort of intuitively we can start sort of
sorting some clear cases into these various categories so something are neither sentient nor Sapient this
includes such things as rocks trees you know maybe some people have suggested that trees are Sapient but sentient
sorry but I take it they they're not uh thermometers are clearly not sentient
they respond to their environment in a certain way but they're not that sort of responsiveness doesn't constitute
sentients there's some things that are sentient but not Sapient this includes such things as lions ferrets octopi I
take it then there are things that are both sentient and Sapient I take it that we're in this category right not only
are we consciously aware of our environment but we have a conceptual understanding of it this includes you
know sort of standard intelligent aliens that you see in science fiction scenarios like clons um and it also
includes certain uh canonical sort of sci-fi characterizations of artificial
intelligence for instance you know in Spielberg's artificial intelligence uh
were at least led to imagine these artificial intelligence systems are both sentient and
Sapient um but this sort of what about this other category of not sentient but
Sapient can anything fill that category um I want to suggest that perhaps uh
llms could in principle occupy this category um so so before I spell that
out let me just say a few more things for fleshing out this idea of sentients versus sapience um um I'll adopt a sort
of call broadly hiigaran picture of sentience so what is it to possess conscious
awareness uh here's this sort of broadly speaking hiigaran answer um it's to have a world in which things show up as
significant in various ways uh as to be responded to in various ways given one's
ends uh so here's a you know oversimplified example uh you know to the lion the gazelle shows up as to be
eaten right uh the Cub shows up as to be nurtured right so the lion sort of occupies this melt this world of
philosophical signific not philosophical sorry world of practical significances in which things show up as to be
responded to in various ways um and I take it you know this is the right way
to understand perception is as one sort of aspect of a perception access NE
perception action Nexus through which you have something that goes about the world achieving its ends things in the
world show up is to be acted upon and it acts upon them achieving its ends and so perception is sort of one aspect of it's
the receptive aspect of this perception action Nexus and the important point
just I want to stress here right I won't go into the details about this sort of philosophy of perception but the important point is that if you think
about perception and conscious awareness in this way it essentially involves as John hogland puts it giving a damn um
being something that in some way cares about achieving its ends only in such a context can things show up as
significant to such a thing so can of have a world of significances okay so that's that's what
I take sentience to be once again I can't sort of spell this out in detail but this is just a gesture at the sort of approach to sentience I'd be inclined
towards what about sapience well here's what brandom says uh characterizing us
as Sapient uh this is Robert brandom one of the notable developers of inferentialism he says our transactions
with other things and with each other in a special and characteristic sense means something to us they have a conceptual
content we understand them in one way rather than another and the idea is we have a sort of a determinant grip on the
various things that show up to us we can think about them uh in effect we we have a grasp on the world as it shows up to
us so what is it to possess this sort of conceptual understanding well they take it that this sort of conceptual
understanding is essentially tied with the ability to speak and understand a language and I've already sort of given
the answer that I take this conceptual understanding to consist in it's Mastery of inferential role right and once again
uh the global strong inferentialist answer says that Mastery of inferential Ro is sufficient for possession of all
Concepts so for us right beings who are sapen right we're human beings we're
animals so sorry beings who are sentient right we're human beings we're animals so we're sentient we have a world in which things show up to us as
practically significant on which we act for us conceptual understanding transforms what it is for us to be
consciously aware of the world here I'm drawing on work of of Matt Bole and some other people so our form of sentience is
a distinctively Sapient sentience we might say but we might wonder right could
there be a form of sapience without sentience right that's not our form of sapience but could there be such a
notion and here I want to consider the case of large language models um and
just sort of for the simple purposes of this talk I'm going to focus on what I'll call Pure
llms um so a lot of today's uh AI models are now are what's called multimodal
models that also include sort of video input and other things like this um but the original version for instance of GPT
4 was was a pure llm where it just was trained on linguistic data and this is
the sort of uh theoretically simple case I want to sort of focus on just to sort of concern myself with this
philosophical point right so these AI models uh they're neural networks that
are that are trained on massive amounts of linguistic data basically they're fed the whole internet give or take some um
and they're trained to predict the next word in a sequence given the previous words right and there's this sort of algorithm through which they can acquire
this capacity to P the next word to predict the next word uh through a lot of training then they're sort of fine
tuned they TR on sort of specific data um for instance question
and answer texts so that they do the right thing that you want them to do when you uh prompt them so for instance
rather than just sort of continuing as if they were you they actually answer your question um and so when you
actually use one of these things the way it works and once again I'm just sort of really oversimplifying here but just to give you the general sort of picture um
you ask it a question right and it just predicts the next word right it it it that that's what it's trained to do
um so it starts predicting the next word and then it feeds that back into the inference engine and predicts the next
word in that sequence and so on recursively and then through that it ends up sort of spitting out um fluent
English sentences that actually sort of answer your question um you know I'm not going to go into details here but the
crucial point I want to emphasize is just sort of given this architecture right they don't care they don't give a damn um these aren't the sort of things
that uh have you know a world of significances um they they're not the
sort of things that give a damn and John Hogan sort of took this to be a sort of basic problem with AI um he says the the
trouble with artificial intelligence is that computers don't give a damn and I want to suggest that he's right that at
least sort of current large language models don't give a damn they're not the sorts of things that could but he's
wrong to think that this is a sort of in-principle problem for them when it comes to sort of understanding things um
so consider just the case of of reading um I take it the current llms are are
are still Limited in a whole bunch of ways um but I do take it that they can read um so by that I mean they can take
a text they can summarize it and they can understand its meaning um Can GP gp4 provide profound
literary analysis no of course not uh but it can score better than 93% of
school students on the reading portion of the SAT uh will GPT 6 uh once again
assuming it's just a pure llm which it probably won't be but if it is you know a few Generations down the road will it
be able to do serious literary analysis uh I want to suggest maybe um so so
hogland against computers reading says only a being that cares about who it is
as some sort of enduring whole can care about guilt or folly self-respect or achievement life or death and he says
only such a being can read um I take it that he just is making an
error here um if the sort of inferentialist view that I've sketched before is Right understanding the
meaning of a fable or a DKI novel doesn't require giving a damn oneself it
just requires knowing what it is to give a dam and so I want to suggest you know like the Mary case right just like Mary
can understand what it is to see Red without herself ever seeing red an llm
can in principle know what it is to give a damn without itself giving a
damn so you know in response to this we might still have this sort of lingering
intuition to to insist right well it doesn't really understand right I mean given the way I just describe this thing
right it can't really understand can it um I want to ask you know what are we
really asking when we when we ask this question um and here I want to draw on a sort of famous quote from Wilfred sers
he says in characterizing an episode or state as that of knowing we're not
giving an empirical description of this episode or state we're placing it in the space of reasons of justifying and being
able to justify what one says so here's a spin on this basic solarian thought in
characterizing a system be it a human being an EXT extraterrestrial or a computer program as reasoning or
understanding we're not giving an empirical description of that individual we're placing them in the logical space of reasons of justifying and being able
to justify what one says so if we stick with this solarian
thought um one way to
AB so the space of justifying and being able to justify what it says and I take
it once again the answer here is at least in principle yes right there's nothing about llms that suggests that in
principle they couldn't be in such a space now I take a contemporary systems
when you really press them on it sometimes you get the sense of well no they they don't really understand what they're saying they're not actually
responding in an intelligible way um but there's nothing about the very idea of an llm that would suggest that in
principle this is impossible so why does this matter um well here's
what I take it we want at least this is the sort of thing that I want in AI systems I mean there's other
applications for instance you know AI Butlers and whatnot that different but in s language models right what we want
is AI systems that truly understand what we're saying to them and what they're saying back to us I mean if you just
think about it right as number of researchers in this uh conference right how nice would it be to have such a
system as a research SE system that would be amazing uh what don't we want um well we don't want AI systems that
kill us all uh that would be that would be very bad um so the first thing what we want is sapience uh however with
sentience right with giving a damn uh with being a thing that actually cares about uh what it is as a continued thing
the risk of the second thing goes exponentially up so perhaps it may be
the case that practically speaking the easiest way to True set sapience is through sentience uh I don't know if this is the
case I'm not an AI researcher um but also I don't think AI researchers currently know either um but I take it
that as we do the risk reward calculations uh as we develop more and more capable systems uh an important
consideration is whether there is at least the theoretical possibility of sapients without sentients and I hope
you I I hope that I've done enough here to convince you that at least the very idea of theoretical possibilities not
crazy uh all right thanks um the slides are available on my website if you want
to take a look at that uh here are the references uh thanks everyone
uh one second
please
e e
uh give us one
second
e
e e
hello e
hello uh thank you for coming here uh next we have uh Alysa vanenburg uh PhD
who is a uh licensed clinical psychologist uh and also an assistant
professor at uh Northwestern University uh she uh is uh was currently
was in the uh uh Rehabilitation department is currently uh moving to the neurology department and today she's
going to speak about the ethics of telepsychology all right thanks Eric can
everyone hear me okay all right
so all right so just to introduce myself um like Eric said um I'm an assistant
professor at Northwestern University fineberg School of Medicine and I've been in the physical medicine and
rehabilitation Department I earned my PhD in Clinical Psychology from Duke
University um I did my clinical internship residency at um Yale School
of Medicine and Behavioral Medicine so my areas um really are
Health psychology and Behavioral Medicine kind of medical psychology um and I did my post-doctoral fellowship at
Northwestern vineberg School of Medicine um and I'm talking about ethics of
telepsychology ology today um I actually got interested in the ethics of teles psychology in 2016 before I had ever
actually done any teles psychology um I didn't get any exposure to it until right before the pandemic when I was at
Yale and was working in smoking sensation um so um yeah that's kind of
when my interest started and now um about half of my practice is um Talis
psychology I'm currently um practicing as a sleep psychologist at Northwestern
um in 2 pm which we're currently at on Eastern time is known as the Circadian
dead zone so this is when your body clocks in its deepest dip so um to try
to get everybody um uh feeling more energized I'm going
to start with a question um how many of you both in person and on Zoom have
experienced with Tal psychology all right we've got some all
right okay um so I like to do some disclosures and disclaimers um I don't
have any conflicts of interest to report I don't receive any industry sponsorship
um this is really for educational purposes today only it's not a substitute for professional consultation
or for Consulting current laws State national regulatory rules um and the
views Express in this presentation are my own and don't necessarily um reflect
those um of Northwestern but uh hopefully there's alignment all right so um you might be
wondering how I fit into this conference um so the ethics of T
psychology represents an application of the methodology of Science in ensuring responsible and effective practice
crucial for maintaining the Integrity of therapeutic relationships in a digital
environment um I'm going to be focusing today on psychologist practice as I'm a
psychologist um and am in formed um by the American Psychological association's
guidelines that were released for the practice of tele psychology in 2013 um and while centered on
Psychotherapy this also applies to testing and assessment um which there
has also been more of um online um I'm going to be using the term
patience um to individuals receiving care rather than clients U this is just
a personal preference also um one that's often common in the hospital system but
I just you know I find clients as more of like a transactional business-like relationship um and um you know patients
can emphasize the clinical and therapeutic nature of care um and again while I'm talking kind
of about um teles psychology specifically these principles um really
apply to Tele medicine more broadly okay so teles psychology what is it um the
American Psychological Association in 2013 Divi um defined it as the provision
of Psychological Services via telecommunication Technologies um
including phone video conferencing email text um these can augment in-person
services or function independently um with Communications being synchronous or
asynchronous um and actually the origins of teles pychology go back further than you might
think um kind of the began in 1960 to address the need for reaching hard to
access populations such as um doing remote forensic assessments um and yeah definitely um
been quite the evolution particularly since the pandemic with h psychology
mental health and substance use out patient visits made up only 9 11% of all outpatient tele Health visits or out of
outpatient visits in 2019 and now it's um just by 2020 is Rose to 40% um so
here's kind of a a graph of um its rising and
evolution all right so to give you kind of an overview of what I'm going to be talking about um the key topics for the
tele psychology ethical considerations um I'm going to be talking about access
um so thinking about technology economic Geographic barriers and cultural and
disability considerations um also informed consent so informed consent being you know the
process by which a psychologist informs patients about what to expect enabling them to make an informed decision about
therapy about Tes psychology um also ideas such as uh
privacy risk and secure environments um addressing technological
risks um also uh I'll separately talk about privacy location privacy for
psychologists and patients uh managing technological security and then
competence so um you know does the psychologist have the technical skills
and are they getting um ongoing training to develop those skills and then what's
can be some of the cross state practic challenges all right so first kind of
talking about um the the ethical issue of access in teles
Psychology um you know things to think about um are you know do patients have
access to the necessary devices internet and the skills to effectively use um
teles psychology platforms um what does this look like when um psychologists
only offer um Tel Psychology Services um and maybe
you know gave up their in-person office also thinking about are there Financial obstacles such as the cost of
purchasing a compatible device or gaps in insurance coverage for Tel Psychology Services that limit patients access um
Medicare currently um you know is covering um telepsychology especially
the um uh the first session but that might change um that um that's kind of
through through the end of 2024 and then they're going to re-evaluate which they've been doing every year but you
know that might be something that goes away at some point um and then Geographic and
infrastructure disparities um how do geographic location and infrastructure
issues like access to high-speed internet um affects patients abilities to utilize telep psychology you know
maybe they're able to um sign on for a session but because of
their access um sessions get interrupted calls get dropped and then also thinking about
cultural and disability access are Tel Psychology Services available to patients with language cultural or
disability related challenges such as those that might require translation service or Adaptive
Technologies um that might hinder their participation prior to being at Northwestern I worked at um
uh Shirley Ryan which is a a rehabilitation hospital in um Chicago
that's affiliated with Northwestern and um worked with a lot of patients who have traumatic brain
injuries spinal cord injuries um and um history of stroke um so you know that um
that's something really to consider so there've been kind of um as
as somebody who um has both done the uh practice and research my um my
post-doctoral fellowship was actually a research Fellowship um I do like to highlight uh research so there have been
um some interesting studies about um access and telepsychology since um the
pandemic so a study in 2020 by Smith um at all examined the tele Health use
among cancer patients at a New York City Cancer Center um and reveal significant
disparities with 50% of video visits were white patients while only 17% were black patients 8% Asian patients and 5%
latinx patients um so yeah just really keeping in mind um you know who what are
some of the disparities here and then another study in 2021 looked at tele
medicine adoption in Milwaukee during the pandemic younger white patients with private insurance favored video visits
while older black patients with public insurance lean toward phone visits um
and latinxs and Asian populations were less likely to use tele medicine overall women used it more than men um and there
was a strong link between college education and video visit adoption so again thinking about those um you know
that there have been a lot of psychologists who maybe have given up their um office spaces in the pandemic
because people didn't want to come in or because of the cost and you know what happens when um there isn't as much
access to to patients who need Services all right so now moving on to
informed consent um thinking about patients location privacy are patients
aware of the Privacy risks in their environment such as being overheard or the potential for compromised safety and
domestic violence situations um have the limits to confidentiality been clearly
communicated by the psychologist to patient stressing the importance of conducting sessions in a quiet quiet
private space um if you've used telepsychology maybe you've experience this yourself having someone else in the
home and you know maybe not being able to to open up as much as you would be in an inperson
setting also thinking about the psychologist location privacy um has the
psychologist environment been discussed to ensure that it's secure and private um is there a dedicated home
office that maintains confidentiality how are records being stored you know if if the the psychologist is working from
home um and what you know what steps are they taking to minimize the risk of electronic communications being
exposed um and then um lastly on this slide um technological and environmental
security have both the patient and psychologist address potential technological risks such as unauthorized
access to sessions or stored information are secure platforms and internet connections being used are you using
public WiFi um or an unsecure connection
and are ethical implications of accessing online patient information really being
considered all right um and then we've touched on some
of this already but thinking more about privacy um uh oh wait
there's a a repeat um all right let me
see okay so um yeah some of the the Privacy um studies that I wanted to
highlight um is um a study from 2023 that um systematically reviewed 18
us-based empirical studies to identify challenges and factors related to privacy and teleah health during the
covid pandemic um and it found you know three main risk factors environmental
like the lack of private spaces challenges with sharing sensitive information um techn technology related
issues like data security issues limited internet access and operational issues like reimbursement problems technology
accessibility training needs um and another study in 2023 examined clinicians perspectives on tal
Health um following its its widespread adoption um many providers Express
concerns that telal Health sessions are less private than in-person care um and
reported that patients frequently lacked private spaces for sessions leading to instances where patients were reluctant
to discuss certain issues with for fear of being overheard or even discuss um
sensitive Topics in front of family members all right
and then lastly um I wanted to um touch on um competence so um the idea that um
for technical competence first of all you know does the psychologist have the necessary technical knowledge um
including proficiency with video chat platforms to deliver effective remote
Services um and psychologists are also responsible for assessing and ensuring the client's technical competence
um and then ongoing professional development is a psychologist regularly updating their Knowledge and Skills to
ensure that telecommunication Technologies are suitable for delivering the highest standard of care um this
includes staying current with advances and technology and ethical standards and then cross state practice
consideration and maybe some of you have encountered this you know how does the psychologist handle the complexities of
practicing across state lines um and what steps are taken um patient moves or travels out of state
um psychologists must navigate legal requirements and en just jurisdictional
practice guidelines and there have been a lot of changes to that since the pandemic all right and so
um that's what I have um I was nervous about with the the timing of these talks
and running over to the next person so I finished a little early um but um some
of the references here are the first one you know if you're curious learning more about the American Psychological
association's guidelines for the practice of telepsychology um their main publication
was that 2013 one I also cite the um other studies that I mentioned um that
have been done since the pandemic um and if you have any questions I'm happy to
take those during the Q&A portion uh what time is that again four four so I'm happy to take
those then thank you so
much e
hello our next speaker is Corey thorough who is a philosopher and U musician and
uh aspiring mathematician and computer scientists uh based in uh Baltimore and
um they uh I'm really uh pleased to uh present uh uh introduce uh Corey today
I've had a lot of conversations with him and he definitely knows uh uh the most of anyone I know about uh um uh
particularly CC HCK work and Henry fln's work in relationship to uh intuitionism
so um uh Corey thanks
there oh the I don't think the uh slide is
displaying for
all right oh I need
that uh all right uh so uh I'm going to be giving a presentation on the 20th
century mathematician poet and Soviet dissident Alexander yenan vulpin uh so
vulpin is could be described as the father of ultra intuitionism which I will present a bit of today um I'm not
going to be arguing for or against one or another um uh given positions in the
philosophy of mathematics so um I I prefer to treat the civilizations philosophical orientations uh somewhat
like the cultural Anthropologist treats the cosmologies of the tribes they study so I'm not trying to convince you or
dissuade you from becoming an ultra intuitionist um that's not my problem uh
so there will be a combination of biography history and theory and given 30 minutes all of it's going to be a
little bit hand wavy uh I was just gonna speak on his uh yeah turn up the
volume uh which is the um the first
one check check um is that better okay uh okay so uh so this is all going to be
a little bit handwavy I was just going to originally talk about his paper um the ultra intuitionistic criticism and
the anti-traditional program for foundations of mathematics uh but his uh political life turned out to be just as
compelling to me as his mathematics and I think they both they sure they they each
contextualize each other nicely uh so my hope would be that the breadth uh just
compels you to look deeper in your own time uh so Alexander yenan vulpin uh he
was born born in 1924 in the Soviet former Soviet Union he was the son of the poet yenan uh probably Russia's most
prominent modern uh poet uh vulpin was a notable dissident political prisoner
poet and mathematician logician he was a the leader or a leader of the human
rights movement and he spent a total of 14 years uh incarcerated and repressed by the Soviet authorities in prisons
pushka and Exile uh pushka were sort of pseudo psychiatric hospitals that um
were used as prisons in order to isolate prisoners and break them physically and mentally and discredit their ideas and
if you look up um the Wikipedia page for uh SE kushka vulpin is on there um so he
he spent a ton of his time incarcerated and sort of politically tortured uh so I'm going to start with his political
activity um vulpin formulated the legalist
strategy of political dissidence which what it came to be called um which went on to predominate in the Russian Civil
Rights movement in the mid and late 20th century uh so I'm going to give some background on this now uh much of what
I'm about to read is either cited or paraphrased from the American mathematician critic and growth and
deque translator Roy lisker who wrote A series on vulpin in his magazine ferment
and uh who called vulpin the Russian Thomas Payne um in Spring of 1958 uh so
a statue of the P poet mayakovski was unveiled in mayakovski square near the
center of ma Moscow and after Stalin's regime uh there ended there was a period
where sort of instead of um instead of being tried and told that you were going
to get 10 years in prison and then being shot uh you actually would be tried and get 10 years in prison um so the uh so
the political uh opposition uh felt had had slightly more freedom
um so this this statue of mayakovski opens up in mayakovski square and um uh
the spontaneous poetry readings start popping up there uh with huge numbers of people coming out five hundreds of
hundreds of people coming out and reading poems from dissidents who had been killed or uh exiled uh and also
poets from all around Russia um coming and reading there uh and it become became such a nuisance for the regime
that they tried to quash it with the Army multiple times over the course of two years and
um the people at these organizations would often like form barricades around the readers uh and um keep the the uh
the Army from breaking them apart and eventually they did sort of break down in 1960 um so at 19 in 1960 earlier than
that at makovski Square according to the poet bukovski uh vulpin delivered his
first speech on the strategy of legality or Le legalism as it came to be called and um legalism says that the most
effective way to combat a lawless government is one to uh obey it uh the
laws of the Constitution uh and also to insist that um the government itself
obey the laws of the Constitution uh so the laws of the Soviet Constitution were
filled with plenty of totally fine laws uh and the problem but but but the
regime was sort of um not following any of the the laws the ones that you would
want or the ones that you wouldn't so the first step was to insist that the
government obey its own laws um so here's the quote from Roy lisker about
1962 uh vulin soon provided a graphic illustration of the kinds of effective tactics possible through the use of his
strategy of legality the government uh rounded up the persons they consider the organizers of the makovski square
readings and used them to Stage the first pro prominent show trial of the
1960s uh the trial of Alexander Ginsburg Ilia boxstein Edward kitov and Vladimir
oov uh they were accused of having conspired in an assassination plot uh
which was an invented fairy tale that even the prosecutors didn't take seriously the real reason uh was their
organization of the m kovski Square readings as was customary in these
things the trial was removed to a venue difficult of access to the general public to whom it was in any case closed
vulpin breached the walls of the courtroom for the first time in Soviet history uh so and then to quote bukovski
again he showed up at the doors of the courthouse brandishing a copy of the criminal code guards naturally tried to
prevent him from entering but by a persistent herang he was actually able to convince them that it was against the
law to refuse him entry they might even get into trouble by keeping him out if he should choose to register a complaint
with the right people uh little did we realize that this absurd incident with the comical Alec vulpen brandishing the
criminal code like a magic wand to melt the doors of the Court was the beginning of our civil rights movement and the
movement for human rights in the USSR so a lot of people disagree uh with bukovski that that was the sort of
springboard moment for the civil rights movement of the 20th C cury in the Soviet Union um but the date that most
people agree was the springboard for the civil rights movement in the Soviet Union uh was also occurred and led and
organized by vulin um so which was December 5th Constitution Day
1965 uh so vulpin organizes the first opposition rally in Soviet Union uh in
four decades uh at Pushkin Square and um comes with with a list of Demands um uh
the first demand is to respect your own Constitution which is you know it's Constitution Day so the authorities are
fine with that and the second is an open trial for two dis two dissident writers who had recently been arrested uh
notably they were arrested for the first time uh in Soviet history uh for the
content of their writing so there was no attempt to sort of make a fairy tale about why they were being arrested uh um
so this is this moment is um is um many claim that this day is is
sort of what what was the springboard for much of the Soviets rights Soviet
Civil Rights Movement um so in 1968 just
moving along uh the government begins a new wave of crackdowns on denters and
vulin is taken from his home uh by the police under the pretense that he had not reported to the psychiatric
outpatients department where he was registered and to which he had not once been summoned in the past four years um
so um Andre korov uh along with PS alexandrov who I believe was also
mathematici uh initiated an appeal to the USSR minister of Health which is uh
goes on to be signed by 99 additional academics uh to improve his situation
and eventually vulpin is allowed to immigrate to the United States and he's the only dissident who allowed to IM
immigrate to the United States uh so he was really annoying them
um um so now I'm going to speak a little bit about some of the uh historical and
cultural context for Bin's mathematical ideas
um so in order to speak about vulpin Ultra intuitionism uh I need to give a
little bit of historical context by talking about Ellie J Brower and his formulation of intuitionism um so this
is an oversimplification but there are often presented uh three modern philosophies of mathematics which are
platonism formalism and intuitionism and each one of these philosophies sort of
makes claims about where mathematics exists uh so platonism claims that
mathematical entities um are um exist in a a platonic sort of
Eternal Heaven uh formalism um claims that uh mathematics
is um is a sort of uh a game of
conventions uh how you um for how you manipulate uh um defined entities um and
the rules yeah the rules for their manipulation um um and intuitionism which was
formulated by Brower uh claims that mathematical entities are uh mental
entities and that mathematics is um concerned with the description of these
entities but that the language of mathematics is sort of a derivative uh
phenomena so Ellie J brow was a famous 20th century Dutch mathematician he was
one of the most distinguished topologists and math mathematicians of the the 20th century he's primarily
known today for his fixed Point theorem but he was um probably most preoccupied
with the problem of the composition of the Continuum for Brower um the linear
Continuum is to be built up from an intuition uh which is abstracted from
the awareness of what he calls a move of time uh in which he dubbed the tuity um
Tu so this intuition is a a diad composed of uh the falling aart of the
given moment um into memory along with the presence of the moment which
replaces it and awareness of this intuition as the sort of primordial
intuition can be gained via introspection according to Brower and um
uh Brower believed that the two was a a way to found mathematics which um which
um sort of achieved an iru which sort of achieved an irreducible status for both
continuity and uh discreetness so discreetness and continuity are both kind of ir irreducible in the um notion
of the uh tuity um and he he was skeptical of the
idea that you could uh that you could construct the Continuum um uh on on discreet points alone uh
so for brow this intuition of time is the sole a priori out of which all of
mathematics and Consciousness are built so this is the thing that sort of everything is structured out of um uh
the mathematician Spencer Gard um in an essay that he wrote about one of vulpin
proteges kathern Chris henx says that it should be stressed that brow's alternative Continuum is a rigorous
mathematical Theory while philosophers going back to Aristotle and earlier speculated on the coherence of the
Continuum Brower proposed a precise mathematical framework modeling one possible viewpoint on these phenomena
out of all this effort came a highly uh came a highly original
conception of the relationship between the discret and the continuous a model for the act of doing mathematics and a
mathematical theory of introspection in total it was one of the most original conceptual developments of the 20th
century and one that overlaps with other fundamental artistic achievements in a manner not yet fully appreciated and
just to clarify brower's intuitionism uh differed in certain ways from the formalization of it by his students most
notably uh hating um and I but I don't really have time to talk more about
that so the Crux uh of what we need in
order to understand vulpin is that for the intuitionist mathematics is a descriptive science which describes
mental entities entities whose construction occurs in time and Via introspective processes this has
consequences for many aspects of mathematics um includ and logic uh
certain principles for example which will hold in classical mathematics do not hold in intuitionistic mathematics
such as brower's own fixed Point theorem uh and vice versa for example one of the
laws of classical Logic the law of the excluded middle uh states that for every proposition either that proposition or
its negation is true um that
um so In classical mathematics to give an example of what this means if I
assert the proposition that some string AB c d appears in the decimal expansion
of Pi according to the law of the excluded middle uh either this assertion
or its negation is true that is either ABC D does exist somewhere in the decimal expansion of Pi or it doesn't um
for the intuitionist however uh while it is the case that if we happen in constructing the decimal expansion of Pi
to find the sequence ABCD then we can assign the proposition asserted above the value of true uh there will never
come a time when we have generated a sufficiently long decimal expansion to decide the question
negatively um um so the the so the law of the excluded middle is sort of
invalidated in intuitionistic logic at least in some context um just a final
quote from Brower uh truth is only in reality that is in the present and past experiences
of Consciousness there are no non-experienced
truths um so now that that's out of the way I'm going to talk a little bit about uh
vulpin paper the ultra intuitionistic criticism and the anti-traditional program for foundations of
mathematics um so this
uh this paper appeared uh in a book called intuitionism and proof Theory
proceedings of the Summer Conference at Buffalo in 1968 uh which was edited by Kino Mill
and Beasley um and so so like I said vulpin immigrated to the US in 19 uh 70
I think when the conference actually happened I'm not sure about this but he might have been imprisoned um and so he
was set to open the conference but um uh in his place uh hating opened the
conference and but in the book um in the book vulpin paper uh does does
open the book um so vul uh
so let's see Vin's paper opens the text oh yeah funnily enough it uh and it
appears in the New Direction section alongside uh what's Martin L snare
famous on the notion of Randomness and Vin's paper is extremely long or pretty
long and uh Technical and it covers a lot of topics so I'm only going to touch
on it csor um just one more thing about the um
this uh book and the the conference I mean there was a a lot of really big
names appeared uh at this conference like I said hating kryal bernes sman
these were all people um who are part of this conference who and whose papers are in this book and um and it's it's it's
interesting to me that uh that Vin's paper is at the beginning of this book because he mostly went on to be
neglected um
vulpin uh so vulpin paper like I said I'm I'm only going to be able to touch
on curly he begins by critiquing some traditional assumptions made by
mathematicians and proceeds to provide a consistency proof of ZF set Theory without the use of infinite entities um
and if you want to learn more about that there's a paper by Jim Geyser in the Journal of symbolic logic that reviews
vulpin paper uh so I'm going to the first paragraph and I think it's interesting to keep in mind everything
um that I mentioned about Vin's political dissidence um so here's the
first paragraph the aim of this program is to banish Faith from the foundations of
mathematics Faith being defined as any violation of the law of sufficient reason for sentences this law is defined
as the identification by definition of Truth with the result of a present or
feasible proof in in spite of the traditional complete incompleteness theorems which deals with only a very
narrow kinds of proofs which I call formal proofs I define proof as any Fair
way of making a sentence incontestable of course this explication is related to ethics the notion fair
means free from any coercion and fraud uh and to the theory of disputes
indicating cases in which a sentence is to be considered as incontestable of course the math the methods of
mathematical logic are not sufficient for this program and I have to enlarge the domains of means explicitly studied
in logic so vulpin tries to shove a bunch of things into the foundations of mathematics like ethics and semi
semitics and um sort of modality theories uh and linguistic theories
um uh and um I again I'm not here to sort of convince you to be um a vulite or an
Ultra intu uh I'm I'm just sort of presenting it so you can take it as you
will vulpin concern with those aspects of an argument from which we draw our confidence of its convincingness compels
him to study the role of linguistic forms such as tense and voice as well as semi semiotics like I said and what he
calls the theory of collations which amounts to theory of um identifications
and distinctions um the traditional assumptions that Open begin by
criticizing are the following uh so I'm just going to list them uh one the
uniqueness up to isomorphism of the natural numbers two the existence of the
values of primitive recursive functions for every system of arguments for an arbitrary natural number series three
the principle of mathematical induction from n to n Prime for what vulpin calls
the locality principle uh which is sort of that uh if the axioms of a formal
system are true and the rules of inference conserve the truth then each theorem is true uh and five the
meaningfulness of the relation of identity and distinction he says I criticize these assumptions above all
simply on the grounds they are assumptions their incontestability has never been established and this is
sufficient reason to doubt them uh so he goes on to show that the
locality principle and induction both require each other and so to him form a vicious circle at the foundations of
mathematics and um uh from this vulpen rejects axium systems as sort of the way
to found mathematics uh he doesn't reject explicit Employments of axium
systems but he requires that prior to the employment of uh axum systems the
aims and rules for the construction of a mathematical system must be specified explicitly so aims as well as
definitions and he provides a whole he provides at least the beginning of a of a theory for for this
um so one of the things that if you look up open you'll see a lot associated with
him is his uh is his assertion of the of
the non-uniqueness of uh natural number series um so he he goes on to ask why 10
to the 12 should be considered a natural number and shows uh because you you can never count to it for him a natural
number is something that that you can count to and so he sort of creates a theory of what he calls feasible numbers
uh which are are numbers that you could feasibly count to and I don't know how you sort of identify what's at at like
what stops becoming a feasible number um but but he's really he's very uh
concerned with um with sort of foregrounding possible
Infinities uh rather than actual Infinities which is part of why he's concerned with modality theories uh
Because he believes that you have to make statements about what you've constructed in the past what it's possible for you to construct in the
future um and things like that uh so um
so he he identifies two different kinds of natural number series one of feasible numbers like I just said and um the
other which corresponds to the traditional notion of an infinite natural number series and he formalizes this
distinction via uh embeddings of the latter and the former um which again he
does over a bunch of pages and it's um more technical so I don't have that much time to go into it but um you can uh
look this up more if you'd like uh these possible Infinities uh or feasible
series which are being constructed require to vulpen a modal Theory because
one speaks of what one has constructed thus far uh what one can or cannot
possibly construct and so forth he then continues his critique of
traditional assumptions adding the following to the list um so six the possibility of
neglecting modalities and aims and Foundations seven the possibility of neglecting rules of attention and
neglecting um and then he um uh arrives
at what he calls the atomic actions for the foundations of mathematics um and um I'm going to read
those off here too so one is preferences two s of establishing or
following Connections in particular acts of attention three collations which are
uh identifications and distinctions between different objects indications
which is his term for a Semitic act which he explicates in the same
paper acceptances of propositions perceptions the acts inverse to the
proceeding so the inverse of each of uh what I just read the abstention from
each of the proceeding raising and answering questions addressing another person and
including some text in in in memory and then the exstension from those um he
then begins uh the constructive part of his paper uh the consistency proof of Z
ofet Theory um which like I said you can um look at Jim geyser's review in the
Journal of symbolic logic um and um so that's sort of the summary
I'm going to give of his um of his text I I find it uh compelling that this text
which um I think is extremely eccentric uh appears in this this book um among a
bunch of well-known mathematicians and that vulpin was uh eventually sort of um
uh there were four or five mathematicians who really took him seriously there was the computer scientist Jim geyser there was uh his
proj Katherine Chris henx who he tried to um uh somehow uh uh um contest uh
girdle's incompleteness theorems with um and um there were there were a few
others um and Roy lisker who I mentioned above who sort of attributed people's uh
uh not giving vulp in the time of day to his eccentricity and their lack of awareness about the political situation
he' come from but I don't know if that matters or not um and uh that's all I've
got thank you
hi uh thank you Corey uh next we have uh Christopher Brown who's the researcher
at The topos Institute and uh I'm pleased to have
them uh participate in this conference uh um they're going to be speaking about
inferentialism and applied category Theory and they also uh in addition to
the work on category Theory they have an uh um one of their passions or Hobbies
rather is uh the works of uh studying The Works of uh brandom and sellers uh
but uh uh I'll hand it off to Chris great thanks guys um all right
everything looking good with the screen share you see a title slide I hope uh
okay great so I want to talk about inferentialism and applied category Theory and I'll start by talking about a
quite practical problem whenever you have cooperation going on in science you
need some sort of fixed structure a common protocol a controlled vocabulary
a database and there's all sorts of affordances you can get once you have something like that but we're immediately pulled in two
directions because it's nice to have things to coordinate over but we might inadvertently bake in some unknown
assumptions that we decide that we want to change later or we're missing something that we realize we need later
and so this this thing that was originally liberating and letting us do something we couldn't do before is now
controlling and restricting what we can do so let me demonstrate this with a toy example imagine some Proto people who
are sending each other little messages like you know a a reacts to Form B and B reacts to form C yada y yada if if
everyone in the world were buying into this protocol like research papers were just lists like this then there would be
some actually cool things you could do you could answer questions you could write a piece of software to answer the question you know is anyone working on
reaction networks with A and C or is there a pathway to get from a to d from anyone in the world and so that's a
really cool powerful uh tool but let's see what happens next someone
else joins and they say Okay I want to get in on this party but when you call a I actually call B and these two things B
and C you're talking about actually they're both the same thing and so there's nothing in this protocol that
gives you the ability to handle this kind of nuance and so you end up not collaborating and it's very realistic
that you discover things that make you change your perspective so we might find it actually practical to describe
certain quote unquote multi-reaction actions which have multiple uh inputs and multiple outputs but it would be a
lot of work to add this new data structure and figure out how does it play with my old notion of of reaction
uh and yeah it's neither here nor there whether or not this is a completely new thing or whether or not we were just
wrong about our previous notion of of reaction there's the same practical problem and so the general problem is
how do we find a synthesis of structure and flexibility and now I want to talk about
about uh another problem which might not seem related at first which is how do we think about semantics uh let me start
with a kind of naive status quo type picture which is I'll call representationalist semantics so the
story here is that we analyze a sentence like the green frog is on the log in terms of covertly being some sort of
logical uh expression and then we can give semantics to uh green and frog and
log as essentially predicates that are subsets of some set of things that there are and this on relation is a set of
pairs of things where one thing is on the other and now that we uh have that
it's actually a really nice tool because I can analyze what the meaning of the sentence is in terms of the meanings of
these uh components so this property would be a compositionality uh and it's really
powerful and a lot of intellectual effort has been trying to essentially generalize this picture to make it work
for more and more of our talk so how you represent possibility well then you get possible World
semantics so uh but this is at odds with something uh called redescription that
I'll I'll give by example so it might be part of our definition of deserve that it's tied up with Notions of markets and
exchange and then later we realize wait no actually there's there are these things called rights and actually
deserving things has to do with that and we we changed we were wrong but uh we
want to say that it's not that we have deserves Prime now and we're just talking about something different we want to say that our notion of deserve
has actually changed and more concretely we have a notion of cat that uh is very
tied up with being a mammal uh they both it's it's essential to being a cat that you're a mammal uh but we're also in
principle open to the idea that future scientists you know really look hard at their DNA and say nope actually cats are aliens and so we were wrong about
something but but what were we wrong about if if we can't be wrong about our definitions then we were wrong because
there actually are no cats but it seems more natural to say no we are we we we got the definition wrong and that's the
sort of thing that can't happen if your notion of meaning is tied to these types of definitions this sort of thing
happens all the time in science where we redescribe our old vocabulary uh you
know things that were contingently true become essential and things that were true just become approximately true and
you know we don't want to be committed to the idea that no one in the 19th century was ever referring to actual
electrons because then we're we're really doomed to skepticism because we're always open to the idea that
future scientists will will show something that was wrong about our current theories and I I don't want this to be
limited to just I don't know moral epiphanies or uh empirical findings even
in mathematics uh there's lots of different ways you could conceptualize what a triangle is uh maybe someone took
it essential that it was a polygon with 180 degrees and accidentally something with you know three uh points that are
non-colinear but it turns out throughout the development of math that one of those definitions actually made more sense for
generalizing so that we can actually talk about Bonafide you know non- ukian triangles as opposed to non- ucan
Triangle primes because we we can't use the word triangle again so these
examples should remind you of the first problem where we want to sort of redefine reaction as we learned more
with our current definition of reaction and if we don't have this ability to redescribe you know we're trapped in
backwards ways of thinking we're not prepared if cats are aliens our scientific vocabulary doesn't refer to
the real world and math becomes oied so we need another a middle way and clearly
we do address this problem in practice but we want a semantic theory that actually is compatible with the fact that we can deal with these
problems so the point of this talk is to just talk about two seemingly completely
different approaches to this problem uh and uh I hope to draw out some similarities between those two
approaches so uh the first first one is going to talk about category Theory so for a context like this uh I can't
really get into all of the definitions involved but I hope to provide some intuition uh that you can do keyword
search later if you're interested so for for present purposes category Theory it's a it's a branch of math it focuses
on relationships between between things rather than the things themselves uh a
category has objects and arrows and because category Theory can not really talk about the objects directly its
definitions really are about certain shapes of arrows so we might give a name called a span to this shape of arrows
that has two two outward arrows and these could also we could give names to certain Global properties such as if
there's an object that has exactly one Arrow into every other object in the category we can call it zero we could
call it an initial object and we've now characterized what that is by this property and the fundamental kind of
relationship that we care about and this is the most complex piece of category theory that will be in this talk so I'll
just say that a funter is a way of relating two different categories to each other uh it's an assignment of the
objects and arrows of one category in the other in a way that respects the local structure so you could think of it
as like a good analogy you're you're preserving the the order and connection of things and so we might think of you
know the original application of category Theory category theory is always applied category Theory it's just the original application was internal to
mathematics itself so if C were some sort of algebraic uh space of things and
D or some more geometric topic then this functor from algebra into geometry is
going to essentially allow us to systematically talk about geometry using the language of algebra and that just
turns out to be a really convenient thing so the power of category Theory to
abstract comes from the fact that its definitions can really be applied in any category so now that I've defined
initial object in this way I can now go to any category I can talk about you know sets and functions I can talk about
graphs I can talk about matrices and I can say what it means to be an initial object in that context and so in
particular you might say that category the doesn't have a proper subject matter itself uh it's but it allows you to make
pre-existing structure that exists in all sorts of other places
explicit so the story for solving that initial problem I posed uh is has to do
with functorial semantics so let me it's all captured in this picture let me try to explain it so we have multiple
theories uh T1 T2 T3 and we're going to think of those theories as categories
themselves and furthermore there's going to be another category called s which is going to be our semantics category very
common this category is the category of sets and functions so let's think what it means to give a functor from your
theory into the category of semantics of set it means for every single object in
your theory you're assigning some sort of set and for every morphism in your theory you're assigning a function so
this has some interesting properties one we're not confined to a single theory in order to be uh meaningful uh we're also
not confined to a fixed semantics like the set of real things or the set of subatomic particle Arrangements uh and
importantly these theories are related to each other so we have arrows between these theories so there's actually a
category of theories and we can use that to ergonomically and rationally move
from one Theory to another uh and I'll note that this space of theories itself is not just this
completely fixed god-given thing uh it's uh when something that plays this role is called a Doctrine and there are other
sorts of doctrines you could have and uh you can trust that there are ways of relating those doctrines to each other
and so we can also flexibly move between those if we properly contextualize what
the doctrine we're talking about is but we can stop at any point and just take that as some sort of
given so this is the story but I think it won't be clear unless I give really concrete examples of what does T1 and S
and what do these dashed arrows look like so let me actually get into some of this so uh an example scientific model
for the examples we're talking about might be a petrinet so a petrinet has a set of species it has a set of
Transitions and it has input and output arcs going in between those and so you could think of this as a Syntax for
chemical reaction networks so we think of these blue circles as chical species the boxes as chemical reactions and
these inputs and outputs tell you how many of each molecule is actually the reactant or the product and so an
interesting thing is that this whole Space of possible Petr Nets is itself
the category of models of a particular Theory so let's go back so there I'm saying there is some fixed Theory and if
we look at all of the possible functors into the category of sets what we're going to recover is precisely this
notion of this space of models of petrinet given this Theory so to unpack
that let me start with graphs because graphs are as we seen a bit simpler than petrinet so graphs the theory of graphs
really is a category that has two objects we'll call it the edge object the vertex object and there's two
morphisms it's called source and Target and now we can ask well what is a model of this Theory well it's a functor into
set which means we're going to assign a set of edges a set of vertices and we're going to have functions that tell us
what each edges source and targe is and so this is what the this particular model might look like as you know a pair
of sets this is what it looks like uh as a graph next and then lastly we have if
it were interpreted as a chemical reaction network uh then this is what the reaction Network would
be so now we can talk about Petr Nets Petr Nets are slightly more complicated
where the theory or the schema for petrin Nets is a category with four objects uh transitions species inputs
outputs and these corresponding morphisms and so if we look at a particular example of this we'll see you
know one transition two species two inputs one output and the actual functions will tell us that what it
really means is that a plus a goes to B so now returning back to our our
picture we've now seen uh what T1 and T2 might look like we've seen what s is and
we've seen what these dashed lines mean so now how do we actually use this to flexibly move when we realize that we're
wrong and need to actually change our our perspective in a in a systematic rational way so what we need a
systematic way of turning graphs into Petr Nets is a functor from this category of graphs to petrinet so
importantly it's not just a function that gives you a petronet for every graph but it's does it in a systematic way that preserves the relationships
between graphs within the relationships between petran so we need something like that in order to systematically move all
of our infrastructure and our code from working with graphs to working with Petr units uh but this is a massive amount of
data there's an infinite number of graphs there's so many so many morphisms so uh this is like quite impractical to
to specify naively but a conceptually legible way of packaging this is to say
no we merely need a funter between the two schemas so uh if we go back to here
there are arrows between these theories well these theories are categories so it's natural that these arrows are functors and we're going to use a funter
at the schema level to actually induce the right notion of a funter between models so here's a particular funter
we're sending species to vertex and we're sending everything else in the petronet schema to the edge table so
Category 3 provides these Notions of functorial data migrations that actually allow you to obtain these functors
purely as a function of these schema mat level functors and uh this is really useful
because we've essentially found a the right way to redescribe our graph vocabulary as petrinet vocabulary
because we're inter in edges as transitions that happen to have one input and output so uh this is all tied
up to con extensions and adint functors if you want to look those up but this is the basic idea and a lot of what we do
uh in during my day job is to actually make this computable so that you can actually you know run this and change
your code to work from graphs to working on Petron Nets I'll say one more thing about applied category Theory which is
that uh it's kind of orthogonal which is that there's really an emphas empasis on open systems so before we can think of a
petronet as a closed system because there's no explicit interface or boundary but we could think of a
petronet as a morphism from one boundary to another and so those boundaries are basically related to certain species and
the petrinet and now if you have two uh petrinet open Petr Nets that share a
boundary you can actually glue them together and get a new Petr net so if we think of assigning semantics not to Petr
Nets but to open petronet we actually get again a really cool idea that we can
uh be committed to some sort of model but still be open to new possibilities that that we don't yet know yet so you
can do this with like assigning a reachability semantics to these individual things and compose them to get reachability for uh the overall
model so just to summarize the the key takeaway points of how applied category theory was useful for this original
problem we work with external structure rather than internal structure uh
categories are essentially making uh pre-existing structure explicit and I didn't get to talk about this Rich field
of categorical logic which says you can do intuitionistic logic classical logic
uh higher order logic in any category if it has enough structure for it so this is a kind of logical pluralism that just comes right out of categorical logic
there's an emphasis on open systems uh you can reason within a theory while still retaining flexibility uh something
interesting is that syntax and semantic are just both categories they're not two ontologically fundamentally different things and that means you can actually
have your semantics be the Syntax for another theory uh and lastly I think it gives
you there's lots of different ways you could get a notion of redescription out of category Theory but one particular one is this notion of functorial data
migration uh next I want to change gears and talk about inferentialism luckily Ryan gave a great introduction to this
so I can rely on that a bit uh it's broadly a class of theories that try to
understand reference in terms of this conceptually prior notion of inferential role and so there isn't any direct
conflict with the earlier picture of the Frog on the log as representational uh we still can and ought think of it that
way uh however in terms of an overarching view of propositional content we want to then understand those
representational relations in terms of for example the role that frog and log and observational reports in general
play and so rather than an atomistic picture of Concepts having reference which then gives sense to sentences
which then tells us which inferences are good it goes the other direction you you have a notion of good judgment and from
that you try to understand what it is we're talking about so the Paradigm case as Ryan showed was uh uh that clearly
this is a good story for logical vocabulary uh to understand and is really to just understand these
circumstances of application and consequences of application uh and the challenge is can we do this with
ordinary Concepts to and for this I'll be drawing from a recent book by uh ol Lobel and Robert random uh and try to
highlight some moves that I think uh have some at least superficial similarity to the story I just told
before so to get into it uh we care about inferences but inferences between
what we we start with a lexicon which is a set of claimes and we can construct positions like a Turn Style CD which is
a notion of implication or a Turn Style nothing which is a way of encoding incompatibility and and so we have this
space of possible implications which is the the power set squared and an implication frame then and we
suggestively call it a vocabulary is a lexicon equipped with a distinguished subset of these implications which we're
endorsing and restal is going to suggest that we uh interpret this as a claim of
incoherence namely that gamma Turn Style Delta uh endorsing that is to say that
uh rejecting all of Delta and accepting all of gamma is an incoherent out of bounds position
so let me give some examples to show how broad this notion of vocabulary is uh you could have these very Niche specific
types of vocabulary such as a theological vocabulary where it does follow that if it's Pious then it's
loved by the gods it doesn't matter if those have reference in a a scientific vocabulary or not uh we also have our
pre-existing pre theoretical notion of just what is a good reason for what and we can call this a notion of material
inference or ordinary ordinary language inference and from that it is a good inference that Pittsburgh is to the west
of New York uh to New York is to the east of Pittsburgh there's something else you
could do with this which is you could uh have the Lexicon of first order logic
with some generators and in that lexicon you have a particular element which is
for all MN east of y y yada is equivalent to west of NM and so these
are just particular claimes but in uh first order logic you can then deduce
that it is incoherent for to accept C while rejecting A and B and there's a particular pathology which says that uh
I have to analyze my ordinary language inferences in terms of this this kind of first order logic system but there's no
need to do that I mean these are just both different types of
vocabularies and so the motivating idea of logical expressivism is that what
logic is doing is not underwriting our claims of good reasons but it's actually making them explicit so gamma entails uh
a material implies B is saying that gamma and a is sufficient for B and or
that it's incoherent to reject B and accept Delta in in uh a and so how do we
make this work well we can extend any base lexicon to include logically complex claimes and not this is quite
different from the the logic on this slide because this logic is internal to the claimes which is invisible to the
the implication frame but this is now going to be legible explicit uh uh uh
syntax such as the claimable A and the claimable B and uh what then are we
supposed to do with the reason relations the sort of implications that involve uh these logically complex bearers so we're
introducing new new elements new claimes and now we need to know well how do I know if it's a good inference or not and
it's important to know that uh if we were to go about this and a naive way we might end up with something that's
called not harmonious so it's possible to make rules for connectives such that
you can deduce things that weren't true originally so the the most classic example of this is Tonk where I can
derive anything if I introduce a tonk connective and this is clearly bad this isn't the type of thing that is logical
we want to be able to observe something without changing it so uh there's this
perspective that logic should be LX elaborated from a base vocabulary but also expressive of it and so classical
logic is LX for certain types of implication frames but not all implication frames and so what we want
is something that's universally LX and so to just reiterate what logical
expressivism expressivism is by contrasting it with its Converse which uh is called logicism there's a question
of what should logic even be about and there's a perspective that well you should just study purely logical
sentences uh ignore uh no content of non-logical things should factor in but
logical expressivism wants to understand the relationship between contentful non-logical vocabulary and logical
vocabulary uh there's the question of uh the relationship between reason relations and logical vocabulary and one
is that uh you should be reducible to logical vocabulary and the other is that you should be expressable by logical
vocabulary the perspective on non-logical content is that it's not really possible to have genuine you know
material inferences there's they're just incompletely specified logical inferences whereas you might want to say
no there is something to the concept of cruel there is the the idea that birds can fly there's something to that even
if it's a diffusible inference and the search for generality ends up going from
trying to just find this covert logical syntax underlying our in ordinary talk to trying to find a logic which is
universally LX now this is not a technical talk so I want to say just declare that that there
is a proposed uh logically universally LX uh uh system uh which they call
non-monotonic multi succedent logic which is conservative for all implication frames and a powerful
representation theorem will tell you that uh this gives us enough vocabulary to make uh the underlying base
vocabulary and its reason relations explicit so once that's been done though
there's something further that's done uh in this work which is uh putting syntax and semantics on the same playing field
so what I told you was just a story about our talk you know saying claiming certain things and certain things being
in bounds or out of bounds and so the opposite perspective is to say uh you know the representationalist picture is
to say nope there are things that we refer to with our talk and that's what underwrites what we say and a sort of uh
Paradigm of that would be truthmaker Theory which generalizes possible World semantics uh and it does this with a
notion of worldly in in a hyper intentional in a way that means you can distinguish meanings that are still true
in all the same possible worlds such as 1 plus 1al 2 and that three is Prime and so the mechanics of this which I won't
be able to get into in a non-technical talk is that you have a notion of worldly States combining States and
possible States and you understand uh gamma Turn Style Delta as there aren't any possible state that makes uh gamma
true and every sentence and Delta false so just stating a punchline is that uh
there's a sort of Duality and isomorphism between the nmms system from before and this truthmaker Theory and
that you can actually move back and forth quite fluidly and this is really interesting because it puts these two
things that are normally very different worlds on a par and that you can be an inferentialist about anything as long as
you have a notion of compatibility or incompatibility uh something else that I
want to stress about this this perspective from this book is that it focuses on open reason relations so normally we think of The Logical
consequence relation as something that's closed when you take consequences of something if you take consequences again
that doesn't add anything new and when you add new premises all you do is strictly add new conclusions uh new new
things that you can draw and that forces you right from the outset to be committed to this idea that nothing
changes if we can derive something and then add it back in as a premise it must have already been a premise all along
but this is really at odds with something that's crucial to rationality which is that we can derive consequences
which when we accept them they actually allow us to derve new things we might invalidate things that we took to be
true and we can Rectify earlier con contradictions and so the idea of
explosion is really implausible when you start talking about non-logical conceptual contents and so this is
relevant to science because we use theories to extract consequences from data and this data can make us change
our theories and then our Theory allows us to extract even new consequences and so for for example we want to be able to
make sense of that gamma and a let's say a is our our Theory uh gamma and a allows us to drive B and C but if we add
that back in we get a contradiction now we're in a crisis perhaps and now but we realize that B and C actually mutually
so support each other but don't support a and actually our new theory is going to be B and C and so this is the sort of
thing that we can express and and show how this progression is rational that was just not expressible uh if you're
committed to this closed reason relation uh perspective and I talked about how
compositionality was this sort of uh nice property of of the representationalist picture uh and we
can say it obeys a strong compositionality property where knowing every mentioned small sentence in in fi
is going to enable knowing this complex sentence fi there's a weaker but non-trivial
compositionality property of uh inferentialism which is that knowing all the small sentences lets you know all
the complex sentences so if I know the reason relation of the base vocabulary I can know the logically extended
vocabulary and so uh I won't be able to get into uh uh the formal definition of implication
space semantics but the idea is that the semantics that were assigned to these things uh in inferentialism is not uh
some fixed set but it's actually an entire uh uh a range of subjunctive
robustness the role that we assign uh gamma Turn Style Delta is going to be all the implications for which we can
add Gamma Delta to and it makes it a good implication and so with this kind of perspective we can assign recursive
uh definitions semantic definitions which is good I consider that a good form of compositionality the downside is
these roles themselves uh are defined holistically I mean the role involves all the possible uh implications of the
base vocabulary so trade-off it's a bit harder or a lot harder I might say to compute actually with these things and
the way it's easy to compute with a uh closed reation so I hope I've drawn some
similarities and if not I I'll make them explicit here there are some similarities between the applied
category Theory story and this logical expressivist story there's a focus on making pre-existing structure explicit
content is being defined in terms of external structure rather than internally uh logical structure is
exists it's special uh and it's useful but it's it's not guaranteed and so there's a kind of pluralism about the
right logic uh traditional thinking about logic is good useful but it's not useful as a foundation
uh there's an emphasis on open systems or open reason relations uh we can
theoretically ground some Norms uh that are binding without requiring some fixed
foundations uh syntax and semantics I think are in in different ways put put on a par and rather than having this
fundamental Gulf between these just two completely different types of things and I think that in both situations we have
a way of making sense of redescription so thank you for listening and I appreciate the convers ation I've
had with people at topos uh as well as Ryan Olen B
thanks Stan is a philosopher and an author most recently of intelligence and
spirit on urbanomic sequence press he directs the critical philosophy program
at the new center for research and practice thank you Eric uh thank you
everyone uh my apologies um I meant to prepare uh you know some notes uh
transfer some notes but given the condition with this hand unfortunately I failed so I'm going to uh give a very
short talk uh you know and try as much as uh I can to read only a few citations
that would be all uh uh so uh what I would like to uh talk
about and particularly given the description of the confurence and the menion of Gabrielle's work uh Gabrielle
katan's work uh is uh essentially a deeper look
into the phenomenological project and how
um within the hosan critique of the Canan distinction between phenomenon and
nenon whoel uh you know foray into the
distinction between noesis NOA or no mat and transcendental
objects uh within a certain sort of methodology of
philosophy uh which would constitute the backbone of the project of
phenomenology um converge Upon A Certain sort of
curious uh what you might call to be uh uh computational and complexity
conundrums uh which truly understood uh can be uh taken or considered as various
way of uh looking at the problem of
nenon uh after the computational turn right uh a transcendental
philosophy and a transcendental skepticism U but now equipped uh with
the computational turn of the 20th century ongoing to through Modern
revolutions uh of logic uh theoretical Computer Science and Mathematics I try
as best as I can to abide by uh you know
approximations um so no one uh uh you know expert knowledge would be offended
in the process so first and foremost uh Eric can I uh
share a couple of
diagrams do I have that
functionality I think Eric is missing so
um just one second oh sure sure
absolutely you should be able to excellent uh one second so it's
uh share okay
does everyone see
this sorry I can't okay excellent so the there are two
diagrams uh one what you might call to be the diagram of transcendental
hylomorphism which would be hell's uh prevailingly neoc Canan critique of
C uh a specific type of hylomorphism
this is the first
diagram we have the poles of subject and object reflexivity and
reference which in can would be that perceptive self and first order outer
appearances thus and so organize for objects you know of mind and
Sensibility which then would break into uh a different
uh sort of uh categories no longer in terms of phenomenon and nenon um but
rather in terms of uh nois and noata or NOA uh in a sense of a of a
deep entanglement uh between acts of Consciousness and objects of
Consciousness and these objects of Consciousness might not be of Consciousness as in usually the cas case
with Transcendent certain sort of transcendental objects or Transcendent
objects and then we see the this distinction uh or the problem of uh
can sorry the the problem of transcendental
hylomorphism uh Tri furiates uh into
sin citation noesis uh
and uh semantic Conta Kum no transcendental
object and logic of Truth or signification or cognition being uh What
uh bring them together and the procedure of
how essentially H uh achieves at this rectification to cans
idea of phenomenon appearances uh to nenon um undertakes this methodology to
achieve this result uh is by way of as we know a
radicalization of the prenal concern of philosophy
meaning a fundamental synthesis or
entanglement between lus and appearances of different kind
ionus aasia and idolan right various forms of
uh appearances of the kind which pertain to
um the first two segments of the analogy of the divided line in
Plato's
work the metodology is a radicalized cartisian one precisely it takes
philosophical meditation uh to a different height uh as we know
um decart in his meditations also reinvigorate this gesture of philosophy
of the synthesis between logos and appearances in a different sort of way in meditations uh from the first to the
sixth meditation nowel taking it to a different uh scale by radicalizing
even uh decart's form of meditation and when I'm saying
meditation here obviously we are talking about uh doubt a special kind of doubt
it's not ponic or academic sort of Doubt but a special kind of doubt and also a
very Co coherentist uh
logic of I the speaker right Consciousness
Consciousness or um eye poles as we see it in deart sometimes
when in throughout the course of meditations he writes sometimes the ey
is biographical sometimes I simply stands for a philosopher
sometimes it is merely uh a logical Act
Right an elementary logical act in order for it uh in order for it to be posited
to be counteracted by another eye right a logic of interaction so to
speak and the name for uh What uh this
radicalized cartisian method of meditation is in hosel to
tackle the problem of phenomenon noma in terms of this
uh Three Forks of uh you know
uh quation cognition patum nois no transcendental uh or
Transcendent objects would be R frag questioning back stripping back that the
only way that we can as uh you know Plato would have probably agreed uh
particularly in his later works maybe not even in Republic he would have agreed uh that the only way that we can
understand uh give a thoroughgoing analysis of all
appearances and that's exactly what phenomenology means forel the thoroughgoing and system itic analysis
of all appearances a descriptive analysis so to
speak such that we can strip back meaning question
back uh behind this appear uh disappearances while never go astray
from you know pretending that we are doing something else as opposed to you
know um entertain in a version of intellectual or rational
Intuition or Supernatural faculty uh so on and so forth would be ruk FR uh
stripping back questioning back uh towards uh foundations but these
foundations are not Givens right there are not data in any sort of uh ways that
people usually in the salarian philosophy uh POS them precisely because
the methodology of R requires
um uh a movement in tandem with a
certain sort of conceptual explication right so it's concept play
in uh throughout it's not it does not strive to
uh essenti um forfeit
uh the linguistic conceptual Labor uh in order to achieve uh certain
sort of Foundations allow the Givens ideological fixations in a kind of a
salarian sort of way of various kind epistemological
categorical uh so on so
forth now if we stretch the diagram above uh this
diagram would kind of look like this in the essential understanding of pel's
idea of phenomenology namely uh the analysis uh
the descriptive analysis of all appearances
right but in Canan terms um their condition of possibility their
uh fundamental condition of possibility was essentially
uh n uh as a condition of possibility uh
epistemological uh particularly in the vein of epist transcendental deduction
uh that to say why is that these experiences uh are exactly as they are
right or do and so right
so if we stretch it out uh the kind of
vertical diagram above into kind of a flat map it would be uh something like
this it is uh it it pertains to the experience of being in the world right
and hence no noesis uh becomes the expression of
being in the world you know as opposed to a
um of this Canan dichotomy uh that there is a thing in
itself and there are appearances or
um a form of uh you know essentially uh kind of a flat
picture uh of subjectivity uh which would lead into a
form of transcendental solipsism right uh so jel wants to he understands
that uh by definition the problems of transcendental philosophy as defined by
a c uh have their origin uh in a form of
transcendental solipsism the question of subject conscious subject the one who sees the
appearances see them or see of them and see them as
right um bosel doesn't want to end up in this sort of Canan
straight jacket or flat Orthodox scism by uh understanding that this is
the experience of being in the world being in the world right which from a
different sort of perspective perspective now that is a very interesting uh thing from the noematic
Pole right than the noetic pole from the noematic
pole the experience of being in the world translates
into the world in its absolute sense
right technically speaking
having a self-expression a localized self-expression of
itself right which that would count as the idea of
manifestation appearances uh always come uh with a
form of manifestation and manifestation is
uh ultimately has fundamental experiential connotations so being in the
world uh on the noetic poll has as its
Counterpoint or counter Poole the self-expression of the world in its
absolute sense that world not wealth uh here but the UN
wealth expresses itself manifests itself in a local uh or
in a localized uh perspective and this
perspective within which manifestation unfolds and hence experience takes
place uh what you might call to be the problem of Consciousness a problem of uh
subjectivity and hence a problem of phenomenon without
having uh you know the necessity to insert the word Nan
here in in its Canan sense at the very least in in this Canan in his pure Canan
sense so in a way the subject from this sort of perspective would be a kind of a
local perspective operator of the UN unto
itself right my apologies let
me stop
sharing this brings us to this idea
that within uh you know the tradition of philosophy at least
as wasel has been radicalizing decart and decart in the name of enlightenment
has been radicalizing the fundamental message of philosophy
itself I would say that the question then it is not
um shallow it is actually a very deep sort of question deep in a sense that
subject but it's very constitution in the sense of
the act that constitutes it and what it constitutes right kind of St austinian
puzzle uh which was loves it to to
really talk about the problem of Consciousness uh by way of you know the
problem of inner time Consciousness uh is the time uh
constitut or experience of time is constituted or
it's constituting right and when
hosel approaches the problem of subjectivity through this sort of
angle in in the philosophical tradition it seems that the the subject
now somehow um being deepened we don't have that sort of
shallow subject that is usually the subject of our natural attitudes or
psychological attitudes toward the subject not even
transcendental uh attitudes uh which are made of
uh what you might call to be inflexible or firm types right hence uh you know various
versions of Orthodox clanism but rather
uh the depth of the subject can be properly
understood in terms of phenomenology understood as the
synthesis between logos and appearances you know which came first sort of uh
question austinian tradition did the egg come first or the chicken first is a
time that is constitut or time experience of time is being
constituted right and to show that entanglement here is fundamental for the
understanding of the problem of the subject and hence by definition uh
the the the question of who is I who is a
philosopher who is talking here right who who is in the
nowhere who is making thus and so judgments who are
making thus and so mathematical uh you know um
structures and this this is I think is very
interesting precisely because once in a very allegory called
met metaphorical way the subject is being shown to be a local perspective of the
abyss of the unwed itself looking back or inflecting back
at itself from a local perspective which shall be which shall
be onmed according to seral to expand this experience the self-
manifestation of the unth onto the subject and subject onto the
um is something that um for some time uh
in the tradition of complexity sciences and uh certain
circles in computer science and algorithmic information Theory uh people have been talking about um this is
often refer to um by various names I mean there are
different measures of complexity but one of them which is of a particular interest here to understand the meaning
of this depth the depth of the subject right with regard to the
question of the transcendental philosophy and also the project of uh
phenomenology would be logical depth as defined by Charles Bennett computer scientist uh physicist
Charles Bennett and then later on in a much more condensed
but enlightening way um expounded by Paul vitany computer
scientist Paul vitany and mingle in uh their uh Masterpiece uh an introduction
to kmog of complexity so I want to just give a very
uh Bri Paul V and mingle's uh idea of a logical
depth for us to understand that look this depth uh
itself the depth of the subject which is the depth of the
phenomenon that can be extended to something what you might call to be a nomenon but the nenon we don't need in
fact uh the concept of nenon the and sense uh to account for this depth right
we can actually go different sort of ways to talk about this such as what waser
does um such as what deart did you know as a progenitor of philosophy of
Enlightenment and what did Plato did as a certain sort of progenitor of uh a
specific kind of philosophy but at the very least if you want to do go and make
a connection of this in order to whether allegorically metaphorically or even
technically understand this in computational sense from the perspective
of complexity we can formulated it as
follows this is uh this is what they say one may consider a book a number Theory
difficult or deep the book will list a number of IAL theorems of number Theory
however it has very low colog of complexity since all theorems are
derivable from the initial few definitions our estimates of the
difficulty or depth of the book is the part of the information in it the
existence of a deep book is itself evidence of primes
being broadcasted to outer space to outer space since it is deemed deep
enough to prove to aliens that it arose as a result of a long Evolution this
obviously um as many of you might notice is a reference to the science fiction
movie contact uh which is based on I think a novel by Carl s you know were
basically aliens which sorry human hum receive a message which then they notice
that it is in a sequence of prime numbers
so from the point of view of an investigator a sequence is deep if it
yields its Secrets only slowly in fact in another part of the book they compare
the Deep object or the deep subject
they call it a deep object object here meaning only in a computational sense uh
of a structure right structure that or a message uh that once we try to put it
together once we try to organize it assemble it give it a structure it would
take uh a long time uh for it to develop right and long time obviously we mean it
in Tandon with computational energy and cost right and so they compare this with
a message in a b that reveals itself that comes out of the bottle very very slowly or in fact extracting it from the
bottle would take a very long time meaning that the Deep object meaning the problem
here that we are talking about that the subject is a manifestation of a deep
object in itself right the the own wealth and the its local perspective
back inflected back into it so the Deep object would be
uh essentially requires a very long time for the
subject to be able to decode it requiring computational cost energy sort of
sacrifice metabolism it can be translated to other sort of avenues
if we take the computational cost and energy seriously um so in that
sense Paul vitany and Ming Lee introduced a deep object and this goes
on for you know uh a number of other Pages
uh they they introduce uh the Deep
object as sort of
organization which can only be
revealed through a very specific route and once this specific root which
we call it the deep subject start to expand its range of
experiences right namely expand the range of manifestations of the deep
object the Deep objects slowly starts to get decoded the message gets slowly get
decoded so they continue that Bennett's uh
Charles Bennett's logical depth is a necessary number of steps in the deductive or causal path connecting an
object with its plausible origin formally it is the time required
by a universal computer to compute the object from its compressed original description it turns out that it's quite
subtle to give a formal definition of depth depth here they have again talked
about this in number of other occasions and so as many other commentators depth
it is literally here is in reference to the hermeneutical
depth as hence early example uh of a book a
shallow book or difficult book right and the hermeneutical depth here is
precisely in the tradition of phenomenology uh we see it from
uh to haiger to gamir and so on so forth uh so from this perspective I
think the question then remains uh what are the sort of techniques te what are
the sort of methods or meditations philosop radicalized philosophical
meditations a political or scientific mathematical or logical
kinds we can have in order to broaden you know
the uh the range of the unw self
manifestation which would be tanton to the range
of subjects possible new experiences
diversification uh of experiential types namely transcendental types thank you so
much
thank you Reza um and uh I guess now we we'll have
the question and answer uh section the concluding the conference uh 30 minutes
question answer um for any of the speakers uh that spoke
today feel free to ask him
so I'm just wondering if uh if you have anything to say about the recession of
LJ browers uh uh ential Tu theory for constructing
and Continuum the recession of that within um professional mathematics and
similarly with uh yes an involement claims of having
Prov and also his proof that three is number could you say a little bit about
his sort of the retion of those IDE
just to kind of complete view of
yes John so everyone
okay yeah I was just curious if if you might um say a few things about LJ
brower's Tu uh construction of the Continuum for found ations how that was
received by the um the profession and similarly with yes and vulpin his uh
claim to have um uh proved the incompleteness theorem
complete and his claim that three is a infinite number like just how was he received by
the um brower's formulation of the Continuum as far as I know uh um well
brower's whole program was formalized by his students uh uh most notably orent hating
and um uh Brower was a solipsist and a Mystic and his students and his teachers
were uh majorly embarrassed by his uh
mysticism and so I mean I think in general uh the uh Community or or
mathematicians The Establishment whatever uh is is kind of trying uh to
um slice and dice for hours uh uh creating subject uh as fast as possible
uh I think the idea now is that it's sort of equated with a computer um and
I'm not sure that Brower would have equated it with a computer um there weren't computers at the time
uh um so mostly I think his student hating had uh had had more sort of that
was accepted by uh by established mathematicians and then uh vulpin uh
claimed to have proved the inconsistency theorems consistent I don't actually know that much about that I know he
focused on his with his student Chris henx and that they put uh a considerable
amount of uh work into it but I don't think that it really had a readership uh in the United States so I'm not sure
that it really went anywhere uh and then as for vulpin idea that um three is an
infinite number I actually um have not specifically come across that idea the
what I came across was that uh he believed that they were uh sort of
different different um types of uh natural number series and maybe that's
connected but um he like I said there were the feasible natural number series
um and then the sort of traditional natural number series and um I I guess
there's maybe some well he's he's essentially saying that 10 to the 12 is an infinite number so he's not saying
that three is an infinite number in the example I know but he's saying he's kind of creating two two different kinds of
um two different types of infinity which is like a a feasible infinity and then a
um and then an infeasible infinity or or maybe like an Infinity that could be completed by a computer
uh this one is for raaza I was uh wondering if you would say I looked at the center of the first diagram where
you have an impersonal kind of representation where everything expands outwards and then you know sub um
transcendental uh subjects have their own part in the corner of the in the other edges of the diagram would you say
that uh those representations talked about in the center of the diagram are basically the same idea as what bergson
would call an image or are there caveats that are missing um may I ask why burkson uh here
because of because of there is a certain sort of uh kind of like a dynamic
dynamicity here there or in what sense I'm thinking of bergson in matter
and memory speaking of uh of trying to avoid either idealism or realism and
talking about the imagees this uh neutral thing um that exists like in a uh in like a single uh univocal
ontology yes yes in in that sense absolutely images by their definitions
should overcome either uh idealism or any other sort of the other pole that's
that's the whole point of the image and Plato was very specific about the nature of images right it's in the
um U analogy of the divided line that they uh that they are between uh uh
certain sort of memories and certain sort of matter
right not any sort of memory so people if they say that memory as an empirical
Trace memory yeah that would be just like going back uh to to the other route
as opposed to the idealism right of the images and we see it with being done
today in terms in in the name of I don't know uh artificial intelligence pattern
recognition so on and so forth right um but if uh we also think about memory in
terms of uh uh the problem of unforgetting
unlearning Amis right uh remembrance then yes absolutely uh that would be
would be the ideal the ideal really from the platonic Oran mology to address the
question of
images yeah this is for Reda um I really was taken by your diagrams and uh they
kind of like reminded me of like Ean araby and like kind of like the sort of like cosmological diagrams and stuff
like suie mysticism just wondering if you like engage with that at all if you like I don't know uh no I'm but I'm
completely familiar with them yes absolutely they are more like I would
say um um I'm not sure if you are you looked
at works of alh right the father of
Optics uh so yes I would say that rather
plagiarized from uh alhazen's Optical diagrams these are Optical diagrams
absolutely hosel also has musical diagrams Plato has Optical diagrams but
yes absolutely they can actually uh fit together uh like a a hand and a fit
glove absolutely yeah yeah know I just thought it was a like beautiful kind of extension of that tradition and like
yeah it's cool to see someone like actively engaging in that um yeah and just kind of like using image like to
communicate like these kind of like really big Concepts that are like yeah just like kind of uh just cool to see
that so yeah no no absolutely there is a there is a there fundamental underground resistance uh after Plato and no
platonism against um you know uh the rather the dark ages of Aristotelian
scholasticism throughout the Middle Ages which are fundamentally concerned with
diagram just as Plato was right Plato was the greatest diagram everything that
he says all the architecture of everything in his dialogues are diagrammatic by by their fundamental
nature and also know plonis and then it goes to people like nicool
orim Oxford Merton College paris's school you know the first people who
essentially doubled uh in uh invention of uh differential equations
of motion you know leading to the enlightenment to Day cards and cernus
and then onwards yeah so there is a a fundamental philosophical input the
nature of these diagrams thanks thank
you um I'm not
I'm not sure who raised their hand first to Andre Matt or the other Matt so Andre I guess you can go
first thank you actually I have a question to to Ryan and and perhaps
Christopher Also may comment if you see because it's about inferentialism so
inferentialism as I understand is primarily about logical inferences right
uh in some standard semantical sense say we have some
premises which are propositions and infer some conclusion uh however
large linguistic models again to my limited understanding it just works like
predicting next uh next symbol or next word right
from the vocabulary just given some uh early given world so
syntactically it it may work similarly right we have some some symbolic
expressions and uh we get a new one but actually there
are still two differences one that's still it's statistical right in a
linguistic model is not uh we have no like rule I don't know if but at least
this is the difference right and the second it still again that we don't have
anything like uh logical semantic nothing like ideal of from say
propositional uh logical inference so it works of course for like logical
semantics like for Genson example for logical vocabulary but
uh uh I somehow can't quite see how this Gap can be really filled
to extend the same principle say of all vocabulary and specifically on large
linguistic model
thanks uh yeah so thanks yeah so I mean the first thing um with respect to the
characterization of inferentialism I mean one the question sort of dividing whether one adopts a restricted approach
or the global approach is which set of inference get included in the inferentialist theory right so if you
have a logical inferentialism right where you're restricting yourself to just saying logical vocabulary those
meanings are constituted by inferential role then the inferences that you're looking at are going to be the logical
inferences um but the crucial thought in extending it to all vocabulary is that
what sellers uh and brandom call material inferences also get included right so these are inferences like you
know something's red so so it's colored right and two Thoughts with respect to
the you know whether you can apply this model of understanding to a to a large language model um the sort of solarian
thought I'm working with is you know obviously if you look under the hood what's going on there is completely
different than what's going on uh when we're you know reasoning um but the thought is you know the question of
whether or not you ought to say right whether or not it's apt to say you know it understands what it's saying isn't to
be decided by you know the processes that are actually going on under the Hood but rather whether we can sort of
afford this sort of uh normative status to it right treat it as responsible for what it says in the sense of it's being
capable of responding to challenges clarifi clarifying and whatnot um and I
don't see any in principle reason why we shouldn't why we couldn't uh afford it
that sort of responsibility um with respect to you know extending you know these material
having these material inferences I take it that that you can principle I mean the thought there is those inferences
are going to be sort of implicit in the entire Corpus of text that it gets fed
right so you know if you take everything that's ever been written
